[{"content":"晕了 import networkx as nx import matplotlib.pyplot as plt n_clique, n_path = 10, 10 clique1 = nx.complete_graph(n_clique) clique1_pos = nx.circular_layout(clique1) clique2 = nx.complete_graph(n_clique) clique2_mapping = {node: node + n_clique for node in clique2} nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes x_diff, y_diff = 8, -1 clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2} path = nx.path_graph(n_path) path_mapping = {node: node + 2 * n_clique for node in path} nx.relabel_nodes(path, path_mapping, copy=False) # avoids repeated nodes path_nodes = list(path.nodes) path_half1_nodes = path_nodes[:n_path//2] path_half2_nodes = path_nodes[n_path//2:] path_dist = 0.9 clique2_entry = n_clique + n_clique // 2 path_half1_pos = {node: clique1_pos[0] + (path_dist + i * path_dist, 0) for i, node in enumerate(path_half1_nodes)} path_half2_pos = {node: clique2_pos[clique2_entry] - (path_dist + i * path_dist, 0) for i, node in enumerate(path_half2_nodes[::-1])} path_pos = {**path_half1_pos, **path_half2_pos} barbell = nx.Graph() barbell.add_edges_from(clique1.edges) barbell.add_edges_from(clique2.edges) barbell.add_edges_from(path.edges) barbell.add_edges_from([(path_half1_nodes[0], 0), (path_half2_nodes[-1], clique2_entry)]) clique_pos = {**clique1_pos, **clique2_pos} barbell_pos = {**clique_pos, **path_pos} plt.figure(figsize=(20, 6)) nx.draw(barbell, pos=barbell_pos, with_labels=True) ","permalink":"https://JhuoW.github.io/posts/barbell_graph/","summary":"晕了 import networkx as nx import matplotlib.pyplot as plt n_clique, n_path = 10, 10 clique1 = nx.complete_graph(n_clique) clique1_pos = nx.circular_layout(clique1) clique2 = nx.complete_graph(n_clique) clique2_mapping = {node: node + n_clique for node in clique2} nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes x_diff, y_diff = 8, -1 clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2} path = nx.path_graph(n_path) path_mapping = {node: node + 2 * n_clique for node in path} nx.","title":"Awesome Barbell Graph with Networkx"},{"content":"Mean Discrepancy (MD)均值差异 判断2个分布$p$ 和$q$是否相同。\n$p$分布生成一个样本空间$\\mathbb{P}$ (从$p$中采样$m$个样本)\n$q$分布生成一个样本空间$\\mathbb{Q}$（从$q$中采样$n$个样本）\n函数$f$的输入为 分布生成的样本空间\n如果 $$ \\begin{equation} \\begin{aligned} \\mathrm{mean}(f(\\mathbb{P})) == \\mathrm{mean}(f(\\mathbb{Q})) \\\\ i.e., \\frac{1}{m}\\sum^m_{i=1}f(p_i) = \\frac{1}{n}\\sum^n_{i=1}f(q_i) \\end{aligned} \\end{equation} $$\n则$p$和$q$是同一分布。\nMD can be defined as $$ \\begin{equation} \\begin{aligned} \\mathrm{MD}\u0026amp;=|\\mathrm{mean}(f(\\mathbb{P})) -\\mathrm{mean}(f(\\mathbb{Q})) | \\\\ \u0026amp;= |\\frac{1}{m}\\sum^m_{i=1}f(p_i) - \\frac{1}{n}\\sum^n_{i=1}f(q_i)| \\end{aligned} \\end{equation} $$\nMaximum Mean Discrepancy (MMD) 最大均值差异 定义 MMD: 在函数集$\\mathcal{F}=\\{f_1, f_2, \\cdots \\}$中， 找到一个函数$f^*$， 使得$|\\mathrm{mean}(f^*(\\mathbb{P})) -\\mathrm{mean}(f^*(\\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的最大均值差异（MMD）。MMD =0 表示两个分布相同。 $$ \\operatorname{MMD}[\\mathcal{F}, p, q]:=\\sup _{f \\in \\mathcal{F}}\\left(\\mathbf{E}_{x \\sim p}[f(x)]-\\mathbf{E}_{y \\sim q}[f(y)]\\right) $$ 其中$\\mathbf{E}_{x \\sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\\sup$为上确界直接理解为max就好。\n条件 为了准确判断分布$p$和$q$之间的距离，需要找到一个合适的函数，使得两个分布在这个函数上的距离尽可能大，但搜索空间不能过于大，所以函数空间$\\mathcal{F}$要满足两个条件：\nC1: 函数集$\\mathcal{F}$要足够丰富， 使得MMD尽可能准确\nC2: 考虑数据集样本数量，随着数据集的增大，MMD要能迅速收敛，要求$\\mathcal{F}$足够restrictive (函数集不能无限大)\n所以利用kernel 方法，即， 将两个分布的样本空间映射到一个高维或者无限维的空间$\\mathcal{H}$中，如果两个分布的样本在$\\mathcal{H}$中的均值依然相等，那么这两个分布相等，MMD=0。两个分布在$\\mathcal{H}$中的最大均值为MMD。\n因此，当$\\mathcal{F}$是再生核Hilbert Space 上的单位球（unit ball）时，可以满足以上两个条件。 即，将$\\mathcal{F}$定义为某个kernel对应的RKHS中的函数， 例如，\n给定一个Gaussian Kernel: $k(u,v) = \\{\\exp({-\\frac{||u-v||^2}{2\\sigma}})\\}_\\sigma$, 这个kernel函数是一个Hilbert Space的再生核，那么这个空间可以表示为\n$$ \\begin{equation} \\mathcal{H}_k = \\operatorname{span}({\\Phi(x): x \\in \\mathcal{X}})=\\left\\{f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right): m \\in \\mathbf{N}, x_{i} \\in \\mathcal{X}, \\alpha_{i} \\in \\mathbf{R}\\right\\} \\tag{1} \\end{equation} $$ 空间$\\mathcal{X}$中的每个元素$x_i$都对应于一个函数$k(\\cdot,x_i)=k_{x_i}(\\cdot)$, 那么$\\mathcal{X}$中的所有元素所产生的函数$\\{k_{x_i}(\\cdot)\\}_{x_i \\in \\mathcal{X}}$ 可以span成一个Function Space, 如公式1所示， 这个function space中的每个function可以由\u0026quot;basis functions\u0026quot;$\\{k_{x_i}(\\cdot)\\}_{x_i \\in \\mathcal{X}}$ 通过线性组合得到。那么\n$$f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right)$$\n可以表示kernel $k(\\cdot, \\cdot)$的RKHS中的每个function。 每个valid kernel都有一个RKHS $\\mathcal{H}_k$与它对应。\n我们将MMD的候选函数集$\\mathcal{F}$定义为某一个kernel $k(\\cdot,\\cdot)$所对应的RKHS $\\mathcal{H}_k$中的函数，这样就可以满足所有候选函数都在$\\mathcal{H}_k$中(足够多)，同时如果kernel是Gaussian Kernel, 相当于把样本空间映射到无限高维来做MD,更加准确。\n另外，我们限制范式norm$||f||_{\\mathcal{H}_k} \\leq 1$来避免上界取到无限大\n回到MMD 已知$\\mathcal{F}=\\{f_1(\\cdot), f_2(\\cdot), \\cdots \\}$中的每个函数都是一个高斯核函数$k(\\cdot,\\cdot)$的RKHS中的函数，要从$\\mathcal{H}_k$中选一个函数$f^*(\\cdot)$，使得两个分布的样本间距离在$k(\\cdot,\\cdot)$的RKHS上最大。\n因为$f(\\cdot)$是$\\mathcal{H}_k$中的一个函数，那么$f(\\cdot)$可以表示为$\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right)$, 此时，下式一定成立（参考这里）：\n$$ f(x) = \\langle f(\\cdot), k(\\cdot, x) \\rangle_{\\mathcal{H}_k} $$ $k(\\cdot, x) = \\Phi(x)$表示将$x$映射到空间$\\mathcal{H}_{k}$上的值，即$x$在$\\mathcal{H}_{k}$上的表示。 若$k$是Gaussian Kernel, 那么$k(\\cdot, x)$就是$x$在无限维空间上的表示。\n连续空间中$\\mathbf{E}_{x \\sim p}[f(x)]$可以写为： $$ \\begin{equation} \\begin{aligned} \\mathbf{E}_{x \\sim p}[f(x)] \u0026amp;= \\int_x p(x)f(x) dx\\\\ \u0026amp; = \\int_x p(x) \\langle f(\\cdot), k(\\cdot, x) \\rangle_{\\mathcal{H}_k} dx \\\\ \u0026amp;= \\langle \\int_x p(x)f(\\cdot) dx, \\int_x p(x)k(\\cdot, x) dx \\rangle_{\\mathcal{H}_k}\\\\ \u0026amp;= \\langle f(\\cdot), \\mu_p\\rangle_{\\mathcal{H}_k} \\end{aligned} \\end{equation} $$ 其中$\\mu_p = \\int_x p(x)k(\\cdot, x) dx$.\n因此，MMD可以改写为： $$ \\begin{equation} \\begin{aligned} \\operatorname{MMD}(\\mathrm{p}, \\mathrm{q}, \\mathcal{H})\u0026amp;:=\\sup_{f \\in \\mathcal{H},|f|_{\\mathcal{H}} \\leq 1}(\\underset{\\mathrm{p}(\\boldsymbol{x})}{\\mathbb{E}}[f(\\boldsymbol{x})]-\\underset{\\mathrm{q}(\\boldsymbol{y})}{\\mathbb{E}}[f(\\boldsymbol{y})])\\\\ \u0026amp;=\\sup_{f \\in \\mathcal{H},|f|_{\\mathcal{H}_k} \\leq 1}\\left(\\left\\langle\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}, f\\right\\rangle_{\\mathcal{H}_k}\\right) \\end{aligned} \\end{equation} $$ 利用内积性质：$\\langle a, b \\rangle \\leq ||a|| ||b||$， 因为 $$ ||f(\\cdot)||_{\\mathcal{H}_k}\\leq 1 $$ , $$ \\left\\langle\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}, f\\right\\rangle_{\\mathcal{H}_k} \\leq ||\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}||_{\\mathcal{H}_k}||f||_{\\mathcal{H}_k} $$ Then, $$ \\sup_{f \\in \\mathcal{H},|f|_{\\mathcal{H}_k} \\leq 1}\\left(\\left\\langle\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}, f\\right\\rangle_{\\mathcal{H}_k}\\right) =||\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}||_{\\mathcal{H}_k} $$ 其中$\\mu_p = \\int_x p(x)k(\\cdot, x) dx$, $\\mu_q = \\int_y q(y)k(\\cdot, y) dy$ 分别表示分布的期望(均值)。 然而期望无法直接计算，因此用样本空间的均值代替分布的期望： $$ \\begin{equation} \\begin{aligned} \\mathrm{M M D}(p,q,\\mathcal{H}_k) \u0026amp; \\approx \\mathrm{M M D}(X,Y,\\mathcal{F}_{\\mathcal{H}_k})\\\\ \u0026amp;=\\left|\\left|\\frac{1}{n} \\sum_{i=1}^{n} f(x_i)-\\frac{1}{m} \\sum_{j=1}^{m} f(x_j)\\right|\\right|_{\\mathcal{H}_k} \\end{aligned} \\end{equation} $$\n$$ \\begin{equation} \\begin{aligned} \\mathrm{M M D}^2(p,q,\\mathcal{H}_k) \u0026amp; \\approx \\mathrm{M M D}^2(X,Y,\\mathcal{F}_{\\mathcal{H}_k})\\\\ \u0026amp;=\\left|\\left|\\frac{1}{n} \\sum_{i=1}^{n} f(x_i)-\\frac{1}{m} \\sum_{j=1}^{m} f(x_j)\\right|\\right|_{\\mathcal{H}_k}^{2}\\\\ \u0026amp;= \\left|\\left|\\frac{1}{n^{2}} \\sum_{i}^{n} \\sum_{i^{\\prime}}^{n} \\left\\langle f(x_i),f(x_i^{\\prime})\\right\\rangle-\\frac{2}{n m} \\sum_{i}^{n} \\sum_{j}^{m} \\left\\langle f(x_i), f(y_j)\\right\\rangle+\\frac{1}{m^{2}} \\sum_{j}^{m} \\sum_{j^{\\prime}}^{m} \\left\\langle f(y_j), f(y_j^{\\prime})\\right\\rangle\\right|\\right|_{\\mathcal{H}_k} \\\\ \u0026amp; = \\frac{1}{n^2} K_{x, x^\\prime}-\\frac{2}{nm} K_{x, y}+\\frac{1}{m^{2}} K_{y, y^{\\prime}} \\end{aligned} \\end{equation} $$\n令 $$ K=\\begin{bmatrix} K_{x, x^{\\prime}} \u0026amp; K_{x, y} \\\\ K_{x, y}\u0026amp; K_{y, y^{\\prime}} \\end{bmatrix} $$\n$$ M=\\begin{bmatrix}\\frac{1}{n^{2}} \u0026amp;-\\frac{1}{n m} \\\\ -\\frac{1}{n m}\u0026amp; \\frac{1}{m^{3}} \\end{bmatrix} $$\n最后： $$ \\mathrm{M M D}^2(X,Y,\\mathcal{F}_{\\mathcal{H}_k}) = tr(KM) $$\n","permalink":"https://JhuoW.github.io/posts/mmd/","summary":"Mean Discrepancy (MD)均值差异 判断2个分布$p$ 和$q$是否相同。\n$p$分布生成一个样本空间$\\mathbb{P}$ (从$p$中采样$m$个样本)\n$q$分布生成一个样本空间$\\mathbb{Q}$（从$q$中采样$n$个样本）\n函数$f$的输入为 分布生成的样本空间\n如果 $$ \\begin{equation} \\begin{aligned} \\mathrm{mean}(f(\\mathbb{P})) == \\mathrm{mean}(f(\\mathbb{Q})) \\\\ i.e., \\frac{1}{m}\\sum^m_{i=1}f(p_i) = \\frac{1}{n}\\sum^n_{i=1}f(q_i) \\end{aligned} \\end{equation} $$\n则$p$和$q$是同一分布。\nMD can be defined as $$ \\begin{equation} \\begin{aligned} \\mathrm{MD}\u0026amp;=|\\mathrm{mean}(f(\\mathbb{P})) -\\mathrm{mean}(f(\\mathbb{Q})) | \\\\ \u0026amp;= |\\frac{1}{m}\\sum^m_{i=1}f(p_i) - \\frac{1}{n}\\sum^n_{i=1}f(q_i)| \\end{aligned} \\end{equation} $$\nMaximum Mean Discrepancy (MMD) 最大均值差异 定义 MMD: 在函数集$\\mathcal{F}=\\{f_1, f_2, \\cdots \\}$中， 找到一个函数$f^*$， 使得$|\\mathrm{mean}(f^*(\\mathbb{P})) -\\mathrm{mean}(f^*(\\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的最大均值差异（MMD）。MMD =0 表示两个分布相同。 $$ \\operatorname{MMD}[\\mathcal{F}, p, q]:=\\sup _{f \\in \\mathcal{F}}\\left(\\mathbf{E}_{x \\sim p}[f(x)]-\\mathbf{E}_{y \\sim q}[f(y)]\\right) $$ 其中$\\mathbf{E}_{x \\sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\\sup$为上确界直接理解为max就好。","title":"Maximum Mean Discrepancy"},{"content":"PDF版\nHilbert Space Definition 1 (Norm) Let $\\mathcal{F}$ be a vector space over $\\mathbb{R}$ (For example $\\mathcal{F}=\\mathbb{R}^n$ is a vector space). A function $||\\cdot||_{\\mathcal{F}}: \\mathcal{F} \\to [0, \\inf)$ is said to be a norm on $\\mathcal{F}$ if ($||\\cdot||_{\\mathcal{F}}$ 是一个有效norm算子要满足以下条件):\n For $f \\in \\mathcal{F}$, $||f||_{\\mathcal{F}}=0$ if and only if $f=0$. (norm separates points) $|\\lambda f|_{\\mathcal{F}}=|\\lambda||f|_{\\mathcal{F}}$, $\\forall \\lambda \\in \\mathbb{R}, \\forall f \\in \\mathcal{F}$ (positive homogeneity). $|f+g|_{\\mathcal{F}} \\leq|f|_{\\mathcal{F}}+|g|_{\\mathcal{F}}, \\forall f, g \\in \\mathcal{F}$ (triangle inequality).  向$||\\cdot||_{\\mathcal{F}}$中输入任意一个向量，只要满足以上条件，那么$||\\cdot||_{\\mathcal{F}}$是一个valid norm operator.\nInner Product An inner product takes two elements of a vector space $\\mathcal{X}$ and outputs a number. An inner product could be a usual dot product: $\\langle\\mathbf{u}, \\mathbf{v}\\rangle=\\mathbf{u}^{\\prime} \\mathbf{v}=\\sum_{i} u^{(i)} v^{(i)}$ (Inner Product can be Dot Product). Or the inner product could be something fancier（即内积不一定表示为点积的形式）. If an Inner Product $\\langle \\cdot,\\cdot \\rangle$ is valid, it MUST satisfy the following conditions:\n  Symmetry $$\\langle u, v\\rangle=\\langle v, u\\rangle \\quad \\forall u, v \\in \\mathcal{X}$$\n  Bilinearity $$\\langle\\alpha u+\\beta v, w\\rangle=\\alpha\\langle u, w\\rangle+\\beta\\langle v, w\\rangle \\quad \\forall u, v, w \\in \\mathcal{X}, \\forall \\alpha, \\beta \\in \\mathbf{R}$$\n  Strict Positive Definiteness $$ \\begin{gathered} \\langle u, u\\rangle \\geq 0 \\forall x \\in \\mathcal{X} \\\\ \\langle u, u\\rangle=0 \\Longleftrightarrow u=0 \\end{gathered}$$\n  An inner product space (or pre-Hilbert space) is a vector space together with an inner product. （包含内积运算的向量空间称为 内积空间，即可以定义内积运算的向量空间）。\nKernel is a kind of Inner Product. For example, the Gaussian kernel is defined as: $$ \\begin{equation} \\langle u, v \\rangle = k(u,v) = \\exp({-\\frac{||u-v||^2}{2\\sigma}}) \\tag{1} \\end{equation} $$\nHilbert Space Definition 2 (Hilbert Space) A Hilbert Space is an Inner Product space that is complete and separable with respect to the norm defined by the inner product.\n\u0026lsquo;Complete\u0026rsquo; means sequences converge to elements of the space - there aren\u0026rsquo;t any \u0026ldquo;holes\u0026rdquo; in the space.\nFinite States Given finite input space ${x_1, x_2, \\cdots x_m }$. I want to be able to take inner products between any two of them using my function $k$ as the inner product ($k$ is customized and satisfy three conditions. For example, $k$ is a Gaussian inner product as Eq.(1)). Inner products by definition are symmetric, so $k(x_i, x_j)=k(x_j, x_i)$ , which yields a symmetric matrix $\\mathbf{K}$.\nSince $\\mathbf{K}$ is real symmetric, and this means we can diagonalize it （实对称阵可以对角化，即特征分解）, and the eigendecomposition takes this form: $$ \\begin{equation} \\begin{aligned} \\mathbf{K} \u0026amp;=\\mathbf{V} \\Lambda \\mathbf{V}^T \\\\ \u0026amp;= \\mathbf{V} \\begin{bmatrix} \\lambda_1 \u0026amp; \u0026amp; \u0026amp; \\\\ \u0026amp; \\lambda_2 \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\cdots \u0026amp;\\ \u0026amp; \u0026amp; \u0026amp;\\lambda_m \\end{bmatrix} \\mathbf{V}^T \\\\ \u0026amp;= \\begin{bmatrix} v_1 \u0026amp; v_2 \u0026amp; \\cdots v_m \\end{bmatrix} \\begin{bmatrix} \\lambda_1 \u0026amp; \u0026amp; \u0026amp; \\\\ \u0026amp; \\lambda_2 \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\cdots \u0026amp;\\\\ \u0026amp; \u0026amp; \u0026amp;\\lambda_m \\end{bmatrix} \\begin{bmatrix} v_1^T\\\\ v_2^T\\\\ \\cdots \\\\ v_m^T \\end{bmatrix}\\\\ \u0026amp;=v_1\\lambda_1 v_1^T + \\cdots + v_m\\lambda_m v_m^T = \\sum_{t=1}^m v_t\\lambda_tv_t^T \\end{aligned} \\tag{2} \\end{equation} $$ Let the $i$-th element of vector $v$ as $v^{(i)}$, then $$ \\begin{equation} \\begin{aligned} \\mathbf{K}_{ij} = k(x_i, x_j) \u0026amp;= [\\sum_{t=1}^m v_t\\lambda_tv_t^T]_{ij}\\\\ \u0026amp;=\\sum^m_{t=1} v_t^{(i)} \\lambda_t v_t^{(j)} \\end{aligned} \\tag{3} \\end{equation} $$ If $\\mathbf{K}$ is a positive semi-definite (PSD) matrix, then $\\lambda_1, \\cdots \\lambda_m \\geq 0$.\n Assumption 1. All $\\lambda_t$ are nonnegative.\n We consider this feature map: $$ \\begin{equation} \\Phi\\left(x_{i}\\right)=\\left[\\sqrt{\\lambda_{1}} v_{1}^{(i)}, \\ldots, \\sqrt{\\lambda_{t}} v_{t}^{(i)}, \\ldots, \\sqrt{\\lambda_{m}} v_{m}^{(i)}\\right] \\in \\mathbb{R}^m \\tag{4} \\end{equation} $$ (writing it for $x_j$ too): $$ \\begin{equation} \\boldsymbol{\\Phi}\\left(x_{j}\\right)=\\left[\\sqrt{\\lambda_{1}} v_{1}^{(j)}, \\ldots, \\sqrt{\\lambda_{t}} v_{t}^{(j)}, \\ldots, \\sqrt{\\lambda_{m}} v_{m}^{(j)}\\right] \\in \\mathbb{R}^m \\tag{5} \\end{equation} $$ 即 $\\Phi: \\mathcal{X} \\to \\mathbb{R}^m$ 将$x\\in \\mathcal{X}$映射到$m$维向量空间$\\mathbb{R}^m$中的一个点。\nWith this choice, the inner product $k$ is just defined as a dot product in $\\mathbb{R}^m$: $$ \\begin{equation} \\left\\langle\\Phi\\left(x_{i}\\right), \\Phi\\left(x_{j}\\right)\\right\\rangle_{\\mathbf{R}^{m}}=\\sum_{t=1}^{m} \\lambda_{t} v_{t}^{(i)} v_{t}^{(j)}=\\left(\\mathbf{V} \\Lambda \\mathbf{V}^{\\prime}\\right)_{i j}=K_{i j}=k\\left(x_{i}, x_{j}\\right) \\tag{6} \\end{equation} $$ If there exists an eigenvalue $\\lambda_s \u0026lt;0$ (即$\\sqrt{\\lambda_s} = \\sqrt{|\\lambda_s|} i$). $\\lambda_s$对应的特征向量$v_s$。用$v_s \\in \\mathbb{R}^m$的$m$个元素$v_s = [v_s^{(1)},\\cdots, v_s^{(m)}]$, 来对$\\Phi(x_1),\\cdots, \\Phi(x_m)$做线性组合： $$ \\begin{equation} \\mathbf{z}=\\sum_{i=1}^{m} v_{s}^{(i)} \\boldsymbol{\\Phi}\\left(x_{i}\\right) \\tag{7} \\end{equation} $$\nIt is obvious that $\\mathbf{z} \\in \\mathbb{R}^m$. Then calculate $$ \\begin{equation} \\begin{aligned} |\\mathbf{z}|_{2}^{2} \u0026amp;=\\langle\\mathbf{z}, \\mathbf{z}\\rangle_{\\mathbf{R}^{m}}=\\sum_{i} \\sum_{j} v_{s}^{(i)} \\boldsymbol{\\Phi}\\left(x_{i}\\right)^{T} \\boldsymbol{\\Phi}\\left(x_{j}\\right) v_{s}^{(j)}=\\sum_{i} \\sum_{j} v_{s}^{(i)} K_{i j} v_{s}^{(j)} \\\\ \u0026amp;=\\mathbf{v}_{s}^{T} \\mathbf{K} \\mathbf{v}_{s}=\\lambda_{s}\u0026lt;0 \\end{aligned} \\tag{8} \\end{equation} $$ which conflicts with the geometry of the feature space.\n如果$\\mathbf{K}$不是半正定，那么feature space $\\mathbb{R}^m$存在小于0的值。所以假设Assumption不成立。即，若$k$表示有限集的内积，那么它的Gram Matrix一定半正定(PSD)，否则无法保证该空间中的norm大于0。\n有效的内积对应的Gram Matrix 必定PSD.\nKernel Definition 3. (Kernel) A function $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is a kernel if\n $k$ is symmetric: $k(x,y) = k(y,x)$. $k$ gives rise to a positive semi-definite \u0026ldquo;Gram matrix,\u0026rdquo; i.e., for any $m\\in \\mathbb{N}$ and any $x_1,\\cdots,x_m$ chosen from $X$, the Gram matrix $\\mathbf{K}$ defined by $\\mathbf{K}_{ij} = k(x_i,x_j)$ is positive semi-definite.  Another way to show that a matrix $\\mathbf{K}$ is positive semi-definite is to show that $$ \\begin{equation} \\forall \\mathbf{c} \\in \\mathbf{R}^{m}, \\mathbf{c}^{T} \\mathbf{K} \\mathbf{c} \\geq 0 \\tag{9} \\end{equation} $$ Here are some nice properties of $k$:\n $k(u,u) \\geq 0$ (Think about the Gram matrix of $m = 1$.) $k(u, v) \\leq \\sqrt{k(u, u) k(v, v)}$ (This is the Cauchy-Schwarz inequality.)  Reproducing Kernel Hilbert Space (RKHS) 给定一个kernel $k(\\cdot, \\cdot): \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$. 定义一个函数空间（space of functions）$\\mathbf{R}^{\\mathcal{X}}:={f: \\mathcal{X} \\rightarrow \\mathbb{R}}$. $\\mathbf{R}^{\\mathcal{X}}$ 是一个 Hilbert Space， 该空间中的每个元素是一个$\\mathcal{X}$映射到$\\mathbb{R}$的函数。\n令$k_x(\\cdot) = k(x, \\cdot)$, 假设$x$是一个定值（Constant），自变量（输入）用$\\cdot$表示。那么$k(x, \\cdot)$ 也是$\\mathbf{R}^{\\mathcal{X}}$空间中的一个函数。\n每个函数$k_x(\\cdot)$ 都与一个特定的$x \\in \\mathcal{X}$有关，即每个$x$对应于一个函数$k_x(\\cdot) = k(\\cdot, x)$. 这种对应关系表示为$\\Phi(x) = k_x(\\cdot) = k(x,\\cdot)$, 即： $$ \\begin{equation} \\Phi: x \\longmapsto k(\\cdot, x) \\tag{10} \\end{equation} $$ 即 $\\Phi$的输入为$x\\in \\mathcal{X}$, 输出一个函数, 输出的函数属于$\\mathbf{R}^{\\mathcal{X}}$空间。\n在连续空间$\\mathcal{X}$中，$x \\in \\mathcal{X}$ 有无穷多种情况，那么$\\Phi(x)=k_x(\\cdot)=k(x, \\cdot)$也有无穷多种情况，即无穷多种函数。 这些函数可以span 一个Hilbert Space: $$ \\begin{equation} \\mathcal{H}_k = \\operatorname{span}({\\Phi(x): x \\in \\mathcal{X}})=\\left\\{f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right): m \\in \\mathbf{N}, x_{i} \\in \\mathcal{X}, \\alpha_{i} \\in \\mathbf{R}\\right\\} \\tag{11} \\end{equation} $$ 其中$k(x,\\cdot)=\\Phi(x)$可以理解为将$x$映射为一个函数（or vector）。上述Hilbert Space是由任意$k(x, \\cdot)$线性组合而成的函数空间，该空间中的每个元素可以表示为 $$ \\begin{equation} f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right) \\tag{12} \\end{equation} $$ 所以$\\mathcal{H}$可以看作是kernel $k$对应的一个Hilbert Space。\n给定$\\mathcal{H}$中的任意两个函数$f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right)$, $g(\\cdot)=\\sum_{j=1}^{m^{\\prime}} \\beta_{j} k\\left(\\cdot, x_{j}^{\\prime}\\right)$。注意$f(\\cdot)$和$g(\\cdot)$可以表示$\\mathcal{H}$中任意两个元素。我们将$\\mathcal{H}$上的内积定义为： $$ \\begin{equation} \\langle f, g\\rangle_{\\mathcal{H}_{k}}=\\sum_{i=1}^{m} \\sum_{j=1}^{m^{\\prime}} \\alpha_{i} \\beta_{j} k\\left(x_{i}, x_{j}^{\\prime}\\right) \\tag{13} \\end{equation} $$ 由Proof证明了该内积符合三个条件，顾上式是$\\mathcal{H}$空间中一个有效的内积算子。注：$\\mathcal{H}_k$表示该Hilbert Space是由函数 $k(x,\\cdot)$ span而成的，与Kernel $k$有关.\n$k(x,\\cdot)$也是$\\mathcal{H}_k$中的一个函数，那么它与 $f$的内积为： $$ \\begin{equation} \\langle k(\\cdot, x), f\\rangle_{\\mathcal{H}_{k}}= \\sum_{i=1}^m \\alpha_i k(x,x_i) =f(x) \\tag{14} \\end{equation} $$ Theorem 1. $k(\\cdot, \\cdot)$ is a reproducing kernel of a Hilbert space $\\mathcal{H}_k$ if $f(x)=\\langle k(x, \\cdot), f(\\cdot)\\rangle$.\n$\\mathcal{H}_k$ 为$k(\\cdot, \\cdot)$的再生核希尔伯特空间。\n同理，$k(\\cdot, x_i)$, $k(\\cdot, x_j)$都为$\\mathcal{H}_k$中的函数， 计算他们的内积: $$ \\begin{equation} \\left\\langle k(\\cdot, x_i), k\\left(\\cdot, x_j\\right)\\right\\rangle_{\\mathcal{H}_{k}}=k\\left(x_i, x_j\\right) \\tag{15} \\end{equation} $$ 因为$ k(\\cdot, x_i) = \\Phi(x_i)$, $ k(\\cdot, x_j) = \\Phi(x_j)$, 所以 $$ \\begin{equation} k\\left(x_i, x_j\\right) = \\left\\langle \\Phi(x_i), \\Phi(x_j)\\right\\rangle_{\\mathcal{H}_{k}} \\tag{16} \\end{equation} $$ 表示将$x_i$和$x_j$ 映射成$\\mathcal{H}_k$中的函数（向量）后再做内积。\n参考文献 [1] https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf\n[2] http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf\n[3] https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf\n","permalink":"https://JhuoW.github.io/posts/rkhs_kernel/","summary":"PDF版\nHilbert Space Definition 1 (Norm) Let $\\mathcal{F}$ be a vector space over $\\mathbb{R}$ (For example $\\mathcal{F}=\\mathbb{R}^n$ is a vector space). A function $||\\cdot||_{\\mathcal{F}}: \\mathcal{F} \\to [0, \\inf)$ is said to be a norm on $\\mathcal{F}$ if ($||\\cdot||_{\\mathcal{F}}$ 是一个有效norm算子要满足以下条件):\n For $f \\in \\mathcal{F}$, $||f||_{\\mathcal{F}}=0$ if and only if $f=0$. (norm separates points) $|\\lambda f|_{\\mathcal{F}}=|\\lambda||f|_{\\mathcal{F}}$, $\\forall \\lambda \\in \\mathbb{R}, \\forall f \\in \\mathcal{F}$ (positive homogeneity). $|f+g|_{\\mathcal{F}} \\leq|f|_{\\mathcal{F}}+|g|_{\\mathcal{F}}, \\forall f, g \\in \\mathcal{F}$ (triangle inequality).","title":"Reproducing Kernel Hilbert Space"},{"content":"最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。PDF版\n本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments\nIntroduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。\n基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} \u0026gt; 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \\sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\\displaystyle \\left(\\begin{array}{ccc}{d_{1}} \u0026amp; {\\ldots} \u0026amp; {\\ldots} \\\\\\ {\\ldots} \u0026amp; {d_{2}} \u0026amp; {\\ldots} \\\\\\ {\\vdots} \u0026amp; {\\vdots} \u0026amp; {\\ddots} \\\\\\ {\\ldots} \u0026amp; {\\ldots} \u0026amp; {d_{n}}\\end{array}\\right) ^{n\\times n} $$ 是一个$n \\times n$的对角阵，对角元素是每个节点的度和。\n定义图的邻接矩阵为$W \\in \\mathbb{R}^{n \\times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \\subset V$， 定义： $$|A|=A 中的节点个数 $$\n$$vol(A) = \\sum_{i \\in A} d_i \\quad 表示A中所有节点的权重之和$$\n基础2：相似矩阵 再提下谱聚类的基本思想： 距离较远的两个点之间权重值较低，距离较近的两个点之间权重值较高\n但是在谱聚类中，我们只能获得数据点的定义，无法给出邻接矩阵，因为是离散的分布在空间中，无法得知其中的连接关系。\n一般来说，通过样本点距离度量的相似矩阵$S$来获得邻接矩阵$W$.\n构建邻接矩阵$W$有两种方法: $\\epsilon$-邻近法， K邻近法和全连接法。\n$\\epsilon$-邻近法 $\\epsilon$为距离的阈值，欧式距离$S_{ij}$表示两点$v_i$,$v_j$的坐标$x_i$,$x_j$之间的距离。 即 $S_{ij} =||x_i-x_j||^2_2$为相似矩阵$S \\in \\mathbb{R}^{n \\times n}$的第$i$行第$j$个元素，则邻接矩阵$W$可以表示为：\n$$ w_{ij}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {s_{i j}\u0026gt;\\epsilon} \\\\ {\\epsilon} \u0026amp; {s_{i j} \\leq \\epsilon}\\end{array}\\right. $$\n意思是如果两点之间的距离大于$\\epsilon$，那么他们之间的权重为0， 如果他们之间的距离小于$\\epsilon$，他们之间的权重为$\\epsilon$。 这种方法的弊端在于点之间的距离只有两种情况，不够精确，故很少采用。\nK邻近法 利用KNN算法遍历所有样本点，取每个样本点最近的$k$个点作为近邻，只有和样本点距离最近的$k$个点的$w_{ij} \u0026gt;0$，但会出现一种情况， $v_i$的k个近邻中有$v_j$，但$v_j$的k个近邻中没有$v_i$，这样会造成邻接矩阵$W$的不对称， 因此提供两种解决办法\n第一种： 只要$v_j$在$v_i$的K邻域中，那么不管$v_i$在不在$v_j$的K邻域中，都把$v_i$加入$v_j$的邻域中，即：\n$$ w_{i j}=w_{j i}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {x_{i} \\notin K N N\\left(x_{j}\\right) \\text { and } x_{j} \\notin K N N\\left(x_{i}\\right)} \\\\ {\\exp \\left(-\\frac{\\left||x_{i}-x_{j}\\right||^2_2}{2 \\sigma^{2}}\\right)} \u0026amp; {x_{i} \\in K N N\\left(x_{j}\\right) \\text { or } x_{j} \\in K N N\\left(x_{i}\\right)}\\end{array}\\right. $$ 第二种，必须$v_i$在$v_j$的K邻域中，且$v_j$在$v_i$的K邻域中，那么才保留两者间的权重，否则都为0，即：\n$$ w_{i j}=w_{j i}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {x_{i} \\notin K N N\\left(x_{j}\\right) \\text { or } x_{j} \\notin K N N\\left(x_{i}\\right)} \\\\ {\\exp \\left(-\\frac{||x_{i}-x_{j}||^2_2}{2 \\sigma^{2}}\\right)} \u0026amp; {x_{i} \\in K N N\\left(x_{j}\\right) \\text { and } x_{j} \\in K N N\\left(x_{i}\\right)}\\end{array}\\right. $$\n全连接法 设所有点之间的权重都大于0，可以选择不同的核函数来定义边的权重，常用的如多项式核函数，高斯核函数， Sigmod核函数。 最常用的为高斯核函数RBF，将两点之间的距离带入高斯核函数RBF中，即： $$ w_{i j}=w_{ji}=s_{i j}=s_{ji}=\\exp \\left(-\\frac{\\left|x_{i}-x_{j}\\right|_{2}^{2}}{2 \\sigma^{2}}\\right) $$ 其中，$sigma$为为函数的宽度参数 , 控制了函数的径向作用范围。\n基础3：拉普拉斯矩阵 拉普拉斯矩阵定义为$L = D-W$ 如上文所示，$D$为度矩阵，$W$为邻接矩阵。\n拉普拉斯矩阵具有如下性质：\n  $L$是对称阵 （因为$D$和$W$都是对称阵）\n  $L$的所有特征值都是实数 （因为$L$是对称阵）\n  对于任意向量$f$， 有$f^TLf = \\displaystyle \\frac{1}{2} \\sum_{i = 1}^n \\sum_{j = 1}^n w_{ij} (f_i-f_j)^2$\n推导： $$ \\begin{aligned} f^TLf \u0026amp;= f^TDf-f^TWf\\\\ \u0026amp;= \\sum^n_{i = 1}d_if_i^2 - \\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij}\\\\ \u0026amp;= \\frac{1}{2}\\left(\\sum^n_{i=1}d_if_i^2 -2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum^n_{i=1}d_if_i^2\\right)\\\\ \u0026amp;由于d_i = \\sum_{j = 1}^nw_{ij}, 将d_i带入上式得\\\\ f^TLf \u0026amp;= \\frac{1}{2} \\left(\\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2-2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2\\right)\\\\ \u0026amp; = \\frac{1}{2} \\left(\\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2-2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum_{j = 1}^n\\sum_{i =1}^n w_{ji}f_j^2\\right)\\\\ \u0026amp;= \\frac{1}{2}\\left(\\sum_{i=1}^n\\sum_{j = 1}^n w_{ij} (f_i-f_j)^2\\right) \\end{aligned} $$\n  拉普帕斯矩阵是半正定的，且$n$个实数特征值都$\\geq$，即 $0=\\lambda_1 \\leq \\lambda_2 \\cdots \\leq \\lambda_n$，且最小的特征值为0。\n证明，因为$f^TLf \\geq 0$ 所以$L$半正定。\n  基础4：无向图切图 对于无向图$G$，目的是将图$G = (v,E)$切成相互没有连接的k个子图，每个子图是一个节点集合：$A_1,A_2,\\cdots, A_k$，满足$A_i \\cap A_j = \\phi$ 且$A_1 \\cup A_2 \\cup \\cdots \\cup A_k = V$，对于两个节点集合$A ,B \\subset V$, $A \\cap B = \\phi$，定义$A$,$B$之间的切图权重为： $$ W(A,B) = \\sum_{v_i\\in A, v_j \\in B} w_{ij} \\quad 表示A中节点到B中节点的权重和 $$ 对于$k$个子图节点集合$A_1,A_2,\\cdots, A_k$，定义切图$Cut$为： $$ Cut(A_1,A_2, \\cdots, A_k) = \\frac{1}{2}\\sum^k_{i = 1} W(A_i,\\overline{A_i}) $$ 其中$\\overline{A_i}$是$A_i$的补集，也就是除$A_i$外其他所有节点的集合，$Cut$计算的是$A_i$中每个节点到$\\overline{A_i}$中每个节点的权重总和，如果最小化$Cut$，就相当于最小化每个子集中节点到自己外节点的权重，但是会存在一个问题，如下图所示：\n如果按左边那条线分割，可以保证有边子图到左边子图的权重最小，但不是最佳分割。\n谱聚类：切图聚类 为了避免上述效果不佳的情况，提供两种切图方式，1.RatioCut, 2.Ncut.\nRatioCut 切图 最小化$Cut(A_1,A_2, \\cdots, A_k)$的同时，最大化每个子图中接待你的个数，即：A_{i}, \\overline{A}_{i}\n$$RatioCut\\left(A_{1}, A_{2}, \\ldots A_{k}\\right)=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\overline{A_i}\\right)}{\\left|A_{i}\\right|}$$\n目标是最小化$RatioCut\\left(A_{1}, A_{2}, \\ldots A_{k}\\right)$。\n为此，我们引入一个指示向量（indicator vector）$h_j \\in {h_1,h_2,\\cdots, h_k}$，其中$j = 1,2,\\cdots,k$，对于其中任意一个向量$h_j$，它是一个$n$维向量，即： $$ h_j = (h_{1j},h_{2j}, \\cdots, h_{nj})^T \\\\ h_{i j}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {v_{i} \\notin A_{j}} \\\\ {\\frac{1}{\\sqrt{\\left|A_{j}\\right|}}} \u0026amp; {v_{i} \\in A_{j}}\\end{array}\\right. $$ $h_ij$表示节点$v_i$ 是否属于子图$A_j$, 如果属于，那么$h_{ij} = \\frac{1}{\\sqrt{\\left|A_{j}\\right|}}$，如果不属于，那么$h_{ij} = 0$。\n那么对于$h_i^TLh_i$有： $$ \\begin{aligned} h_i^T L h_i \u0026amp;= \\frac{1}{2}\\sum_{m=1}\\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\\\ \u0026amp;= \\frac{1}{2}\\left(\\sum_{m\\in A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2+\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\\\ \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2 + \\sum_{m\\notin A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2\\right)\\\\ \u0026amp; 上式中的mn是图中任意选取的两个不同的节点， 针对子图A_i及其对应的指示向量h_i,\\\\ \u0026amp;任意选取的节点对有四种情况。其中 根据h_{ij}的定义，第一项和第四项为0，所以\\\\ \u0026amp;=\\frac{1}{2} \\left(\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2\\right) \\\\ \u0026amp;=\\frac{1}{2} \\left(\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(\\frac{1}{\\sqrt{\\left|A_{i}\\right|}})^2 + \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(-\\frac{1}{\\sqrt{\\left|A_{i}\\right|}})^2\\right) \\\\ \u0026amp;=\\frac{1}{2}\\left(\\frac{1}{|A_i|}Cut(A_i,\\overline{A_i}) + \\frac{1}{|A_i|}Cut(A_i,\\overline{A_i})\\right)\\\\ \u0026amp;=\\frac{Cut(A_i,\\overline{A_i})}{|A_i|} = RatioCut(A_i) \\end{aligned} $$ 上式$h_i^TLh_i$可以看做是子图$A_i$的RatioCut，那么： $$ \\begin{aligned} RatioCut(A_1,A_2,\\cdots,A_k) \u0026amp;=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\overline{A_i}\\right)}{\\left|A_{i}\\right|} = \\sum_{i = 1}^k \\frac{Cut(A_i,\\overline{A_i})}{|A_i|} \\\\ \u0026amp;= \\sum_{i=1}^k h_i^TLh^i = \\sum_{i=1}^k (H^TLH)_{ii} = tr(H^TLH) \\end{aligned} $$ 每个$h_i^TLh^j$的值对应于矩阵$H^TLH$在位置$ij$处的值： $$ H=(h_1,h_2,\\cdots,h_k) \\in \\mathbb{R}^{n\\times k} $$ $$ h_i^TLh_j = (H^TLH)_{ij} \\to h^T_iLh_i = (H^TLH)_{ii} $$\n由于$h_i\\cdot h_j = 0, h_i \\cdot h_i = 1$, 所以$H^TH = I$是一个单位矩阵\n所以切图优化函数为： $$ \\underbrace{\\arg \\min }_{H} RatioCut\\left(A_1,A_2,\\cdots A_k\\right) = \\underbrace{\\arg \\min }_{H} \\operatorname{tr}\\left(H^{T} L H\\right) \\quad \\text { s.t. } \\quad H^{T} H=I $$ $H$中的每个指示向量$h$是$n$维的，每个向量中的元素有两种取值，分别是0和$\\frac{1}{\\sqrt{\\left|A_{j}\\right|}}$， 所以每个$h$有$2^n$种可能性，所以整个$H$有$k2^n$中，因此上述目标函数是个NP-hard问题。\n注意到$tr(H^TLH)$中每个优化子目标$h_i^TLh_i$，其中，$h$是单位正交基，基每个元素的平方和等于1，$L$为对称矩阵，此时$h^T_iLh_i$的最大值为$L$的最大特征值， $h^T_iLh_i$的最小值是$L$的最小特征值。在谱聚类中，我们的目标是找到目标函数的最小特征值，从而使目标函数最小，得到对应的特征向量。\n对于$h^T_iLh_i$，目标是找到$L$最小的特征值，这个值就是$h^T_iLh_i$的最小值，对于$tr(H^TLH) = \\sum^k_{i=1} h^T_iLh_i$，目标是找到$k$个最小的特征值，从而使$tr(H^TLH)$最小。\n通过找到$L$最小的$k$个特征值，可以对应得到$k$个特征向量，这$k$个特征向量可以组成一个$n \\times k$的矩阵，这个矩阵就是我们需要的指示向量矩阵$H$，里面包含了每个节点所属子图的信息。一般来说 我们需要对$H$按行做标准化： $$ h_{ij}^* = \\frac{h_{ij}}{\\sqrt{\\sum_{t=1}^kh^2_{it}}} $$ 注意到，$H$的每行是一个$k$维行向量，即$H_i = (H_{i1},H_{i2},\\cdots, H_{ik})$， 表示节点$i$属于每个子图的指标值， 我们可以把$H_i$当做节点$v_i$的表示向量， 由于归一化后的$H$还不能明确指示各个样本的归属， 我们还需要对$H$代表的所有节点做一次传统聚类，如K-Means。\nNCut切图 把$RatioCut$的分母从$|A_i|$换成$vol(A_i) = \\sum_{j \\in A_i}d_j$， 为$A_i$中所有节点的权重之和，一般来说$NCut$效果好于$RatioCut$: $$ NCut(A_1,A_2,\\cdots,A_k) = \\frac{1}{2}\\sum_{i=1}^k\\frac{W(A_i,\\overline{A_i})}{vol(A_i)} = \\sum^k_{i = 1}\\frac{Cut(A_i)}{vol(A_i)} $$ $NCut$指示向量$h$做了改进，$RatioCut$切图在指示向量中使用$\\frac{1}{\\sqrt{|A_i|}}$表示某节点归属于子图$A_i$，而$NCut$切图使用子图权重$\\frac{1}{\\sqrt{vol{A_i}}}$来表示某节点归属子图$A_i$如下： $$ h_{i j}=\\left{\\begin{array}{ll}{0} \u0026amp; {v_{i} \\notin A_{j}} \\ {\\frac{1}{\\sqrt{v o l\\left(A_{j}\\right)}}} \u0026amp; {v_{i} \\in A_{j}}\\end{array}\\right. $$ 上式表示如果节点$v_i$在子图$A_j$中，那么指示向量$h_j$的第$i$个元素为$\\frac{1}{\\sqrt{vol{A_j}}}$。\n那么对于$h_i^TLh_i$有： $$ h^T_iLh_i = \\frac{1}{2}\\sum_{m=1}\\sum_{n=1}w_{mn}(h_{im}-h_{in})^2 = \\frac{Cut(A_i)}{vol(A_i)} =NCut(A_i) $$ 目标函数： $$ NCut(A_i,A_2,\\cdots,A_k) = \\sum^k_{i = 1} NCut(A_i) = \\sum^k_{i=1}h^T_iLh_i =\\sum^k_{i=1}(H^TLH)_{ii} = tr(H^TLH) $$ 此时，$h_i \\cdot h_j = 0$，$h_i\\cdot h_i = \\frac{|A_i|}{vol(A_i)} \\neq 1$， 所以$H^TH \\neq I$。\n但是， 由于：$h^T_iDh_i = \\sum^n_{j = 1} h_{ij}^2d_j$, $d_j$为节点$v_j$的权重和，$h_{ij}$的值表示节点j是否在子图$A_i$中，如果在子图$A_i$中，那么$h_{ij}^2 = \\frac{1}{vol(A_i)}$，否则为0。\n$$h^T_iDh_i = \\frac{1}{vol(A_i)} \\sum_{v_j \\in A_i} d_j = \\frac{1}{vol(A_i)} \\cdot vol(A_i) = 1$$\n最终目标函数为： $$ \\underbrace{\\arg \\min } _{H}\\operatorname{tr}\\left(H^{T} L H\\right) \\quad \\text { s.t. }\\quad H^{T} D H=I $$ 由于$H$中的指示向量$h$不是标准正交基，所以RatioCut中加粗的定理不能直接使用，所以，令$H = D^{-\\frac{1}{2}}F$, $D^{-\\frac{1}{2}}$表示对$D$对角线上元素开方后求逆，那么： $$ H^TLH = F^TD^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}F $$\n$$ H^TDH = F^TD^{-\\frac{1}{2}}DD^{-\\frac{1}{2}}F = F^TF=I $$ 所以目标函数转化为： $$ \\underbrace{\\arg \\min }_{F} \\operatorname{tr}\\left(F^{T} D^{-1 / 2} L D^{-1 / 2} F\\right) \\quad \\text { s.t. } \\quad F^{T} F=I $$ 同$RatioCut$，$D^{-1 / 2} L D^{-1 / 2}$是对称矩阵，$F$中的每个向量为标准正交基，只需要求出$D^{-1 / 2} L D^{-1 / 2}$的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，最后得到特征矩阵$F$，然后对$F$的行向量做一次传统聚类，如K-means.\n一般来说， $D^{-1 / 2} L D^{-1 / 2}$相当于对拉普拉斯矩阵$L$做了一次标准化，即$\\frac{L_{i j}}{\\sqrt{d_{i} * d_{j}}}$.\n","permalink":"https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/","summary":"最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。PDF版\n本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments\nIntroduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。\n基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} \u0026gt; 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \\sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\\displaystyle \\left(\\begin{array}{ccc}{d_{1}} \u0026amp; {\\ldots} \u0026amp; {\\ldots} \\\\\\ {\\ldots} \u0026amp; {d_{2}} \u0026amp; {\\ldots} \\\\\\ {\\vdots} \u0026amp; {\\vdots} \u0026amp; {\\ddots} \\\\\\ {\\ldots} \u0026amp; {\\ldots} \u0026amp; {d_{n}}\\end{array}\\right) ^{n\\times n} $$ 是一个$n \\times n$的对角阵，对角元素是每个节点的度和。\n定义图的邻接矩阵为$W \\in \\mathbb{R}^{n \\times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \\subset V$， 定义： $$|A|=A 中的节点个数 $$\n$$vol(A) = \\sum_{i \\in A} d_i \\quad 表示A中所有节点的权重之和$$","title":"谱聚类 Spectral Clustering 笔记"},{"content":"文章链接：Embedding_IC\nIntroduction 本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。\n对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：\n 用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。 用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。 不同应用中的级联长度变化很大，难以学习和预测。  本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：\n 影响传播是二元的（被感染或不被感染）， 扩散网络未知， 影响关系不依赖于传播的内容， 用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。  本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：\nModel Notations 传播事件集$\\mathcal{D}={D_1,D_2,\u0026hellip;,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。\n给定一个社交网络，有$N$个用户：$\\mathcal{U}={u_1,u_2,\u0026hellip;,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \\in \\mathcal{U} \\wedge t^D(u)\u0026lt; \\infty}$， 其中，$t^D: \\mathcal{U} \\to \\mathbb{R}^+$ 表示用户被传染的时间戳， $\\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \\quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \\in \\mathcal{U} | t^D(u)\u0026lt;t}$。对称地，用$\\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\\infty)$表示最终所有被感染的用户，$\\bar{D}(infty)$表示最终所有没被感染的用户。\nDiffusion Model 本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。\n在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \\in \\mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \\in \\mathcal{U} \\backslash I$: $$ P(v|I) = 1-\\prod_{u \\in I}(1-P_{u,v}) $$ 上式中，$\\prod_{u \\in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。\n接下来就需要给出$P_{u,v}$的定义了，即$v$被$u$传染的概率。$z_u \\in \\mathbb{R}^d$是传染源用户$u$的表示向量，$\\omega_{v} \\in \\mathbb{R}^d$是传染目标用户$v$的表示向量。那么$P_{u,v}$可以定义如下： $$ P_{u,v} = f(z_u,\\omega_{v}) $$ 其中，$f: \\mathbb{R}^d \\times \\mathbb{R}^d \\to [0,1]$，是一个映射函数，把两个表示向量映射到概率空间： $$ f\\left(z_{u}, \\omega_{v}\\right)=\\frac{1}{1+\\exp \\left(z_{u}^{(0)}+\\omega_{v}^{(0)}+\\sum_{i=1}^{d-1}\\left(z_{u}^{(i)}-\\omega_{v}^{(i)}\\right)^{2}\\right)} $$ 其中，$z_{u}^{(i)}$和$\\omega_{v}^{(i)}$分别表示$z_u$和$\\omega_v$的第$i$个分量。表示随距离增加而递减的传输概率，即$\\left(z_{u}^{(i)}-\\omega_{v}^{(i)}\\right)$越大$f$越小。上式使用了sigmoid函数:$\\frac{1}{1+e^{-x}}$返回一个$[0,1]$的概率。\n值得注意的是，偏置项$z_{u}^{(0)}$和$\\omega_{v}^{(0)}$的作用是反映$u$传入$v$的一般趋势，这样做的目的是避免不同的$u$和$v$产生相同的概率。\nLearning Algorithm 考虑所有节点对的传播概率$\\mathcal{P}={P_{u,v} | (u,v) \\in \\mathcal{U}^2}$ (涉及所有节点对)。那么对于特定级联$D$的概率为： $$ P(D)=\\prod_{v \\in D(\\infty)} P_{v}^{D} \\prod_{v \\in \\overline{D}(\\infty)}\\left(1-P_{v}^{D}\\right) $$ 上式中，$\\prod_{v \\in D(\\infty)} P_{v}^{D}$表示$D$中所有被影响的用户存在的概率，$\\prod_{v \\in \\overline{D}(\\infty)}\\left(1-P_{v}^{D}\\right)$表示$D$中所有未被影响的用户存在的概率。所以$P(D)$就是级联$D$存在的概率。同时，可以用对数似然来表示训练级联集$\\mathcal{D}$: $$ \\mathcal{L}(\\mathcal{P} ; \\mathcal{D})=\\sum_{D \\in \\mathcal{D}}\\left(\\sum_{v \\in D(\\infty)} \\log \\left(P_{v}^{D}\\right)+\\sum_{v \\in \\overline{D}(\\infty)} \\log \\left(1-P_{v}^{D}\\right)\\right) $$ 上式就是模型的目标函数。\n","permalink":"https://JhuoW.github.io/posts/2019-05-12-embedding-ic/","summary":"文章链接：Embedding_IC\nIntroduction 本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。\n对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：\n 用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。 用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。 不同应用中的级联长度变化很大，难以学习和预测。  本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：\n 影响传播是二元的（被感染或不被感染）， 扩散网络未知， 影响关系不依赖于传播的内容， 用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。  本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：\nModel Notations 传播事件集$\\mathcal{D}={D_1,D_2,\u0026hellip;,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。\n给定一个社交网络，有$N$个用户：$\\mathcal{U}={u_1,u_2,\u0026hellip;,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \\in \\mathcal{U} \\wedge t^D(u)\u0026lt; \\infty}$， 其中，$t^D: \\mathcal{U} \\to \\mathbb{R}^+$ 表示用户被传染的时间戳， $\\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \\quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \\in \\mathcal{U} | t^D(u)\u0026lt;t}$。对称地，用$\\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\\infty)$表示最终所有被感染的用户，$\\bar{D}(infty)$表示最终所有没被感染的用户。\nDiffusion Model 本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。\n在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \\in \\mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \\in \\mathcal{U} \\backslash I$: $$ P(v|I) = 1-\\prod_{u \\in I}(1-P_{u,v}) $$ 上式中，$\\prod_{u \\in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。","title":"《Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model》阅读笔记"},{"content":"影响力传播模型 社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择\nLC IT模型（独立级联模型和线性阈值模型） WC（权重级联模型） HD（热传播模型） SIR（传染病模型） MIA模型（路径相关） 投票模型 巴斯模型 影响力最大化算法 目前有的几个影响力最大化的算法\n   基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）\n  A note on maximizing a submodular set function subject to a knapsack constraint 这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样\n  Cost-effective outbreak detection in networks （CELF算法）\n  Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法） 这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路\n    基于中心性的启发式算法\n  Efficient influence maximization in social networks W.Chen （DegreeDiscount算法） 这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1\n  A potential-based node selection strategy for influence max- imization in a social network （TM算法） 这个算法也是一种启发式算法，它是以节点地潜在地影响力作为启发的因素去选择初始的节点，同时设置了一个参数c，分为两个阶段，第一个阶段启发式的选择潜在影响力最大的若干个节点，第二阶段用贪心算法对这些第一阶段选择的种子节点进行筛选，当参数c为0，也就是没有第一阶段，那么算法就变成了传统的贪心算法。算法的缺点也很明显，首先参数c的选择依赖于经验判断，其次是启发式，准确率没有保证，再次也需要使用贪心算法，因此时间复杂度很高\n  A new centrality measure for influence maxi- mization in social networks 考虑到了传播模型中的传播度，利用这个进行启发式选择传播度最高的节点，从而得到更加精确的结果。这个传播度包括了两个方面，第一是节点自己影响他的邻居，第二个是他的邻居影响其他节点，结合影响概率一起构建的模型。优点在于启发式的选择的同时考虑了传播模型。\n    基于影响路径的算法\n  Tractable models for information diffusion in social networks（SP1M算法） 基于影响路径的算法考虑某个节点只会尽可能地影响从这个节点开始地最短或者次最短路径上地节点，因此，可以递归地计算influenc spread而不用像贪心算法那样使用蒙特卡洛模拟，从而导致大量地计算时间，因此提高了算法地效率\n  Scalable influence maximization for prevalent viral marketing in large-scale social networks （MIA算法） 借鉴了基于路径地影响力最大化算法的思路，提出一种利用局部图结构的树状近似算法来近似influence spread从而也是避免了蒙特卡罗模拟。此算法中，每条路径具有一个传播概率，定义为在这条路径上的每条边的传播概率的乘积。只有具有最大传播概率的路径才能够作为影响力路径来扩散影响力。同时给每一个节点计算树状度，定义为从节点出发的各条路径中，路径传播概率大于阈值 $\\theta$的路径上的所有点的集合\n  Scalable and parallelizable processing of influence maximization for large-scale social networks （IPA算法）\n    该算法不同于chen等人提出的算法，认为每条路径是相互独立的，chen等人只选择了具有最大的propagation 概率的那条路径，但是本论文则选择所有大于阈值 $\\theta$的路径，并行的计算他们的influence spread。基于路径的算法也具有缺点，比如没有理论上的准确度保证，同时，针对于特别复杂的图，空间复杂度非常大\n  基于社区的算法\n Oasnet: an optimal allocation approach to in- fluence maximization in modular social networks（OASNET算法）  这个算法假设社交网络划分社区后的每个社区是相互独立的，社区之间不会存在相互的影响力传播，利用CNM算法进行社区发现。种子节点的选择则分为两个阶段，第一个节点在每个社区内部利用贪心算法选择k个节点，第二个阶段则使用动态规划的方法在$C \\times k$个节点中选择最终的k个节点\n Identifying influential nodes in complex networks with community structure  这个算法基于利用社区结构发现社交网络中的最具有影响力的几个节点的研究。首先根据加权图构造概率转移矩阵，然后使用$K-Mediods$聚类方法找到最具有影响力的若干个节点。\n Cim: community-based influence maximization in social networks（CIM算法）  chen等人基于HD（热传播）模型提出的基于社区结构的影响力最大化算法。算法分为好三个阶段，首先是社区发现，作者给出了一种$H_{Clustering}$算法用于社区发现，然后是候选节点迭代，作者根据节点的拓扑结构和它的社区特征进行选择，最后是种子节点的选择，同时考虑了诸多因素，个人认为是一个比较合理的影响力最大化算法。\n Conformity-aware influence maximization in online social networks （CINEMA算法）  基于节点的一致性来设计的算法。传播模型中的概率定义为让第一个节点的影响力指标乘以第二个节点的一致性指标作为传播概率。\n当然，基于社区发现的算法也有自己的缺点，首先是在社区内部进行初步的节点的选择，也需要进行蒙特卡洛模拟，因此时间复杂度也会比较大，其次，社区发现的思路，是用节点在社区内的influence spread去模拟它在whole network上的influence spread，近似效果依赖于网络结构，如果社区之间的连接边都比较少，那么近似结果是非常接近的，但是如果社区之间的连接边比较多，及即是hub节点比较多，那么近似效果可想而知\n   1.基于的贪心算法KK（kempe等学者提出的算法） 2.基于贪心算法的改进算法，利用启发式规则改进的NewGreedyIC，MixedGreedyIC，NewGreedyWC算法 相关论文：Efficient influence maximization in social networks 2009 3.基于贪心算法的改进算法，利用子模性质改进的CELF算法，改进的CELF++算法 相关论文：Cost-effective outbreak detection in networks 2007 CELF++：optimizing the greedy algorithm for influence maximization in social networks 2011 4.启发式算法：随机算法，度中心算法，MaxDegree算法，Degree Discount算法 相关论文：Efficient influence maximization in social networks 5.基于社区划分的算法：OASNET算法，CGA算法等（后面加上现阶段阅读的论文） 相关论文：Community-based greedy algorithm for mining top-k influential nodes in mobile social networks 2010 6.MIA算法 混合式算法 社区划分的算法   基于模块优化的算法 光谱聚类的算法 层次分级的算法 基于标签传播的算法  LPA算法（目前的最快的社区划分算法，几乎是线性时间复杂度）     论文思考的几个点 基于社区发现的影响力最大化算法的分析，论文研究的目的：\n 算法的效率保证，时间复杂度尽可能低。 算法的性能保证，尽可能接近最优解。（利用到子模拟性质？）  毕业论文的大致的框架，总体是基于社区划分的思路，具体需要做的工作如下：\n   传播模型的选择，如何改进传播模型使得切合实际的传播过程？IC，LT改进？结合PageRank算法或者思想？或者考虑改进HDM传播模型？引入时间空间的因素使得模型更加充分？\n（1）传播模型的改进，传播模型中，针对于某个节点的从邻居获得的影响力，不应该简单的直接叠加，而是考虑每个邻居并不是等同对待的，应当区分不同的权重，针对节点之间的互动频率，互动频率高的节点，应该具有更加高的信任度，同时，也可能存在负面的影响力，即反而让节点更不可能选择新产品，这点应该在改进的模型中有所反馈。\n（2）至于这个信任程度如何计算出来反应在传播模型中，则可以考虑，根据邻居之间的互动信息，每个节点的活跃程度，邻居节点本身是否是具有很高的度的节点，邻居节点和本节点的观点是正相关还是反相关，从而决定邻居对本节点的信任度。\n（3）应该考虑影响力的时效性，是否可以考虑结合HDM和LTM模型一起，加上信任度参数这个观点，一起构建一个新的传播模型。\n  社区发现算法的选择，社区发现的选择是非常重点的，社区发现本质上是社交网络节点的聚类，应该涉及比较有效率的聚类算法或者选择其他的距离算法？\n社区发现聚类算法，一般都是先设置每个节点作为单独的一个社区，然后进行合并，在进行社区聚类发现的时候，不应当单独仅仅考虑边，仅仅利用边的关系，比如CGA算法就利用到了传播模型，结合传播模型进行标签传播，然后获得相应的划分的社区。同时，可以加以改进的地方，比如，社交网络的社区发现不应当仅仅考虑到拓扑结构，还有考虑节点之间的互动交流的信息，互动程度越频繁，那么两个节点在一个社区内部的概率就越大，因此要考虑这个改进点。\n同时，借鉴了CGA算法的思想，一个节点的社区内部的影响力和整个社交网络的影响力如何区别？如何用社区内部的影响力去近似？或者考虑hub节点，社区之间的这些连接节点也有着非常重要的作用。\n  社区发现是否可以处理重叠社区的情况，重叠社区会导致影响力的重复传播，如何减少这种情况的出现，如何设计算法实现重叠社区的处理？\n  各个社区的重要性也是不同的，应该有选择的摒弃一些社区，先给出一个社区选择的模型，比如说利用PageRank先计算出哪些社区比较重要，有的社区人数多，有的社区人数少，但是处在中心位置，并且一些非重要的社区，往往会关注这些重要社区的传递出来的信息。考虑种子节点选择的时候，应当把社区这些因素考虑进去。我们可以忽略那些不重要的小社区，重要的社区给与比较大的加权值，同时注意影响力避免重叠传播。\n  社区发现之后，如何分配每个社区的种子节点数目？，直接按照比例分配？亦或是选择一种度量社区重要程度的模型？\nCGA算法，使用了动态规划进行贪心选择，在各个社区内部选择相应的种子节点。但是时间复杂度仍然是非常大的，是否可以考虑先在社区内部基于启发式规则，或者PageRank，计算重要的节点，然后全局进行贪心的选择？\n  基于社区发现的算法，实际上是利用节点在社区内的传播来近似它在整个网络上传播的效果，因此这种近似肯定存在误差，如何减少这种误差的产生？而且这种误差还是和网络中的社区结构有关系的。\n  注意充分利用社区结构的特点，划分社区之后，把社区也视为一个点，作为整体去考虑\n  社区内的候选节点选择，候选节点应该按照什么标准进行选择，是按照启发式的度选择？还是设计另外的模型？结合PageRank模型？\n  种子节点的获取策略，如何在这些候选节点上选择出最终的种子节点？直接按照贪心策略暴力选择还是参考CELF算法进行选择？或者是涉及其他的方法？\n   思路总结  基于社区发现的影响力最大化算法框架：\n 社区划分。 充分考虑社区作为一个整体性，来体现社区的一个作用，可以利用PageRank模型，来代表社区的重要程度，这个是社区的一个属性，利用这个模型，选择一些重要的社区，同时摒弃一些小的，没那么重要的社区。而且利用PageRank进行迭代，应该比传统的算法会快一些。 社区内部种子节点的选择，考虑的是社区内部的种子节点在社区内部的影响力传播。如何选择社区内部的种子节点。？？？？？这一点目前还需要多家考虑。 社区出现重叠，如何考虑？ 种子节点在社区内的影响力只是对种子节点在全局的影响力的近似，那么需要一种方式来弥补这种误差。显然，种子节点的影响力如果想要传播到另外的社区，那么是通过社区之间的边界节点进行传播的，同时和社区本身的重要性有关，那么这部分的误差通过边界节点来弥补。 最终会得到若干的候选节点，这些节点使用贪心算法进行选择出最终的种子节点，考虑贪心算法的时间复杂度，那么可以考虑使用CELF思路或者是其他的CGA这类的动态规划思路去求解，不过这还是基于蒙特卡罗模拟，时间复杂度仍然相对比较大，对算法进行加速。 影响力传播模型，基于线性阈值模型进行改变，加上信任度参数，因为每个影响力的叠加不是平权的，和用户之间的互动，观点信息，兴趣爱好是否一直存在着相关的关系，因此在影响力传播模型中加入这个考虑因素。   算法的实验部分，考虑一些经典的BaseLine Algorithm   传统贪心爬山算法-KK算法 CELF算法 CGA算法 CIM算法 启发式算法（度启发式，中心性启发式）DegreeDiscount算法 本论文的算法     算法中参数的选择的影响 控制变量对比实验   相关工作总结    Richardson和Domingos在2002年的论文，首次把这个问题作为一个研究方向提出。\n  首先应该数说到的式Kempe的2003年的论文，主要提出了\n  a）LT IC 模型，并说明了这个问题的NP完全性\nb）给出了贪心算法\nc）说明了影响力递增的边界递减性质，利用子模性质说明了算法的性能保证\n 近期论文阅读总结   传播模型的改进，基于PageRank的改进，传统PageRank在考虑某个节点的PR值是均匀分配给链出的节点的（链出的概率为出度的倒数）（即权重级联模型），但是实际上，PR高的节点具有更高的影响力，因此考虑链出的概率不用度，而用PR值的占比，从而更加切合实际的情况。\n  还有的改进算法，改进了PageRank计算模型，把节点自身的属性，节点之间互动的属性，加入到了PageRank模型计算中，使得PageRank能够适用于社交网络中节点重要性的计算。\n  我们可以考虑把以上的两者结合起来给出一种新的信息传播模型（给出概率计算的方法）。\n  数据集选择\n 参考宫秀云那篇文章\n   总结：基于PageRank思想的影响力计算，都是在PageRank的基础上进行模型的改进，加入其他的影响因子，给出不同的权重，从而更加符合实际的应用场景。\n  ","permalink":"https://JhuoW.github.io/posts/2019-05-06-im-conclusion/","summary":"影响力传播模型 社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择\nLC IT模型（独立级联模型和线性阈值模型） WC（权重级联模型） HD（热传播模型） SIR（传染病模型） MIA模型（路径相关） 投票模型 巴斯模型 影响力最大化算法 目前有的几个影响力最大化的算法\n   基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）\n  A note on maximizing a submodular set function subject to a knapsack constraint 这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样\n  Cost-effective outbreak detection in networks （CELF算法）\n  Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法） 这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路\n    基于中心性的启发式算法\n  Efficient influence maximization in social networks W.Chen （DegreeDiscount算法） 这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1","title":"Influence Maximization Conclusion"},{"content":"这篇笔记用于收藏别人的博客\n技术博客    Blog Author     https://michael-bronstein.medium.com/ Michael Bronstein   https://geometricdeeplearning.com/ Michael Bronstein   https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c Vitaly Kurin (Many Paper Notes)   https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html UvA DL Notebooks   https://graph-neural-networks.github.io/index.html GNN Books   http://prob140.org/sp17/textbook/ Probability for Data Science class at UC Berkeley   https://graphreason.github.io/schedule.html Learning and Reasoning with Graph-Structured Representations ICML 2019 Workshop   https://chuxuzhang.github.io/KDD21_Tutorial.html KDD2021 Tutorial: Data Efficient Learning on Graphs   http://songcy.net/posts/ Changyue Song (Kernel)   https://www.cs.mcgill.ca/~wlh/grl_book/ William L. Hamilton   https://kexue.fm/ BoJone   https://danielegrattarola.github.io/blog/ Daniele Grattarola (EPFL)   https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html Google AI Blog   https://zhiyuchen.com/blogs/ Zhiyu Chen   https://andreasloukas.blog/ Andreas Loukas (EPFL)   https://irhum.pubpub.org/pub/gnn/release/4 Understanding Graph Neural Networks   https://lilianweng.github.io/ Lilian Weng   https://www.zhihu.com/column/marlin 深度学习与图网络   https://github.com/roboticcam/machine-learning-notes Yida Xu   https://www.dgl.ai/pages/index.html DGL   https://www.kexinhuang.com/tech-blog Kexin Huang   https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8 Rishabh Anand   https://saashanair.com/blog Saasha Nair   http://www.huaxiaozhuan.com/ 华校专   https://github.com/dglai/WWW20-Hands-on-Tutorial DGL   https://github.com/tianyicui/pack 背包9講   https://www.fenghz.xyz/    https://sakigami-yang.me/2017/08/13/about-kernel-01/ kernel   https://davidham3.github.io/blog    https://fenghz.github.io/index.html    https://archwalker.github.io/     Survey    Repo Name     https://github.com/naganandy/graph-based-deep-learning-literature links to conference publications in graph-based deep learning (Very, Very, Very Important)   https://github.com/SherylHYX/pytorch_geometric_signed_directed PyTorch Geometric Signed Directed is a signed/directed graph neural network extension library for PyTorch Geometric.   https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning Paper Lists for Fair Graph Learning   https://github.com/thunlp/PromptPapers Must-read papers on prompt-based tuning for pre-trained language models.   https://github.com/zhao-tong/graph-data-augmentation-papers A curated list of graph data augmentation papers.   https://github.com/Thinklab-SJTU/ThinkMatch Code \u0026amp; pretrained models of novel deep graph matching methods.   https://github.com/FLHonker/Awesome-Knowledge-Distillation Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)。   https://github.com/zlpure/awesome-graph-representation-learning A curated list for awesome graph representation learning resources.   https://github.com/basiralab/GNNs-in-Network-Neuroscience A review of papers proposing novel GNN methods with application to brain connectivity published in 2017-2020.   https://github.com/flyingdoog/awesome-graph-explainability-papers Papers about explainability of GNNs   https://github.com/yuanqidu/awesome-graph-generation A curated list of graph generation papers and resources.   https://github.com/benedekrozemberczki/awesome-decision-tree-papers A collection of research papers on decision, classification and regression trees with implementations.   https://github.com/AstraZeneca/awesome-explainable-graph-reasoning A collection of research papers and software related to explainability in graph machine learning.   https://github.com/LirongWu/awesome-graph-self-supervised-learning Awesome Graph Self-Supervised Learning   https://github.com/Chen-Cai-OSU/awesome-equivariant-network Paper list for equivariant neural network   https://github.com/mengliu1998/DL4DisassortativeGraphs Papers about developing DL methods on disassortative graphs   https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers A curated list of graph reinforcement learning papers.   https://github.com/ChandlerBang/awesome-self-supervised-gnn Papers about pretraining and self-supervised learning on Graph Neural Networks (GNN).   https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks Paper Lists for Graph Neural Networks   https://github.com/jwzhanggy/IFMLab_GNN Graph Neural Network Models from IFM Lab   https://github.com/ChandlerBang/awesome-graph-attack-papers Adversarial attacks and defenses on Graph Neural Networks.   https://github.com/safe-graph/graph-adversarial-learning-literature A curated list of adversarial attacks and defenses papers on graph-structured data.   https://github.com/benedekrozemberczki/awesome-graph-classification A collection of important graph embedding, classification and representation learning papers with implementations.   https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers A curated list of gradient boosting research papers with implementations.   https://github.com/benedekrozemberczki/awesome-community-detection A curated list of community detection research papers with implementations.    好用的工具    Name Info     http://acronymify.com/ Model Name   https://csacademy.com/app/graph_editor/ Graph Editor   https://github.com/guanyingc/python_plot_utils A simple code for plotting figure, colorbar, and cropping with python   https://github.com/guanyingc/latex_paper_writing_tips Tips for Writing a Research Paper using LaTeX   https://github.com/JhuoW/Pytorch_Program_Templete Pytorch Program Templete GNN   https://github.com/graph4ai/graph4nlp Graph4nlp is the library for the easy use of Graph Neural Networks for NLP. Welcome to visit our DLG4NLP website (https://dlg4nlp.github.io/index.html) for various learning resources!   https://github.com/benedekrozemberczki/pytorch_geometric_temporal PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models (CIKM 2021)    Social Diffusion    Title Unscramble     Social Influence Locality for Modeling Retweeting Behavior 解读 代码   Role-Aware Conformity Influence Modeling and Analysis in Social Network 解读   DeepInf:Social Influence Prediction with Deep Learning 解读   Reverse Influence Sampling in Python 解读及代码   Cost-effective Outbreak Detection in Networks（CELF） 解读及代码 解读   DeepCas: an End-to-end Predictor of Information Cascades 解读   Inf2vec: Latent Representation Model for Social Influence Embedding 解读   POI2Vec: Geographical Latent Representation for Predicting Future Visitors 解读    Network Embedding and GNN    Title Unscramble     Heterogeneous Graph Attention Network 解读   Attributed Social Network Embedding 解读   Self-Translation Network Embedding 解读1 解读2   Self-Paced Network Embedding 解读   CANE: Context-Aware Network Embedding for Relation Modeling 解读   HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning 解读1 解读2   Embedding Temporal Network via Neighborhood Formation 解读   Network Representation Learning with Rich Text Information 解读1 解读2   Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks 解读   SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction 解读   Context-Aware Network Embedding for Relation Modeling 解读   GAN在网络特征学习中的应用 this   一文读懂「Attention is All You Need」 附代码实现 解读   node2vec: Scalable Feature Learning for Networks 解读1 解读2 解读3   PTE:Predictive Text Embedding through Large-scale 解读   struc2vec: Learning Node Representations from Structural Identity 解读   AspEm: Embedding Learning by Aspects in HINs 解读   Max-Margin DeepWalk: Discriminative Learning of Network Representation 解读1 解读2   Structural Deep Network Embedding 解读   Learning Structural Node Embeddings via Diffusion Wavelets 解读   HARP: Hierarchical Representation Learning for Networks 解读   Dynamic Network Embedding by Modeling Triadic Closure Process 解读   RaRE: Social Rank Regulated Large-scale Network Embedding 解读   TransNet: Translation-Based Network Representation Learning for Social Relation Extraction 解读   GraphGAN: Graph Representation Learning with Generative Adversarial Nets 解读   Graph Convolutional Network 解答   GraRep: Learning Graph Representations with Global Structural Information 解读   Deep Dynamic Network Embedding for Link Prediction 解读   Representation Learning for Attributed Multiplex Heterogeneous Network GATNE   Inductive Representation Learning on Large Graphs GraphSAGE   Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba 解读   Graph Convolutional Neural Networks for Web-Scale Recommender Systems PinSAGE   Semi-Supervised Classification with Graph Convolutional Networks Semi-GCN   Joint Type Inference on Entities and Relations via Graph Convolutional Networks 解读   图卷积神经网络（GCN） GCN   Adaptive SamplingTowards Fast Graph Representation Learning 解读   ProNE: Fast and Scalable Network Representation Learning ProNE   ROTATE:Knowledge graph embedding by relational rotate in complex space Rotate   Signed Graph Convolutional Network SGCN   GAT: Graph Attention Network GAT GAT2   Large-Scale Learnable Graph Convolutional Networks LGCN LGCN2 LGCN3 LGCN4 LGCN5 代码分析   Hierarchical Graph Representation Learning with Differentiable Pooling DiffPool DiffPool    ML \u0026amp; DL    Title Unscramble     常见散度与距离(KL散度，JS散度，Wasserstein距离，互信息MI) Here   【简化数据】奇异值分解(SVD) Here   AUC的计算与近似 Here   PCA Here Here Here   AdaBoost Here Here   Batch Normalization Here   矩阵的正定及半正定 Here   精确率、召回率、F1 值、ROC、AUC Here   Hierarchical Softmax Here   傅立叶变换 Here   卷积神经网络（CNN）之一维卷积、二维卷积、三维卷积详解 Here   变分自编码器（VAE） Here    ","permalink":"https://JhuoW.github.io/posts/2019-04-28-paper-unscramble/","summary":"这篇笔记用于收藏别人的博客\n技术博客    Blog Author     https://michael-bronstein.medium.com/ Michael Bronstein   https://geometricdeeplearning.com/ Michael Bronstein   https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c Vitaly Kurin (Many Paper Notes)   https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html UvA DL Notebooks   https://graph-neural-networks.github.io/index.html GNN Books   http://prob140.org/sp17/textbook/ Probability for Data Science class at UC Berkeley   https://graphreason.github.io/schedule.html Learning and Reasoning with Graph-Structured Representations ICML 2019 Workshop   https://chuxuzhang.github.io/KDD21_Tutorial.html KDD2021 Tutorial: Data Efficient Learning on Graphs   http://songcy.","title":"长期更新-好论文解读收藏"},{"content":"The Independent Cascade Model (IC Model) IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。\n在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。\n值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。\nThe Linear Threshold Model (LT Model) LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。\n形式上， 在图$G$中每条边$e=(u,v) \\in E$有一个权重$b_{u,v}$。 我们定义$\\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\\sum_{u \\in \\mathcal{N}_I (v)} b_{u,v} \\leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\\theta_v$。 LT模型首先为每个节点$v$的阈值$\\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。\n","permalink":"https://JhuoW.github.io/posts/2019-03-20-ic-lt/","summary":"The Independent Cascade Model (IC Model) IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。\n在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。\n值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。\nThe Linear Threshold Model (LT Model) LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。\n形式上， 在图$G$中每条边$e=(u,v) \\in E$有一个权重$b_{u,v}$。 我们定义$\\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\\sum_{u \\in \\mathcal{N}_I (v)} b_{u,v} \\leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\\theta_v$。 LT模型首先为每个节点$v$的阈值$\\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。","title":"社交网络影响最大化（Influence Maximization）中的IC，LT模型"},{"content":"论文地址：BiNE\nIntroduction Bipartite Network(二分网络):如下图所示：\n二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。\n另一个问题，\n如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。\n另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。\n针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过HITS来衡量。\nModel 如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \\times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\\overrightarrow{u_i}]$, $V=[\\overrightarrow{v_i}]$，结构如下图所示：\n（取自作者的讲解ppt)\nExplicit Relations 同LINE一样， 基于直接连接的目标函数表示为：\n$$\\mathrm{minimize} \\quad O_1=-\\sum_{e_{ij} \\in E}w_{ij}\\log \\hat{P}(i,j)$$\nImplicit Relations 构造随机游走序列 这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：\n$$w^U_{ij}=\\sum_{k \\in V}w_{ik}w_{jk}$$\n$$w^V_{ij}=\\sum_{k \\in U}w_{ki}w_{kj}$$\n其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。 其中$l=\\max(H(v_i)\\times \\max T,\\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。\n$$D_{v_i}=\\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$\n表示其中一次随机游走的节点集合$p$表示停止概率。\n通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。\n对间接关系建模 如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。 对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \\in C_s(u_i)$的条件概率。所以目标函数如下： $$\\mathrm{maximize} \\quad O_2 = \\prod_{u_i \\in S \\land S \\in D^U} \\prod_{u_c \\in C_s(u_i)}P(u_c|u_i)$$\n对于$D^V$同理。其中,$p(u_c|u_i) = \\frac{\\exp(\\overrightarrow{u}_i^T \\overrightarrow{\\theta}_c)}{\\sum^{|U|}_{k=1} \\exp(\\overrightarrow{u}_i^T \\overrightarrow{\\theta}_k))}$。\nNegative Sampling 本文的负采样方法是基于局部敏感哈希（LSH）来对与中心节点不相似的节点进行采样。 该方法的strategy是，给定一个中心节点，随机选取一个bucket（序列）并且这个序列不包含给定的中心节点，以此来获得和给定节点尽量不相似的负采样节点。\n$N^{ns}_S (u_i)$ 表示$ns$个负采样节点，对于中心节点$u_i$, 那么上文提到的条件概率$p(u_c|u_i)$可以被定义为下式：\n$$p(u_c,N^{ns}_S (u_i)|u_i) = \\prod_{z \\in {u_c} \\cup N^{ns}_S (u_i)} P(z|u_i)$$\n其中条件概率$P(z|u_i)$定义为：\n其中$\\sigma$表示sigmoid函数，这样就减少了上文softmax函数造成的计算量过大的问题。\n联合优化 通过随机梯度上升对3部分损失函数进行加权优化：\n$$\\mathrm{maximize} \\quad L = \\alpha \\log O_2+\\beta \\log O_3 - \\gamma O_1$$ 最终BiNE的整体算法流程如下：\nConclusion 这篇文章提出的分布式训练以及负采样策略还是很值得学习的。\n","permalink":"https://JhuoW.github.io/posts/bine/","summary":"论文地址：BiNE\nIntroduction Bipartite Network(二分网络):如下图所示：\n二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。\n另一个问题，\n如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。\n另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。\n针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过HITS来衡量。\nModel 如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \\times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\\overrightarrow{u_i}]$, $V=[\\overrightarrow{v_i}]$，结构如下图所示：\n（取自作者的讲解ppt)\nExplicit Relations 同LINE一样， 基于直接连接的目标函数表示为：\n$$\\mathrm{minimize} \\quad O_1=-\\sum_{e_{ij} \\in E}w_{ij}\\log \\hat{P}(i,j)$$\nImplicit Relations 构造随机游走序列 这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：\n$$w^U_{ij}=\\sum_{k \\in V}w_{ik}w_{jk}$$\n$$w^V_{ij}=\\sum_{k \\in U}w_{ki}w_{kj}$$\n其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。 其中$l=\\max(H(v_i)\\times \\max T,\\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。\n$$D_{v_i}=\\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$\n表示其中一次随机游走的节点集合$p$表示停止概率。\n通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。\n对间接关系建模 如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。 对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \\in C_s(u_i)$的条件概率。所以目标函数如下： $$\\mathrm{maximize} \\quad O_2 = \\prod_{u_i \\in S \\land S \\in D^U} \\prod_{u_c \\in C_s(u_i)}P(u_c|u_i)$$","title":"BiNE"},{"content":"论文地址: Enhanced Network Embeddings via Exploiting Edge Labels\nIntroduction 这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。\n具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个半监督NE模型。\nProblem Definition 假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,\u0026hellip;,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \\cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \\in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \\subseteq L$。目的还是一样，学习一个映射函数$\\Phi \\to \\mathbb{R}^{|V| \\times d}$, 其中$d \\ll |V|$。\nMethod 首先定义损失函数:\n$$\\mathcal{L}=(1-\\lambda)\\mathcal{L}_s+\\lambda\\mathcal{L}_r$$\n其中$\\mathcal{L}_s$表示预测节点邻域的损失。$\\mathcal{L}_r$表示预测边label的损失。$\\lambda$是两种损失的权重。\nStructural Loss 第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：\n$$\\mathcal{L}_s=-\\sum_{u \\in C(v)} \\log Pr(u|v)$$\n这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：\n$$Pr(u|v)=\\frac{\\exp(\\Phi(u) \\cdot \\Phi\u0026rsquo;(v))}{\\sum_{u\u0026rsquo; \\in V} \\exp(\\Phi(u\u0026rsquo;) \\cdot \\Phi\u0026rsquo;(v))}$$\n这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\\Phi(v)$和$\\Phi\u0026rsquo;(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。\n剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：\n$$C(v)={v_{i-w},\u0026hellip;,v_{i-1}} \\cup {v_{i+1},\u0026hellip;,v_{i+w}}$$\nRelational Loss 由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \\in E$,可以用一下方法来表示这条边:\n$$\\Phi(e)=g(\\Phi(u),\\Phi(v))$$\n其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：\n$$\\Phi(e)=\\Phi(u) \\oplus \\Phi(v)$$\n这样我们就获得了edge embedding。直接将它输入前馈神经网络，前馈神经网络第$k$层的定义为:\n$$h^{(k)}=f(W^{(k)}h^{(k-1)}+b^{(k)})$$\n其中 $h^{(0)}=\\Phi(e)$，$f$是除最后一层外采用relu激活函数，最后一层采用sigmoid函数激活,最后一层输出为$\\hat{y_i}$。最后最小化二元交叉熵损失函数：\n$$\\mathcal{L}_r=\\sum^{|L|}_{i=1} H(y_i,\\hat{y_i}) + (1-y_i) \\cdot \\log (1-\\hat{y_i})$$\nConclusion 这篇论文从原理到方法实现都非常简单，稍后我也将尝试复现这篇论文，边的标签信息是以前NE方法所没有考虑到的，但这篇论问的局限性是没有考虑边的方向以及权重，这是可以拓展的方向。\n","permalink":"https://JhuoW.github.io/posts/2019-01-22-ne-edge-labels/","summary":"论文地址: Enhanced Network Embeddings via Exploiting Edge Labels\nIntroduction 这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。\n具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个半监督NE模型。\nProblem Definition 假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,\u0026hellip;,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \\cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \\in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \\subseteq L$。目的还是一样，学习一个映射函数$\\Phi \\to \\mathbb{R}^{|V| \\times d}$, 其中$d \\ll |V|$。\nMethod 首先定义损失函数:\n$$\\mathcal{L}=(1-\\lambda)\\mathcal{L}_s+\\lambda\\mathcal{L}_r$$\n其中$\\mathcal{L}_s$表示预测节点邻域的损失。$\\mathcal{L}_r$表示预测边label的损失。$\\lambda$是两种损失的权重。\nStructural Loss 第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：\n$$\\mathcal{L}_s=-\\sum_{u \\in C(v)} \\log Pr(u|v)$$\n这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：\n$$Pr(u|v)=\\frac{\\exp(\\Phi(u) \\cdot \\Phi\u0026rsquo;(v))}{\\sum_{u\u0026rsquo; \\in V} \\exp(\\Phi(u\u0026rsquo;) \\cdot \\Phi\u0026rsquo;(v))}$$\n这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\\Phi(v)$和$\\Phi\u0026rsquo;(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。\n剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：\n$$C(v)={v_{i-w},\u0026hellip;,v_{i-1}} \\cup {v_{i+1},\u0026hellip;,v_{i+w}}$$\nRelational Loss 由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \\in E$,可以用一下方法来表示这条边:\n$$\\Phi(e)=g(\\Phi(u),\\Phi(v))$$\n其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：","title":"《Enhanced Network Embeddings via Exploiting Edge Labels》阅读笔记"},{"content":"最近遇到一个问题，如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积\n之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：\n我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：\n{% codeblock %}\nimport cv2 import numpy as np # Input image img = cv2.imread('cut.jpeg', cv2.IMREAD_GRAYSCALE) # Needed due to JPG artifacts _, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY) # Dilate to better detect contours temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) # Find largest contour _, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) largestCnt = [] for cnt in cnts: if len(cnt) \u0026gt; len(largestCnt): largestCnt = cnt # Determine center of area of largest contour M = cv2.moments(largestCnt) x = int(M[\u0026quot;m10\u0026quot;] / M[\u0026quot;m00\u0026quot;]) y = int(M[\u0026quot;m01\u0026quot;] / M[\u0026quot;m00\u0026quot;]) # Initiale mask for flood filling width, height = temp.shape mask = img2 = np.ones((width + 2, height + 2), np.uint8) * 255 mask[1:width, 1:height] = 0 # Generate intermediate image, draw largest contour, flood filled temp = np.zeros(temp.shape, np.uint8) temp = cv2.drawContours(temp, largestCnt, -1, 255, cv2.FILLED) _, temp, mask, _ = cv2.floodFill(temp, mask, (x, y), 255) temp = cv2.morphologyEx(temp, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) # Count pixels in desired region area = cv2.countNonZero(temp) # Put result on original image img = cv2.putText(img, str(area), (x, y), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, 255) cv2.imshow('Input', img) cv2.imshow('Temp image', temp) cv2.waitKey(0)  {% endcodeblock %}\n最后我们可以得到一个比较准确的轮廓：\n面积如图中所示：\n参考：\nhttps://stackoverflow.com/questions/55467031/how-to-get-the-area-of-the-contours\n","permalink":"https://JhuoW.github.io/posts/pic-closed-edge/","summary":"最近遇到一个问题，如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积\n之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：\n我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：\n{% codeblock %}\nimport cv2 import numpy as np # Input image img = cv2.imread('cut.jpeg', cv2.IMREAD_GRAYSCALE) # Needed due to JPG artifacts _, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY) # Dilate to better detect contours temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) # Find largest contour _, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) largestCnt = [] for cnt in cnts: if len(cnt) \u0026gt; len(largestCnt): largestCnt = cnt # Determine center of area of largest contour M = cv2.","title":"OpenCV轮廓提取并计算图片中某一封闭区域的面积"},{"content":"最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。\nGradient Desent(梯度下降) 目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。\n(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。\n(2).目标函数关于参数$x$在epoch $t$时的梯度：\n$$g_t = \\nabla_x f(x_t)$$\n(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：\n$$x_{t+1} = x_t-\\eta_t g_t$$\n其中$x_{t+1}$为$t+1$时刻的参数值。\nStochastic Gradient Desent(随机梯度下降) 梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。\n首先给出无偏估计的定义，稍后会用到：\n无偏估计：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。\n深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \\frac{\\displaystyle\\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：\n$$\\nabla f_{batch}(x) = \\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\nabla f_i(x)$$\n如果使用GD来优化：\n$$x_{t+1} = x_{t}- \\eta_t \\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\nabla f_i(x) \\ = x_t-\\eta_t \\nabla f_{batch}(x)$$ 上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。\n随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \\in {1, \\cdots,n}$, 每次从中随机选取一个样本来更新参数$x$。\n$$x_{t+1} = x_{t}-\\eta_t \\nabla f_i(x)$$\n这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\\nabla f_i(x)$是对梯度$\\nabla f_{batch}(x)$的无偏估计，因为：\n$$E_i \\nabla f_i(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\boldsymbol{x}) = \\nabla f_{batch}(\\boldsymbol{x})$$\n符合无偏估计的定义。\nMomentum(动量法) Exponentially weighted moving averages(EMA) EMA,指数加权移动平均数。\n在GD中,如果学习率过大，会导致目标函数发散，而无法逼近最小值，如下图所示：\n如果学习率很低，那么会缓慢接近最优点，如下图红色轨迹：\n我们希望在学习率较小的时候可以更快逼近最优点，在学习率大的时候自变量可以不发散，即在正确的方向上加速下降并且抑制震荡，也就是达到如下的效果：\n因此引入EMA。给定参数$0 \\leq \\gamma \u0026lt; 1$,当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合。\n$$y_t = \\gamma y_{t-1} + (1-\\gamma) x_t$$\n展开上式:\n$$\\begin{split}\\begin{aligned} y_t \u0026amp;= (1-\\gamma) x_t + \\gamma y_{t-1}\\\\ \u0026amp;= (1-\\gamma)x_t + (1-\\gamma) \\cdot \\gamma x_{t-1} + \\gamma^2y_{t-2}\\\\ \u0026amp;= (1-\\gamma)x_t + (1-\\gamma) \\cdot \\gamma x_{t-1} + (1-\\gamma) \\cdot \\gamma^2x_{t-2} + \\gamma^3y_{t-3}\\\\ \u0026amp;\\ldots \\end{aligned}\\end{split}$$\n上式可以看出当前时刻变量是对过去时刻变量做指数加权，离当前时刻越近，加权越大（越接近1）。\n在现实中，我们将$y_t$看作是最近$1/(1-\\gamma)$个时间步的$x_t$的加权平均，当$\\gamma = 0.95$时，是最近20个时间步的$x_t$值的加权平均。当$\\gamma=0.9$时,可以看做是最近10个时间步加权平均。\n动量法 $$\\begin{split}\\begin{aligned} \\boldsymbol{v}_t \u0026amp;= \\gamma \\boldsymbol{v}_{t-1} + \\eta_t \\boldsymbol{g}_t, \\\\ \\boldsymbol{x}_t \u0026amp;= \\boldsymbol{x}_{t-1} - \\boldsymbol{v}_t, \\end{aligned}\\end{split}$$\n其中$g_t = \\nabla f_i(x)$上式可以看出，如果$\\gamma=0$，则上式就是一个普通的随机梯度下降法。$0 \\leq \\gamma \u0026lt; 1$. $\\gamma$一般取0.9。\n一般，初始化$v_0=0$, 则\n$$v_1=\\eta_t g_t \\\\ v_2=\\gamma v_1+\\eta_t g_t = \\eta_t g_t(\\gamma+1) \\\\ v_3 = \\eta_t g_t (\\gamma^2+\\gamma+1) \\\\ v_{inf} = \\frac{(\\eta_t g_t)\\cdot(1-\\gamma^{inf+1})}{1-\\gamma}\\approx \\frac{(\\eta_t g_t)}{1-\\gamma}$$\n相比原始梯度下降算法，动量梯度下降算法有助于加速收敛。当梯度与动量方向一致时，动量项会增加，而相反时，动量项减少，因此动量梯度下降算法可以减少训练的震荡过程。\n换种方式理解动量法：\n如上图所示，A点为起始点，首先计算A点的梯度$\\nabla a$，下降到B点，\n$$\\theta_{new} = \\theta-\\eta\\nabla a$$\n其中$\\theta$为参数， $\\eta$为学习率\n到达B点后要加上A点的梯度，但是A点的梯度有个衰减值$\\gamma$,推荐取0.9，相当于加上一个来自A点递减的加速度。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。所以B点的参数更新公式是这样的：\n$$v_t = \\gamma v_{t-1}+\\eta \\nabla b$$\n$$\\theta_{new} = \\theta-v_t$$\n其中$v_{t-1}$表示之前所有步骤累计的动量和，$\\nabla b$为B点的梯度方向。这样一步一步下去，带着初速度的小球就会极速的奔向谷底。\nAdaGrad 假设目标函数有两个参数分别为$x_1$,$x_2$,若梯度下降迭代过程中，始终使用相同的学习率$\\eta$:\n$$x_{1_{new}} = x_1-\\eta \\frac{\\partial f}{\\partial x_1}$$ $$x_{2_{new}} = x_2-\\eta \\frac{\\partial f}{\\partial x_2}$$\nAdaGard算法根据自变量在每个维度的梯度值来调整各个维度上的学习率，避免学习率难以适应维度的问题。adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。\n$\\nabla_{\\theta_i} J(\\theta)$表示第$i$个参数的梯度，其中$\\theta=(\\theta_1,\\theta_2,\u0026hellip;)$有$n$个参数。如果使用SGD来优化第$i$个参数，我们可以表示为:\n$$\\theta_{i_new} = \\theta_i-\\eta \\nabla_{\\theta_i}J(\\theta)$$\n如果使用Adagrad，则可以表示为这样:\n$$\\theta_{i,t+1}=\\theta_{i,t}-\\frac{\\eta}{\\sqrt{G_{i,t}+\\epsilon}} \\nabla_{\\theta_{i,t}}J(\\theta)$$\n$i,t$ 表示优化参数$\\theta_i$时的第$t$次迭代，$\\epsilon$防止分母为0，可以取$10^{-6}$,$G_{i,t}$表示对参数$\\theta_i$优化的前$t$步的梯度的累加：\n$$G_{i,t} = G_{i,t-1}+\\nabla_{\\theta_{i,t}}J(\\theta) $$\n新公式可以简化成:\n$$\\theta_{t+1}= \\theta_t-\\frac{\\eta}{\\sqrt{G_t+\\epsilon}}\\nabla_{\\theta_t}J(\\theta)$$\n可以从上式看出，随着迭代的推移，新的学习率$\\frac{\\eta}{\\sqrt{G_t+\\epsilon}}$在缩小，说明Adagrad一开始激励收敛，到了训练的后期惩罚收敛，收敛速度变慢\nRMSprop 主要解决Adagrad学习率过快衰减问题，类似动量的思想，引入一个超参数，在积累梯度平方项进行衰减.\n$$s = \\gamma \\cdot s +(1-\\gamma) \\cdot \\nabla J(\\theta) \\odot \\nabla J(\\theta) $$\n参数$\\theta$的迭代目标函数可以改写为:\n$$\\theta_{new} = \\theta - \\frac{\\eta}{\\sqrt{s+\\varepsilon}} \\odot \\nabla J(\\theta)$$\n可以看出$s$是梯度的平方的指数加权移动平均值，$\\gamma$一般取0.9，有助于解决 Adagrad中学习率下降过快的情况。\nAdaptive moment estimation(Adam) Adam可以说是用的最多的优化算法，Adam通过计算一阶矩估计和二阶矩估计为不同的参数设计独立的自适应学习率。\nAdabound 正在学习中\n参考文献：\nhttps://zhuanlan.zhihu.com/p/32626442\nhttps://zhuanlan.zhihu.com/p/31630368\nhttps://zh.gluon.ai/\nhttps://blog.csdn.net/tsyccnh/article/details/76270707\n","permalink":"https://JhuoW.github.io/posts/optimizer/","summary":"最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。\nGradient Desent(梯度下降) 目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。\n(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。\n(2).目标函数关于参数$x$在epoch $t$时的梯度：\n$$g_t = \\nabla_x f(x_t)$$\n(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：\n$$x_{t+1} = x_t-\\eta_t g_t$$\n其中$x_{t+1}$为$t+1$时刻的参数值。\nStochastic Gradient Desent(随机梯度下降) 梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。\n首先给出无偏估计的定义，稍后会用到：\n无偏估计：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。\n深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \\frac{\\displaystyle\\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：\n$$\\nabla f_{batch}(x) = \\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\nabla f_i(x)$$\n如果使用GD来优化：\n$$x_{t+1} = x_{t}- \\eta_t \\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\nabla f_i(x) \\ = x_t-\\eta_t \\nabla f_{batch}(x)$$ 上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。\n随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \\in {1, \\cdots,n}$, 每次从中随机选取一个样本来更新参数$x$。\n$$x_{t+1} = x_{t}-\\eta_t \\nabla f_i(x)$$\n这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\\nabla f_i(x)$是对梯度$\\nabla f_{batch}(x)$的无偏估计，因为：","title":"深度学习中的优化算法总结"},{"content":"🌟自己好菜呀，希望可以多多刷paper~\n本站搭建于Github\n联系方式： e-mail: AdversarialAttack@protonmail.com\nQQ/Wechat: 暂不公开，发邮件给我我告诉你\n其他 🏀休斯顿火箭，⚽巴萨\n💘昆汀~科恩兄弟\n📷摄影爱好者\n🌜严重失眠患者 （顺便求助睡眠方法）\nMay The Force Be With You ","permalink":"https://JhuoW.github.io/about/","summary":"🌟自己好菜呀，希望可以多多刷paper~\n本站搭建于Github\n联系方式： e-mail: AdversarialAttack@protonmail.com\nQQ/Wechat: 暂不公开，发邮件给我我告诉你\n其他 🏀休斯顿火箭，⚽巴萨\n💘昆汀~科恩兄弟\n📷摄影爱好者\n🌜严重失眠患者 （顺便求助睡眠方法）\nMay The Force Be With You ","title":"About"},{"content":"论文地址：HTNE\nIntroduction 本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。\n另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。\n因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。\n通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。\n另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。\n值得注意的是，本文目标是优化邻域生成序列的极大似然估计即条件强度函数（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数\nModel Definition 本文通过跟踪节点邻域的形成来捕获网络的形成过程。\nDefinition 1 : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \\in E$ 被表示为按时间顺序的时间序列，例如， $\\mathbf{a}_{x,y}={a_1\\to{a_2}\\to{…}}\\subset\\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。\n因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。\nDefinition 2 : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2\u0026hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\\to(y_2,t_2)\\to\u0026hellip;\\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。\nHawkes Process 点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。\n对于一个给定的节点$x \\in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：\n$$ \\tilde{\\lambda}_{y|x}(t)=\\mu_{x,y}+\\sum_{t_h\u0026lt;t}{\\alpha_{h,y}\\kappa(t-t_{h})}$$\n其中，$\\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\\sum_{t_h\u0026lt;t}$表示遍历t时刻前$x$的所有邻居。$\\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：\n$$\\kappa(t-t_h)=\\exp(-\\delta_s(t-t_h))$$\n其中，减少率 $\\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\\delta_s(t-t_h)$越大, $\\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。\n综上所述，$\\kappa$的具体意义是随时间衰减的影响，其中$\\delta_s$参数表示对于不同的源节点，影响是不同的。\n如果$\\tilde{\\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。\n直观的来看，基本率（base rate）$\\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\\mu_{x,y}=f(\\mathbf{e}_x,\\mathbf{e}_y)=-||\\mathbf{e}_x-\\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\\alpha_{h,y}=f(\\mathbf{e}_h,\\mathbf{e}_y)=-||\\mathbf{e}_h-\\mathbf{e}_y||^2$。\n因为条件强度函数必须为正，所以使用如下公式: $\\lambda_{y|x}(t)=\\exp(\\tilde\\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$. 这就与传统的NE方法差不多了。。。\nAttention 根据论文中（3）式，可以看出，$\\sum_{t_h\u0026lt;t}{\\alpha_{h,y}\\kappa(t-t_{h})}$这一部分主要描述了历史邻居对当前邻居的影响，但是完全忽略了源节点$x$，因为源节点$x$的变化也会影响到历史邻居对当前邻居的亲近程度(affinity)。因此，本文引入了attention model。as follows：\n$$w_{h,x} = \\frac{\\exp(-||\\mathbf{e}_x-\\mathbf{e}_h||^2)}{\\sum_{h\u0026rsquo;}{\\exp(-||\\mathbf{e}_x-\\mathbf{e}_{h\u0026rsquo;}||^2)}}$$\n这是一个softmax函数 来根据源节点$x$的不同为它的邻居赋予不同权重。\n最后， 历史邻居与当前邻居的连接紧密程度可以表示为: $$\\alpha_{h,y}=w_{h,x}f(\\mathbf{e}_h,\\mathbf{e}_y)$$\nOptimization 目标函数即为给定节点$x$以及基于邻域形成序列的霍克斯过程, 生成节点$y$的条件概率。 公式如下： $$p(y|x, \\mathcal{H}_x(t)) = \\frac{\\lambda_{y|x}(t)}{\\sum_{y\u0026rsquo;}{\\lambda_{y\u0026rsquo;|x}(t)}}$$ 目标函数即为所有节点对的极大似然： $$\\log \\mathcal{L}=\\sum_{x\\in{\\mathcal{V}}}{\\sum_{y\\in{\\mathcal{H}_x}}}{\\log{p(y|x,\\mathcal{H}(t))}}$$\n最后，由于softmax过程是calculating expensive，所以采用负采样优化损失函数。\n","permalink":"https://JhuoW.github.io/posts/htne/","summary":"论文地址：HTNE\nIntroduction 本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。\n另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。\n因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。\n通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。\n另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。\n值得注意的是，本文目标是优化邻域生成序列的极大似然估计即条件强度函数（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数\nModel Definition 本文通过跟踪节点邻域的形成来捕获网络的形成过程。\nDefinition 1 : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \\in E$ 被表示为按时间顺序的时间序列，例如， $\\mathbf{a}_{x,y}={a_1\\to{a_2}\\to{…}}\\subset\\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。\n因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。\nDefinition 2 : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2\u0026hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\\to(y_2,t_2)\\to\u0026hellip;\\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。\nHawkes Process 点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。\n对于一个给定的节点$x \\in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：\n$$ \\tilde{\\lambda}_{y|x}(t)=\\mu_{x,y}+\\sum_{t_h\u0026lt;t}{\\alpha_{h,y}\\kappa(t-t_{h})}$$\n其中，$\\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\\sum_{t_h\u0026lt;t}$表示遍历t时刻前$x$的所有邻居。$\\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：\n$$\\kappa(t-t_h)=\\exp(-\\delta_s(t-t_h))$$\n其中，减少率 $\\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\\delta_s(t-t_h)$越大, $\\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。\n综上所述，$\\kappa$的具体意义是随时间衰减的影响，其中$\\delta_s$参数表示对于不同的源节点，影响是不同的。\n如果$\\tilde{\\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。\n直观的来看，基本率（base rate）$\\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\\mu_{x,y}=f(\\mathbf{e}_x,\\mathbf{e}_y)=-||\\mathbf{e}_x-\\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\\alpha_{h,y}=f(\\mathbf{e}_h,\\mathbf{e}_y)=-||\\mathbf{e}_h-\\mathbf{e}_y||^2$。\n因为条件强度函数必须为正，所以使用如下公式: $\\lambda_{y|x}(t)=\\exp(\\tilde\\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$.","title":"HTNE:《Embedding Temporal Network via Neighborhood Formation》阅读笔记"}]