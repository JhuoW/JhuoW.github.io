<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on JhuoW‘s Notes</title>
    <link>https://JhuoW.github.io/posts/</link>
    <description>Recent content in Posts on JhuoW‘s Notes</description>
    <image>
      <url>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 26 Oct 2025 13:59:35 +0800</lastBuildDate><atom:link href="https://JhuoW.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ICML2025《AutoGFM：Automated Graph Foundation Model with Adaptive Architecture Customization》 阅读笔记</title>
      <link>https://JhuoW.github.io/posts/2025-10-26-autogfm/</link>
      <pubDate>Sun, 26 Oct 2025 13:59:35 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2025-10-26-autogfm/</guid>
      <description>ICML2025 &amp;#34;AutoGFM：Automated Graph Foundation Model with Adaptive Architecture Customization&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>GFM旨在实现跨领域和任务之间的知识共享来提升图机器学习，但是现有的GFM依赖于手工设计并且fixed的backbone GNN架构，不能根据任务或者领域来自适应调整最优的GNN backbone架构。对于此问题，本文通过发现跨领域和任务的不变“图-架构”关系（invariant graph-architecture relationship）来解决该问题。什么是invariant graph-architecture relationship，就是不管是什么领域，图和架构之间存在一种不变的关系，什么样的图就应该对应于什么样的架构。这会带来3个挑战：</p>
<ol>
<li>如何捕获invariant和variant的pattern，其中invariant pattern指的是可以用来可靠预测图的架构的pattern，variant pattern指的是图中不能用来稳定预测图架构的pattern。<strong>换句话说，invariant pattern指的是图中可以决定架构的固有图结构，可以形成与对应GNN架构之间的一一对应的不变关系，而variant pattern指的是一个可变pattern，他可能对应于各种GNN架构，对于这类pattern，GNN架构是不确定的。</strong> Invariant relationship between graph data and corresponding architecture 指的是一个架构适用于的固有图结构，即一个GNN架构对什么样的图结构一定适用。</li>
<li>如何为不同的领域和任务调整GFM中的GNN架构。</li>
<li>如何减轻架构搜索中的数据支配显现（Data domination）。</li>
</ol>
<p>AutoGFM的<strong>核心思想</strong>是训练一个架构映射函数 $\pi: \mathcal{G} \to \mathcal{A}$，用于将图数据映射为特定GNN架构。</p>
<h1 id="architecture-inconsistency-in-gfm">Architecture Inconsistency in GFM</h1>
<p>当前GFM模型存在一个架构不一致（Architecture Inconsistency）问题，也就是说，不同领域不同任务的最优架构不一致。这边拿GFT这个foundation model为例，它在预训练图上预训练一个GNN模型，然后基于预训练图的embedding来构造一堆词汇表，然后下游任务的图基于这些预训练好的词汇表来作预测。其他GFM模型也都会在预训练阶段预训练好一个GNN模块。这个GNN模块的架构，比如可能是GCN也可能是GAT，是固定的。但这样就存在一个问题，就是下游任务的图不一定适用于这个固定的GNN架构。</p>
<p><img loading="lazy" src="/posts/2025-10-26-autogfm/1.png#center" alt="autogfm"  />
</p>
<h1 id="preliminary">Preliminary</h1>
<h2 id="problem-formulation">Problem Formulation</h2>
<p><strong>TAG</strong>：所有图用Text-attributed graph $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{R})$来统一所有图，其中 $\mathcal{R}=r_1, \ldots, r_{|\mathcal{R}|}$是关系数量。</p>
<p><strong>Node of Interest (NOI) Subgraph</strong>：用子图分类任务来统一节点级，边级，图级任务。NOI subgraph $G_h$定义为一个在NOI node周围的子图。用 $S_h(v)=\left\{\mathcal{V}_v^h, \mathcal{E}_v^h, \mathcal{R}_v^h\right\}$表示以节点 $v$为中心的 $h$-hop ego-subgraph。</p>
<p>节点级：NOI node 就是节点 $v$本身，因此 $\mathcal{T} = {v}$，NOI graph 表示为 $G_h(\mathcal{T}) = S_h(v)$。</p>
<p>边级：对于一条边 $(v_i,v_j$)，定义 $\mathcal{T} = \{v_i, v_j\}$，该任务的NOI graph为 $G_h\left(\left\{v_i, v_j\right\}\right)=S_h\left(v_i\right) \cup S_h\left(v_j\right)$。</p>
<p>图级：NOI graph是整个图。</p>
<p>所有的任务均可以统一成对NOI graph的分类任务。</p>
<h2 id="graph-neural-architecture-search-gnas">Graph Neural Architecture Search (GNAS)</h2>
<p>对于一个图数据 $\mathcal{D} = (\mathcal{G}, \mathcal{Y})$，图架构搜索旨在搜索一个从graph 到 label的函数 $F_{\alpha,w}: \mathcal{G} \to \mathcal{Y}$，其中 $\alpha \in \mathcal{A}$是架构参数， $\mathcal{A}$是架构空间。 $w\in \mathcal{W}$是权重。最优架构指的是，在该GNN架构下，模型的最优参数对图预测的loss最小，是一个bi-level optimization problem。如下图所示，上面的式子表示找到最有架构 $\alpha^*$，满足该架构的最优参数可以使模型与 $\mathcal{Y}$的loss最小。</p>
<p><img loading="lazy" src="/posts/2025-10-26-autogfm/2.png#center" alt="autogfm"  />
</p>
<p><strong>Invariant View of Architecture Customization</strong></p>
<p>The goal is to identify <em><strong>invariant relationships between graph data and the correspond architecture</strong></em>, addressing the issue of <em><strong>architecture inconsistency</strong></em> by tailoring graph neural architecture for each data individually. （不变关系：一个图结构一定对应固定的架构，而不会对应于变化的架构，这样的图结构相对于架构空间称为invariant pattern）。</p>
<p>基于此，本文构建了4个核心变量：输入图 $G$，架构 $A$，invariant pattern $Z_I$，variant pattern $Z_V$。将图到架构的映射分为2个成分：encoder $\theta: G \to Z_I$和 predictor $\psi: Z_I \to A$。分别从图中提取invariant pattern，以及用invariant pattern预测最适用于该图的架构。</p>
<p><strong>Assumption</strong>：（1） $Z_I=G \backslash Z_V$， invariant pattern $Z_I$用于可靠预测架构；（2） $Z_V \not \perp A$， $Z_V$虽然不能稳定预测架构，但他并不是与架构相互独立；（3） $A \perp Z_V \mid Z_I$， $\mathrm{A}=\psi\left(\mathrm{Z}_I\right)$，表示给定invariant pattern $Z_I$，架构 $A$的选择不会受到 variant pattern $Z_V$的影响。也就是说如果确定了图中的invariant pattern $Z_I$可以决定该图的架构 $A$，那么其他部分 $Z_V$不应该影响 $Z_I$对架构的选择。</p>
<p>上面三个假设可以写为下式：</p>
<p>$$
\max _{\theta, \psi} I\left(\mathrm{Z}_I, \mathrm{~A}\right)-\lambda I\left(\mathrm{Z}_I, \mathrm{Z}_V\right)-\beta I\left(\mathrm{~A}, \mathrm{Z}_V \mid \mathrm{Z}_I\right)
$$</p>
<p>即 学到的invariant pattern $Z_I$要和架构完全相关，并且与variant pattern分离，同时在 $Z_I$可以确定架构的情况下， $Z_V$不能对架构的选择产生影响。AutoGFM就是要实现该目标。</p>
<h1 id="autogfm">AutoGFM</h1>
<h2 id="disentangled-contrastive-graph-encoder">Disentangled Contrastive Graph Encoder</h2>
<p><img loading="lazy" src="/posts/2025-10-26-autogfm/3.png#center" alt="autogfm"  />
</p>
<p>输入是一堆NOI graphs。对于一个NOI graph $G_i$，它的invariant和variant部分需要被disentangle (解纠缠)。Disentangled NOI-graph Encoder用于学习两个channel 的表示：</p>
<p>$$
\begin{aligned}\mathrm{H}_k^{(l)} &amp; =\mathrm{GNN}_k\left(\mathrm{H}_k^{(l-1)}, \mathbf{A}\right), \quad k=1,2, \\ \mathrm{z}_k &amp; =\mathrm{MLP}_k\left(\mathrm{~h}_k\right) \\ \mathrm{h}_k &amp; =\operatorname{Readout}_k\left(\mathrm{H}_k^{(L)}\right), \quad k=1,2 .\end{aligned}
$$</p>
<p>这里用两个GNN $\mathrm{GNN}_1$和 $\mathrm{GNN}_2$并行的学习一个图的2个表示 $z_1$和 $z_2$，作为图的invariant表示 $Z_I$和variant表示 $Z_V$。然后，本文提出NOI-graph-level contrastive learning方法来解纠缠的表示可以反应图数据的架构需求。这里要求两个表示被disentangle：</p>
<p>$$
p_\theta\left(\mathrm{z}_{i, k} \mid \mathrm{G}_i\right)=\frac{\exp \phi\left(\mathrm{z}_{i, k}, \mathrm{p}_k\right)}{\sum_{j=1}^2 \exp \phi\left(\mathrm{z}_{i, k}, \mathrm{p}_j\right)}
$$</p>
<p>$\mathrm{p}_k$（ $k=1,2$）是第 $k$个表示chunk的prototype，用于表示invariant或variant的<strong>可学习的protontype</strong>，上面的对比函数表示NOI graph $G_i$的表示 $z_{i,1}$要和 $\mathrm{p}_1$相似，$z_{i,2}$要和 $\mathrm{p}_2$相似，而 $z_{i,1}$要和 $\mathrm{p}_2$不相似。以此来区分图中的invariant和variant成分。最大化上面的概率目的是最小化 NOI graph $G_i$ 的 $I\left(\mathrm{Z}_I, \mathrm{Z}_V\right)$。另外，不同NOI graph的invariant representation $Z_I$，需要encourage $Z_I$可以为不同的图数据捕获不同的架构需求，因此对于同一个图的invariant表示 $Z_I$，同一个图的 $Z_I$要相似，不同NOI graph的 $Z_I$要不相似，下面式子用到的 $z$全是 $Z_I$：</p>
<p>$$
p_\theta\left(s\left(\mathrm{z}_{i, k}\right) \mid \mathrm{G}_i, \mathrm{z}_{i, k}\right)=\frac{\exp \phi\left(\mathrm{z}_{i, k}, \mathrm{z}_{i, k}^{\prime}\right)}{\sum_{j=1}^N \exp \phi\left(\mathrm{z}_{i, k}, \mathrm{z}_{j, k}^{\prime}\right)}
$$</p>
<p>模型参数 $\theta$的优化目标如下：</p>
<p>$$
\mathcal{L}_{\text {dis }}=\sum_i-\log \mathbb{E}_{p_\theta\left(\mathrm{z}_{i, k} \mid \mathrm{G}_i\right)} p_\theta\left(s\left(\mathrm{z}_{i, k}\right) \mid \mathrm{G}_i, \mathrm{z}_{i, k}\right)
$$</p>
<p>旨在区分不同NOI graph的 $Z_I$，使得 $Z_I$可以为不同的图捕获不同的架构需求，同时要让同一个图的invariant和variant表示被区分。</p>
<h2 id="invariant-guided-architecture-customization">Invariant-guided Architecture Customization</h2>
<p>接下来就是要构建不变表示和图架构之间的映射关系。 Invariant representation $Z_I$要可以映射到正确的架构上，对于一个GFM来说，之前的方法是定义一个GNN backbone，这个工作把backbone定义为 $|O|$个候选backbone的组合 $\{GCN, GAT, GIN, \ldots, SAGE\}$，每个GNN都有自己的参数，被联合训练，如下图所示。</p>
<p><img loading="lazy" src="/posts/2025-10-26-autogfm/4.png#center" alt="autogfm"  />
</p>
<p>这些GNN组成的超网络表示为：</p>
<p>$$
\mathrm{H}^{(l)} \leftarrow \sum_{i=1}^{|\mathcal{O}|} \alpha_{l, i} \mathrm{GNN}_i^{(l-1)}\left(\mathrm{H}^{(l-1)}, \mathbf{A}\right),
$$</p>
<p>也就是当前NOI graph得到的表示是一堆候选GNN的组合。其中 $\alpha_{l, i}$决定了每个GNN操作对给定数据集的贡献。</p>
<p>架构预测器（Architecture Predictor）：对于一个图的表示 $z$（不变表示），invariant mapping predictor 定义为 $\psi: z \to \{\alpha_{l,i}\}$。用于将invariant 表示映射为选择每个架构的概率：</p>
<p>$$
\begin{gathered}\hat{\alpha}_{l, i}=\mathrm{z} \cdot \frac{\mathrm{p}_{l, i}}{\left|\left|\mathrm{p}_{l, i}\right|\right|_2} \\ \alpha_{l, i}=\frac{\exp \left(\hat{\alpha}_{l, i}\right)}{\sum_{j=1}^{|\mathcal{O}|} \exp \left(\hat{\alpha}_{l, j}\right)},\end{gathered}
$$</p>
<p>其中 $\mathrm{p}_{l, i}$是可学习的prototype，表示第$i$层第$i$个操作的表示，可以看作是architecture representation。若 $\mathrm{H}^{(l)}$可以最好的匹配标签 $\mathcal{Y}$，那么等价于最大化 $I\left(\mathrm{Z}_I, \mathrm{~A}\right)$。</p>
<h2 id="invariant-guided-customization">Invariant-guided Customization</h2>
<p>上面的优化目标已经可以最大化 $I\left(\mathrm{Z}_I, \mathrm{~A}\right)$以及最小化 $I\left(\mathrm{Z}_I, \mathrm{Z}_V\right)$。最后还有一项，需要在给定 $Z_I$的情况下， $Z_V$不会对架构的选择产生影响，即最小化条件互信息 $I\left(\mathrm{~A}, \mathrm{Z}_V \mid \mathrm{Z}_I\right)$。具体来说，首先用不变pattern $Z_I$，基于架构预测器 $\psi_I$来预测架构 $A_I$。并且用 $Z_I$和 $Z_V$的混合通过辅助预测器 $\psi_E$来预测架构为 $A_E$：</p>
<p>$$
\mathrm{A}_I=\psi_I\left(\mathrm{Z}_I\right), \quad \mathrm{A}_E=\psi_E\left(\mathrm{Z}_I, \mathrm{Z}_V\right) .
$$</p>
<p>然后遍历所有图数据，优化目标是使得 所有$A_I$和 $A_E$的差异尽可能小，也就是加上 $Z_V$后，对架构的预测改变要尽可能小：</p>
<p>$$
\begin{aligned}\mathcal{L}_{\mathrm{inv}} &amp; =\sum_i^{||\mathcal{D}||} \sum_j^{||\mathcal{D}||}\left|\left|\mathrm{A}_{I, i}-\mathrm{A}_{E,(i, j)}\right|\right|, \\ \text { s.t. } \quad \mathrm{A}_{I, i} &amp; =\psi_I\left(\mathrm{z}_{I, j}\right), \mathrm{A}_{E, i, j}=\psi_E\left(\mathrm{z}_{I, i}, \mathrm{z}_{V, j}\right),\end{aligned}
$$</p>
<p>架构的预测的任务目标是使得最后一层输出 $\mathrm{H}^{(L)}$ 可以做出最好的预测：</p>
<p>$$
\mathcal{L}_{\text {task }}=\ell\left(F_{\psi\left(\mathrm{Z}_I\right)}(\mathrm{G}), \mathrm{y}\right) .
$$</p>
<p>AutoGFM的最终loss为：</p>
<p><img loading="lazy" src="/posts/2025-10-26-autogfm/5.png#center" alt="autogfm"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2023《Prodigy：Enabling In-context Learning Over Graphs》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/prodigy-icl/</link>
      <pubDate>Mon, 28 Jul 2025 15:19:07 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/prodigy-icl/</guid>
      <description>NeurIPS2023 &amp;#34;Prodigy：Enabling In-context Learning Over Graphs&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>在llm里面，In-Context Learning就是用一些新任务的问答例子来提示LLM，使得LLM不需要更新参数，也能基于这些例子来对新的任务生成输出。</p>
<p><img loading="lazy" src="/posts/2025-07-28-prodigy/1.png#center" alt="icl"  />
</p>
<p>为LM的预训练任务实际上是next token prediction的任务，由于prompt和回答只是相邻的tokens，因此 Transformer 的自注意力可以计算要补全的句子和之前token之间的关系，从而让模型动态推断出新的决策规则。In-Context Learning实际上也是一种next token prediction的任务，所以In-Context Learning是和预训练任务相关的。</p>
<p><img loading="lazy" src="/posts/2025-07-28-prodigy/2.png#center" alt="icl"  />
</p>
<p>Prodigy和All in One (KDD 2023)是同期工作，目的都是为了去提示Pretrained graph model。 All in One 是已经有一个Pretrained graph model，然后用meta learning基于冻结的pretrained model，去学一个prompt graph，使得这个prompt graph和原图结合起来之后可以使预训练模型泛化到新的任务上。</p>
<p>All in One 存在的问题是，All in one要对测试图做finetune。但实际上我们知道foundation model是不需要fine tune的。遇到一个新任务，因为支持in-context learning，可以基于给出的prompt example 来对新任务直接做出预测。而Prodigy就是不需要fine tune的graph prompt方法，它是直接以in-context learning的方式来预训练模型。Prodigy在预训练图上构造一堆prompt example和queries，预训练的优化目标是使在给定prompt examples和query set的情况下，query set的预测损失最小，也就是说<strong>模型的参数由prompt examples来调整，优化目标是使得query set的损失最小</strong>。</p>
<p><img loading="lazy" src="/posts/2025-07-28-prodigy/3.png#center" alt="icl"  />
</p>
<h2 id="prompt-graph-representation">Prompt Graph Representation</h2>
<p>首先要基于预训练的图构造一个支持in-context learning的prompt graph。然后在这个prompt graph上以in-context learning的方式来预训练模型。具体来说，在预训练图上构造一个 $m$-way $k$-shot的分类任务。预训练图为MAG240M，该图中有122M个节点，1.3B条边，153个类。 在每次训练epoch中，采样 $m=30$个class，每个class选择 $k=3$个样本作为prompt examples $\mathcal{S}$，也就是每个epoch有90个prompt examples。每个类别取4个样本作为query set $\mathcal{Q}$。也就是 $\mathcal{S} = \{(x_i, y_i)\}^{mk}_{i = 1}$ 作为预训练阶段的support set。 $\mathcal{Q} = \{x_i\}^n_{i = 1}$作为预训练阶段的query set，也就是预训练阶段的优化目标是要在query set上的损失最小。</p>
<p><strong>Data Graphs</strong>：若预训练任务是node-level的任务，那么每个数据点集 $\mathcal{V}_i$是一个目标节点，如果是edge level的任务，那么每个数据点集 $\mathcal{V}_i$是一对节点。然后，通过采样每个数据点集的 $k$跳局部子图来contextualize每个数据点集，从而为所有prompt examples和query set构造Data Graphs：
$$
\mathcal{G}_i^{\mathrm{D}}=\left(\mathcal{V}_i^{\mathrm{D}}, \mathcal{E}_i^{\mathrm{D}}\right) \sim \bigoplus_{i=0}^k \operatorname{Neighbor}\left(\mathcal{V}_i, \mathcal{G}, i\right)
$$</p>
<p><strong>Message Passing on Data Graphs</strong>：然后每个Data Graph有一个Super Node $v_{x_i}$，来得到上下文化的目标节点表示。具体来说，在Data Graph上做message，然后把目标节点的embedding拿出来作为super node的node feature。</p>
<p><strong>Task Graph</strong>：在每次预训练迭代中，有30个class参与预训练，那么就有30个label nodes $v_y$。然后将所有super nodes和label nodes 连接起来，用边来反映数据点和label之间的关系，具体来说，每条边的特征有2个binary value构成，如果目标数据点是来自prompt set的节点，那么第一个属性为 $0$，如果是来自query set的数据点，那么第一个属性为 $1$。在task graph中，每个超节点和所有label node连接，如果prompt example的超节点连接到正确的label node，那么第二个属性为 $1$，如果连到错误的label，那么边的第二个属性为 $-1$。</p>
<p><img loading="lazy" src="/posts/2025-07-28-prodigy/4.png#center" alt="icl"  />
</p>
<p><strong>Message Passing on Task Graph</strong>：在得到每条边的feature $e_{ij}$后，在Task Graph上做attention-based message passing。也就是每个label node要聚合所有super nodes，每个super nodes也要聚合所有label nodes。
$$
\begin{aligned}\beta_{i j} &amp; =M L P\left(W_q^T H_i^l\left|\left|W_k^T H_j^l\right|\right| e_{i j}\right) \\ \alpha_{i j} &amp; =\frac{\exp \left(\beta_{i j}\right)}{\sum_{k \in \mathcal{N}(i) \cup{i}} \exp \left(\beta_{i k}\right)} \\ H_i^{l+1} &amp; =\operatorname{ReLU}\left(B N\left(H_i^l+W_o^T \sum_{j \in \mathcal{N}(i) \cup{i}} \alpha_{i j} W_v^T H_j^l\right)\right)\end{aligned}
$$</p>
<h2 id="pretraining-task-generation">Pretraining Task Generation</h2>
<p>2个预训练任务：<strong>Neighbor Matching</strong>和<strong>Multi-Class Classification</strong>。</p>
<p><strong>Neighbor Matching</strong>：在每次training epoch，随机采样 $m=30$个节点作为 $30$个way，然后每个采样的节点选择 $k=3$个邻域内节点作为这个类的prompt examples。然后所有label nodes的特征从Uniform distribution上初始化：</p>
<p>$$
\begin{gathered}\mathcal{C}=\left\{c_i\right\}_{i=1}^m \quad c_i \sim \operatorname{Uniform}\left(\mathcal{V}_{\text {pretrain }}\right) \\ N_i=\text { Neighbor }\left(c_i, \mathcal{G}_{\text {pretrain }}, l\right) \\ \mathcal{S}_i=\left\{\left(x_j, y_j=c_i\right)\right\}_{j=1}^k \quad x_j \sim \operatorname{Uniform}\left(N_i\right) \\ \mathcal{Q}_i=\left\{\left(x_j, y_j=c_i\right)\right\}_{j=1}^{\left\lceil\frac{n}{m}\right\rceil} \quad x_j \sim \operatorname{Uniform}\left(N_i\right)\end{gathered}
$$</p>
<p><strong>Multi-Class Classification</strong>：用原始label来构造预训练任务，label node的feature依旧从uniform distribution中采样：</p>
<p>$$
\begin{gathered}\mathcal{C}=\left\{c_i\right\}_{i=1}^m \quad c_i \sim \operatorname{Uniform}(\mathcal{Y}) \\ \mathcal{S}_i=\left\{\left(x_j, y_j=c_i\right)\right\}_{j=1}^k \quad x_j \sim \operatorname{Uniform}\left(\left\{x_i \mid f\left(x_i\right)=c_i\right\}\right) \\ \mathcal{Q}_i=\left\{\left(x_j, y_j=c_i\right)\right\}_{j=1}^{\left\lceil\frac{n}{m}\right\rceil} \quad x_j \sim \operatorname{Uniform}\left(\left\{x_i \mid f\left(x_i\right)=c_i\right\}\right)\end{gathered}
$$</p>
<p>通过计算每个super node和所有label node的cosine similarity 来得到预测的logits：</p>
<p>$$
O_i=\left[\operatorname{cosine} \_ \text {similarity }\left(H_{x_i}, H_y\right), \forall y \in \mathcal{Y}\right]
$$</p>
<p>模型的优化目标是在NM和MT这两个预训练任务的query set上的损失最小：</p>
<p>$$
\mathcal{L}=\underset{x_i \in \mathcal{Q}_{\mathrm{NM}}}{\mathbb{E}} \operatorname{CE}\left(O_{\mathrm{NM}, i}, y_{\mathrm{NM}, i}\right)+\underset{x_i \in \mathcal{Q}_{\mathrm{MT}}}{\mathbb{E}} \operatorname{CE}\left(O_{\mathrm{MT}, i}, y_{\mathrm{MT}, i}\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2024《LLaGA：Large Language and Graph Assistant》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/llaga/</link>
      <pubDate>Mon, 28 Jul 2025 15:09:11 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/llaga/</guid>
      <description>ICML2024 &amp;#34;LLaGA：Large Language and Graph Assistant&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>目的：将图结构调整为适配LLM的输入。具体来说，将图节点表示为<strong>图结构感知</strong>和<strong>属性感知</strong>的序列，然后将序列映射到token embedding space中，从而使LLM可以用处理text tokens的方式来对Graph token embeddings进行处理。为了实现这个目标，就要求node sequence必须充分保存中心节点的结构信息。</p>
<h2 id="structure-aware-graph-translation">Structure-Aware Graph Translation</h2>
<p>LLaGA的目的是将graph翻译成可被LLM理解的token embedding sequence的形式。同时，这可以利用LLM固有的推理能力来处理图的任务，并且无需改变LLM的参数（foundation model）。为了实现这个目标，LLaGA将图结构转化为node embedding sequences，这些sequences融合了图的局部和更大范围的结构信息，然后将node embedding sequences通过一个projector转化为LLM可以处理的token embedding sequence。</p>
<p>第一步是将图转换为node embedding sequences。由于图分析的基本单位是节点，所以本工作开发了2个节点级templates，这些templates是多功能的，不仅可以用于节点级任务，也可以用于边级任务。分别是一个<strong>Neighborhood Detail Template</strong>，提供对中心节点及其周围环境的深入观察；<strong>Hop-Field Overview Template</strong>，提供了一个节点邻居的总结视角，可以拓展到更大的域。<strong>这两个模板都旨在编码节点周围的结构信息</strong>，为分析提供不同的视角。</p>
<h3 id="neighborhood-detail-template"><strong>Neighborhood Detail Template</strong></h3>
<p>Neighborhood Detail Template用于描述节点和其周围邻居的详细信息。给定一个中心节点 $v$，需要构造一个形状固定的tree。对于中心节点 $v$的每跳邻居，定义一组邻居采样的size： $n_1$, $n_2$, …，其中 $n_i$表示第 $i$跳邻居的采样数量。对于 $1$-hop邻居集合 $\mathcal{N}_v^1$，从其中随机采样 $n_1$个邻居，表示为 $\widetilde{\mathcal{N}}_v^1$。如果 $\left|\mathcal{N}_v^1\right|&lt;n_1$，那么用placeholder nodes来补全 $n_1$个邻居。注意，这里定义的每跳邻居的采样数量 $n_1, n_2, \cdots$是应用于所有节点的。得到的 $v$-centered tree如下图所示：</p>
<p><img loading="lazy" src="/posts/2025-07-28-llaga/0.png#center" alt="icl"  />
</p>
<p>这里定义 $n_2 = 3$，如果2跳邻居不满3个节点，那么用 placeholder node  $[\text{pad}]$来占位。然后我们从tree的中心节点开始遍历，可以把这棵计算树转换为一个固定长度的node sequence。这个node sequence描述了以 $A$为中心的局部邻域内的节点相对结构位置（从近到远）。</p>
<p>上面的步骤将中心节点和它的结构信息编码成一个节点序列，然后我们需要把这个节点序列映射到embedding space中。对于TAG（Text-Attributed Graph），使用现成的语言模型 $\phi$，例如SBERT，RoBERTa或SimTeG来编码文本信息，placeholder node的特征被编码为 $0$向量。然后进一步融合结构信息，每个节点在tree中的结构信息用Laplacian Embedding来表示，也就是拉普拉斯矩阵的特征向量在node id的位置来表示这个node在Tree中的结构信息，也就是相邻的节点有相似的embedding：</p>
<p>$$
L=I-\mathcal{D}^{-\frac{1}{2}} \mathcal{A}_{\text {tree }} \mathcal{D}^{-\frac{1}{2}}=U^T \Lambda U
$$</p>
<p>其中 $U$的每行表示对应节点的Laplacian Embedding。注意到，我们预训练的时候，只需要计算一次Laplacian Embedding，以为对于任何图中的任何节点，都为它构造一样的computational tree，所以Laplacian Embedding是不变的。对于计算树induced node sequence $v_1, v_2, \cdots, v_n$，那么这个计算树中的节点 $v_i$的最终node embedding表示为：</p>
<p>$$
h_{v_i}= \begin{cases}\mathbf{0} || U_i, &amp; \text { if } v_i=[p a d] ; \ \phi\left(x_{v_i}\right) || U_i, &amp; \text { otherwise }\end{cases}
$$</p>
<p>它结合了文本信息和节点与领域内其他节点的相对位置信息。</p>
<h3 id="hop-field-overview-template">Hop-Field Overview Template</h3>
<p><img loading="lazy" src="/posts/2025-07-28-llaga/1.png#center" alt="icl"  />
</p>
<p>Hop-Field Overview Template提供的是一个关于中心节点和其邻居的总结视角。**Neighborhood Detail Template中sequence的每个元素是一个节点，Hop-Field Overview Template中每个元素是一跳节点的总结。**首先，用LM来初始话每个节点的特征，节点 $v$的初始特征为 $h_x^0=\phi\left(x_v\right)$。定义parameter-free message passing on encoded text features：</p>
<p>$$
h_v^i=\frac{1}{\left|\mathcal{N}_v^1\right|} \sum_{v^{\prime} \in \mathcal{N}_v^1} h_{v^{\prime}}^{i-1}
$$</p>
<p>$h_v^1=\frac{1}{\left|\mathcal{N}_v^1\right|} \sum_{v^{\prime} \in \mathcal{N}_v^1} h_{v^{\prime}}^{0}$表示节点 $v$的 $1$-hop邻居的summarized embedding，即直接聚合邻居的原始特征。 $h_v^i$表示聚合邻居的 $i-1$阶特征，也就是 $v$的 $i$hop以内邻居的summarized embedding。那么可以依据中心节点和hop顺序关系构造一个序列 $h_v^1, h_v^2,\cdots$，用来表示中心节点和它的相对结构信息。这个方法牺牲了更多细节，但是保留了更广的感受野。</p>
<h3 id="mapping-node-embeddings-into-llms-token-space">Mapping Node Embeddings into LLM’s token space</h3>
<p>用于对齐节点的embedding space和LLM的token space，使得节点序列可以作为LLM的输入。定义一个可训练的projector $f_\theta$：</p>
<p>$$
e_i=f_\theta\left(h_i\right)
$$</p>
<p>用来将序列中的每个节点embedding映射到token space。然后，node embedding sequence $h_1, h_2,\cdots, h_n$可以被转换成token embeddings $e_2, e_2,\cdots, e_n$。LLaGA中唯一的训练参数就是这个projector $f_\theta$。</p>
<h3 id="alignment-tuning">Alignment Tuning</h3>
<p><img loading="lazy" src="/posts/2025-07-28-llaga/2.png#center" alt="icl"  />
</p>
<p>LLaGA采用3个预训练任务：节点分类，链路预测和<strong>节点描述（Node description）。<strong>其中，Node description任务用于将node embedding和特定文本描述对齐。这个特殊任务能够提供图的丰富语义解释，从而更深入地了解基于图的预测背后的逻辑。将这些预训练projector的任务统一成</strong>Questions and Answers</strong>的形式：</p>
<p>Questions: Please describe the center node: <!-- raw HTML omitted -->.</p>
<p>Answers: The center node represents a [paper / products /&hellip;], it’s about [node description].</p>
<p>在训练过程中，以chat的形式在组织问答，Vicuna-v1.5 (Chiang et al., 2023) 作为LLaGA的LLM模型。如上图step 2所示，在处理Freezed LLM的输入阶段，将SYSTEM MESSAGE，USER: [Give you a node] 都tokenize，然后将node sequence 替换为projected node embedding $e_1, e_2, \cdots$，然后将后面描述任务的prompt也tokenize，训练的优化目标是最大化生成正确答案的概率：</p>
<p>$$
\underset{\theta}{\operatorname{maximize}} \quad p\left(X_{\text {answer }} \mid X_{\text {graph }}, X_{\text {question }}, X_{\text {system }}\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2024《GFT：Graph Foundation Model with Transferable Tree Vocabulary》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gft/</link>
      <pubDate>Mon, 28 Jul 2025 14:49:00 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gft/</guid>
      <description>NeurIPS2024 &amp;#34;GFT：Graph Foundation Model with Transferable Tree Vocabulary&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>本文的目的是在预训练图上训练一组通用的图词汇表（vocabulary）作为图上可迁移的通用pattern，这些词汇表可应用于跨领域的多任务中。</p>
<h2 id="gnn的可迁移性">GNN的可迁移性</h2>
<p>可迁移性指的是模型从source task中提取模式，这些模式的知识可以增强对目标任务的预测。近期的工作通过识别与各种任务相关的关键子结构，来研究图上可迁移的词汇表。例如，三元闭包（triadic closure），同质性，异质性对于节点分类至关重要；一些motif，比如三角形，k-cliques，和star，是图分类的基本组成成分，这些子结构可以作为通用可迁移的pattern，基于这些子结构在不同图中的形式，或者研究图具有怎样的子结构组成，可以推断图在各种任务上的表现。</p>
<p>拿自然语言来类比，positive sub-vocabulary 可以是 [happy, nice, great, … ]；negative sub-vocabulary 可以是[upset, bad, worse, … ]，对于一个句子，只需要用它来检索这个词汇表，就可以对这个句子是积极或者消极做出分类。</p>
<p>对于Graph也是一样，构建一些通用的子结构，下游graph去检索这些子结构，基于子结构的性质来对图的性质做出推理。</p>
<h2 id="计算树computation-tree作为可迁移的graph-pattern">计算树（computation tree）作为可迁移的graph pattern</h2>
<p>Computation Tree：对于一个图 $\mathcal{G}(\mathcal{V}, \mathcal{E})$，关于节点 $v$，它的 $L$ 层计算树为 $\mathcal{T}_v^L$，且 $\mathcal{T}_v^1 = v$。这个tree通过递归融合邻居的子树得到。对于图 $\mathcal{G}$，它的 $L$层子树的集合可以表示为一个multiset： $\mathcal{T}_{\mathcal{G}}^L:=\left\{\mathcal{T}_v^L\right\}_{v \in \mathcal{V}}$ 。</p>
<h3 id="为什么计算树比其他子结构如motif更适合做通用的vocabulary">为什么计算树比其他子结构（如motif）更适合做通用的vocabulary？</h3>
<p>首先，节点级、边级、图级任务都可以表示为计算树的分类任务。如下图所示：</p>
<p><img loading="lazy" src="/posts/2025-07-28-GFT/0.png#center" alt="icl"  />
</p>
<p>计算树可以捕获图中重要的局部子树模式。如果两个节点的 $L$层计算树相似（同时考虑节点和特征），那么表明这两个节点共享相似的邻域信息，则这两个节点具有类似现象（analogous phenomena）。</p>
<p><strong>两个具有相似计算树的图，是否以为这模型在这两个图上的迁移性更好？（Yes）</strong></p>
<p>两个图具有相似的其他子结构（如motif），并<strong>不一定</strong>意味着两个图之间模型的迁移性更好</p>
<p>Theorem 2.2：</p>
<p>$$
\Delta \triangleq\left|\left|\phi\left(\mathcal{T}_{v_1}\right)-\phi\left(\mathcal{T}_{v_2}\right)\right|\right|_2 \leq \mathcal{C}_1 \left| \left| \mathbf{x}_{v_1}-\mathbf{x}_{v_2}\right|\right|_2+\mathcal{C}_2 \sum_{j \in \mathcal{N}(v)} \Delta_{v_1, v_2, j}^{L-1} \leq 2 \mathcal{B}_{\mathbf{x}}\left(\mathcal{C}_1+\sum_{l=1}^L \mathcal{C}_2^l D_l\right)
$$</p>
<p>上面定理表示，一个GNN模型 $\phi$对两个计算树计算embedding的相似度，一定会被两个计算树的子树相似度bound，也就是说，<strong>如果两个计算树相似，那么GNN一定可以为他们计算相似的embedding，从而做出相似度预测，所以GNN在这两个子树上具有较好的可迁移性</strong>。如下图所示，motif的相似度并不意味着更高的准确率，99.01的motif相似度，实际上的准确率很低，但计算树的相似度可以反映相似度，所以可以作为图上可迁移的vocabulary，下游图可以检索vocabulary计算树来推测性质。</p>
<p><img loading="lazy" src="/posts/2025-07-28-GFT/1.png#center" alt="icl"  />
</p>
<h2 id="pre-training-with-computation-tree-reconstruction">Pre-training with Computation Tree Reconstruction</h2>
<p>GFT采用Vector Quantization (VQ, 参考VQ-VAE)来开发计算树词汇表。对于一个图数据库 $\mathcal{D} = \{\mathcal{G}_i\}^n_{i = 1}$，从中提取一个集合的计算树 $\mathcal{T}=\left\{\mathcal{T}_i\right\}_{i=1}^m$。通过GNN $\phi$可以得到 $m$个computation tree的embedding： $\mathcal{Z}=\left\{\mathbf{z}_i\right\}_{i=1}^m$。然后定义一组codebooks为可学习的tokens $\mathbf{C}=\left\{\mathbf{c}_1, \ldots, \mathbf{c}_K\right\}$，那么tree embedding space被量化为将每个tree embedding分配给最近的codebook token（类似于k-means，但没有明确的k）。那么对于tree embedding $\mathbf{z}_i$，它所属的codebook token 为 $\mathbf{q}_i=\mathbf{c}_j$，其中 $j=\arg \min _j\left|\left|\mathbf{z}_i-\mathbf{c}_j\right|\right|_2 .$ 那么对于 $m$个数，可以将他们分配給 $K$个codebook tokens 作为长度为 $K$的词汇表。通过vocabulary loss和commitment loss来优化tree embedding和codebook tokens，可以得到预训练目标函数：</p>
<p>$$
\mathcal{L}_{\text {pretrain }}=\mathcal{L}_{\text {tree }}+\underbrace{\frac{1}{m} \sum_{i=1}^m\left|\left|\operatorname{sg}\left[\mathbf{z}_i\right]-\mathbf{c}_i\right|\right|_2^2}_{\text {vocabulary loss }}+\beta_1 \cdot \underbrace{\frac{1}{m} \sum_{i=1}^m\left|\left|\mathbf{z}_i-\operatorname{sg}\left[\mathbf{c}_i\right]\right|\right|_2^2}_{\text {commitment loss }}
$$</p>
<p>先看后面两项，目的是可微地将 $m$个tree embedding压缩为 $K$个向量， $\mathrm{sg}[\mathbf{z}_i]$是stop-gradient的意思，即前向传播得到 $\mathbf{z}_i$后，block $\mathbf{z}_i$回传的梯度，也就是把 $\mathrm{sg}[\mathbf{z}_i]$视为constant来优化 $\mathbf{c}_j$。commitment loss同理。</p>
<p><strong>Computation Tree Reconstruction.</strong> 除了vocabulary loss和commitment loss联合训练codebooks和tree embedding外，本文还引入了一个树重建任务使得学习到的词汇表可以深入理解计算树的结构和语义属性。</p>
<ol>
<li>对于每个计算树 $\mathcal{T}_i$所属的token $\mathbf{q}_i \in \mathbf{C}$，它应当具备重构计算树root node 特征 $\mathbf{x}_i$的能力（codebook token要充分学习它包含的计算树的结构和属性信息）：</li>
</ol>
<p>$$
\mathcal{L}_{\text {feat }}=\frac{1}{m} \sum_{i=1}^m\left|\left|\hat{\mathbf{q}_i^2}-\mathbf{x}_i\right|\right|_2^2
$$</p>
<ol>
<li>计算树token $\mathbf{q}_i$ 要能够重构树 $\mathcal{T}_i$的总体语义信息 $\mathbf{z}_i$：</li>
</ol>
<p>$$
\mathcal{L}_{s e m}=\frac{1}{m} \sum_{i=1}^m\left(1-\frac{\hat{\mathbf{q}}_i^{1^T} \hat{\mathbf{z}_i}}{\left|\left|\hat{\mathbf{q}_i^1}\right|\right|\left|\left|\hat{\mathbf{z}}_i\right|\right|}\right)^\gamma
$$</p>
<ol>
<li>对于有边连接的节点，他们的两个计算树 $\mathcal{T}_i$ 和 $\mathcal{T}_j$所属的codebook token $\mathbf{q}_i$和 $\mathbf{q}_j$要相似：</li>
</ol>
<p>$$
\mathcal{L}_{\text {topo }}=\sum_{(i, j) \in \mathcal{E},\left(i, j^{\prime}\right) \in \hat{\mathcal{E}}}-\frac{1}{|\mathcal{E}|} \log \left(\sigma\left(\hat{\mathbf{q}}_i^{\mathbf{3}^T} \hat{\mathbf{q}}_j^3\right)\right)-\frac{1}{|\hat{\mathcal{E}}|} \log \left(1-\sigma\left(\hat{\mathbf{q}}_i^{\mathbf{3}^T} \hat{\mathbf{q}}_{j^{\prime}}^3\right)\right)+\frac{1}{|\mathcal{E}|}\left|\left|\left[\mathbf{q}_i^4 || \mathbf{q}_j^4\right]-\mathbf{e}_{i j}\right|\right|_2^2
$$</p>
<p>所以预训练loss $\mathcal{L}_{\text {pretrain }}$中的 $\mathcal{L}_{\text {tree }}$定义为如下形式：</p>
<p>$$
\mathcal{L}_{\text {tree }}=\beta_2 \cdot \mathcal{L}_{\text {feat }}+\beta_3 \cdot \mathcal{L}_{\text {sem }}+\beta_4 \cdot \mathcal{L}_{\text {topo }}
$$</p>
<p>为了增强tree vocabulary的质量，不同的词汇应具备一定的区分性，所以regularize the tree vocabulary space by intentionally <strong>increasing the distance between distinct tokens</strong>：</p>
<p>$$
\mathcal{L}_{\text {ortho }}=\lambda \frac{1}{K^2}\left|\left|\mathbf{C C}^T-\mathbf{I}_K\right|\right|_F^2, \quad \mathbf{C}=\left[\mathbf{c}_1, \ldots, \mathbf{c}_K\right]^T \in \mathbb{R}^{K \times d^{\prime}}
$$</p>
<h2 id="fine-tuning-with-computation-tree-classification">Fine-tuning with Computation Tree Classification</h2>
<p>预训练阶段将通用知识编码到Tree vocabulary中，Fine-tuning阶段将这些知识应用于特定的预测任务。首先将节点级、边级、图级任务全部统一成tree-level task。</p>
<ul>
<li>节点分类：任务特定的树表示为 $\mathcal{T}_{node}  =\mathcal{T}_i$，embedding为 $\mathbf{z}=\phi\left(\mathcal{T}_i\right)$。</li>
<li>链接预测：任务特定的树表示为 $\mathcal{T}_{\text {link }}=\operatorname{Combine}\left(\mathcal{T}_s, \mathcal{T}_t\right)$， embedding为 $\mathbf{z}=\operatorname{mean}\left(\phi\left(\mathcal{T}_s\right), \phi\left(\mathcal{T}_t\right)\right)$。</li>
<li>图级分类：任务特定的树表示为 $\mathbf{z}=\operatorname{mean}\left(\phi\left(\mathcal{T}_s\right), \phi\left(\mathcal{T}_t\right)\right)$。</li>
</ul>
<p>然后，任务特定的计算树通过查询tree vocabulary来做出预测，从而将词汇表中的通用知识用于各种任务和领域。</p>
<p><strong>Prototype Classifier</strong>. 给定一组任务特定的计算树 $\left\{\left(\mathcal{T}_i, y_i\right)\right\}_{i=1}^n$包含 $|C|$个类别，需要预测这个计算树的类别。对于一个预训练好的GNN $\phi$，可以为这些计算树生成embeddings $\mathcal{Z}=\left\{\mathbf{z}_i\right\}_{i=1}^n$。这些embedding用于查询tree vocabulary 然后可以得到每个计算树所属的codebook token $\mathcal{Q}=\left\{\mathbf{q}_i\right\}_{i=1}^n$。然后，对于每个类别有一定数量的计算树用于训练，对于训练计算树，构造一个memory bank $\mathbb{S}=\left\{\mathbb{S}^1, \ldots, \mathbb{S}^{|C|}\right\}$用来保存构构成每个类别的codebook token，其中 $\mathbb{S}^k = \{\mathbf{q}_i \in \mathcal{Q} | y_i = k\}$，表示那些用于构成类别 $k$的codebook tokens。那么，class prototype就是用于表示这些类的tokens的结合：
$$
\mathbf{p}_k=\frac{1}{ \left|\mathbb{S}^k\right|} \sum_{\mathbf{q}_i \in \mathbb{S}^k} \mathbf{q}_i
$$</p>
<p>这些类prototype可以用于对测试计算树的预测。若测试计算树的GNN embedding为 $\mathbf{z}$，那么它的第 $k$个类别的预测结果如下：</p>
<p>$$
p(y=k \mid \mathbf{z})=\frac{\exp \left(-\operatorname{sim}\left(\mathbf{z}, \mathbf{p}_k\right) / \tau\right)}{\sum_c \exp \left(-\operatorname{sim}\left(\mathbf{z}, \mathbf{p}_c\right) / \tau\right)}
$$</p>
<p>通过上面的概率在下游图的训练集上fine-tuning模型。为什么要fine tuning，因为对于不同的图，同一个计算树可能的作用也是不一样的，所以要通过这种方式来让计算树的embedding变得task specific，使得每个类别有最适合它的计算树。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2025《GOFA：A Generative One-For-All Model for Joint Graph Language Modeling》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gofa/</link>
      <pubDate>Mon, 28 Jul 2025 14:35:16 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gofa/</guid>
      <description>ICLR2025 &amp;#34;GOFA：A Generative One-For-All Model for Joint Graph Language Modeling&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>GOFA是OFA（ICLR 2024）的后续工作，用来解决OFA中存在的一些问题。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/6.png#center" alt="icl"  />
</p>
<p>OFA用NOI Prompt Node来统一不同的分类任务，也就是用不同的NOI Prompt Node来表示不同的分类任务，然后用任务的文本描述作为这些表示任务的节点的文本特征，LLM 编码来数值化这个节点的初始特征，任务对应的label也用节点（Class Node）来表示，然后用每个class的文本描述作为这个Class Node的特征。GNN的训练目标就是训练NOI Prompt node的embedding 和class node的embedding，对于某个节点，如果我们要预测它关于某个任务的标签，只需要把它和这个任务对应的NOI Prompt Node连接起来，根据输出就可以知道这个节点在该任务下的标签。</p>
<p>但是，OFA存在以下问题：必须是已知任务，无法泛化到新的任务，因为新的任务NOI Prompt Node 未知，也不知道新任务的Class Node是什么， 因此下游任务必须得是预训练阶段见过的任务，不能是一个全新的任务。此外，可以看出由于OFA是一个supervised foundation model，所以他和参与训练的任务标签是强相关的，很难泛化到训练阶段没有见过的任务，GOFA就是用来解决OFA存在的问题，GOFA认为foundation model的训练过程应该不能有标签信息加入的，也就是需要是自监督的。</p>
<h2 id="unified-task-formats">Unified Task Formats</h2>
<p>模型的输入必须要统一，所以和过去的很多方法一样，GOFA用TAG来统一所有graph：</p>
<p>$$
G=\left\{V, E, X_V, X_E\right\}
$$</p>
<p>自监督语言模型里面，模型训练的目标叫next-token prediction 通常是补全句子，比如给定一个句子，自监督语言模型的训练目标是基于这个句子来预测生下来的tokens，假设剩下来的token是apple，参数优化的目标就是使生成apple的likelihood要最大， 如下图所示：</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/1.png#center" alt="icl"  />
</p>
<p>GOFA提出的自监督图模型，也继承这种做法，模型学习的目标是补全TAG图，有个目标节点，叫node of generation，也就是自监督学习要补全的节点，把它的文本属性截断，然后自监督学习的目标就是要生成这个NOG node 的剩下内容。但是，如果要生成这个节点剩下的内容的话，不能无视这种图中的边信息，因为图的结构可以帮助这个自监督任务生成正确的信息，所以自监督图模型需要充分理解图结构。比如上图中的Target就需要在理解边关系的情况下补全句子，因此，模型需要充分学习图结构。</p>
<h2 id="gofa-generative-one-for-all-model">GOFA: Generative One-For-All Model</h2>
<p>怎么才能在充分学习图结构的情况下，补全目标节点的text，是GOFA的主要目标。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/2.png#center" alt="icl"  />
</p>
<p>GOFA主要由2个模块组成，一个是Graph Language Encoder，还有一个是LLM Decoder。其中Graph Language Encoder的目的是用来学习节点的表示向量， 其中交替训练了LLM compressor和GNNlayer，LLM compressor用来学习TAG graph中的语义信息，GNN用来学习结构关系信息（也就是NOG Node和其他节点的交互）。因为GNN已经可以学到图的结构信息了，LLM compressor的作用就是把文本信息压缩到GNN学习的节点表示中。 第二个模块是一个LLM decoder，它的训练目标是，基于节点的向量表示，预测下一个token。</p>
<h3 id="graph-language-encoder">Graph Language Encoder</h3>
<p>Graph Language Encoder 包含LLM compressors和GNN Layers。对于一个NOG Node的text attribute，若它由 $l$个tokens组成 $\{x_i\}_{i = 1}^l$，通过一个pretrained LLM 比如Mistral-7B，可以将每个token映射为一个token embedding $q(x_i)$。那么将NOG Node的所有token $\left\{q\left(x_i\right)\right\}_{i=1}^l$ pooling成一个向量可以作为NOG Node的node feature vector。但是这种做法丢失太多semantic information。</p>
<p><strong>Solution:</strong> 将NOG Node的text attribute压缩为 $K$个向量，且 $K &laquo;l$。具体来说，首先定义 $K$个memory tokens $\{m_1, \cdots. m_K \}$ with their lookup initial embeddings $\left\{q\left(m_j\right)\right\}_{j=1}^K$用于接收压缩后的text tokens。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/3.png#center" alt="icl"  />
</p>
<p>然后把 $l$个text tokens和 $K$个memory tokens 拼起来输入到LLM Compressor Layer1，由于LLM的是<strong>multiple transformer layers</strong> (self-attention mechanisms)，因此可以捕获memory tokens和text tokens之间的dependencies。所以通过这种方式，text tokens的信息可以被压缩到memory tokens中。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/4.png#center" alt="icl"  />
</p>
<p>此时，这个目标节点（NOG Node）的特征向量就被压缩到 $K$个token embeddings，但是这个时候 $K$个memory tokens并没有学到图的结构信息。为了编码图的结构信息，将这 $K$个memory token embeddings作为node features输入GNN中，做一个 $K$通道的聚合GNN。通过上图中的Token-level Message Passing，可以将图的结构信息编码到 NOG Node的 $K$个memory tokens中，然后将GNN处理过的memory token embeddings再和text token embeddings拼起来，用LLM compressors将文本信息压缩到 memory token embeddings中。经过 $L$次的交替LLM压缩和训练GNN后，这里得到的 $K$个memory token可以认为是充分节点的text 信息和图的结构信息。</p>
<h3 id="llm-decoder">LLM Decoder</h3>
<p><strong>The memory tokens $Q_{m,x}$ ( $m$:memory; $x$: 节点 $x$) of every node contain information about the text on the node, the surrounding node text, and the graph structure due to the message-passing process in the GNN layers.</strong> 然后，对于NOG节点 $v$，它的completion target 是 $y$，GOFA将NOG Node的memory tokens $Q_{m,x}$插入到target text tokens的前面，然后输入LLM Decoder中来生成下一个token。优化目标是最大化completion target 的log likelihood，也就是生成正确tokens的概率。</p>
<p>$$
\operatorname{CrossEntropy}\left(\left\{l\left(m_K\right), l\left(x_1\right), \ldots, l\left(x_{l-1}\right)\right\},\left\{x_1, \ldots, x_l\right\}\right) .
$$</p>
<p>在GOFA的influence阶段，输入一个新图和每个节点的text attribute，并且给出目标节点NOG Node，就可以补全该NOG Node。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/5.png#center" alt="icl"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Fair Graph Learning</title>
      <link>https://JhuoW.github.io/posts/fairnessgnn/</link>
      <pubDate>Wed, 21 May 2025 12:13:13 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fairnessgnn/</guid>
      <description>Fair Graph Learning, Fair GNNs相关论文阅读笔记</description>
      <content:encoded><![CDATA[<h1 id="overview">Overview</h1>
<p>The following two works reduce prediction discrimination by optimizing adjacency matrices, which can improve fairness for link prediction tasks:</p>
<ul>
<li>On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections （ICLR 2021）</li>
<li>All of the Fairness for Edge Prediction with Optimal Transport (AISTATS 2021)</li>
</ul>
<p>通过修改原图的敏感属性，使用对比学习来实现模型对敏感属性的鲁棒性，即敏感属性的修改不会影响模型的输出:</p>
<ul>
<li>Towards a Unified Framework for Fair and Stable Graph Representation Learning (UAI 2021)</li>
</ul>
<p>使用对抗训练策略来增强图，使得增强图与敏感属性的关系（MI）最小，基于增强图训练的representation可以实现fairness:</p>
<ul>
<li>Learning Fair Graph Representations via Automated Data Augmentations (ICLR 2023)</li>
</ul>
<p>证明了message passing的neighbor aggregation会使得拓扑偏差积累到node representation中，在GNN的signal denoising优化框架中加入fairness regularization，使得学习到的节点表示向量要满足，不同敏感group具有相同的期望logits:</p>
<ul>
<li>FMP: Toward Fair Graph Message Passing against Topology Bias (Arxiv 2022)</li>
</ul>
<p>跨group的属性联合分布在model处理后的分布差异最小化:</p>
<ul>
<li>EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks (WWW 22)</li>
</ul>
<h1 id="1-on-dyadic-fairness-exploring-and-mitigating-bias-in-graph-connections-iclr-2021">1. On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections (ICLR 2021)</h1>
<h2 id="introduction">Introduction</h2>
<p>为了减轻算法的歧视性，涉及公平的算法至关重要。具体来说本文关注二元公平性（Dyadic fairness），即<strong>2个实例间的预测关系独立于敏感属性（敏感属性不会对节点间的预测关系分数产生影响）</strong>。本文揭示了调节现有边的权重有益于实现二元公平，并且进一步提出<strong>FairAdj</strong>来学习一个<strong>具有适当图结构约束的公平邻接矩阵</strong>，从而实现公平的链路预测。</p>
<p>在推荐系统中，敏感属性可能会主导带有偏见的推荐，比如用户的宗教信可能会对用户的相互推荐偏向产生影响，导致用户更可能被推荐具有相同宗教信仰的其他用户，从而导致社会关系产生隔离。因此，对于两个实例而言，算法应该在不受他们敏感属性影响的情况下执行链路预测。二元公平性的概念来自于统计中的group fairness。首先，图中的节点根据敏感属性的取值分为几个组，二元公平要求<strong>一些统计数据在组内和组间大致相等</strong>。</p>
<h2 id="dyadic-fairness">Dyadic Fairness</h2>
<p>对于某一个敏感属性，每个节点有一个该属性的值，用$S(v)$表示节点 $v$的敏感属性， $\Gamma(v)$表示节点 $v$的一阶邻居集合。如果节点 $u$和 $v$的敏感属性值相同，那么Edge $(u,v)$为<strong>intra (Group内边)</strong>；如果 $u$和 $v$的敏感属性值不同，即 $S(u) \neq S(v)$则Edge $(u,v)$为<strong>inter(Group间边)</strong>。对于一个binary sensitive attribute，即sensitive attribute有两个可选值（如男/女），可以将图中的节点分为不同敏感属性值的两个组 $S_0$和 $S_1$。 $\widetilde{S_0}:=\left\{v \in S_0 \mid \Gamma(v) \cap S_1 \neq \varnothing\right\}$表示 $S_0$中与 $S_1$有边连接的节点，同理 $\widetilde{S_1}$表示 $S_1$中与 $S_0$ 有边连接的节点。也就是说， $\widetilde{S_0}$和 $\widetilde{S_1}$表示<strong>具有跨group边的节点集合</strong> 。链路预测模型 $g(\cdot, \cdot): \mathbb{R}^D \times \mathbb{R}^D \rightarrow \mathbb{R}$表示将两个节点的embeddings映射为一个分数来判断是否有边连接。</p>
<p><em>Definition 2.1 如果link prediction的预测分数满足：</em></p>
<p>$\operatorname{Pr}(g(u, v) \mid S(u)=S(v))=\operatorname{Pr}(g(u, v) \mid S(u) \neq S(v))$</p>
<p><em>那么该link prediction算法满足Dyadic fairness。</em></p>
<p>也就是两个节点是否处在同一个group并不会模型对他们的预测概率。换句话说，任意调整两个节点的敏感属性值，模型对他们之间是否存在边的预测结果不变。</p>
<p><strong>Demographic Parity</strong>:  <strong>A fairness metric that is satisfied if the results of a model&rsquo;s classification are not dependent on a given sensitive attribute</strong>.</p>
<h2 id="图连通如何影响公平性">图连通如何影响公平性</h2>
<p><strong>Proposition 4.1</strong>：若链路预测函数 $g(\cdot, \cdot)$定义为一个内积函数 $g(v, u)=v^{\top} \Sigma u$，其中 $\Sigma$是一个正定矩阵，那么 $\exists Q&gt;0$， $\forall v \sim \mathcal{V}$， $||v||_2 \leq Q$，对于 $\mathbb{E}_{v \sim U}[v] \in \mathbb{R}^M$（ $U$是节点集 $\mathcal{V}$上的离散均匀分布），如果 $\left|\left|\mathbb{E}_{v \sim U}\left[v \mid v \in S_0\right]-\mathbb{E}_{v \sim U}\left[v \mid v \in S_1\right]\right|\right|_2 \leq \delta$ （即由敏感属性划分的两个集合的期望特征差异小于 $\delta$），则有：
$$
\Delta_{\mathrm{DP}}:=\left|\underbrace{\mathbb{E}_{(v, u) \sim U \times U}[g(v, u) \mid S(v)=S(u)]}_{u和v属于同一个group时，模型预测的期望分数}-\underbrace{\mathbb{E}_{(v, u) \sim U \times U}[g(v, u) \mid S(v) \neq S(u)]}_{u和v属于不同group时，模型预测的期望分数}\right| \leq Q||\Sigma||_2 \cdot \delta
$$</p>
<p><em><strong>Proof:</strong> 令 $p:=\mathbb{E}_{v \sim U}\left[v \mid v \in S_0\right] \in \mathbb{R}^M$表示group $S_0$中节点特征的期望， $q:=\mathbb{E}_{v \sim U}\left[v \mid v \in S_1\right] \in \mathbb{R}^M$表示group $S_1$中节点特征的期望。那么对于两个节点 $u$和 $v$，<strong>他们是跨group节点时表示向量的期望相似度</strong> 与 <strong>他们是同一个group时表示向量之间的期望相似度</strong> 之间的差异应该越小越好，表示模型的预测是独立于划分group的敏感属性。这个差异可以表示为以下形式：</em></p>
<p><em>若 $u$和 $v$的敏感属性不同，即他们属于不同的group，那么他们通过模型输出的embeddings相似度的期望表示为如下形式：</em></p>
<p>$$
\begin{aligned}
\mathbb{E}_{\text {inter }} &amp;= \mathbb{E}\left[v^{\top} \Sigma u \mid v \in S_0, u \in S_1\right] \\
&amp;=\frac{1}{|S_0||S_1|} \sum_{v \in S_0} \sum_{u \in S_1} v^\top \Sigma u \\
&amp;=\frac{1}{|S_0|} \sum_{v \in S_0} v^\top \Sigma \frac{1}{|S_1|}\sum_{u \in S_1} u \\
&amp; = p^\top \Sigma q
\end{aligned}
$$</p>
<p><em>若 $u$和 $v$的敏感属性相同，即他们属于同一个敏感group，那么他们通过模型输出的embeddings相似度期望可以表示为如下形式：</em></p>
<p>$$
\begin{aligned}
\mathbb{E}_{{\text {inter }}} &amp;= \mathbb{E}\left[v^{\top} \Sigma u \mid (v \in S_0, u \in S_0) \vee (v \in S_1, u \in S_1)\right]  \\
&amp;= \frac{1}{|S_0|^2 + |S_1|^2}(\sum_{v \in S_0}v^\top \Sigma \sum_{u \in S_0}u + \sum_{v \in S_1}v^\top \Sigma \sum_{u \in S_1}u) \\
&amp;=  \frac{1}{|S_0|^2 + |S_1|^2} (|S_0|p^\top\Sigma|S_0|p + |S_1|q^\top\Sigma|S_1|q)
\\
&amp;=\frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} p^{\top} \Sigma p+\frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} q^{\top} \Sigma q
\end{aligned}
$$</p>
<p><em>那么跨group节点表示之间的相似度的期望与group内节点表示之间相似度的期望 之间的差异可以表示为如下形式</em></p>
<p>$$
\begin{aligned}
\left|\mathbb{E}_{\text {inter }}-\mathbb{E}_{\text {intra }}\right| &amp; =\left|\mathbb{E}\left[v^{\top} \Sigma u \mid v \in S_0, u \in S_1\right]-\mathbb{E}\left[v^{\top} \Sigma u \mid v \in S_0, u \in S_0 \vee v \in S_1, u \in S_1\right]\right| \\
&amp; =\left|p^{\top} \Sigma q-\left(\frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} p^{\top} \Sigma p+\frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} q^{\top} \Sigma q\right)\right| \\
&amp; =\left|(q-p)^{\top}\left(\frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} \Sigma p-\frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} \Sigma q\right)\right|
\end{aligned}
$$</p>
<p><em>令 $\alpha = \frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2}$， $\beta = \frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2}$，那么上式可以改写为：</em>
$$
\left|(q-p)^{\top}\left(\frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} \Sigma p-\frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} \Sigma q\right)\right| = \left|(q-p)^{\top} (\alpha \Sigma p-\beta\Sigma q)\right|
$$</p>
<p><em>|根据柯西-施瓦茨不等式 (Cauchy–Schwarz inequality)，有 $|\langle\mathbf{u}, \mathbf{v}\rangle|^2 \leq\langle\mathbf{u}, \mathbf{u}\rangle \cdot\langle\mathbf{v}, \mathbf{v}\rangle$，那么下式成立：</em>
$$
\left|(q-p)^{\top} (\alpha \Sigma p-\beta\Sigma q)\right| \leq ||q-p||_2 \cdot||\alpha \Sigma p-\beta\Sigma q||_2
$$</p>
<p><em>因为敏感group期望的差异被 $\delta$ upper bounded所以下式成立：</em>
$$
||q-p||_2 \cdot||\alpha \Sigma p-\beta\Sigma q||_2 \leq \delta \cdot ||\alpha \Sigma p-\beta\Sigma q||_2 = \delta \cdot ||\Sigma (\alpha p - \beta q)||_2
$$</p>
<p><em>根据matrix norm，有 $||AB|| \leq ||A||\cdot ||B||$，因此下面不等式成立：</em></p>
<p>$$
\delta \cdot ||\Sigma (\alpha p - \beta q)|| \leq \delta ||\Sigma||_2  \cdot ||(\alpha p - \beta q)||_2 = \delta ||\Sigma||_2  \cdot ||(\alpha p + (- \beta q))||_2
$$</p>
<p><em>由于 $||A+B|| \leq ||A||+ ||B||$，所以下式成立：</em></p>
<p>$$
\delta ||\Sigma||_2  \cdot ||(\alpha p + (- \beta q))||_2  \leq \delta ||\Sigma||_2  \cdot (||\alpha p||_2 + ||\beta q||_2) = Q\delta ||\Sigma||_2
$$</p>
<p><em>因此，Proposition 4.1得证。</em></p>
<p><strong>由Proposition 4.1可以得出，模型在link prediction上实现demographic parity的充分条件是模型可以为敏感属性划分的group学习到期望相似的embeddings。</strong> 下面证明图结构如何影响demographic parity。</p>
<p>令由敏感属性划分的group中节点表示的期望为 $\mathbb{E}_{v \sim U}\left[v \mid v \in S_0\right] = \mu_0$； $\mathbb{E}_{v \sim U}\left[v \mid v \in S_1\right] = \mu_1$。用 $\sigma$表示节点表示和所在group期望之间的最大偏差，即 $\forall v \in S_0$， $\left|\left|v-\mu_0\right|\right|_{\infty} \leq \sigma$ 且 $\forall v \in S_1$， $\left|\left|v-\mu_1\right|\right|_{\infty} \leq \sigma$。图中节点的最大加权度为 $D_{\max }:=\max_{v \in \mathcal{V}} \operatorname{deg}_w(v)$， $m_w = \sum_{S(v)\neq S(u)} a_{vu}$表示所有跨group边的权重和。下面定理展示了单次GNN平滑后，两个group的期望的差异会如何变化：</p>
<p><strong>Theorem 4.1.</strong> 对于一个没有负边权重的图，在执行一次mean aggregation后，2个sensitive group中的表示差异 $\Delta_{\mathrm{DP}}^{\mathrm{Aggr}}:=\left|\left|\mathbb{E}_{v \sim U}\left[\operatorname{Agg}(v) \mid v \in S_0\right]-\mathbb{E}_{v \sim U}\left[\operatorname{Agg}(v) \mid v \in S_1\right]\right|\right|_2$会被下式bounded:<br>
$$
\max \left\{\alpha_{\min }\left|\left|\mu_0-\mu_1\right|\right|_{\infty}-2 \sigma, 0\right\} \leq \Delta_{\mathrm{DP}}^{\mathrm{Aggr}} \leq \alpha_{\max }\left|\left|\mu_0-\mu_1\right|\right|_2+2 \sqrt{M} \sigma
$$</p>
<p>其中 $\alpha_{\min }=\min \left\{\alpha_1, \alpha_2\right\}$， $\alpha_{\max}=\max \left\{\alpha_1, \alpha_2\right\}$， $\alpha_1=\left|1-\frac{m_w}{D_{\max }}\left(\frac{1}{\left|S_0\right|}+\frac{1}{\left|S_1\right|}\right)\right|$， $\alpha_2=\left|1-\frac{\left|\widetilde{S_0}\right|}{\left|S_0\right|}-\frac{\left|\widetilde{S_1}\right|}{\left|S_1\right|}\right|$ ， $M$是输入特征的维度。 $\alpha_2$只和图相关，所以是定值。</p>
<p>Theorem 4.1 中，只有跨group边的总权重 $m_w$和图中最大加权度 $D_{\max}$是可调节的，因此通过调节图中边的权重，可以调节一层GNN后得到的两个group期望表示差异，通过最小化这个差异 $\Delta_{\mathrm{DP}}^{\mathrm{Aggr}}$的上界，可以得到关于sensitive group公平的representations。</p>
<p>只看右边的upper bound， $\alpha_{\max}$中 $|S_0|$和 $|S_1|$是定值，对于 $\alpha_1$，如果跨group边权重很小（ $m_w \rightarrow 0$）会使得 $\alpha_1$趋近于1，如果跨group边的权重很大（一个group中的所有边都连到另一个group： $m_w \rightarrow D_w \cdot \min \left\{\left|S_0\right|,\left|S_1\right|\right\}$）也会使得 $\alpha_1$趋近于1。（这里不理解？？？为什么 $m_w$的上限是这种形式？） 因此，增加跨group边的权重并不能够达到demographic parity，因为增得太大会使 $\alpha_{\max}$增大，导致无法取到最小上界。</p>
<h2 id="learning-fair-graph-connections">Learning Fair Graph Connections</h2>
<p>上面的分析说明了调整图结构有助于模型有条件地获得公平性。然而，在多层GNN中优化图结构并不容易，因此FairAdj在保留原始图结构的基础上更新 $\widetilde{A}$，即更新已有边的权重来实现学习表示的公平。</p>
<p>因为是链路预测任务，GNN编码器用于重构原始图中的边，使得GNN可以预测原图中的连接关系，本文采用VGAE作为GNN编码器：</p>
<p>$$
\max _\theta \quad \mathcal{L}_{\text {util }}:=\mathbb{E}_{\mathrm{GNN}_\theta(Z \mid X, \widetilde{A})}[\log p(A \mid Z)]-K L\left[\operatorname{GNN}_\theta(Z \mid X, \widetilde{A}) || \mathcal{N}(0,1)\right]
$$</p>
<p>即 GNN编码器基于$\widetilde{A}$和 $X$生成高斯分布，再基于reparameterization trick把构造decoder来重构原图结构 $A$，这样的GNN具备链路预测能力。接下来为模型注入关于敏感属性公平性，公平性要求GNN对具有跨group边的节点间（inter 边）预测的期望分数 与group内边（intra边）预测的期望分数的差异要尽可能小，表示模型的预测独立于敏感属性：</p>
<p>$$
\begin{array}{cl}
\min_{\widetilde{A}} &amp; \mathcal{L}_{\text {fair }}:=\left|\left|\mathbb{E}_{v, u \sim U \times U}\left[\hat{a}_{v u} \mid S(v)=S(u)\right]-\mathbb{E}_{v, u \sim U \times U}\left[\hat{a}_{v u} \mid S(v) \neq S(u)\right]\right|\right|^2 \\
\text { s.t. } &amp; (1) .[\widetilde{A}]_{v u}=0, \text { if }[A]_{v u}=0, \quad(2) . \widetilde{A} \mathbb{1}=\mathbb{1}, \widetilde{A} \geq 0,
\end{array}
$$</p>
<h1 id="2-all-of-the-fairness-for-edge-prediction-with-optimal-transport-aistats-2021">2. All of the Fairness for Edge Prediction with Optimal Transport (AISTATS 2021)</h1>
<p>给定两个节点 $(V, V^\prime)$，用 $\mathbb{P}_1(h)=\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S \neq S^{\prime}\right)$表示两个节点敏感属性不同时，模型 $h$为他们预测的边概率；</p>
<p>$\mathbb{P}_0(h)=\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=S^{\prime}\right)$表示两个节点敏感属性相同时，模型 $h$为他们预测的边概率。</p>
<p>Definition (<strong>Disparate Impact (DI)</strong>) ：</p>
<p>$$
D I\left(h, \mathbb{V}, S \oplus S^{\prime}\right)=\frac{\mathbb{P}_1(h)}{\mathbb{P}_0(h)}
$$</p>
<p>其中 $S \oplus S^{\prime}$是XOR操作，不同为1，相同为0。分子 $\mathbb{P}_1(h)$表示 $h$为跨group节点预测有边的概率，分布表示 $h$为group内节点预测有边的概率。如果模型 $h$是完美平衡的，那么DI等于1，也就是无论两个节点的敏感属性如何设置， $h$为他们预测的边概率都相同。</p>
<p>Definition (<strong>Balanced Error Rate (BER)</strong>):</p>
<p>$$
\operatorname{BER}\left(h, \mathbb{V}, S \oplus S^{\prime}\right)=\frac{\mathbb{P}_1(h)-\mathbb{P}_0(h)+1}{2}
$$</p>
<p>BER表示模型预测两个节点敏感属性相同的概率，BER的最佳值为 $\frac{1}{2}$，即模型预测两个节点敏感属性相同的概率和不同的概率一样，都为 $\frac{1}{2}$。</p>
<p>Remark 1: DI 的定义可以改写成关于单个节点fairness的形式：</p>
<p>$$
D I(h, \mathbb{V}, S)=\frac{\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=0\right)}{\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=1\right)}
$$</p>
<p>分子 $\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=0\right)$表示某节点敏感属性是0时， $h$预测它与其他节点产生边的概率;</p>
<p>分母 $\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=1\right)$表示某节点敏感属性是1时， $h$预测它与其他节点产生边的概率。</p>
<p>Theorem 1: 给定一个图 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ 和一个边预测函数 $h: \mathbb{V} \times \mathbb{V} \rightarrow{0,1}$，假设 $D I(h, \mathbb{V}, S) \leq \tau$，那么下式成立：</p>
<p>$$
D I\left(h, \mathbb{V}, S \oplus S^{\prime}\right) \leq D I(h, \mathbb{V}, S) \leq \tau
$$</p>
<p>Corollary 1: 在Theorem 1成立的前提下，下式成立：</p>
<p>$$
\begin{aligned}
B E R\left(h, \mathbb{V}, S \oplus S^{\prime}\right) &amp; \leq \frac{1}{2}-\frac{\mathbb{P}_1(h)}{2}\left(\frac{1}{\tau}-1\right) \\
\min_{h \in \mathcal{H}} B E R\left(h, \mathbb{V}, S \oplus S^{\prime}\right) &amp; =\frac{1}{2}\left(1-\frac{1}{2} W_{1 . \neq \cdot}\left(\gamma_0, \gamma_1\right)\right)
\end{aligned}
$$</p>
<p>其中 $W_{1 . \neq \cdot}$是Wasserstein Distance on the joint distributions $\gamma_0$ and $\gamma_1$. 其中 $\gamma_0$ 和 $\gamma_1$分别是group $S=0$和group $S=1$的节点分布。因此邻接矩阵的优化目标是两个group节点的分布要相似（Wasserstein distance）。</p>
<h2 id="group-graph-fairness-with-ot">Group Graph Fairness with OT</h2>
<p>从Corollary 1可以看出，使得模型 $h$学习到的节点表示在敏感属性划分的group上达到fairness需要两个group上节点的分布 $\gamma_0$ 和 $\gamma_1$之间的Wasserrstein distance尽可能小。</p>
<p>对于邻接矩阵 $\mathcal{A} \in \mathbb{R}^{N \times N}$，分布 $\gamma_0$ 是由 $\mathcal{A}$中敏感属性 $S=0$的行 $\mathcal{A}_0 \in \mathbb{R}^{N_0 \times N}$构成，分布 $\gamma_1$ 是由 $\mathcal{A}$中敏感属性 $S=1$的行 $\mathcal{A}_1 \in \mathbb{R}^{N_1 \times N}$构成。 $\gamma \in \Pi\left(\frac{1}{N_0}, \frac{1}{N_1}\right)$表示两个节点分布之间的transport plan。 $\frac{1}{N_0}, \frac{1}{N_1}$分别表示有 $N_0$和 $N_1$个元素的均匀分布，表示2个group中节点的分布。 两个group中节点到节点的运输距离用矩阵 $M \in \mathbb{R}^{N_0 \times N_1}$表示，其中 $M_{i j}=l\left(a_0^{(i)}, a_1^{(j)}\right)$表示两个跨group节点间的距离， $a_0^{(i)}$分别表示 $\mathcal{A}_0$的第 $i$行， $a_1^{(j)}$表示 $\mathcal{A}_1$的第 $j$列。 $l$是距离度量。因此，两个group中节点分布之间的距离（Wasserstein distance）是以下OT问题的解：</p>
<p>$$
\mathrm{inf}_{\gamma \in \Pi\left(\frac{1}{N_0}, \frac{1}{N_1}\right)} \langle\gamma, M\rangle
$$</p>
<p>要使上式达到最小，需要优化 $M$，也就是图的邻接矩阵，使得两个group分布的Wasserstein distance达到最小:</p>
<p>$$
\tilde{\mathcal{A}}_{\text {bary }}=\underset{\mathcal{A} \in \mathbb{R}^{N \times N}}{\operatorname{argmin}} \frac{1}{|S|} \sum_{i=1}^{|S|} \min _{\gamma_i \in \Pi\left(\frac{1}{N}, \frac{1}{N_i}\right)} \langle\gamma, M\rangle
$$</p>
<h1 id="3-towards-a-unified-framework-for-fair-and-stable-graph-representation-learning-uai-2021">3. Towards a Unified Framework for Fair and Stable Graph Representation Learning (UAI 2021)</h1>
<h2 id="notations">Notations</h2>
<p>令图 $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathbf{X})$表示无向图，其中 $\mathbf{x}_v \in \mathbf{X}$是节点 $v$的一个 $M$维属性向量。令 $\mathbf{I}_u \in{0,1}^N$表示节点 $u$的局部邻居，即如果 $v$与 $u$有边连接，那么 $\mathbf{I}_{uv}=1$，否则 $\mathbf{I}_{uv}=0$。令 $\mathbf{b}_u=\left[\mathbf{x}_u ; \mathbf{I}_u\right]$表示节点 $u$所有信息（局部邻居和属性特征）。</p>
<h2 id="fairness-and-stability">Fairness and Stability</h2>
<p><em><strong>Counterfactual Fairness （模型关于敏感属性的改变是robust的）:</strong></em> 对于节点 $u$和它的增强 $\tilde{u}^s$（将 $u$的敏感属性翻转后得到），编码器 $\text { ENC }$若满足下面条件，那么$\text { ENC }$满足counterfactual fairness:</p>
<p>$$
\operatorname{ENC}(u)=\operatorname{ENC}\left(\tilde{u}^s\right)
$$</p>
<p><em><strong>Stability via Lipschitz Continuity:</strong></em> 对于节点 $u$和它的增强 $\tilde{u}$（对$u$扰动节点特征/边），如果编码器 $\text { ENC }$满足以下条件：</p>
<p>$$
||\operatorname{ENC}(\tilde{u})-\operatorname{ENC}(u)||_p \leq L\left|\left|\tilde{\mathbf{b}}_u-\mathbf{b}_u\right|\right|_p
$$</p>
<p>则编码器 $\text { ENC }$关于Lipschitz continuity是稳定的。 $L$ is the Lipschitz constant，对于固定的编码器函数，比如ReLU是一个1-Lipschitz连续函数 $L=1$。对于一个有固定激活函数的编码器， $L$的值是固定的。</p>
<h2 id="图增强">图增强</h2>
<p>a) <strong>节点属性扰动</strong>：给定一个 $M$维的attribute masking向量 $\mathbf{r} \in\{0,1\}^M$用于选择需要被扰动的属性， $\mathbf{r}$中的每个值从Bernoulli distribution  $p_n$中采样，表示每个属性（除了敏感属性）是否要被扰动，而扰动量从normal distribution中采样 $\delta \in \mathbb{R}^M$。那么节点 $u$特征的扰动量为 $\mathbf{r} \circ \delta$，扰动后的节点特征为： $\tilde{\mathbf{x}}_u=\mathbf{x}_u+\mathbf{r} \circ \delta$。</p>
<p>b) <strong>敏感属性的反事实扰动</strong>：将节点的敏感属性值翻转，这里考虑二值敏感属性分布，即 $s \in\{0,1\}$。</p>
<p>c) <strong>图结构扰动</strong>：Bernoulli distribution $\mathcal{B}(p_e)$ 中每条边以 $p_e$概率被drop，由此生成扰动后的图结构 $\tilde{\mathbf{A}}$。</p>
<h2 id="contrastive-objective">Contrastive Objective</h2>
<p>为了使模型可以对属性/结构的扰动，以及敏感属性的修改鲁棒，则需要在原图和增强后的图上，GNN可以学习到相似的embeddings，这样等价于学习扰动/敏感属性鲁棒的节点表示：</p>
<p>$$
\mathcal{L}_s=\mathbb{E}_u\left[\frac{1}{2}\left(D\left(t\left(\mathbf{z}_u\right), \operatorname{sg}\left(\tilde{\mathbf{z}}_u\right)\right)+D\left(t\left(\tilde{\mathbf{z}}_u\right), \operatorname{sg}\left(\mathbf{z}_u\right)\right)\right)\right]
$$</p>
<h2 id="stability">Stability</h2>
<p>若模型要满足稳定性，那么编码器应满足<em><strong>Lipschitz Continuity</strong></em>。将第 $k$层GNN编码器定义为 $\mathbf{h}^k_u = \sigma\left(\mathbf{W}_a^k \mathbf{h}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{k-1}\right)$，那么下式成立：</p>
<p>$$
||\operatorname{ENC}(\tilde{u})-\operatorname{ENC}(u)||_p \leq \prod_{k=1}^K\left|\left|\mathbf{W}_a^k\right|\right|_p\left|\left|\left(\tilde{\mathbf{b}}_u-\mathbf{b}_u\right)\right|\right|_p
$$</p>
<p>Proof: 第 $k$层编码器关于节点 $u$的扰动后差异可以写为：</p>
<p>$$
\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k=
\sigma\left(\mathbf{W}_a^k \tilde{\mathbf{h}}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(\bar{u})} \mathbf{h}_v^{k-1}\right)-\sigma\left(\mathbf{W}_a^k \mathbf{h}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{k-1}\right)
$$</p>
<p>其中 $\sigma(\cdot)$为非线性激活函数。<strong>假设 $\sigma(\cdot)$满足1-Lipschitz continuity（例如ReLU）</strong>，那么下式成立：</p>
<p>$$
||\sigma(b)-\sigma(a)||_p \leq||b-a||_p     \quad \quad \text{(1-Lipschitz, Lip_Constant=1)}
$$</p>
<p>所以下式成立：</p>
<p>$$
\begin{aligned}
||\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k||_p&amp;=
\left|\left|\sigma\left(\mathbf{W}_a^k \tilde{\mathbf{h}}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(\bar{u})} \mathbf{h}_v^{k-1}\right)-\sigma\left(\mathbf{W}_a^k \mathbf{h}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{k-1}\right)\right|\right|_p\\
&amp;\leq ||\mathbf{W}_a^k(\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1}) + \mathbf{W}_n^k (\sum_{v \in \mathcal{N}(\bar{u})} \mathbf{h}_v^{k-1}- \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{k-1})||_p
\end{aligned}
$$</p>
<p>由于对图中的边做扰动，边drop概率 $p_e$通常来说非常小，本文中设置为0.001，因此结构改变不大，所以上式的第二项可以近似为0，那么上式可以改写为如下形式：</p>
<p>$$
||\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k||_p \leq  ||\mathbf{W}_a^k(\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1})||_p
$$</p>
<p>根据Cauchy-Schwartz inequality $|\langle\mathbf{u}, \mathbf{v}\rangle|^2 \leq\langle\mathbf{u}, \mathbf{u}\rangle \cdot\langle\mathbf{v}, \mathbf{v}\rangle$，上式可以改写成：</p>
<p>$$
||\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k||_p \leq  ||\mathbf{W}_a^k(\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1})||_p \leq  ||\mathbf{W}_a^k||_p ||\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1}||_p
$$</p>
<p>进而得到下式：</p>
<p>$$
||\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k||_p \leq ||\mathbf{W}_a^k||_p ||\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1}||_p \leq ||\mathbf{W}_a^k||_p ||\mathbf{W}_a^{k-1}||_p ||\tilde{\mathbf{h}}_u^{k-2}-\mathbf{h}_u^{k-2}||_p \cdots
$$</p>
<p>因此下式成立：</p>
<p>$$
\begin{aligned}
||\operatorname{ENC}(\tilde{u})-\operatorname{ENC}(u)||_p &amp; =\left|\left|\tilde{\mathbf{z}}_u-\mathbf{z}_u\right|\right|_p=\left|\left|\tilde{\mathbf{h}}_u^K-\mathbf{h}_u^K\right|\right|_p
\leq \prod_{k=1}^K\left|\left|\mathbf{W}_a^k\right|\right|_p\left|\left|\left(\tilde{\mathbf{b}}_u-\mathbf{b}_u\right)\right|\right|_p,
\end{aligned}
$$</p>
<p>这里考虑 $p=2$的情况，根据spectral norm，有 $||A||_2 =\sqrt{\lambda_{\max}(A^\star A)} =  \sigma_{\max}(A)$，即最大 $A$的最大奇异值。所以下式成立：</p>
<p>$$
\begin{aligned}
||\operatorname{ENC}(\tilde{u})-\operatorname{ENC}(u)||_p &amp; =\left|\left|\tilde{\mathbf{z}}_u-\mathbf{z}_u\right|\right|_p=\left|\left|\tilde{\mathbf{h}}_u^K-\mathbf{h}_u^K\right|\right|_p
\leq \prod_{k=1}^K \sigma_{\max} (\mathbf{W}_a^k)\left|\left|\left(\tilde{\mathbf{b}}_u-\mathbf{b}_u\right)\right|\right|_p,
\end{aligned}
$$</p>
<p>因为GNN的激活函数为ReLU，严格满足1-Lipschitz，因此 $\prod_{k=1}^K \sigma_{\max} (\mathbf{W}_a^k)$应等于1，为了是模型每层权重的最大奇异值为1，需要对权重矩阵做spectral normalization：</p>
<p>$$
\tilde{\mathbf{W}}_a^k=\mathbf{W}_a^k / \sigma\left(\mathbf{W}_a^k\right)
$$</p>
<h1 id="4-learning-fair-graph-representations-via-automated-data-augmentations-iclr-2023">4. Learning Fair Graph Representations via Automated Data Augmentations (ICLR 2023)</h1>
<p><a href="https://openreview.net/pdf?id=xgGS6PmzNq6">FairAdj</a>和<a href="http://proceedings.mlr.press/v130/laclau21a/laclau21a.pdf">FairGraph</a>对所有图均使用统一的邻接矩阵增强策略。FairAdj要求优化图中边的权重，使得模型预测两个节点无论他们是否在同一个group，他们之间有边连接的概率相同（对所有图均使用这一原则来增强图结构）；而FairGraph要求优化图结构，使得不同group节点的分布Wasserstein distance最小。这些方法通过添加**启发式的fairness regularization （上面提到的原则就是这些fairness regularization的启发式规则）**来对表示学习过程施加公平性约束。然而对所有图均使用统一的公平性策略来做数据增强并不合适，不同图的敏感属性不同，敏感属性的分布也不同，因此不同的图可能需要不同的图结构或者节点特征增强策略来适应这些多样化的敏感属性。因此，针对不同图的敏感属性，采用自适应的图结构和节点特征的增强策略，而不是局限在固定的启发式的公平性规则中，是highly desirable solution。</p>
<h2 id="automated-graph-augmentations">Automated Graph Augmentations</h2>
<p>给定一个GNN encoder $g_{\text {enc }}:(A, X) \rightarrow Z \in \mathbb{R}^{n \times d_r}$将每个节点映射为表示向量 $Z$。基于 $Z$可以自适应生成增强图。</p>
<p><strong>Edge Perturbation</strong>: 基于 $Z$生成增强图的结构：</p>
<p>$$
Z_A=\operatorname{MLP}_A(Z), \quad \widetilde{A^{\prime}}=\sigma\left(Z_A Z_A^T\right), \quad A_{i j}^{\prime} \sim \operatorname{Bernoulli}\left({\widetilde{A^{\prime}}}_{i j}\right) \text { for } i, j=1, \cdots, n
$$</p>
<p>其中 $\operatorname{MLP}_A$是结构增强器的训练参数， $\sigma(\cdot)$是sigmoid函数。增强图的邻接矩阵从Bernoulli分布 $\operatorname{Bernoulli}\left({\widetilde{A^{\prime}}}\right)$中采样。</p>
<p><strong>Node Feature Masking:</strong> 基于 $Z$来自适应mask节点特征矩阵中的元素：</p>
<p>$$
\begin{aligned}
&amp;Z_X =\operatorname{MLP}_X(Z), \\ &amp;\widetilde{M}=\sigma\left(Z_X\right), \\
&amp;M_{i j} \sim \operatorname{Bernoulli}\left(\widetilde{M}_{i j}\right) \text { for } i, j=1, \cdots, n, \\
&amp;X^{\prime}=M \odot X
\end{aligned}
$$</p>
<p>$\operatorname{MLP}_A$是特征增强器的参数，node feature mask矩阵从Bernoulli分布 $M_{i j} \sim \operatorname{Bernoulli}\left(\widetilde{M}_{i j}\right)$采样，然后与原特征矩阵 $X$。</p>
<p>在端到端的模型优化中，从Bernoulli distribution中采样邻接矩阵和featre masking矩阵是不可微的。为了使 $g$是端到端可训练的，Bernoulli distribution的采样操作可以使用Gumbel-Softmax reparameterization trick来模拟。具体来说，给定一个由模型 $\varphi$输出的概率 $\widetilde{P}$，那么可以通过添加一个从Gumbel分布中采样的随机变量 $G$来作为概率 $\widetilde{P}$的连续近似（continuous approximation）： $\hat{P}=\frac{1}{1+\exp (-(\log \widetilde{P}+G) / \tau)}$。那么该概率下的伯努利分布采样值为 $P=\left\lfloor\hat{P}+\frac{1}{2}\right\rfloor$，这个从 $\hat{P}$中得到采样值可以近似作为 $\widetilde{P}$的采样值。在反向传播计算梯度的时候，计算的使 $\hat{P}$关于可训练参数 $\varphi$的梯度。</p>
<h2 id="adversarial-training">Adversarial Training</h2>
<p>由于图中那些边以及节点的哪些属性会导致出现不公平的预测是未知的，因此使用对抗训练的方法来学习一个图增强器 $g$，用上文提到的方式来生成增强图的结构和节点特征，所得到的增强要尽可能与敏感属性无关，也就是<strong>判别器 $k$要无法建立增强图与敏感属性之间的关系</strong>。此时可以认为图增强器 $g$生成的增强图与敏感属性无关。</p>
<p>具体来说，判别器 $k:\left(A^{\prime}, X^{\prime}\right) \rightarrow \hat{S} \in[0,1]^n$用来基于增强图来预测每个节点的敏感属性值，预测损失越低说明增强图和敏感属性之间的关系越弱。因此对抗训练来学习图增强器的过程为以下优化问题：</p>
<p>$$
\begin{aligned}
\min_g \max_k L_{\mathrm{adv}}&amp;=\min_g \max_k \frac{1}{n} \sum_{i=1}^n\left[S_i \log \hat{S}_i+\left(1-S_i\right) \log \left(1-\hat{S}_i\right)\right] \\
&amp;= \min_g \max_k -\operatorname{CE}\left(S, \hat{S}\right)
\end{aligned}
$$</p>
<p>上面的优化目标可以改写为：</p>
<p>$$
\max_g \min_k \operatorname{CE}\left(S, \hat{S}\right) = \max_g \min_k \operatorname{CE} \left(S, k(g(A, X))\right)
$$</p>
<p>上式的优化过程可以总结如下：固定判别器 $k(\cdot, \cdot)$，优化z</p>
<p>固定判别器 $k(\cdot, \cdot)$，优化图增强器 $g$，使得CE loss最大化，即通过 $g$来生成的增强图 $g(A,X) = A^\prime, X^\prime$要使得 $k$无法利用它们来预测每个节点的敏感属性值，也就是 $g$需要生成与敏感属性无关的增强图。</p>
<p>固定图增强器 $g$，优化判别器 $k(\cdot, \cdot)$，使得CE loss最小化，即对于当前得到的增强图 $\left(A^{\prime}, X^{\prime}\right)$，需要一个更强的判别器  $k(\cdot, \cdot)$来建立增强图与敏感属性的关系，也就是用增强图来预测每个节点的敏感属性。</p>
<p>这种对抗训练的过程可以让增强器 $g$生成尽可能难以被用来预测敏感属性的图结构，而判别器 $k$则要尽可能建立增强图与敏感属性之间的关联。当对抗训练收敛时，可以认为 $g$足以生成无法与敏感属性建立起关系的增强图 $\left(A^{\prime}, X^{\prime}\right)$。</p>
<p><strong>这种图增强的方法来减轻敏感属性对预测的影响是自适应的，并没有基于确定的规则regularization（比如intra/inter group相似度，distribution相似度），对于不同的图会自适应地学习与敏感属性关系最低的对应增强图。因此相对于基于fairness regularization的方法，更加通用。</strong></p>
<h1 id="5-fmp-toward-fair-graph-message-passing-against-topology-bias-arxiv-2022">5. FMP: Toward Fair Graph Message Passing against Topology Bias (Arxiv 2022)</h1>
<p>Topology bias 指的是在许多现实世界graph中，具有相同敏感属性（除了节点特征外的敏感属性，如性别，国籍等）的节点更可能相互连接。本文证明了message passing中的聚合过程会在node representation中积累偏差。</p>
<p>给定一个有 $n$个节点的图 $\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$，节点的（binary）敏感属性是一个 $n$维向量 $\mathbf{s} \in\{-1,1\}^n$，表示每个节点的敏感属性取值。图中节点敏感属性的指示向量（incident vector）定义为：</p>
<p>$$
\Delta_{\mathbf{s}}=\frac{\mathbb{1}_{&gt;0}(\mathbf{s})}{\left|\left|\mathbb{1}_{&gt;0}(\mathbf{s})\right|\right|_1}-\frac{\mathbb{1}_{&gt;0}(-\mathbf{s})}{\left|\left|\mathbb{1}_{&gt;0}(-\mathbf{s})\right|\right|_1}
$$</p>
<p>其中 $\left|\left|\mathbb{1}_{&gt;0}(\mathbf{s})\right|\right|_1$等于敏感属性为1的节点数 $n_1$， $\left|\left|\mathbb{1}_{&gt;0}(-\mathbf{s})\right|\right|_1$等于敏感属性为-1的节点数。若 $s_i=1$，那么 $\mathbb{1}_{&gt;0}(\mathbf{s}_i)=1$， $\mathbb{1}_{&gt;0}(-\mathbf{s}_i)=0$，则 $\Delta_{\mathbf{s}_i} = \frac{1}{n_1}$；同理，若 $s_i=0$，那么 $\Delta_{\mathbf{s}_i} = -\frac{1}{n_{-1}}$，其中 $n_{-1}$表示敏感属性为-1的节点数。因此normalized incident vector $\Delta_{\mathbf{s}} = \{\frac{1}{n_1}, -\frac{1}{n_{-1}}\}^n$。</p>
<p><strong>Definition 1 (Label and Sensitive Homophily Coefficient)</strong> Label homophily coefficient:
$$
\epsilon_{\text {label }}(\mathcal{G}, \mathbf{y})=\frac{1}{|\mathcal{E}|} \sum_{(i, j) \in \mathcal{E}} \mathbb{1}\left(\mathbf{y}_i=\mathbf{y}_j\right)
$$</p>
<p>表示连接相同label节点的边占总边数的比例。</p>
<p>Sensitive homophily coefficient:</p>
<p>$$
\epsilon_{\text {sens }}(\mathcal{G}, \mathbf{s})=\frac{1}{|\mathcal{E}|} \sum_{(i, j) \in \mathcal{E}} \mathbb{1}\left(\mathbf{s}_i=\mathbf{s}_j\right)
$$</p>
<p>表示连接相同敏感属性节点的边占总边数的比例。</p>
<h2 id="gnns-as-graph-signal-denoising">GNNs as Graph Signal Denoising</h2>
<p>现有的GNN如GCN, SGC, APPNP等均可以看作以不同的方式（直接求导或梯度下降）来优化signal denoising函数：</p>
<p>$$
\min_\mathbf{F} h_s(\mathbf{F}) = \min_\mathbf{F}\frac{\lambda_s}{2} \operatorname{tr}\left(\mathbf{F}^T \tilde{\mathbf{L}} \mathbf{F}\right)+\frac{1}{2}\left|\left|\mathbf{F}-\mathbf{X}_{\text {trans }}\right|\right|_F^2
$$</p>
<p>用来enforce smoothnes以及保持和输入特征的相似度。</p>
<h2 id="the-optimization-framework">The Optimization Framework</h2>
<p>在优化 $\mathbf{F}$的同时， $\mathbf{F}$要满足公平性约束fairness objective $\left|\left|\boldsymbol{\Delta}_s S F(\mathbf{F})\right|\right|_1$:</p>
<p>$$
\min_{\mathbf{F}} \underbrace{\frac{\lambda_s}{2} \operatorname{tr}\left(\mathbf{F}^T \tilde{\mathbf{L}} \mathbf{F}\right)+\frac{1}{2}\left|\left|\mathbf{F}-\mathbf{X}_{\text {trans }}\right|\right|_F^2}_{h_s(\mathbf{F})}+\underbrace{\lambda_f\left|\left|\boldsymbol{\Delta}_s S F(\mathbf{F})\right|\right|_1}_{h_f\left(\boldsymbol{\Delta}_s S F(\mathbf{F})\right)}
$$</p>
<p>其中 $S F(\mathbf{F})_{ij} = \hat{P}\left(y_i=j \mid \mathbf{X}\right)$表示将节点 $v_i$预测label为 $j$的概率，而 $S F(\mathbf{F})_{i:}$ 表示节点 $v_i$的预测logits向量，所以 $S F(\mathbf{F}) = \operatorname{Softmax}(\mathbf{F})$。</p>
<p>由于$\Delta_{\mathbf{s}} = \{\frac{1}{n_1}, -\frac{1}{n_{-1}}\}^n$用来指示每个节点的敏感属性，因此 $\boldsymbol{\Delta}_s S F(\mathbf{F}) \in \mathbb{R}^n$。那么 $\boldsymbol{\Delta}_s S F(\mathbf{F})$的第 $j$个元素 $\boldsymbol{\Delta}_s S F(\mathbf{F})_j = \boldsymbol{\Delta}_s \cdot \operatorname{Softmax}(\mathbf{F})_{:,j}$，因此， $\boldsymbol{\Delta}_s S F(\mathbf{F})_j$等于敏感属性$s=1$的节点预测为label $j$的平均概率 与 $s=-1$的节点预测为label  $j$的平均概率 之差：</p>
<p>$$
\left(\Delta_s S F(\mathbf{F})\right)_j = \hat{P}\left(y_i=j \mid \mathbf{s}_i=1, \mathbf{X}\right)-\hat{P}\left(y_i=j \mid \mathbf{s}_i=-1, \mathbf{X}\right)
$$</p>
<p>其中 $\hat{P}\left(y_i=j \mid \mathbf{s}_i=1, \mathbf{X}\right)$表示敏感属性为 $1$的节点预测为标签 $j$的期望概率。如果 $\left(\boldsymbol{\Delta}_s S F(\mathbf{F})\right)_j$越小，说明模型学习到的node representations $\mathbf{F}$ is equivalent to demographic parity，也就是模型对通过敏感属性划分的sensitive group的预测不会产生偏差，即模型的预测与sensitive attribute无关。</p>
<h1 id="6-edits-modeling-and-mitigating-data-bias-for-graph-neural-networks-www-22">6. EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks (WWW 22)</h1>
<p>Bias in attribute：敏感属性划分的demographic group的attribute distribution （除了敏感属性之外的node feature）是不同的</p>
<p>Bias in network structure：敏感属性划分的demographic groups之间的节点邻居分布存在差异</p>
<p>上面两个bias意味着图的<strong>feature信息</strong>或图的<strong>结构信息</strong>与敏感属性高度相关</p>
<p><strong>Case 1: 有偏属性和无偏结构。</strong> 对于两个demographic group （例如通过敏感属性性别来划分group），bias in attribute意味着两个group中节点的属性分布（除了敏感属性之外的node feature）不同，但是图的连通性与敏感属性无关。这里将male节点的node feature从分布 $\mathcal{N}\left(-1.5,1^2\right)$采样，female节点的node feature从分布 $\mathcal{N}\left(1.5,1^2\right)$中采样。通过这种方式可以让敏感属性和节点特征保持强相关。为了实现无偏网络结构，图中的节点对以 $2\times 10^{-3}$的概率连接。</p>
<p><img loading="lazy" src="/posts/2025-05-21-Fairness/1.png#center" alt=""  />
</p>
<p>上图(a)是在propagation之前的两个敏感group的node feature分布，(b)可以看出图连通与group无关，(c) 可以看出propagation之后的node representations与(a)中的输入特征相比，2个敏感group节点之间的表示差异变小，说明无偏的结构可以使GNN学到的node representations与敏感属性的相关性降低。</p>
<p><strong>Case 2: 无偏属性和有偏结构。</strong> 这里假设所有节点的特征均采样自同一个分布$\mathcal{N}\left(0,1^2\right)$来构造无偏属性。为了构造关于敏感属性的有偏结构，对于每个节点，它的邻居的敏感属性与中心节点的敏感属性相关。首先，为每个节点累加它们的属性值，并且按照降序排列所有节点。取排名最高的前 $t$个male和最低的后 $t$个female构造两个community，在每个community内，节点以 $5\times 10^{-2}$的概率构造边。对于其它节点，以 $1\times 10^{-2}$的概率构造边。如下图所示：</p>
<p><img loading="lazy" src="/posts/2025-05-21-Fairness/2.png#center" alt=""  />
</p>
<p>从上图中可以看出，虽然(a)中的原始node feature是与sensitive attribute无关的，但是有偏的network structure会导致propagation之后的node representation会产生与sensitive attribute相关的聚类。</p>
<h2 id="bias">Bias</h2>
<p>Definition 1 (<strong>Attribute bias</strong>) 敏感属性 $i$将节点分为2个demographic groups。如果node feature中的任意属性在2个groups间存在分布差异，那么属性偏差存在。也就是node feature的分布与敏感属性相关。</p>
<p>Definition 2 (<strong>Structural bias</strong>) Propagate后2个group的节点属性分布出现差异，即 $AX$会导致representation在group间出现分布差异。因此图结构与敏感属性相关。</p>
<h2 id="objective-function">Objective Function</h2>
<p>考虑到节点特征有 $M$个属性，那么用 $P_{0,m}$表示group 0的第 $m$个属性的值分布，用 $P_{1,m}$表示group 1的第 $m$个属性的值分布。</p>
<p>若在structural debiasing阶段，模型propagate $H$次，并计算每次propagate的跨group属性差异。那么第 $h$层（ $1 \leq h \leq H$）group 0中节点的第 $m$个属性的值分布为 $P_{0,m}^{(h)}$，group 1中节点的第 $m$个属性值的分布为 $P_{1,m}^{(h)}$。那么对于所有 $H+1$层，group 0 的第 $m$个属性的联合概率分布 $P_{0,m}^{Joint} = P_{0,m}^{(0)} \cdot  P_{0,m}^{(1)} \cdot P_{0,m}^{(2)} \cdots P_{0,m}^{(H)}$ ，同理，group 1的第 $m$个属性的联合概率分布 $P_{1,m}^{Joint} = P_{1,m}^{(0)} \cdot  P_{1,m}^{(1)} \cdot P_{1,m}^{(2)} \cdots P_{1,m}^{(H)}$。这两个分布可以看作 $H+1$维的随机变量，每个随机变量服从一个分布。模型的参数可以分为2个部分，分别是 $g_{\theta_m}: \mathbb{R} \to \mathbb{R}$，用于mitigate每个节点第 $m$个属性的bias。参数化的邻接矩阵 $\tilde{A}$ 用于mitigate bias from structure。因此，对于第 $m$个attribute，模型的目标函数定义为如下形式：</p>
<p>$$
\min_{\theta_m, \tilde{A}} W(P_{0,m}^{Joint}, P_{1,m}^{Joint})
$$</p>
<p>优化目标是优化第 $m$个属性的“去偏器”参数 $\theta_m$和图结构 $\tilde{A}$，使得2个group原始节点属性分布以及propagate之后的属性分布之间的差异最小，即原始属性分布要在group间无差异，且propagate后的属性分布也要在group间无差异。</p>
<p>联合分布 $P_{0,m}^{Joint}$中的一个样本 $\mathrm{x}_{0, m} \sim P_{0,m}^{Joint}$  定义为每个概率分布的一个样本 $\mathrm{x}_{0, m}=\left[x_{0, m}^{(0)}, x_{0, m}^{(1)}, \ldots, x_{0, m}^{(H)}\right]$。而group 1的第 $m$个属性服从联合分布 $P_{1,m}^{Joint}$，那么它的一个样本可以视为从每个分布中采样一个样本的组合 $\mathrm{x}_{1, m}=\left[x_{1, m}^{(0)}, x_{1, m}^{(1)}, \ldots, x_{1, m}^{(H)}\right]$。</p>
<p>上面目标函数用Wasserstein distance 来表示，其中 $\gamma$表示两个分布间的一种传输方案，Wasserstein distance则表示两个分布中样本的平均差异:</p>
<p>$$
W(P_{0,m}^{Joint}, P_{1,m}^{Joint})= \inf_{\gamma\in \Pi(P_{0,m}^{Joint}, P_{1,m}^{Joint})} \mathbb{E}_{(\mathrm{x}_{0, m},\mathrm{x}_{1, m}) \sim \gamma} \left[||\mathrm{x}_{0, m}-\mathrm{x}_{1, m}||_1\right]
$$</p>
<p>因此模型优化的目标函数是要使2个group联合概率分布中样本的平均差异最小。</p>
<p>考虑所有 $M$个属性，模型的目标函数为：</p>
<p>$$
\min_{\theta, \tilde{A}} \frac{1}{M} \sum_{1 \leq m \leq M} W\left(P_{0, m}^{Joint }, P_{1, m}^{Joint }\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2024《One for All：Towards Training One Graph Model for All Classification Tasks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/ofa/</link>
      <pubDate>Wed, 21 May 2025 11:57:35 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/ofa/</guid>
      <description>ICLR2024 &amp;#34;One for All：Towards Training One Graph Model for All Classification Tasks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><strong>What is in-context learning?</strong></p>
<p>“In-Context Learning is a way to use LLMs to learn tasks given only a few examples. During in-context learning, we give the LM a prompt that consists of a list of input-output pairs that demonstrate how to perform a task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens. For example, to answer the two prompts below, the model needs to examine the training examples to figure out the input distribution (financial or general news), output distribution (Positive/Negative or topic), input-output mapping (sentiment or topic classification), and the formatting. “</p>
<p><img loading="lazy" src="/posts/2025-05-21-OFA/1.gif#center" alt="icl"  />
</p>
<p>如上图所示，在prompt中包含一些上下文input-output pairs来演示（demonstrate）如何执行任务。在这些prompt的最后添加text input token来让LLM学习上下文中的演示来执行text input token的任务。</p>
<h2 id="challenge"><strong>Challenge</strong></h2>
<p>Foundation Model指用单一模型来解决多个任务。但是Graph Foundation Model面临以下挑战：（1）不同来源的图数据在feature表征上通常完全不同。比如molecular graph中的节点特征是其中原子nominal feature的索引，e-commerce networks中的节点特征通常用Bag-of-Word来编码。这些不同来源的图数据特征维度、语义信息和尺度的差别都很大，几乎不可能用同一个模型来学习他们的表示。（2）不同的下游任务涉及图的不同部分（节点级、边级、图级）需要不同的策略和方法来学习表示。（3）如何设计统一的模型来实现跨领域和in-context learning是不确定的。</p>
<h1 id="ofa">OFA</h1>
<h2 id="用tag来统一不同领域的图都转化为tag形式">用TAG来统一不同领域的图（都转化为TAG形式）</h2>
<p><strong>将不同Domian的图转化为统一的TAG形式</strong></p>
<p>用human-interpretable language来描述节点和边的属性，从而使LLM可以将这些text attribute编码到同一个空间。具体来说，通过以下方式来生成每个节点的text feature:
将不同domain的graph的节点属性用统一形式的文本来描述，如上图中，对于一个节点的特征，它的文本描述以<strong>Feature node</strong>开头，后面的每个</p>
<p><img loading="lazy" src="/posts/2025-05-21-OFA/node_text_feature.jpg#center" alt="node_text_feature.jpg"  />
</p>
<p>将不同domain的graph的节点属性用统一形式的文本来描述，如上图中，对于一个节点的特征，它的文本描述以<strong>Feature node</strong>开头，后面的每个 $&lt;\text{feature describe}&gt;:&lt;\text{feature content}&gt;$是该节点的一个type-content文本对。例如对于一个molecule graph，其中的一个节点是原子，那么它的type是<strong>Atom</strong>，它的content是该原子的属性文本描述<strong>Carbon, Atomic number 6, helix chirality</strong>。如果是Citation Network， type是“Paper title and abstract”，content是“Attention is all you need. balabala”  （该文章具体的标题和摘要）
同理，边的text feature以<strong>Feature edge</strong>开头进行文本描述。</p>
<p>在用以上方式得到节点 $v_i$的text feature $s_{v_i}$和边 $e_{ij}$的text feature $s_{e_{ij}}$后，用LLM（sentence transformer, e5-large-v2, or Llama2）来将每个节点和边的text feature编码为vector embeddings：</p>
<p>$$
x_i = \operatorname{LLM}(s_{v_i}), \quad x_{ij} = \operatorname{LLM}(s_{e_{ij}})
$$</p>
<h2 id="用nodes-of-interest-noi来统一不同的图任务">用Nodes-of-Interest (NOI)来统一不同的图任务</h2>
<p>图上的任务主要分为3中：node-level tasks，link-level tasks和graph-level tasks。如果要用一个模型来处理这些任务的话，<strong>需要将这些任务统一为一个任务以便于在图数据上训练</strong>。</p>
<p><img loading="lazy" src="/posts/2025-05-21-OFA/NOI.jpg#center" alt="NOI.jpg"  />
</p>
<p>Nodes-of-Interest (<strong>NOI</strong>)指的是一个任务的<strong>目标节点（任务感兴趣的节点）</strong>，如上图的蓝色节点所示，表示为 $\mathcal{T}$。</p>
<ol>
<li>对于节点级任务，NOI是待预测节点集合。</li>
<li>边级任务NOI是待预测是否有边的节点对。</li>
<li>图级任务NOI是待预测图中的所有节点。</li>
</ol>
<p>待训练的有很多图，有的图要做节点分类，有的图做链路预测，有的图做图分类，通过构造NOI subgraph的方式，无论是什么任务，都把每个图中所有的 <strong>与任务相关的节点</strong> （nodes-of-interest）提取出来。</p>
<p>若一个NOI节点 $v$的 $h$-hop局部子图表示为 $\mathcal{S}_h(v)$，那么NOI中所有目标节点的局部子图共同构成了一个<strong>NOI subgraph</strong>，表示为 $\mathcal{G}_h (\mathcal{T})$：</p>
<p>$$
\mathcal{G}_h(\mathcal{T})=\bigcup_{v \in \mathcal{T}} \mathcal{S}_h(v)=\left\{\bigcup_{v \in \mathcal{T}} \mathcal{V}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{E}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{R}_v^h\right\} \quad \text{NOI中所有节点的局部子图共同构成}
$$</p>
<p><strong>NOI prompt node</strong>：定义NOI prompt node，该节点的文本信息用来描述任务，比如某个数据集上的节点分类、图分类等。NOI prompt node的节点text feature以如下形式构建：</p>
<p><img loading="lazy" src="/posts/2025-05-21-OFA/NOI_prompt_node.jpg" alt="NOI_prompt_node.jpg"  />
</p>
<p>NOI prompt node的节点特征以 “Prompt node.” 开头，该节点的文本表述为需要在NOI上进行的task，如果需要对NOI做node classification，那么Prompt node的文本特征为：“Node classification on the literature category of the paper.”。通过这种方式，即使是图上任务有不同，只需要用不同的NOI prompt node就可以描述不同的任务。</p>
<h2 id="graph-in-context-learning的图提示范式graph-prompting-paradigm">Graph In-Context Learning的图提示范式（Graph Prompting Paradigm）</h2>
<p>LLM的一大特性就是它可以通过prompt来实现in-context learning，使得模型可以在不用fine-tuning的情况下适应于不同的任务。比如在few-shot场景下，目标是基于一篇paper的摘要和内容预测它的类别，我们可以为LLM提供每个类别的 $k$篇papers作为context加入prompt中，来指导模型基于这些提供的context来生成目标摘要和内容的类别预测。（通过一些<strong>任务相关的其他信息</strong>来指导模型对目标样本的预测）</p>
<p>本文发现实现图上in-context learning的核心在于操作输入图使其和下游任务对齐。<strong>Graph Prompting Paradigm (GPP)</strong> 即图提示范式旨在操作输入图使其可以从输入数据本身获得任务相关的信息。如图2中的虚线所示，用来描述任务的NOI prompt node与所有NOI node建立连接（任务的目标节点），表示在这些NOI nodes上执行对应prompt的任务（如节点分类，链路预测，图分类等）。</p>
<p><strong>Zero-shot Learning:</strong> 对于一个NOI $q$，若是节点分类任务 $q$是一个节点；若是边分类任务 $q$是一条边的两端节点；若是图分类任务， $q$是图中的所有节点，如Figure 2中的蓝色节点所示。 $q$中节点构成的局部子图，即NOI subgraph 表示为 $\mathcal{G}^q_h (\mathcal{T}^q) = (\mathcal{V}^h_q, \mathcal{E}^h_q, \mathcal{R}^h_q)$。</p>
<p>对于一个NOI subgraph $\mathcal{G}^q_h (\mathcal{T}^q)$, 该子图的NOI的任务可能是节点级任务也可能是边级或图级任务，根据该NOI subgraph的目标任务不同，将对应任务的NOI prompt node $p_q$与该NOI subgraph中的所有NOI节点 $q$相连。如Figure 2中的虚线所示，任务描述节点NOI prompt node与该任务的目标节点用<strong>虚线</strong>相连。</p>
<p>除此之外，定义class node与NOI prompt node相连，class node用于描述NOI prompt node的分类任务的类别信息，有多少个类别就有多少个class nodes。最终可以得到所有NOI的prompt graph，如Figure 2的左边框中3个prompt graph所示。用同一个GNN模型来训练不同分类任务的Prompt Graph （任务仅仅通过NOI prompt nodes 和class nodes来做区分）。在半监督训练完成后，我们可以得到一个GNN模型，3种任务的NOI prompt nodes的embedding，以及每种任务的class nodes的embedding，以及一个从embedding映射到label的MLP。对于一个新的节点/边，首先提取他的NOI subgraph，如Figure 2的彩色框中所示，然后将新的NOI nodes连接到NOI prompt nodes，再将NOI prompt nodes连接到任务的class nodes。先用训练好的GNN应用在这个新的prompt graph上，然后可以得到每个class node的embedding, 再用训练好的MLP将class node embeddings映射到label space，从而得到该NOI属于不同类别的概率。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2023《All in One：Multi-Task Prompting for Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/allinone/</link>
      <pubDate>Tue, 20 May 2025 14:22:08 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/allinone/</guid>
      <description>KDD2023 &amp;#34;All in One：Multi-Task Prompting for Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>图神经网络的预训练任务和下游任务之间可能存在较大gap，直接将预训练模型应用在下游任务上可能会产生负迁移现象（“negative transfer”）。例如，binary edge prediction经常用于pretrain graph model。这样的预训练模型使得有边连接的节点在representation space中接近。但是下游任务可能是node-level 或graph-level tasks，下游的任务如果是节点分类任务，那么预训练模型需要针对额外的节点类别标签搜索更高维度的参数空间。如果图中相连节点的类别不同（heterophilic），那么基于edge prediction pretrained的模型会对下游任务参数负面效果。</p>
<p>为了解决上述问题，一个潜在的方向是将“pretraining and fine-tuning”拓展为“pretraining, <strong>prompting</strong>, and fine-tuning”。例如在自然语言处理中，如果要赋予预训练语言模型预测句子情感的能力（sentiment analysis），可以通过prompt来完成，而不需要优化pretrained model。</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/image.png#center" alt="你想输入的替代文字"  />
</p>
<p>以上图为例，对于一个fronzen LLM（参数固定），如果要为这个模型赋予情感分析的能力，我们可以额外训练一个最佳的prompt，训练数据为prompt parameters，要求这个prompt tokens在tasker $\phi$的优化下，生成的下一个token是正确的情感（label为excited）。即训练得到一个最佳的prompt tokens，比如训练得到的最佳prompt tokens是“I feel so [mask]”，使得frozen LLM应用在“KDD2023 will witness many high-quality papers. I feel so ” 这个句子上时，可以将下一个词预测为情感词，这样LLM在输入包含prompt tokens的情况下，可以具备预测句子情感的能力。也就是说， <strong>Prompt Learning的目的是训练得到一堆tokens，使得这些tokens与原来的context拼起来可以使得LLM具备新的能力。</strong></p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/1.png#center" alt="你想输入的替代文字"  />
</p>
<p><strong>这篇文章的目的是在图上做Prompt Learning，也就是在训练一个prompt graph，使得现有图拼上这个prompt graph后，预训练的GNN可以在新的任务上（预训练阶段没有接触过的任务上）也表现的较好。</strong> 但是在graph上做Prompt Learning存在以下挑战：</p>
<ul>
<li>自然语言处理中，prompt tokens是一个一维线性的句子，可以放在content的开头或结尾，但是在graph中，节点是非欧结构，因此如何组织prompt tokens，以及如何将graph prompts与input graph结合是一个挑战。</li>
<li>在自然语言处理中，类似于情感分析，和问答任务，这些任务都可以简单的重构为next token prediction（单词预测）的任务，所以只需要使用单词预测来训练prompt就可以。但是在图中，节点级任务、边级任务和图级任务难以统一成一种形式。因此如何将各种prompt任务统一来训练graph prompt也是一个挑战。</li>
</ul>
<p>训练好prompt token的向量化信息、连接结构、以及插入到原图的方式，然后Frozen Pretrained Model应用在这个combined graph上后，就可以为Pretrained Model赋予处理新任务的能力。</p>
<h2 id="reformulating-downstream-tasks">Reformulating Downstream Tasks</h2>
<p>将节点级和边级的下游任务统一为induced graph的标签预测问题。</p>
<p>对于节点预测任务，将它重构为图分类任务：</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/2.png#center" alt="你想输入的替代文字"  />
</p>
<p>将节点标签设置为它的 $k$-hop诱导子图的标签。</p>
<p>将边预测（存在性预测和类别预测都可以，即二分类和多分类）任务也重构为图分类任务，如下：</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/3.png#center" alt="你想输入的替代文字"  />
</p>
<h2 id="prompt-graph-design">Prompt Graph Design</h2>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/4.png#center" alt="你想输入的替代文字"  />
</p>
<p><strong>Prompt Tokens</strong> 首先Prompt graph 定义为 $\mathcal{G}_p(\mathcal{P},\mathcal{S})$，其中每个token $p_i \in \mathcal{P}$是Prompt graph中的节点，它的特征是 $\mathbf{p}_i$，维度与node features相同。</p>
<p><strong>Token Structures</strong> 每个tokens的特征 $\mathbf{p}_i$是随机初始化可学习的，然后可以通过计算相似度并用sigmoid和相应的阈值来控制 $\mathcal{G}_p$的结构。然后可以计算 $\mathbf{p}_k$和节点 $\mathbf{x}_i$之间的相似度来确定该prompt node如何将自身信息与节点 $\mathbf{x}_i$的信息结合，来赋予预训练模型处理节点 $i$相应任务的能力。
$$
w_{i k}= \begin{cases}\sigma\left(\mathrm{p}_k \cdot \mathrm{x}_i^T\right) &amp; \sigma\left(\mathrm{p}_k \cdot \mathrm{x}_i^T\right)&gt;\delta \\ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>通过 $\hat{\mathbf{x}}_i=\mathbf{x}_i+\sum_{k=1}^{|\mathcal{P}|} w_{i k} \mathbf{p}_k$ 来为节点 $i$添加提示。</p>
<h2 id="如何训练-prompt-graph使其可以作为预训练模型的有效提示multi-task-prompting-via-meta-learning">如何训练 Prompt Graph，使其可以作为预训练模型的有效提示：Multi-task Prompting via Meta Learning</h2>
<p>学习最好的Prompt，使得该Prompt可以帮助Pretrained Model更好的适应下游任务。</p>
<p><strong>Phase 1</strong> 有一些 Source Task去训练Prompt 的初始值。</p>
<p><strong>Phase 2</strong> 对Target Task做测试。</p>
<p>具体来说，首先对于要训练的prompt graph $\theta$ （即所有token的特征）让它在多个training task上训练，如下图所示，又3个节点分类的tasks，每个tasks里又自己的训练集（support set）和测试集（query set）。每个task都有一个共同的起点，在几个tasks上训练上训练后，每个tasks上有个优化后的task-specific prompt (Prompt 1, Prompt 2, Prompt 3)。</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/5.png#center" alt="你想输入的替代文字"  />
</p>
<p>在更新了之后，评估组合起来的Prompt是否会让整体的性能更好，如下图所示，把所有task-specific prompt组合起来，来看组合后的prompt是不是足够好。</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/6.png#center" alt="你想输入的替代文字"  />
</p>
<p>如果当前的Initial prompt训练出来的模型在三个tasks上不够好，那么换一个Initial Prompt，直到某个Initial Prompt在所有training tasks经过训练后都表现的较好时，就说明这是一个 <strong>较好的初始prompt (训练起点)</strong>，它适合做training tasks的初始化prompt。</p>
<p>在Meta Training 结束后，我们可以得到一个较好的prompt初始化值，即上图中的Adapt prompt initialization。</p>
<p>在Meta Testing阶段，将我们通过Training Tasks学到的最佳初始Prompt在Test tasks的support set上做微调，然后在Test Task的query set上验证prompt initialization的效果。</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/7.png#center" alt="你想输入的替代文字"  />
</p>
<p>通过上面方式得到的prompt可以作为graph prompt与input graph 结合，使其具备link prediction和node classification的能力。</p>
<p><strong>可以看作时学习一个图增强，使得预训练的图模型在这个增强图上可以适用于更多任务。</strong></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2023《Evaluating GNN Performance On Unseen Graphs Without Labels》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gnn-evaluator/</link>
      <pubDate>Tue, 20 May 2025 13:45:31 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gnn-evaluator/</guid>
      <description>NeurIPS2023 &amp;#34;Evaluating GNN Performance On Unseen Graphs Without Labels&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2310.14586">Paper</a></p>
<p>目的：在一个图上训练好的GNN，在未知的testing graph上的结果由于training和test数据分布的不同，可能存在很大的不确定性。通常来说，in-service的GNN在已知graph with labels的图上训练好后，需要部署在label未知的testing graph上，但是由于label未知，无法估计GNN在testing graph上的效果。</p>
<p><img loading="lazy" src="/posts/2025-05-20-GNNEvaluator/1.png#center" alt="你想输入的替代文字"  />
</p>
<p>如上图所示，对于一个已经在训练图 $\mathcal{S}=(\mathbf{X}, \mathbf{A}, \mathbf{Y})$上well-trained &amp; fixed GNN model  $\mathrm{GNN}_{\mathcal{S}}^\star$，并且将它deploy in service。对于一个不知道label的测试图 $\mathcal{T}$，由于不知道label，如何评估GNN在该测试图上的性能是一个挑战。由于可见的训练graph和不可见的test graph的分布差异可能很大，因此GNN评估器<strong>GNNEvaluator</strong>需要充分学习多样化的图结构with diverse node context and graph structure distributions，从而可以评估不同数据分布的测试图潜在的效果。</p>
<p>假设：<strong>Covariate shift</strong> between the training graph $\mathcal{S}$ and the label-unlabeled graph $\mathcal{T}$ with respect to the label space。 即无论输入图是什么样的，输出的label space相同。</p>
<h3 id="如何生成数量足够的graph-set来训练gnnevaluator-f_theta">如何生成数量足够的graph set来训练GNNEvaluator $f_\theta$?</h3>
<p>Solution：采样一个seed subgraph $\mathcal{S}_{seed}$ from the observed training graph $\mathcal{S}$。采样原则是seed graph 要和 training graph之间满足相同的label space，所以原图中采样可以使采样图$\mathcal{S}_{seed}$的分布尽可能少的偏离原图，从而满足Covariate shift。如下图左边所示。</p>
<p><img loading="lazy" src="/posts/2025-05-20-GNNEvaluator/2.png#center" alt="你想输入的替代文字"  />
</p>
<p>在得到采样seed graph $\mathcal{S}_{seed}$后，对 $\mathcal{S}_{seed}$做 $K$次增强，其中涉及的增强包括 $\texttt{EdgeDrop}$， $\texttt{NodeDrop(Subgraph)}$， $\texttt{AttrMask}$和 $\texttt{NodeMix}$。选择哪种增强有特定的概率 $\epsilon$。基于seed graph  $\mathcal{S}_{seed}$，可以得到一个 meta-graph集合 $\mathcal{G}_{\text{meta}}=\left\{g_{\text {meta }}^i\right\}_{i=1}^K$，其中的每个meta-graph都是由seed graph 扰动而来，和原图具有相同的label space。每个meta-graph $g_{\text {meta }}^i=\left\{\mathbf{X}_{\text {meta }}^i, \mathbf{A}_{\text {meta }}^i, \mathbf{Y}_{\text {meta }}^i\right\}$，分别表示meta-graph的节点特征，结构和标签。通过这种方式可以为原图生成label space相同，但结构/特征都不相同图，拓展了差异性，基于这些图学习到的GNNEvaluator可以评估不同分布图的效果。</p>
<h3 id="如何学习well-trained-gnn在不同分布图上的差异">如何学习well-trained GNN在不同分布图上的差异？</h3>
<p>给定一个在 training graph $\mathcal{S}$上 well-trained的GNN模型 $\mathbf{Z}_{\mathcal{S}}^\star=\operatorname{GNN}_{\mathcal{S}}^\star(\mathbf{X}, \mathbf{A})$， $\mathbf{Z}_{\mathcal{S}}^\star$是学习到的原图embeddings。将这个well-trained GNN  $\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$直接应用在每个meta graph $g_{\text {meta }}^i$上：</p>
<p>$$ \mathbf{Z}_{\text {meta }}^{(i, \star)}, \hat{\mathbf{Y}}_{\text {meta }}^{(i, \star)}=\operatorname{GNN}_{\mathcal{S}}^\star\left(\mathbf{X}_{\text {meta }}^i, \mathbf{A}_{\text {meta }}^i\right)$$</p>
<p>可以得到$\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$为meta graph $g_{\text{meta}}^i$学习到的节点embedding和预测值，可以用来评估在$\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$该分布的图上的效果。若 $g_{\text {meta }}^i$中有 $M_i$个节点，那么 $\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$对该图的准确率为：
$$
y_{\mathrm{disc}}^i=\operatorname{Acc}\left(g_{\text {meta }}^i\right)=\frac{\sum_{j=1}^{M_i}\left(\hat{y}_{\operatorname{meta}(i, *)}^j==y_{\operatorname{meta}(i)}^j\right)}{M_i},
$$
表示 $\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$为该图中节点预测的平均准确率。</p>
<p>那么对于meta graph集合  $\mathcal{G}_{\text {meta }}=\left\{g_{\text {meta }}^i\right\}_{i=1}^K$中的所有图，由于这些图来组training graph，有明确的节点标签信息，因此$\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$可以计算在每个meta graph上的准确率。由于不同的meta graph的数据分布不同，所以我们可以得到 $\operatorname{GNN}_{\mathcal{S}}^*(\cdot, \cdot)$对不同数据分布图的适用性。</p>
<p>此外，在well-trained模型下，meta graph $g_{meta}^i$的表示 $\mathbf{Z}_{\text {meta }}^{(i, \star)}$和训练图表示 $\mathbf{Z}_{\mathcal{S}}^\star$之间差异用下式衡量：
$$
\mathbf{X}_{\mathrm{disc}}^i=D\left(\mathbf{Z}_{\text {meta }}^{(i, \star)}, \mathbf{Z}_{\mathcal{S}}^\star\right)=\frac{\mathbf{Z}_{\text {meta }}^{(i, \star)} \mathbf{Z}_{\mathcal{S}}^{\star \mathrm{~T}}}{\left|\left|\mathbf{Z}_{\text {meta }}^{(i, \star)}\right|\right|_2 \cdot\left|\left|\mathbf{Z}_{\mathcal{S}}^\star\right|\right|_2}
$$</p>
<h3 id="gnnevaluator如何估计无标签图的准确率">GNNEvaluator如何估计无标签图的准确率？</h3>
<p>训练GNNEvaluator:
$$
\min_\phi \sum_{i=1}^K \mathcal{L}_{\mathrm{reg}}\left(f_{\boldsymbol{\phi}}\left(\mathbf{X}_{\mathrm{disc}}^i, \mathbf{A}_{\mathrm{disc}}^i\right), y_{\mathrm{disc}}^i\right)
$$
对于每个meta graph $g_{\text {meta }}^i=\left\{\mathbf{X}_{\text {meta }}^i, \mathbf{A}_{\text {meta }}^i, \mathbf{Y}_{\text {meta }}^i\right\}$，基于结构 $\mathbf{A}_{\text {meta }}^i$和差异属性 $\mathbf{X}_{\text {meta }}^i$使用逻辑回归来预测准确率 $y_{\mathrm{disc}}^i$。损失函数采用MSE回归损失。</p>
<p>测试阶段，对于一个label不可见的预测图 $\mathcal{T} = (\mathbf{X}^\prime, \mathbf{A}^\prime)$，将其带入 $\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$得到它的表示 $\mathbf{Z}_{\mathcal{T}}^\star$，并与 $\mathbf{Z}_{\mathcal{S}}^\star$一同计算得到差异节点特征$\mathbf{X}_{\text {disc }}^{\mathcal{T}}$。再将$\mathbf{X}_{\text {disc }}^{\mathcal{T}}$和 $\mathbf{A}^\prime$带入 $f_\phi (\cdot,\cdot)$中，得到估计准确率 $\operatorname{Acc}(\mathcal{T})=\hat{y}_{\text {dist }}^{\mathcal{T}}$。该过程无需任何测试集标签参与评估。</p>
<p>图的差异与图的准确率匹配，所以可以用图差异来衡量与真实准确率的差异。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2023《Personalized Subgraph Federated Learning》Reading Notes</title>
      <link>https://JhuoW.github.io/posts/fedpub/</link>
      <pubDate>Mon, 19 May 2025 13:42:56 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fedpub/</guid>
      <description>ICML2023 &amp;#34;Personalized Subgraph Federated Learning&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>本文旨在解决不同子图作为不同的client，由于client是locally accessible，所以client间存在的missing edges无法被server捕获的情况。与FedSage仅仅基于自身连接来expand missing neighbors（并且最小化生成neighbor和其他所有子图的距离） 的missing neighbors生成方式不同，本文提出的FedPub考虑了子图的社区结构，即不同社区的子图相互之间的边连接关系应该较弱，处于同一个社区的子图应存在更强的边连接关系。</p>
<h2 id="personalized-subgraph-fl-overall">Personalized Subgraph FL (Overall)</h2>
<p>对于每个子图 $G_i \in \mathcal{G}$，传统的方法优化方式是学习global optimal parameters $\bar{\theta}$，使得该参数在所有clients上的总loss最小： $\min_{\overline{\theta}} \sum_{G_i \subset \mathcal{G}} \mathcal{L}\left(G_i ; \overline{\theta}\right)$。但是由于处在不同社区中的子图异质性很严重，并且不同社区的子图间的edges也很稀疏，这种强异质性很难学习到一个对所有子图都最优的 $\bar{\theta}$。因此本文提出，在同一个community的子图间共享参数，不同community的子图不共享参数，而不是使用全局通用参数 $\bar{\theta}$。因此，Personalized Subgraph FL的形式如下：</p>
<p>$$
\begin{aligned}
&amp; \min_{\left\{\boldsymbol{\theta}_i, \boldsymbol{\mu}_i\right\}_{i=1}^K} \sum_{G_i \subseteq \mathcal{G}} \mathcal{L}\left(G_i ; \boldsymbol{\theta}_i, \boldsymbol{\mu}_i\right), \quad \boldsymbol{\theta}_i \leftarrow \boldsymbol{\mu}_i \odot\left(\sum_{j=1}^K \alpha_{i j} \boldsymbol{\theta}_j\right) \\
&amp; \text { with } \alpha_{i k} \gg \alpha_{i l} \text { for } G_k \subseteq C \text { and } G_l \nsubseteq C
\end{aligned}
$$
其中 $K$是client数量， $\theta_i$是属于community $C$ 的client $G_i$的可训练权重， $\alpha_{ij}$是client $i$ 和 $j$的权重聚合coefficient。如果client $k$和 $i$属于同一个community，而client $l$和 $i$不属于同一个community，那么它们关于 $i$的参数聚合系数 $\alpha_{i k} \gg \alpha_{i l}$。通过这种方式模型可以隐式地考虑不同subgraph之间的关系。</p>
<p>虽然 $\alpha_{i,:}$指示了client之间的相关程度，但是未能指示那些参数与 $i$ 相关，这里用一个size和 GNN 权重相同的mask矩阵 $\mu_i$来过滤无关权重。</p>
<h2 id="fedpub-detailed">FedPub (Detailed)</h2>
<p>为了实现上式，首先要基于子图之间的相似度为图中的子图分配communities。由于每个子图是local accessibility的，服务器只能接收到每个client的GNN参数，所以需要从子图的GNN模型参数中获得辅助信息来计算子图之间的相似度。</p>
<h3 id="functional-embeddings-for-subgraph-similarities"><strong>Functional Embeddings for Subgraph Similarities</strong></h3>
<p>为了解决上述问题，本文通过向所有 local GNN 模型提供相同的输入，然后使用它们的输出计算相似度来测量 GNN 的functional similarity。基于stochastic block model构建一个有5个communities，每个community 100个节点的random community graph $\tilde{G}=(\tilde{\mathcal{V}}, \tilde{\mathcal{E}})$。令 $\tilde{\boldsymbol{h}}_i=\operatorname{AVG}\left(f\left(\tilde{G} ; \boldsymbol{\theta}_i\right)\right)$表示client $i$上的局部模型对 $\tilde{G}$的所有node embedding输出的均值。那么client $i$和 $j$之间的GNN functional similarity 可以用下式表示：</p>
<p>$$ S(i, j)=\frac{\tilde{\boldsymbol{h}}_i \cdot \tilde{\boldsymbol{h}}_j}{\left|\left|\tilde{\boldsymbol{h}}_i\right|\right|\left|\left|\tilde{\boldsymbol{h}}_j\right|\right|} $$</p>
<h3 id="personalized-weight-aggregation"><strong>Personalized Weight Aggregation</strong></h3>
<p>在得到子图之间的相似度衡量标准后，对于client $i$，可以计算client $j$与它的normalized subgraph similarity：</p>
<p>$$ \alpha_{i j}=\frac{\exp (\tau \cdot S(i, j))}{\sum_k \exp (\tau \cdot S(i, k))} $$</p>
<p>其中 $\tau$是一个相似度放缩参数。然后在同一个社区中的client尽可能共享更多的参数：</p>
<p>$$\overline{\boldsymbol{\theta}}_i \leftarrow \sum_{j=1}^K \alpha_{i j} \cdot \boldsymbol{\theta}_j$$</p>
<h3 id="adaptive-weight-masking">Adaptive Weight Masking</h3>
<p>上面加权聚合所有client local GNN参数的方式得到 $\overline{\boldsymbol{\theta}}_i$只考虑到how much each local model from other clients is relevant to $i$，因为 $\alpha_{i j}$仅用来衡量client $j$与client $i$在社区层面的相关程度，然后就依据相关性聚合来自client $j$的所有参数 $\boldsymbol{\theta}_j$。并没有考虑到聚合而来的参数 $\overline{\boldsymbol{\theta}}_i$中，哪些与client $i$是相关的。因此这里用一个可学习的参数mask $\boldsymbol{\mu}_i$来从 $\overline{\boldsymbol{\theta}}_i$中过滤出与client $i$有关的参数：</p>
<p>$$ \boldsymbol{\theta}_i=\overline{\boldsymbol{\theta}}_i \odot \boldsymbol{\mu}_i $$</p>
<p>这里得到的 $\boldsymbol{\theta}_i$作为client $i$上local GNN的参数。对于local client的训练过程，由于 $\boldsymbol{\mu}_i$仅在本地数据上训练，并不与其他client共享参数，因此容易与本地数据过拟合，即完全mask掉所有其他client的信息。为了缓解该问题，在局部损失函数上添加一个proximal term来使得 $\boldsymbol{\theta}_i$保留更多全局模型信息：
$$
\min _{\left(\boldsymbol{\theta}_i, \boldsymbol{\mu}_i\right)} \mathcal{L}\left(G_i ; \boldsymbol{\theta}_i, \boldsymbol{\mu}_i\right)+\lambda_1\left|\left|\boldsymbol{\mu}_i\right|\right|_1+\lambda_2\left|\left|\boldsymbol{\theta}_i-\overline{\boldsymbol{\theta}}_i\right|\right|_2^2
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021《Subgraph Federated Learning with Missing Neighbor Generation》Reading Notes</title>
      <link>https://JhuoW.github.io/posts/fedsage/</link>
      <pubDate>Sun, 18 May 2025 14:59:38 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fedsage/</guid>
      <description>NeurIPS2021 &amp;#34;Subgraph Federated Learning with Missing Neighbor Generation&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Subgraph Federated Learning旨在多个分布式的子图上训练一个图模型，并且子图之间没有数据共享。面临2个挑战：</p>
<p><strong>Q1:</strong> 如何将各个子图上的模型融合成一个全局可用的模型，使其可以处理来自全局图的任何请求？</p>
<p><strong>A1:</strong> 针对该挑战，本文提出FedSage将GraphSage和FedAvg结合。</p>
<p><strong>Q2:</strong> 如何处理local subgraph之间缺失的边？</p>
<p><strong>A2:</strong> 在FedSage的基础上添加一个<strong>邻居生成器</strong>。具体来说，为了得到missing neighbor generator，每个client首先通过随机移除一些节点和他们的边来<strong>损坏subgraph</strong>，然后基于损坏的图来和移除的节点来训练邻居生成器。邻居生成器不仅为子图中的每个节点生成missing neighbor number，也为每个节点生产missing neighbor features。然后用生成的节点邻居去修补subgraph，所谓模拟的跨子图交互。最后在修补好的子图上使用FedSage。</p>
<h2 id="collaborative-learning-on-isolated-subgraphsfedsage">Collaborative Learning on Isolated Subgraphs：FedSage</h2>
<p>对于一个被查询节点 $v\in V$，一个全局 $K$层GraphSage分类器 $F$通过融合 $v$和它的 $K$跳邻居来得到节点的预测值，其中每层的可学习参数为 $\phi = \{\phi^k\}^K_{k = 1}$，其中 $\phi^k$表示第 $k$层的权重矩阵。考虑一个子图 $G_i = \{V_i, E_i, X_i\}$， GraphSage通过一下式子计算节点 $v \in V_i$的第 $k \in [K]$层表示：</p>
<p>$$
h_v^k=\sigma\left(\phi^k \cdot\left(h_v^{k-1} || \operatorname{Agg}\left(\left\{h_u^{k-1}, \forall u \in \mathcal{N}_{G_i}(v)\right\}\right)\right)\right)
$$</p>
<p>其中 $\mathcal{N}_{G_i}(v)$表示节点 $v$在子图 $G_i$中的邻居，上式表示邻居节点聚合后和中心节点拼接，然后再用参数 $\phi^k$做变换后得到中心节点的表示。</p>
<p>在FedSage中，全局模型 $F$的参数 $\phi$通过 $e_c$次训练迭代得到。在每次训练迭代 $t$中，每个client $D_i$首先拷贝全局参数 $\phi$到本地，作为本地模型的初始参数，再基于自身的数据更新参数 $\phi_i$：</p>
<p>$$
\phi_i \leftarrow\phi-\eta \nabla \ell\left(\phi \mid\left\{\left(G_i(v), y_v\right) \mid v \in V_i^t\right\}\right)
$$</p>
<p>其中 $\eta$是学习率。然后中心服务器 $S$将所有 $M$个客户端上的最新参数 $\{\phi_i | i \in [M]\}$聚合并做平均，作为中心服务器上的模型参数。再下一次迭代时，将新的参数广播到所有客户端上进行新一轮的优化。</p>
<h2 id="fedsage">FedSage+</h2>
<h3 id="missing-neighbor-generator-neighgen">Missing Neighbor Generator (NeighGen)</h3>
<p><img loading="lazy" src="/posts/2025-05-18-Fedsage/image.png#center" alt=""  />
</p>
<p>邻居生成器NeighGen由编码器 $H^e$和生成器 $H^g$组成：</p>
<p><strong>编码器 $H^e$</strong>: 对于一个输入subgraph  $G_i = (V_i, E_i, X_i)$，一个由 $\theta^e$参数化的GNN模型为子图中的每个节点学习embeddings $Z_i = \{z_v | v \in V_i\}$ 。</p>
<p><strong>生成器 $H^g$</strong>: 基于node embedding生成它的missing neighbors。如图2所示， $H^g$包含两个模块dGen和fGen。其中dGen是一个由 $\theta^d$参数化的线性回归模型，用来预测给定节点的missing neighbors数量：
$$
\tilde{n}_v=\sigma\left(MLP_{\theta^d} ( z_v)\right) \in \mathbb{R}
$$</p>
<p>基于子图中的节点 $v$的embedding $z_v$生成它的预测missing neighbors数量 $\tilde{n}_v$。</p>
<p>fGen是一个由 $\theta^f$参数化的MLP用来基于节点 $v$的embedding $z_v$生成它的missing neighbor features。由于fGen为节点 $v$生成missing neighbor特征的个数取决于 $v$由多少个predicted missing neighbors，因此这里用变分的方式来基于 $z_v$生成它的missing neighbor features。具体来说，fGen有一个Gaussian noise生成器 $\mathbf{N}(0,1)$来生成随机噪声，以及一个随机采样器 $R$来从生成的邻居中采样 $\tilde{n}_v$个邻居特征：</p>
<p>$$
\widetilde{x}_v=R\left(\sigma\left(MLP_{\theta^f}\left(z_v+\mathbf{N}(0,1)\right)\right), \widetilde{n}_v\right)
$$</p>
<p>通过这种方式，可以基于 $z_v$定义一个随机邻居生成器。</p>
<p>图修补仿真：假设一个client中只有一个特定集合的节点具有跨subgraph missing neighbors，因此可以通过图损坏（impairing）和修复（mending）过程来模拟missing neighbor的情况。具体来说，为了训练NeighGen，对于每个subgraph $G_i$，随机取出 $h%$的节点和与这些节点相关联的边作为simulated missing neighbors of $G_i$，由此可以得到一个损坏后的图 $\bar{G}_i=\left\{\bar{V}_i, \bar{E}_i, \bar{X}_i\right\}$。对于 $\bar{G}_i$中的每个节点 $v$，它的真实被移除的邻居数量为 $n_v$， 使用NeighGen预测它的missing neighbor数量表示为 $\tilde{n}_v$，NeighGen的优化目标是使得预测的missing neighbors数量和被移除的neighbors数量相似：</p>
<p>$$
\mathcal{L}^d = \frac{1}{|\bar{V}_i|} \sum_{v \in \bar{V}_i} L_1^S(\tilde{n}_v - n_v)
$$</p>
<p>其中 $L_1^S (\cdot, \cdot)$是 L1 distance。对于特征，要求对于每个节点 $v \in \bar{V}_i$，它在 $G_i$中被移除的邻居表示为 $\mathcal{N}_{G_i}(v) \cap V_i^h$，其中 $\mathcal{N}_{G_i}(v)$表示 $v$在 $G_i$中的真实邻居集合， $V_i^h$表示impair后的邻居集合，因此 $\mathcal{N}_{G_i}(v) \cap V_i^h$共有 $n_v$个被移除邻居，对于节点 $v$的所有被移除邻居 $u \in \mathcal{N}_{G_i}(v) \cap V_i^h$， 要求真实被移除的邻居特征 $x_u$与为 $v$生成的邻居 $\widetilde{x}_v$之间的距离应该最小：</p>
<p>$$
\mathcal{L}^f = \frac{1}{\left|\bar{V}_i\right|} \sum_{v \in \bar{V}_i} \sum_{p \in\left[\widetilde{n}_v\right]} \min _{u \in \mathcal{N}_{G_i}(v) \cap V_i^h}\left(\left|\left|\widetilde{x}_v^p-x_u\right|\right|_2^2\right)
$$</p>
<p>其中 $\widetilde{x}_v^p$表示NeighGen为节点 $v$生成的第 $p$个missing neighbor。</p>
<p>为了使模型生成的邻居特征可以模拟跨subgraph的特性，因此将特征loss $\mathcal{L}^f$改写为如下形式：</p>
<p>$$
\mathcal{L}_i^f=\frac{1}{\left|\bar{V}_i\right|} \sum_{v \in \bar{V}_i} \sum_{p \in\left[\tilde{n}_v\right]}\left(\min_{u \in \mathcal{N}_{G_i}(v) \cap V_i^h}\left(\left|\left|\widetilde{x}_v^p-x_u\right|\right|_2^2\right)+\alpha \sum_{j \in[M] / i} \min_{u \in V_j}\left(\left|\left|H_i^g\left(z_v\right)^p-x_u\right|\right|_2^2\right)\right)
$$</p>
<p>其中后面一项 $\min _{u \in V_j}\left(\left|\left|H_i^g\left(z_v\right)^p-x_u\right|\right|_2^2\right)$中的 $x_u$表示另一个子图 $G_j$中的节点特征 $H_i^g\left(z_v\right)^p$表示NeighGen为 $G_i$中的节点 $v$生成的missing neighbor feature。<strong>因此这一项表示生成的新邻居要和其他子图中与它最相似的邻居尽可能一致，也就是生成的邻居要尽可能接近其他子图中的节点特征。</strong></p>
<p>但是这存在另一个问题， $x_u$在 $G_j$中，不符合FL的设定。为了解决该问题，这里将 $z_v$ send到client $D_j$中来计算 $\min _{u \in V_j}\left(\left|\left|H_i^g\left(z_v\right)^p-x_u\right|\right|_2^2\right)$，并计算生成器的梯度，然后将梯度send back到 $D_i$。因为编码器 $H_i^e$在 $D_i$上是local的，所以 $D_j$无法通过 $z_v$恢复出 $v$的原始特征。</p>
<p><strong>注意：<strong>从上面的 $\mathcal{L}^f$的式子第二项可以看出，由 $z_v$生成的邻居embedding $H_i^g\left(z_v\right)$要和</strong>所有其他子图中的所有节点</strong>的相似度尽可能高。也就是认为 $v$的missing neighbors需要来自于其他所有subgraphs。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>LLMs and Graphs</title>
      <link>https://JhuoW.github.io/posts/graphllm/</link>
      <pubDate>Sat, 25 May 2024 01:04:50 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/graphllm/</guid>
      <description>Large Language Model, GNNs, and Foundation Models</description>
      <content:encoded><![CDATA[<h1 id="1-harnessing-explanations-llm-to-lm-interpreter-for-enhanced-text-attributed-graph-representation-learning-tape">1. Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning (TAPE)</h1>
<p><strong>Shallow model:</strong> Encoding the textual attributes using shallow or hand-crafted features such as skip-gram or bag-of-words (BoW) which used in PgG and DGL <strong>are limited in the complexity of the semantic features they can capture.</strong></p>
<p><strong>LM:</strong> 指相对较小并且可以被fine-tune的模型。GIANT <strong>fine-tune</strong> an LM using neighborhood prediction task （在neighborhood prediction task上来微调LM模型）. GLEM <strong>fine-tune</strong> an LM to predict the label distribution from GNN&rsquo;s output (GLEM 用GNN预测的伪标签作为监督信号，让LM来fine tune 节点的文本表示)。这些工作需要大量的计算资源，并且由于要微调模型的参数，所以选取的LM相对较小，比如BERT和DeBERTa，因此缺乏LLM的推理能力。</p>
<p>LLMs refer to very large language models such as GPT-3/4.</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/TAPE.png#center" alt="TAPE"  />
</p>
<p><strong>The present work: LLM augmentation using explanations:</strong> 使用解释作为node feature。通过LLM来解释它的预测，这些解释传达了text与prediction的相关知识和LLM的推理步骤，这些解释信息更易于小LM模型吸收消化。如上图所示，首先绿框中的节点text属性通过自定义的prompt来询问LLM，比如GPT-3.5，让GPT来生成关于文本类别的<strong>预测排名list</strong>（黄色框所示），并且<strong>提供这些得到这些预测的解释理由</strong>。接着，原始text，LLM的预测，以及解释共同用来fine-tune LM，比如BERT或DeBERTa。然后，LM将他们转化为节点的features用于下游预测。</p>
<h2 id="formalization">Formalization</h2>
<p><strong>LM for text classification</strong>
$$
h_n=\mathbf{L M}\left(s_n\right) \in \mathbb{R}^d
$$
其中$s_n \in \mathcal{D}^{L_n}$是节点$n$的文本属性。LM是已经被预训练的模型如BERT和DeBERTa。LM可以将文本属性编码为文本的表示向量$h_n$。</p>
<p><strong>LLM and prompting</strong></p>
<p>输入token序列 $x=\left(x_1, x_2, \ldots, x_q\right)$，目标是输出token序列 $y=\left(y_1, y_2, \ldots, y_m\right)$。并且在输入token序列中加入prompt $p$来对输出施加约束。LLM旨在优化一下条件概率：
$$
p(y \mid \hat{x})=\prod_{i=1}^m p\left(y_i \mid y_{&lt;i}, \hat{x}\right)
$$
其中$\hat{x}=\left(p, x_1, x_2, \ldots, x_q\right)$是使用prompt约束的input token sequence。</p>
<h2 id="tape">TAPE</h2>
<h3 id="使用llms生成预测和解释">使用LLMs生成预测和解释</h3>
<p>LLMs的prompt包括文章的title和abstract，并要求LLMs预测paper的一个或多个类别标签，并且将这些类别标签按从高到低的概率排序，并且要求LLMs提供预测的解释理由，完整prompt如下图所示：</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/tape_prompt.png#center" alt="TAPE"  />
</p>
<p>Abstract输入节点对应paper的摘要，Title输入paper的题目，question则要求模型输出一个或多个paper的预测类别标签并按照可能性排序，同时给出预测的解释。</p>
<h3 id="fine-tuning-lm-interpreter-and-node-feature-extraction">Fine-Tuning LM Interpreter and Node Feature Extraction</h3>
<p>将LM作为LLM得到文本解释的“理解器”。给定两个预训练好的LMs $\mathrm{LM}_{\text {orig }}$ 和$\mathrm{LM}_{\text {expl }}$，他们的输入分别是原始文本特征$s^{\text {orig }}$和该文本特征的解释$s^{\text {expl }}$，这样我们可以分别得到原始文本和解释的text embeddings:
$$
h_{\text {orig }}=\mathrm{LM}_{\text {orig }}\left(s^{\text {orig }}\right) \in \mathbb{R}^{N \times d}, \quad h_{\text {expl }}=\mathrm{LM}_{\text {expl }}\left(s^{\text {expl }}\right) \in \mathbb{R}^{N \times d}
$$
然后要对LM进行Fine-tuning使其与下游任务。首先使用MLP将原始文本和解释的text embedding分别映射到标签空间：
$$
y_{\text {orig }}=\operatorname{MLP}_{\text {orig }}\left(h_{\text {orig }}\right) \in \mathbb{R}^{N \times C}, \quad y_{\text {expl }}=\operatorname{MLP}_{\text {expl }}\left(h_{\text {expl }}\right) \in \mathbb{R}^{N \times C}
$$
通过最小化分类的cross-entropy loss来对LM进行fine-tuning，从而使$\mathrm{LM}_{\text {orig }}$ 和$\mathrm{LM}_{\text {expl }}$可以分别学习到文本和解释与标签之间的关联（什么样的原始文本对应于什么样的标签、什么样的解释对应于什么样的标签）。</p>
<p><strong>Ranked prediction features</strong> LLM同时也给出了对于每个节点文本的类别可能性排名，同样也是有价值的信息。假设每个节点有$C=5$个可能的类别数，对这些类别分别做one-hot编码，节点$i$排名第1的类别为类别4，那么$p_{i,1} = [0,0,0,1,0]$，那么LLM为节点$i$预测的top-$k$个label可以拼接成一个$kC$维的向量。然后通过一个线性变换:
$$
h_{\text{pred}} = \operatorname{MLP}_{\text {pred }}(\operatorname{Concat}(p_{i,1}, \cdots, p_{i,k})) \in \mathbb{R}^{N\times d_P}
$$
可以得到每个节点的排序预测特征，如Figure 1中的$h_{\text{pred}}$所示。</p>
<p><strong>TAPE node feature: $\{[h_{\text {orig }}, h_{\text {expl }}, h_{\text{pred}}]\}$</strong>。其中$h_{\text {orig }}$和$h_{\text {expl }}$是通过LM 对下游任务标签做fine-tuning后的文本特征和解释特征，用来简历原始文本和解释与label之间的联系，$h_{\text{pred}}$是LLMs预测类别标签编码。</p>
<h3 id="gnn-training-with-tape-features">GNN Training with TAPE features</h3>
<p>对于TAPE的三种特征：LM fine-tuning的原始text embedding $h_{\text {orig }}$, LM fine-tuning的解释text embedding $h_{\text {expl }}$，以及LLM的prediction embedding $h_{\text{pred}}$，使用3个GNN模型分别预测labels:
$$
\begin{aligned}
\hat{y}_{\text{orig}} &amp;= \operatorname{GNN}_{\text{orig}}(h_{\text {orig }}, A) \in \mathbb{R}^{N \times C}, \\
\hat{y}_{\text{expl}} &amp;= \operatorname{GNN}_{\text{expl}}(h_{\text {expl }}, A) \in \mathbb{R}^{N \times C}, \\
\hat{y}_{\text{pred}} &amp;= \operatorname{GNN}_{\text{pred}}(h_{\text {pred }}, A) \in \mathbb{R}^{N \times C}
\end{aligned}
$$</p>
<p>然后基于不同特征得到的预测取平均可以得到模型最终的预测label：
$$
\hat{y}=\operatorname{mean}\left(\hat{y}_{\text {orig }}, \hat{y}_{\text {expl }}, \hat{y}_{\text {pred }}\right) \in \mathbb{R}^{N \times C}
$$</p>
<h1 id="2-exploring-the-potential-of-large-language-models-llms-in-learning-on-graphs">2. Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</h1>
<p><a href="https://arxiv.org/abs/2307.03393">paper</a></p>
<p>Q1: 能否利用LLMs来弥补图神经网络对contextualized knowledge和semantic comprehension理解不足的缺陷？</p>
<p>Q2: LLM能否独立运行于图结构任务？</p>
<h2 id="llms">LLMs</h2>
<h3 id="embedding-visible-llms">Embedding-visible LLMs</h3>
<p>可以获得words, sentences, documents的具体representations（embeddings），如BERT, Sentence-BERT和Deberta。</p>
<h3 id="embedding-invisible-llms">Embedding-invisible LLMs</h3>
<p>用户无法获取和操作embeddings，通常部署在web服务上，如ChatGPT，只能通过text来进行交互</p>
<h3 id="detailed-four-types-of-llms">Detailed four types of LLMs</h3>
<p><strong>Pre-trained Languagge Models (PLMs):</strong> 指相对较小的LLMs，比如Bert和Deberta，并且可以根据下游数据集进行fine-tuning，比如在下游数据集上fine-tune  Deberta然后取最后一个hidden state的embeddings作为text embedding。</p>
<p><strong>Deep Sentence Embedding Models:</strong> 使用PLMs作为base encoders，并且将训练好的PLMs进一步进行监督或对比学习<strong>预训练</strong>。这样的模型通常不需要Fine-tuning。</p>
<p><strong>Large Language Models:</strong> 与PLM相比，LLMs具有更强的能力和更多数量级的参数。</p>
<h2 id="llms-as-enhancers">LLMs-as-Enhancers</h2>
<p><strong>利用LLMs来增强节点的文本属性特征，然后用GNN生成预测。</strong></p>
<p>How LLMs can enhance GNNs by leveraging their <strong>extensive knowledge</strong> and <strong>semantic comprehension</strong> capability?</p>
<p>Challenge: 不同的LLMs能力不同，越强大的模型有更多使用限制，因此需要对不同的LLMs针对性设计使用策略来充分利用他们的能力。</p>
<h3 id="feature-level-enhancement">Feature-level Enhancement</h3>
<h4 id="1-cascading-structure-级联结构">1. Cascading Structure 级联结构</h4>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/cascading.png#center" alt="Cascading Structure"  />
</p>
<p>先试用embedding-visible的LLMs来对dataset中的text attribute做fine-tuning，然后生成每个节点的文本特征的embeddings，然后将这些embeddings作为node features，与图结构一起数据GNN中来训练GNN模型。</p>
<h4 id="2-iterative-structure-迭代结构">2. Iterative Structure 迭代结构</h4>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/iterative.png#center" alt="iterative Structure"  />
</p>
<p>如GLEM[1]，在E步根据真实标签和GNN预测的伪标签来训练PLM，在M步根据真实标签和PLM预测的伪标签和PLM学到的text embedding作为node feature来训练GNN模型，然后训练好的GNN模型和PLM模型都可以用来作为节点标签预测器。</p>
<h4 id="node-classification-comparison">Node Classification Comparison</h4>
<p>实验结果来看，</p>
<ol>
<li>在下游数据集上fine-tuned PLM模型取最后一层作为text embedding，然后GNN作为predictor的效果来看，fine-tuned PLM并不比简单点TF-IDF强。对于不同的text embedding方式（Fine-tuned PLM，PLM without fine-tuning，online sentence embedding）GNN的表现各不相同。</li>
<li>在监督训练数据较少的情况下，fine-tuned PLM和迭代结构的GLEM学习到的text embedding用到GNN后，得到的结果比普通的TF-IDF差。</li>
<li>使用Deep Sentence Embedding Models 如sentence-bert学习到的text embedding + GNN predictor的效果较好。</li>
<li>LLama的效果弱于deep sentence embedding models，说明简单的增加参数并不能生成对GNN有用的text embedding。</li>
</ol>
<h3 id="text-level-enhancement">Text-level Enhancement</h3>
<h4 id="1-tape">1. TAPE</h4>
<p>使用文本和LLM解释来fine-tuning PLM，LLM的分类解释和分类排序作为增强text embedding。</p>
<h4 id="2-knowledge-enhanced-augmentation-kea">2. Knowledge-Enhanced Augmentation (KEA)</h4>
<p>使用额外的knowledge来增强PLM。</p>
<h4 id="node-classification-comparison-1">Node Classification Comparison</h4>
<p>实验结果来看，</p>
<ol>
<li>TAPE的效果主要受益于LLMs生成的文本解释。</li>
<li>TAPE原文中采用PLM （Deberta）作为LM，将text attribute和explanation以及任务的label用来fine-tune PLM。而local sentence embedding model e5-large不fine-tune 直接用text attribute以及LLM的explanation来输出embeddings，相比于TAPE中使用的fine-tuned PLM，e5得到了更好的效果。</li>
</ol>
<h2 id="llms-as-predictors">LLMs-as-Predictors</h2>
<p><strong>LLMs作为独立的predictor</strong></p>
<p>How LLMs can be adapted to explicit graph structures as a <strong>predictor</strong>?</p>
<p>Challenge: 如何设计prompt使得LLM可以处理图中的structure和attribute信息。</p>
<p>直接使用LLM来预测节点类别标签可以使用以下几种prompt策略：</p>
<ol>
<li><strong>Zero-shot prompts:</strong> 给定单一节点的文本信息，让LLM预测它的类别标签，如下面的prompt所示。</li>
</ol>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/zero_shot_prompt.png#center" alt="zero-shot prompt"  />
</p>
<p>​       其中，Paper是一个节点的text attribute，Task提供可能的类别标签，然后prompt要求LLM输出一个最可能的类别。</p>
<ol start="2">
<li><strong>Few-shot prompts:</strong> 如下图所示，先按照zero-shot的形式给出一些node samples的 prompts，<strong>以及这些samples的ground-truth标签</strong>。最后，给出目标节点的prompt，要求LLMs输出节点的类别。</li>
</ol>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/few_shot_prompt.png#center" alt="few-shot prompt"  />
</p>
<ol start="3">
<li><strong>Zero-shot prompts with CoT (Chain-of-Thoughts):</strong> 基于zero-shot prompt，在prompt中进一步要求LLM生成思考过程，也就是<strong>Think it step by step and output the reason in one sentence</strong>  (用一句话来概括推理步骤)。</li>
</ol>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/zero_shot_cot.png#center" alt="zero-shot cot"  />
</p>
<ol start="4">
<li><strong>Few-shot prompts with CoT:</strong> 使用第三个prompt的zero-shot prompts with CoT来分别为多个sample生成推理步骤的sentences，然后将这些这些samples的文本内容、Ground-truth标签和CoT process共同作为prompt，并且要求LLM为当前的目标node输出预测标签以及CoT。</li>
</ol>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/few_shot_cot.png#center" alt="few-shot cot"  />
</p>
<h1 id="3-one-for-all-towards-training-one-graph-model-for-all-classification-tasks">3. One for All: Towards Training One Graph Model for All Classification Tasks</h1>
<p><strong>What is in-context learning?[2]</strong></p>
<p>&ldquo;In-Context Learning is a way to use LLMs to learn tasks given only a few examples. During in-context learning, we give the LM a prompt that consists of a  list of input-output pairs that demonstrate how to perform a task. At  the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next  tokens. For example, to answer the two prompts below, the model needs to examine the training examples to figure out the input distribution  (financial or general news), output distribution (Positive/Negative or  topic), input-output mapping (sentiment or topic classification), and  the formatting. &quot;</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/icl.gif#center" alt="ICL"  />
</p>
<p>如上图（from [2]）所示，在prompt中包含一些上下文input-output pairs来演示（demonstrate）如何执行任务。在这些prompt的最后添加text input token来让LLM学习上下文中的演示来执行text input token的任务。</p>
<h2 id="challenge">Challenge</h2>
<p>Foundation Model指用单一模型来解决多个任务。但是Graph Foundation Model面临以下挑战：（1）不同来源的图数据在feature表征上通常完全不同。比如molecular graph中的节点特征是其中原子nominal feature的索引，e-commerce networks中的节点特征通常用Bag-of-Word来编码。这些不同来源的图数据特征维度、语义信息和尺度的差别都很大，几乎不可能用同一个模型来学习他们的表示。（2）不同的下游任务涉及图的不同部分（节点级、边级、图级）需要不同的策略和方法来学习表示。（3）如何设计统一的模型来实现跨领域和in-context learning是不确定的。</p>
<h2 id="ofa-one-for-all">OFA (One-for-All)</h2>
<h3 id="将不同domian的图转化为统一的tag形式">将不同Domian的图转化为统一的TAG形式</h3>
<p>用human-interpretable language来描述节点和边的属性，从而使LLM可以将这些text attribute编码到同一个空间。具体来说，通过以下方式来生成每个节点的text feature:</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/node_text_feature.png#center" alt="text feature"  />
</p>
<p>将不同domain的graph的节点属性用统一形式的文本来描述，如上图中，对于一个节点的特征，它的文本描述以<strong>Feature node</strong>开头，后面的每个$&lt;\text{feature describe}&gt;:&lt;\text{feature content}&gt;$是该节点的一个type-content文本对。如对于一个molecule graph，其中的一个节点是原子，那么它的type是<strong>Atom</strong>，它的content是该原子的属性文本描述<strong>Carbon, Atomic number 6, helix chirality</strong>。同理，边的text feature以<strong>Feature edge</strong>开头进行文本描述。</p>
<p>在用以上方式得到节点$v_i$的text feature $s_{v_i}$和边$e_{ij}$的text feature $s_{e_{ij}}$后，用LLM（这里用sentence transformer）来将每个节点和边的text feature编码为vector embeddings：
$$
x_i = \operatorname{LLM}(s_{v_i}), \quad x_{ij} = \operatorname{LLM}(s_{e_{ij}})
$$</p>
<h3 id="用nodes-of-interest-noi来统一不同的图任务">用Nodes-of-Interest (NOI)来统一不同的图任务</h3>
<p>图上的任务主要分为3中：node-level tasks，link-level tasks和graph-level tasks。如果要用一个模型来处理这些任务的话，<strong>需要将这些任务统一为一个任务以便于在图数据上训练</strong>。</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/NOI.png#center" alt="NOI"  />
</p>
<p>Nodes-of-Interest (<strong>NOI</strong>)指的是一个任务的<strong>目标节点</strong>，如上图的蓝色节点所示，表示为$\mathcal{T}$。对于节点级任务，NOI是待预测节点集合，边级任务NOI是待预测是否有边的节点对，图级任务NOI是待预测图中的所有节点。若一个NOI节点$v$的$h$-hop局部子图表示为$\mathcal{S}_h(v)$，那么NOI中所有目标节点的局部子图共同构成了一个<strong>NOI subgraph</strong>，表示为$\mathcal{G}_h (\mathcal{T})$。
$$
\mathcal{G}_h(\mathcal{T})=\bigcup_{v \in \mathcal{T}} \mathcal{S}_h(v)=\left\{\bigcup_{v \in \mathcal{T}} \mathcal{V}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{E}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{R}_v^h\right\}   \quad \text{NOI中所有节点的局部子图共同构成}
$$
<strong>NOI prompt node</strong>：定义NOI prompt node，该节点的文本信息用来描述任务，比如某个数据集上的节点分类、图分类等。NOI prompt node的节点text feature以如下形式构建：</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/NOI_prompt_node.png#center" alt="NOI"  />
</p>
<p>即每个数据集的一个task对应于一个prompt node，该node用来描述task。这样，把不同的图任务都用节点的text feature这种统一的形式来表达。</p>
<h3 id="graph-in-context-learning的图提示范式graph-prompting-paradigm">Graph In-Context Learning的图提示范式（Graph Prompting Paradigm）</h3>
<p>LLM的一大特性就是它可以通过prompt来实现in-context learning，使得模型可以在不用fine-tuning的情况下适应于不同的任务。比如在few-shot场景下，目标是基于一篇paper的摘要和内容预测它的类别，我们可以为LLM提供每个类别的$k$篇papers作为context加入prompt中，来指导模型基于这些提供的context来生成目标摘要和内容的类别预测。（通过一些<strong>任务相关的其他信息</strong>来指导模型对目标样本的预测）</p>
<p>本文发现实现图上in-context learning的核心在于操作输入图使其和下游任务对齐。<strong>Graph Prompting Paradigm (GPP)</strong> 即图提示范式旨在操作输入图使其可以从输入数据本身获得任务相关的信息。如图2中的虚线所示，用来描述任务的NOI prompt node与所有NOI node建立连接（任务的目标节点），表示在这些NOI nodes上执行对应prompt的任务（如节点分类，链路预测，图分类等）。图中的$p2t$和$t2p$ edge表示NOI节点和prompt node之间的边。下一步建立NOI prompt node和具体类别之间的联系，如图2中的<strong>Class Node</strong>用来描述该分类任务的每个类别的类信息，任务有多少个类，就有多少个Class Node。Class Node的Text feature如下图所示：</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/class_node.png#center" alt="NOI"  />
</p>
<p><strong>Zero-shot Learning:</strong> 对于NOI中的一个节点$q$，用于描述它的任务的NOI prompt node $p_q$，以及Class Node $\{c_i | i \in [N]\}$ ，其中$N$为任务的类别数。prompt node，class node，以及和prompt node 连接的所有边（包括与NOI连接的边）共同构成了<strong>prompt graph</strong> $\mathcal{P}=\left(\mathcal{V}_p, \mathcal{E}_p, \mathcal{R}_p\right)$。</p>
<p>然后，prompt graph $\mathcal{P}=\left(\mathcal{V}_p, \mathcal{E}_p, \mathcal{R}_p\right)$和NOI subgraph (即NOI节点的局部子图)相结合为<strong>通用graph model</strong>的输入图$\mathcal{G}_m=\left(\mathcal{V}_q^h \cup \mathcal{V}_p, \mathcal{E}_q^h \cup \mathcal{E}_p, \mathcal{R}_q^h \cup \mathcal{R}_p\right)$。如图2中的（a）(b)（c）都是graph model的输入图。通过模型的学习，可以得到每个Class node 的embeddings，如类别$c_i$的Class node embedding为$h_{c_i}$。因为$c_i$与任务相关的prompt node 连接，而prompt node与目标NOI node 连接，因此可以用$h_{c_i}$来推断NOI node属于类别$c_i$的概率：
$$
P[\text { NOI belongs to class } i]=\sigma\left(\operatorname{MLP}\left(h_{c_i}\right)\right)
$$</p>
<h1 id="4-talk-like-a-graph-encoding-graphs-for-large-language-models">4. Talk like a Graph: Encoding Graphs for Large Language Models</h1>
<p>prompt engineering的目的是找到一个合适的方式使LLM $f$可以解析问题$Q$，并且得到他的Answer $\mathcal{A}$，即$\mathcal{A} = f(Q)$。本工作的目标是为LLM$f$提供图$G$，使得LLM可以对图做推理QA，即$\mathcal{A} = f(G,Q)$。在该工作中，固定LLM $f$的参数不变，并引入一个图编码函数（graph encoding function）$g(G): G \to W$用于将图结构数据编码成text，以及一个问题重解析函数$q(Q): W \to W$。训练数据$D$有图$G$，问题$Q$和回答$S$组成，即每个训练数据$(G,Q,S) \in D$。训练目标是固定大语言模型$f$不变，找到最佳的图编码函数$g$和问题重解析函数$q$，使得给定训练$G$和$Q$，得到回答$S$的分数最高：
$$
\max _{g, q} \mathbb{E}_{G, Q, S \in D} \operatorname{score}_f(g(G), q(Q), S)
$$</p>
<h3 id="graph-encoding-function-gg">Graph encoding function $g(G)$</h3>
<p>$g(G)$用于将$G$映射为LLM可以处理的token：首先，encode图中的节点，然后encode图中的边。具体编码技术如下图所示：</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/encoding_graph.png#center" alt="NOI"  />
</p>
<p><strong>Encoding Nodes</strong>:</p>
<ol>
<li>
<p>整型节点编码：$G$ describes a graph among nodes 0,1,2,3,4,5,6,7</p>
</li>
<li>
<p>使用well-known English first names：$G$ describes a friendship among James, Robert, Michael, Mary.</p>
</li>
<li>
<p>使用电视剧《权力的游戏》和《南方公园》中流行的角色名字。</p>
</li>
<li>
<p>包括美国政治家的名字。</p>
</li>
<li>
<p>用字母表示的。</p>
</li>
</ol>
<p><strong>Representing Edges</strong>:</p>
<ol>
<li>用括号表示边：The edge in $G$ given as (0,1), (0,2), &hellip;, (6,7), (7,8)</li>
<li>Friendship: source node and target node are friends. 如 We have the following edges in $G$: James and Robert are friends, &hellip; Jennifer and Linda are friends。对应于上面的well-known English first name表示节点</li>
<li>Coauthorship: 比如 James and Robert wrote a paper together 来表示一条边</li>
<li>Social network: James and Robert are connected来表示一条边</li>
<li>Arrows: A-&gt;B 来表示边</li>
<li>Incident: Node 8 is connected to nodes 3,7 来表示每个节点的<strong>邻域</strong>。</li>
</ol>
<h2 id="experiments">Experiments</h2>
<p>使用PaLM 62B作为LLM，在不同图任务以及不同图编码器下的准确率比较，最有效的prompt (zero-shot、zero-cot、few-show、cot、cot-bag)用下划线标出，最佳图编码器用加粗标出。实验使用ER Graph作为数据集，ER Graph的统计数据如下表所示，可以看出平均节点数为12.37，平均边数为39.79，平均度为5.70。对于边存在任务，有53.96%的情况不存在边，对于cycle check任务，有81.96%的情况存在cycle（因为ER graph很可能存在cycle）。</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/graphqa_dataset.png#center" alt="graphqa_datase"  />
</p>
<p>从下面的实验结果可以看出，LLM在所有prompt heuristic的所有graph encoding function上预测的最高边存在概率为44.5%，76%的情况存在cycle。而在Node degree 任务上，均与真实平均度5.7差距较大，平均边数也与真实情况差距较大。</p>
<p>简单的Prompt比如zero-shot 在简单的任务上比复杂的prompt比如zero-cot效果更好，因为简单的任务无需多跳推理。</p>
<p>graph encoding function对LLM影响巨大。</p>
<p>整型节点编码可以提升算数性能，比如node degree、node count和edge count的预测。</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/graphqa_exp.png#center" alt="graphqa_exp"  />
</p>
<h1 id="5-can-language-models-solve-graph-problems-in-natural-language">5. Can Language Models Solve Graph Problems in Natural Language?</h1>
<p>[1] Learning on Large-scale Text-attributed Graphs via Variational Inference.</p>
<p>[2] <a href="https://medium.com/@francescofranco_39234/in-context-learning-icl-a775cd8b7261">https://medium.com/@francescofranco_39234/in-context-learning-icl-a775cd8b7261</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2023《Ordered GNN：Ordering Message Passing to Deal with Heterophily and Over-smoothing》 Reading Nodes</title>
      <link>https://JhuoW.github.io/posts/orderedgnn/</link>
      <pubDate>Sun, 16 Jul 2023 17:47:14 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/orderedgnn/</guid>
      <description>ICLR2023 &amp;#34;Ordered GNN：Ordering Message Passing to Deal with Heterophily and Over-smoothing&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2302.01524">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>多层message passing后，GNN会导致Over-smoothing使得节点表示趋同。另一方面，标签不同的相邻节点特征会混合，导致不同标签节点边的难以区分，即Heterophily问题。在本文中，提出对传递到节点的message进行排序，即表示向量的特定neuron块编码特定hop的消息。通过将节点rooted computation tree的层次和表示向量neuron 块对齐。如下图所示，3层GNN对节点$v$的输出$h_v^{(3)}$的前$P_v^{(0)}$个neurons 编码1 hop邻居信息，$[P_v^{(0)}, P_v^{(1)}]$编码了第1层另据的信息。通过以确定的顺序编码邻居信息，来避免hops的特征融合，即一个节点的embedding的神经元要和计算书的层次对齐，不同层分配不同的neuron。也就是按照顺序将不同层的邻居信息编码到最终表示$h_v^{(k)}$的不同维度区间中。</p>
<p><img loading="lazy" src="/posts/2023-07-16-OrderedGNN/1.png" alt=""  />
</p>
<h1 id="approach">Approach</h1>
<h2 id="aligning-rooted-tree-with-node-embedding">Aligning Rooted-Tree with Node Embedding</h2>
<p>对于一个节点$v$，显然它的第$k-1$层rooted-tree $\mathcal{T}^{(k-1)}_v$是$k$层rooted-tree  $\mathcal{T}^{(k)}_v$的子树:
$$
\mathcal{T}_v^{(0)} \subseteq \mathcal{T}_v^{(1)} \subseteq \cdots \subseteq \mathcal{T}_v^{(k)} \subseteq \cdots \subseteq \mathcal{T}_v^{(K)}
$$
随着$k$的增加，$\mathcal{T}_v^{(K)}$会变得越来越大且复杂，且包含之前层的所有信息。所以$\mathcal{T}_v^{(K)}$需要更多neuron (最终表示向量$h_v^{(k)}$中的维度) 来编码信息。由于$\mathcal{T}_v^{(k-1)}$是$\mathcal{T}_v^{(k)}$的子树，所以在表示向量$h_v^{(k)}$中，编码tree $\mathcal{T}_v^{(k)}$信息的neurons要包含编码 tree$\mathcal{T}_v^{(k-1)}$信息的neurons。具体来说，关于节点$v$的$k-1$层rooted-tree $\mathcal{T}_v^{(k-1)}$，它的信息会被编码到$h_v^{(K)}$的前$P_v^{(k-1)}$个neuron中（维度），对于下一个层次的rooted-tree $\mathcal{T}_v^{(k)}$，它会被编码到$h_v^{(K)}$的前$P_v^{(k)}$个neuron（维度）中。因为$\mathcal{T}_v^{(k-1)}$是$\mathcal{T}_v^{(k)}$的子树，所以$P_v^{(k-1)} \leq P_v^{(k)}$。  $v$的$K$层最终表示$h_v^{(K)}$，它的每个维度是一个neuron，前$P_v^{(k-1)}$个neurons编码了前$k-1$ hop邻居的信息，前$P_v^{(k)}$个neurons编码了前$k$ hop邻居的信息， 在两个分割点$P_v^{(k-1)}$和$P_v^{(k)}$之间的neurons要编码的是<strong>第$k$ hop邻居</strong>的信息。所以节点$v$在$K$层GNN下的最终embedding$h_v^{(K)} \in \mathbb{R}^D$ 会被$K+1$个分割点分成$K+1$块，其中前$P_v^{(0)}$个neurons编码的是节点$v$的自身信息，$P_v^{(k)}$为$h_v^{(K)}$的split point，且：
$$
P_v^{(0)} \leq P_v^{(1)} \leq \cdots \leq P_v^{(k)} \leq \cdots \leq P_v^{(K)} = D
$$</p>
<h2 id="the-split-point">The Split Point</h2>
<p>分割点$P_v^{(k)}$是一个index，会将$D$维node embedding 分为2块，$[0, P_v^{(k)}-1]$的neurons编码了前$k$层邻居的信息。 定义一个$D$维gating向量$g^{(k)}_v = [1,1,1,1,1,1,0,0,0,0,0]$其中前$P_v^{(k)}$个entries是1， 后面为0，即筛选出前$k$层要编码进的neurons：
$$
h_v^{(k)}=g_v^{(k)} \circ h_v^{(k-1)}+\left(1-g_v^{(k)}\right) \circ m_v^{(k)}
$$
其中第$k$层的信息$h_v^{(k-1)}$保留在第$k+1$层embedding $h_v^{(k)}$的前$P_v^{(k)}$个neuron中，而$h_v^{(k)}$的后面部分neuron编码新聚合的邻居信息，通过这种方式，将每一个hop的信息分开。 再下一层时， $h_v^{(k)}$的信息就会被编码到$D$个neurons中的前$P_v^{(k+1)}$个neuron中，那么其实$P_v^{(k)}$到$P_v^{(k+1)}$之间的neuron实际上只包含了$m_v^{(k)}$的信息，即第$k$个hop的信息。以这种方式将每一个hop的邻居信息按顺序编码到最终表示向量$h_v^{(K)}$中。</p>
<p>然而binary gating vector $g^{(k)}_v$ 来自于离散操作，导致$g^{(k)}_v$的学习不可微，使得每个hop用多少神经元来保存不可自适应学习。 为了解决该问题，将$g^{(k)}_v$定义为关于root node $h_v^{(k-1)}$和messge $m_v^{(k)}$ 的函数，$g_v^{(k)}$的split point $P_v^{(k)}$分割了前面所有层的信息和第$k$ hop的邻居信息，这里将$\hat{g}_v^{(k)}$定义为一个从右向左的累加向量：
$$
\hat{g}_v^{(k)}=\operatorname{cumax}_{\leftarrow}\left(f_{\xi}^{(k)}\left(h_v^{(k-1)}, m_v^{(k)}\right)\right)=\operatorname{cumax}_{\leftarrow}\left(W^{(k)}\left[h_v^{(k-1)} ; m_v^{(k)}\right]+b^{(k)}\right)
$$
进一步 由于$\hat{g}_v^{(k)}$是可学习的，它的split point为$P_v^{(k)}$ ， 因此并不能保证$\hat{g}_v^{(k+1)}$的split point $P_v^{(k+1)}$会相对于$P_v^{(k)}$右移，因此本文提出了可微OR操作来确保$P_v^{(k+1)}\geq P_v^{(k)}$：
$$
\tilde{g}_v^{(k)}=\operatorname{SOFTOR}\left(\tilde{g}_v^{(k-1)}, \hat{g}_v^{(k)}\right)=\tilde{g}_v^{(k-1)}+\left(1-\tilde{g}_v^{(k-1)}\right) \circ \hat{g}_v^{(k)}
$$</p>
<h2 id="putting-it-all-together">Putting it All Together</h2>
<p>Ordered GNN的第$k$层message passing如下：</p>
<p>首先计算第$k$hop邻居的message：
$$
m_v^{(k)}=\operatorname{MEAN}\left(\left\{h_u^{(k-1)}: u \in \mathcal{N}(v)\right\}\right)
$$
计算当前层的split point $P_v^{(k)}$和门控向量$\tilde{g}_v^{(k)}$：
$$
\begin{aligned}
&amp; \hat{g}_v^{(k)}=\operatorname{cumax}_{\leftarrow}\left(f_{\xi}^{(k)}\left(h_v^{(k-1)}, m_v^{(k)}\right)\right) \
&amp; \tilde{g}_v^{(k)}=\operatorname{SOFTOR}\left(\tilde{g}_v^{(k-1)}, \hat{g}_v^{(k)}\right)
\end{aligned}
$$
利用门控向量分块聚合当前节点embedding和新加入的hop信息：
$$
h_v^{(k)}=\tilde{g}_v^{(k)} \circ h_v^{(k-1)}+\left(1-\tilde{g}_v^{(k)}\right) \circ m_v^{(k)}
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2023《MLPInit：Embarrassingly Simple GNN Training Acceleration with MLP Initialization》 Reading Nodes</title>
      <link>https://JhuoW.github.io/posts/mlpinit/</link>
      <pubDate>Sat, 15 Jul 2023 15:43:09 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/mlpinit/</guid>
      <description>ICLR2023 &amp;#34;MLPInit：Embarrassingly Simple GNN Training Acceleration with MLP Initialization&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2210.00102">paper</a></p>
<p>GNN中的层次叠加需要稀疏矩阵乘法计算带来较大的计算开销，而MLP仅使用node feature可以避免此问题。本文发现大多数message-passing通过将训练参数设置为相同shape，可以推导出等效的MLP（PeerMLP），而使用PeerMLP来作为GNN的初始化参数可以相较于仅使用PeerMLP，效果提升极大。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/1.png" alt=""  />
</p>
<p>从上图的蓝线可以看出，GNN通常需要更多的训练迭代次数才可以达到收敛，因为其中涉及复杂的稀疏矩阵乘法计算。而MLP不使用结构信息，训练速度更快，因此本文发现MLP和GNN可以有相同的训练权重空间，因此 <strong>Can we train GNNs more efficiently by leveraging the weights ofconverged MLPs?</strong>  本文进一步发现，对于一个GNN和它对应的PeerMLP （相同的weight），在PeerMLP上训练的权重可以优化GNN。基于该发现，图上训练好的PeerMLP作为GNN的权重矩阵$W$, 然后再考虑结构信息，可以发现GNN的效果相较于PeerMLP有很大的提升。 如表2所示，其中PeerMLP和GNN有相同的权重空间，首先在图上训练PeerMLP，得到收敛时的最有参数$w^\star_{mlp}$，PeerMLP的预测结果为$f_{m l p}\left(\mathbf{X} ; w_{m l p}^\star\right)$， 然后直接使用不训练而直接使用$w_{m l p}^\star$作为GNN的参数，即$f_{g n n}\left(\mathbf{X}, \mathbf{A} ; w_{m l p}^\star\right)$,可以看出，在考虑图结构后，GNN即使不训练，直接使用PeerMLP的权重矩阵，效果也有巨大提升。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/2.png" alt=""  />
</p>
<p>受此启发，本文提出了用收敛的PeerMLP最优权重矩阵，作为GNN的初始化权重。从图1的红线可以看出，相较于随机初始化的GNN，MLPInit初始化的GNN在更少的epoch到达收敛 并且可以达到和相似的准确率。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/3.png" alt=""  />
</p>
<p>从上表可以看出GNN的Propagation操作$AZ$的前向计算和反向梯度传播的耗时都远远超过Feature Transformation操作$WX$。Feature Tran的操作相对与Propagation，计算成本几乎可以忽略不计，所以如果预训练操作得到的$W$可以使得训练GNN时的epoch大幅下降，可以使模型更加高效。如下表所示，训练PeerMLP的时间再加上的权重迁移到GNN后的fine-tuning时间， 远少于在GNN上直接训练随机初始化参数的时间。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/5.png" alt=""  />
</p>
<p>从下图同样可以看出PeerMLP的参数$w_{mlp}$的训练趋势，PeerMLP训练过程中每个epoch的$w_{mlp}$直接迁移到GNN上计算CE损失，可以发现使得MLP 的CE Loss下降的$w_{mlp}$同样可以使得GNN以同样的趋势下降。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/4.png" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Fraud Detection based on Graph Neural Networks</title>
      <link>https://JhuoW.github.io/posts/fd/</link>
      <pubDate>Sat, 13 May 2023 16:14:56 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fd/</guid>
      <description>基于图神经网络的异常检测</description>
      <content:encoded><![CDATA[<h1 id="1-label-information-enhanced-fraud-detection-against-low-homophily-in-graphs-www-23">1. Label Information Enhanced Fraud Detection against Low Homophily in Graphs (WWW &lsquo;23)</h1>
<h2 id="introduction">Introduction</h2>
<p>GNN4FD存在问题： 大多数基于GNN的欺诈检测器难以泛化到low homophily网络中，除此之外，如何充分利用label信息也是Fraud detection的重要因素。即如果一个Fraud node的邻居都是benign nodes，那么这样的图就是heterophily or low homophily graph，由于GNN的neighborhood aggregation机制，target node的表示会和它的邻居相似，无论他们的label是否不同，这样会使得GNN难以区分位于异质邻域内的Fraud nodes。另外， 现有的GNN4FD方法利用label信息的能力有限，这些方法仅在训练阶段使用label信息作为监督信号，但是在设计message passing 机制的过程中并没有使用label信息。</p>
<p>为了解决上述2个挑战，本文提出GAGA: 基于分组聚合的Transformer。 GAGA首先提出了一种预处理策略Group Aggregation (GA, 分组聚合)，然后每个节点的原始邻居特特征被分组为序列数据。 然后提出一种科学系的编码方式来编码structural，relational 和label信息 （全局），即整个图的relational encoding，group encoding 和 hop encoding （图中又几个relation就有几个relational embedding，取几个hop就又几个hop embedding..）。 最后用多头attention为每个节点聚合embedding sequence.</p>
<h2 id="preliminaries">Preliminaries</h2>
<p><strong>Multi-relational fraud graph construction</strong>  Multi-relational fraud graph $\mathcal{G}(\mathcal{V}, \mathcal{E}, \mathcal{X}, \mathcal{Y})$, 其中节点集$\mathcal{V}=\left\{v_1, v_2, \ldots, v_N\right\}(N=|\mathcal{V}|)$，$R$ 个邻接矩阵$\mathcal{E}=\left\{\mathbf{A}_1, \mathbf{A}_2, \ldots, \mathbf{A}_R\right\}(R=|\mathcal{E}|)$的多关系图 （$R$个关系）。节点feature vectors $X=\left\{\mathbf{x}_1, \mathrm{x}_2, \ldots, \mathrm{x}_N\right\}$以及节点的label集合$\mathcal{Y}$。 对于一个relation的邻接矩阵$\mathbf{A}_r$，如果$\mathbf{A}_1[u,v]=1$，那么在关系$r$下节点$u$和$v$被连接。每个节点$v \in \mathcal{V}$ 有一个$d$维feature vector $\mathbf{x}_v \in \mathbb{R}^d$。 在基于Graph的fraud detection中，我们考虑半监督场景，其中一小部分节点 $\hat{\mathcal{V}} \supset \mathcal{V}$是有label的 （$y=1$表示该节点为fraud node，$y=0$表示该节点为benign node）所以对于fraud graph，节点class数为2。</p>
<h2 id="gaga">GAGA</h2>
<p><img loading="lazy" src="/posts/2023-05-13-FD/1.png#center" alt=""  />
</p>
<p>上图为GAGA的框架。第一步为Group Aggregation，为预处理过程，为每个节点计算多条邻居信息，并且每跳内的信息分组表示（一跳内 label=0，label=1，label=None的节点分别聚合）。这样会为每个节点生成一系列embeddings。第二步中，定义三种类型可学习的embeddings：hop embeddings, relation embeddings,group embeddings，即如果每个节点有$K$-hop邻居参与聚合，那么hop embeddings 是一个$K \times d_H$ 矩阵，每个hop（结构特征）用一个$d_H$维向量表示。 同理relational embedding是一个$R \times d_H$矩阵，每个relation 用一个$d_H$维向量表示。一共存在3种group（$y=1$的group， $y=0$的group， 无标签邻居的group），所以group embeddings是一个$3 \times d_H$的矩阵，每种group 表示为一个$1 \times d_H$的向量。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/2.png#center" alt=""  />
</p>
<p>对于第一步得到的某一个节点$v$在relation 0的邻接矩阵$\mathbf{A}_0$下的第<strong>2</strong>跳邻居的fraud neighbors ($y=1$)的group embedding $g_v (r=0, h=2, y=1)$，为这个embedding融合hop信息 + relation信息+ group 信息得到 $x_v = g_v (r=0, h=2, y=1) + E_h(1) + E_r(0) + E_g(1)$    关于节点$v$的某个group的融合结构，关系特征的表示向量。最后一步将一个节点的所有多关系多hop的group向量用transformer合并然后输入MLP种来预测节点label。</p>
<p>即一个节点会生成 $\#relation (\#hop * (\#class+1) + 1)$个group 向量，每个group向量属于某个relation下的某个hop，这个group向量属于那个relation就加上这个relation的一维encoding，属于那是个hop就加上这个hop的1维encoding，这个group是0/1/None group就再加上对应group的encoding，从而得到这个group的最终encoding。</p>
<h3 id="group-aggregation">Group Aggregation</h3>
<p>对于Fraud detection任务，每个节点的label有3种情况，分别为 benign node $y=0$, Fraud node $y=1$, unlabeled node $y = None$，所以对于每个节点，它的第$k$hop邻居可以被分为3个group，每个group的节点做聚合：
$$
\mathbf{H}_g^{(k)}=\left[\mathbf{h}^{-}, \mathbf{h}^{+}, \mathbf{h}^*\right]^{(k)} \text { given } \hat{\mathcal{N}}_k(v)
$$
表示节点$k$ hop内的3个group表示向量 （由每个group节点取平均得到）。那么对于关系$r$下的所有$K$个hop内的group embedding可以表示为：
$$
\mathbf{H}_r=||_{k=1}^K \mathbf{H}_g^{(k)},
$$
那么对于所有$R$个关系，所涉及的group embedding sequence表示为：
$$
\mathbf{H}_s=||_{r=1}^R \mathbf{H}_{v, r} .
$$
关于每个节点，共有$S=R \times(P \times K+1)$个group embeddings。其中$R$为relation数， $K$为hop数，$K = \#class +1$ 为label数+1 （有多少种group）。</p>
<h3 id="learnable-encoding-for-graph-transformer">Learnable Encoding for Graph Transformer</h3>
<p>先将每个节点的所有$S$个group embeddings过一下MLP得到$\mathbf{X}_s \in \mathbb{R}^ {S\times d_H}$。用nn.Embedding来定义一个$K \times d_H$的可训练的Hop encoding 矩阵$E_h(\cdot)$，每行表示一种hop的embedding。 对于节点的$S$个group 向量，每个group向量都属于一个hop种，那么这个group embedding 就+对应hop的embedding，从而融合结构特征。 比如$X_s[3]$是hop 2的 group embedding，那么这个group embedding 就要加上 $E_h(1)$ 来保留hop结构特征。所有$S$个group embedding 都要融合各自的hop特征，他们的hop 特征为：
$$
\begin{gathered}
\mathbf{X}_h=[\underbrace{\mathbf{E}_h(0), \overbrace{\mathbf{E}_h(1), \mathrm{E}_h(1), \mathrm{E}_h(1)}^{1 \text { st hop }}, \ldots, \overbrace{\mathrm{E}_h(K), \mathrm{E}_h(K), \mathbf{E}_h(K)}^{K-\text { th hop }}}_{1 \text { st relation }}, \\
\ldots, \underbrace{\mathbf{E}_h(0), \mathrm{E}_h(1), \mathrm{E}_h(1), \mathrm{E}_h(1), \ldots, \mathrm{E}_h(K), \mathrm{E}_h(K), \mathrm{E}_h(K)}_{R \text {-th relation }}]
\end{gathered}
$$
$S$中1-st到R-th relation的所有1hop group embedding都要加上hop 1 的encoding $E_h(1)$，对于其他hop的group embedding 同理。$\mathbf{X}_s$ 表示一个节点的所有$S$个group embedding，每个group embedding 要加上它所在的relation encoding $E_r(\text{relation of group})$，hop encoding $E_h(\text{hop of group})$ 以及它属于那个group $E_g (\text{label of group})$:
$$
\mathrm{X}_{i n}=\mathrm{X}_s+\mathrm{X}_h+\mathrm{X}_r+\mathrm{X}_g
$$
$\mathbf{X}_in$为一个节点新的group embeddings。每个节点的每个group embedding 都要融合它所在的hop 特征，所在的relation特征和所在的label特征（group 特征）然后用transformer将一个节点所有$S$个融合丰富特征的group embedding 做聚合，从而得到这个节点的最终embedding，用这个最终embedding来计算binary classification loss。</p>
<h1 id="2-gccad-graph-contrastive-coding-for-anomaly-detection-tkde">2. GCCAD: Graph Contrastive Coding for Anomaly Detection （TKDE）</h1>
<p>本文的目标：拉近normal nodes和global embedding的距离，拉远fraud nodes和global embedding的距离。inference阶段通过计算testing node和global embedding的距离来判断节点是否为fraud node。</p>
<h2 id="preliminary-observations">Preliminary Observations</h2>
<p><img loading="lazy" src="/posts/2023-05-13-FD/3.png#center" alt=""  />
</p>
<p>上图中N-N表示Normal nodes之间的相似度，AB-AB表示Abnormal nodes之间的相似度，N-AB表示Normal nodes和Abnormal nodes之间的相似度，从图（a）中可以发现N-N节点原始之间的相似度差别很大，即normal nodes之间的相似度差别很大，相似度范围在$[0.2,0.8]$， 而abnormal nodes之间的相似度差别也很大。从图(a)中还可以看出，normal nodes (N)和abnormal nodes （AB）原始特征之间有很大一部分是相似的。从图(b)中可以看出，当使用GCN学习到新的节点feature vectors后，normal nodes之间的相似度(N-N)得到了提升，即从$[0.2,0.8]$改善到$[0.4,1.0]$，但是N-N，AB-AB内部的相似度依然变化较大，并且依然存在大量高相似度的N-AB。</p>
<p>从图（c）可以看出normal nodes的原始特征和global embedding （N-GL）之间相似度较高，并且相似度变化范围小。而Abnormal nodes和global embedding (AB-GL)之间的相似度较低，并且N-GL相似度和AB-GL相似度更好区分。所以通过与global embedding之间的相似度来区分normal nodes和abnormal nodes可能更加有效。而本文提出的GCCAD会进一步提升normal nodes和global emb之间的相似度，并且更加容易区分N-GL相似度与AB-GL相似度。</p>
<h2 id="gccad-model">GCCAD Model</h2>
<p>GCCAD基于监督对比学习来优化node embeddings和global embeddings，目标函数如下：
$$
\mathcal{L}_{\text {con }}=\underset{\substack{i: y_i=0 \\ j: y_j=1}}{\mathbb{E}}\left[-\log \frac{\exp \left(\mathbf{q}^{\top} \boldsymbol{h}_i / \tau\right)}{\sum_j \exp \left(\boldsymbol{q}^{\top} \boldsymbol{h}_j / \tau\right)+\exp \left(\boldsymbol{q}^{\top} \boldsymbol{h}_i / \tau\right)}\right]
$$
其中$\boldsymbol{q}$为global embedding，$\boldsymbol{h}_i$为normal node $v_i$的embedding。基于上述supervised contrastive loss，训练目标为增大训练集中normal nodes和global embedding的相似度，减少abnormal nodes和global embedding的相似度。即使得global embedding尽可能不受abnormal nodes的影响。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/4.png#center" alt=""  />
</p>
<p>另外 本文不直接在原图上训练contrastive loss，而是先优化图结构，然后在优化的图结构上训练contrastive loss。由于message passing过程中会使得节点特征局部平滑，而abnormal node通常和位于normal node中，所以MPNN无论如何都会使得abnormal node和周围的normal node变得相似。所以在MPNN前先使用<strong>Edge Update</strong>模块对图更新，移除潜在的可以links，使得abnormal nodes尽可能少的接受到normal node的信息。本文提出Context-Aware Link Predictor来衡量原图中两个节点的边的保留概率：
$$
\begin{array}{r}
p_{i j}^{(l)}=\operatorname{MLP}\left(\left(\boldsymbol{h}_i^{(l-1)}-\boldsymbol{h}_j^{(l-1)}\right) \oplus\left(\boldsymbol{h}_i^{(l-1)}-\boldsymbol{q}^{(l-1)}\right)\right.
\left.\oplus\left(\boldsymbol{h}_j^{(l-1)}-\boldsymbol{q}^{(l-1)}\right)\right)
\end{array}
$$
两个节点间边的保留概率和两个节点间的embedding相似度有关（第1项），也和节点与global emb相似度有关。然后用训练集中标注好的normal nodes和abnormal nodes来训练$p_{i j}^{(l)}$：
$$
\mathcal{L}_{\text {link }}=\mathbb{E}\left[\sum_{i, j: y_i=y_j=0}-\log p_{i j}^{(l)}-\sum_{i, j: y_i \neq y_j=0}\left(1-\log p_{i j}^{(l)}\right)\right]
$$
通过这种方式，来将潜在的与abnormal nodes连接的边移除，从而使得abnormal node在message passing过程减少收到normal node的影响。基于每条边的保留概率，基于Bernoulli 分布来采样edges从而得到边mask 矩阵 $I^{(l)}$。新的图结构定义为：
$$
A_{i j}^{(l)}=\left(\alpha A_{i j}^{(l-1)}+(1-\alpha) p_{i j}^{(l)}\right) \odot I_{i j}^{(l)}
$$
反向传播时$I$视为常量，梯度从$p_{ij}$走。得到新的图结构后用GNN学习图的node embeddings。最后基于得到的node embeddings计算每个node embedding 和global embedding $\boldsymbol{q}$ 的相似度 （$\boldsymbol{q}$初始化为所有节点初始feature的均值）：
$$
s_i^{(l)}=\operatorname{cosine}\left(\boldsymbol{h}_i^{(l)}, \boldsymbol{m}\right)
$$
然后基于每个节点和global emb之间的相似度来计算聚合权重：
$$
\alpha_i^{(l)}=\frac{\exp \left(s_i^{(l)}\right)}{\sum_{j=1}^N \exp \left(s_j^{(l)}\right)}
$$
聚合node emb得到global emb:
$$
\boldsymbol{q}^{(l)}=\sum_{i=1}^N \alpha_i^{(l)} \cdot \boldsymbol{h}_i^{(l)}
$$
<strong>Training and Inference</strong></p>
<p>每个epoch 基于得到的global emb $\boldsymbol{q}$以及node embeddings $\boldsymbol{h}$ 来计算supervised contrastive loss，从而同时优化node embs和global embs。测试阶段，将测试节点的emb计算和global emb之间的相似度，相似度越低，测试节点是abnormal nodes的可能性越大。</p>
<h1 id="3-h2-fdetector-a-gnn-based-fraud-detector-with-homophilic-and-heterophilic-connections-www-22">3. H2-FDetector: A GNN-based Fraud Detector with Homophilic and Heterophilic Connections (WWW &lsquo;22)</h1>
<h2 id="introduction-1">Introduction</h2>
<p>Fraud graph通常包含2种类型的实体关联：1. homophilic connections: 相同label的节点被连接。 2. heterophilic connections: 不同label的节点被连接（fraudster 和 benign）。对于同时存在homophily 和heterophily的fraud graph，存在以下挑战：（1）如何学习一个边判别器，来判断图中的边是homophilic （两端都是benign 或 fraud） 还是 heterophilic (边一端是fraud一端是benign)。（2）如何为同时包含homophilic和heterophilic connections的fraud graph设计GNN。 (3) 如何利用整个类别的特征来判别新的fraud node? 即fraud node除了捕获与其邻居中benign nodes不相似的信息，还要捕获其他fraudster的信息，所以本方法让每个节点的表示和它所在类别的category feature相似，来捕获其他fraud 节点的特征。</p>
<h2 id="methodology">Methodology</h2>
<h3 id="h2-connection-identification">H$^2$-connection Identification</h3>
<p>训练一个边判别器来预测图中任意一条边是homophilic edge还是heterophilic edge，基于训练集中的节点label。对于第$l$层的node embedding $H^{(l-1)}=\left\{h_1^{(l-1)}, h_2^{(l-1)}, \ldots, h_N^{(l-1)}\right\}$。对于图中的每条边 $e_{uv}$，定义一个可训练的判别器来判断该边是homophilic还是heterophilic。 首先：
$$
\begin{aligned}
&amp; \bar{h}_u^{(l)}=\sigma\left(W_t^{(l)} h_u^{(l-1)}\right) \\
&amp; \bar{h}_v^{(l)}=\sigma\left(W_t^{(l)} h_v^{(l-1)}\right)
\end{aligned}
$$
其中$W_t^{(l)} \in \mathbb{R}^{d_l \times d_{l-1}}$是边判别器的可学习参数。然后基于$\bar{h}_u^{(l)}$和$\bar{h}_v^{(l)}$来计算边$e_{uv}$的homophilic分数。边的homophilic分数通过对两个节点的拼接 以及两个节点的不同来计算，$W_c^{(l)}$也是边判别器的参数，用于输出边分数：
$$
m_{u v}^{(l)}=\tanh \left(W_c^{(l)}\left[\bar{h}_u^{(l)}||\bar{h}_v^{(l)}||\left(\bar{h}_u^{(l)}-\bar{h}_v^{(l)}\right)\right]\right)
$$
其中 $\mathrm{tanh}(\cdot) \in (-1,1)$。根据$m_{u v}^{(l)}$的符号来判断$e_{uv}$是homo还是hetero：
$$
c_{u v}^{(l)}=\operatorname{SIGN}\left(m_{u v}^{(l)}\right)
$$
基于第$l$层的embedding输入边判别器中，可以得到所有边是homophilic还是heterophilic：
$$
C^{(l)}=\left\{c_{u v}^{(l)}\right\}_{e_{u v} \in \mathcal{E}}
$$
因为边判别器的输出是$\{-1,1\}$，所以对于训练集中的homophilic边，$y_{uv}=1$，那么$m_{u v}^{(l)}$要逼近1。同理对于heterophilic边，$y_{uv}=-1$，那么$m_{u v}^{(l)}$要逼近-1。即最小化以下目标：
$$
\mathcal{L}_{H I}^{(l)}=\frac{1}{\mathcal{E}_t} \sum_{e_{u v}}^{\mathcal{E}_t} \max \left(0,1-y_{u v} m_{u v}^{(l)}\right)
$$</p>
<h3 id="h2-connection-aggregation">H$^2$-connection Aggregation</h3>
<p>对于第$r$个relation下的图$\mathcal{G}_r=\left\{\mathcal{V}, X,\left\{\mathcal{E}_r\right\}, Y\right\}$，$\mathcal{N}_r(v)$表示关系$r$下节点$v$的邻居，$u \in \mathcal{N}_r(v)$。计算$u$对中心节点$v$重要性分数时考虑他们之间的边是homo边还是hetero边，所以在计算边$e_{vu}$间的重要性系数时考虑$c_{uv}^{(l)}$:
$$
e_{u v}^{(l), r}=a^{(l), r}\left[W_r^{{l}} h_v^{(l-1)} || c_{u v}^{(l)} W_r^{(l)} h_u^{(l-1)}\right]
$$
其中 attention mechanism权重向量$a^{(l), r} \in \mathbb{R}^{1 \times 2d_l}$。类似于GAT，邻居聚合的attention系数如下：
$$
\alpha_{u, v}^{(l), r}=\frac{\exp \left\{\operatorname{LeakyReLU}\left(e_{u v}^{(l), r}\right)\right\}}{\sum_{k \in \mathcal{N}_r(v)} \exp \left\{\operatorname{LeakyReLU}\left(e_{k v}^{(l), r}\right)\right\}}
$$
考虑多头attention，并且在邻居聚合的时候考虑边类型：
$$
h_v^{(l), r}=||_{k=1}^K \sigma\left(\sum_{u \in \mathcal{N}_r(v)} \alpha_{u, v}^{(l), r, k} c_{u v}^{(l)} W_r^{(l), k} h_u^{(l-1)}\right)
$$
对于$R$个relation，将每个节点每层输出$h_v^{(l), r}$的所有$R$个关系拼接后做特征变换，得到融合多关系的节点embedding：
$$
\begin{aligned}
&amp; h_v^{(l), \text { all }}=||_{r=1}^R h_v^{(l), r} \\
&amp; h_v^{(l)}=W_d^{(l)} h_v^{(l), \text { all }}
\end{aligned}
$$
其中$W_d^{(l)} \in \mathbb{R}^{d_l \times R d_l}$。最后一层输出维度为2，并做softmax：
$$
p_v=\operatorname{softmax}\left(h_v^{(L)}\right)
$$
用cross-entropy 训练GNN：
$$
\mathcal{L}_o=-\sum_{v \in \mathcal{V}_t}\left[y_v \log \left(p_v\right)+\left(1-y_v\right) \log \left(1-p_v\right)\right]
$$</p>
<h3 id="prototype-extraction">Prototype Extraction</h3>
<p>除了训练边类型判别器H$^2$-connection Identification $\mathcal{L}_{H I}$，节点embedding类型判别器$\mathcal{L}_o$外，节点的每层embedding要和该节点所属的类embedding（prototype embedding）相似。类的prototype embedding:
$$
\begin{aligned}
\operatorname{prototype}_{\text {fraud }}^{(l)} &amp; =\frac{1}{\left|\mathcal{V}_f\right|} \sum_{v \in \mathcal{V}_f} h_v^{(l)} \\
\operatorname{prototype}_{\text {benign }}^{(l)} &amp; =\frac{1}{\left|\mathcal{V}_b\right|} \sum_{v \in \mathcal{V}_b} h_v^{(l)}
\end{aligned}
$$
distance between node $v$ and two prototype:
$$
\begin{aligned}
&amp; \mathcal{D}_f^{{l}}(v)=|| h_v^{(l)}-\text { prototype }_{f r a u d}^{(l)} ||_2 \\
&amp; \mathcal{D}_b^{{l}}(v)=|| h_v^{(l)}-\text { prototype }_{\text {benign }}^{(l)} ||_2
\end{aligned}
$$
$v$到两个prototype 的距离可以用softmax来输出一个2维概率向量，用来匹配他的ground truth one-hot label:
$$
\begin{gathered}
\mathcal{L}_{P E}^{(l)}=-\sum_{v \in \mathcal{V}_t}\left[y_v \log \left(q_v^{(l)}\right)+\left(1-y_v\right) \log \left(1-q_v^{(l)}\right)\right] \\
q_v^{(l)}=\operatorname{softmax}\left(-\mathcal{D}_{C(v)}^{(l)}(v)\right)
\end{gathered}
$$
最终的训练目标为：
$$
\mathcal{L}=\mathcal{L}_o+\gamma_1 \sum_{l-1}^L \mathcal{L}_{H I}^{(l)}+\gamma_2 \sum_{l=1}^L \mathcal{L}_{P E}^{(l)}
$$</p>
<h1 id="4-care-gnn-enhancing-graph-neural-network-based-fraud-detectors-against-camouflaged-fraudsters-cikm-20">4. Care-GNN: Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters (CIKM &lsquo;20)</h1>
<p>Fraud nodes 在图中有2中类型的伪装（Camouflage）。（1）Feature Camouflage：通过添加一些特殊属性，从而骗过基于特征的一场检测器。（2）Relation Camouflage：Fraud nodes 隐藏在benign nodes中。为了解决两种伪装问题，对于<strong>特征伪装</strong>，提出一种标签感知的节点相似度衡量指标（label-aware similarity measure）用来为节点找到在特征层面和它最相似的邻居，节点特征基于它的label训练得到。（2）相似度感知的邻居提取器。基于强化学习，在GNN训练过程中自适应的寻找和他最相似的邻居。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/4.png#center" alt=""  />
</p>
<h2 id="label-aware-similarity-measure">Label-aware Similarity Measure</h2>
<p>在关系$r$下，中心节点$v$在第$l$层的表示为$\mathbf{h}_v^{(l-1)}$，对于该关系下的关于$v$的边$\left(v, v^{\prime}\right) \in \mathcal{E}_r^{(l-1)}$，他们在该层embeddings之间的$l_1$-distance为：
$$
\mathcal{D}^{(l)}\left(v, v^{\prime}\right)=||\sigma\left(M L P^{(l)}\left(\mathbf{h}_v^{(l-1)}\right)\right)-\sigma\left(M L P^{(l)}\left(\mathbf{h}_{v^{\prime}}^{(l-1)}\right)\right)||_1
$$
基于距离可以直接得到相似度：
$$
S^{(l)}\left(v, v^{\prime}\right)=1-\mathcal{D}^{(l)}\left(v, v^{\prime}\right)
$$
其中MLP输出的是一个scalar，然后输出到一个激活函数$\sigma = \tanh \in [-1,1]$中，通过衡量两个节点1维实数表示的距离来评价$v$和它邻居$v^{\prime}$的相似度。其中$M L P^{(l)}$是相似度评价器的参数，目标是基于node label $\{-1,1\}$来训练scalar embedding：
$$
\mathcal{L}_{\mathrm{Simi}}^{(l)}=\sum_{v \in \mathcal{V}}-\log \left(y_v \cdot \sigma\left(M L P^{(l)}\left(\mathbf{h}_v^{(l)}\right)\right)\right)
$$
$MLP$要使得训练集节点可以正确分类，在这种情况下计算两个节点的相似度。</p>
<h2 id="similarity-aware-neighbor-selector">Similarity-aware Neighbor Selector</h2>
<p>对于第$r$个relation，设置第$l$层的邻居采样阈值$p_r^{(l)} \in[0,1]$，表示当前epoch，在关系$r$下第$l$层每个fraud node仅采样和他相似度最高的前$p_r^{(l)}$比例个数的邻居参与聚合。这样可以尽可能为fraud node提取出和他相连的fraud nodes。注意，本文只针对fraud node计算$p_r^{(l)}$，但是该$p_r^{(l)}$会应用到关系$r$第$l$层的所有节点上。因为benign周围的同类型节点占比有绝对优势，所以对$p_r^{(l)}$的大小不敏感，$p_r^{(l)}$不管很大还是很小，都能为它聚合到同类的节点，所以基于fraud node计算的$p_r^{(l)}$来通用在所有节点上。</p>
<p>那么如何设置$p_r^{(l)}$，使得fraud node可以聚合到和它最相似的邻居，从而尽可能过滤掉和它不同类的邻居？由于在训练过程中$p_r^{(l)}$是一个采样概率，采样出的邻居参与聚合，所以$p_r^{(l)}$没有梯度，无法在端到端的训练过程中基于梯度优化。所以为了优化$p_r^{(l)}$，CARE-GNN采用一种基于强化学习的方式，在每个epoch中优化$p_r^{(l)}$。具体来说，代码中只设置了一层GNN，对于一个3relation的图，每个relation下有一个采样概率，$[p_1 ,p_2, p_3]$，初始化为$[0.5, 0.5, 0.5]$，reward初始话为$[0,0,0]$。对于关系$i$下的采样概率$p_i$，第一个epoch先基于初始化概率采样邻居，然后聚合采样出的邻居：
$$
\mathbf{h}_{v, r}^{(l)}=\operatorname{ReLU}\left(\mathrm{AGG}_r^{(l)}\left(\left\{\mathbf{h}_{v^{\prime}}^{(l-1)}:\left(v, v^{\prime}\right) \in \mathcal{E}_r^{(l)}\right\}\right)\right)
$$
然后用当前不同relation下的概率$[p_1 ,p_2, p_3]$加权聚合节点$v$在3个relation下的邻居embedding，然后和节点$v$的self-feature聚合，得到每个节点$v$在当前epoch的输出embedding：
$$
\mathbf{h}_v^{(l)}=\operatorname{ReLU}\left(\mathrm{AGG}_{a l l}^{(l)}\left(\left.\mathbf{h}_v^{(l-1)} \oplus\left\{p_r^{(l)} \cdot \mathbf{h}_{v, r}^{(l)}\right\}\right|_{r=1} ^R\right)\right)
$$
因为只有一层，所以$\mathbf{h}_v^{(l)} = z_v$。基于$z_v$构造cross-entropy loss来预测node label，其中$z_v$过MLP+softmax：
$$
\mathcal{L}_{\mathrm{GNN}}=\sum_{v \in \mathcal{V}}-\log \left(y_v \cdot \sigma\left(M L P\left(\mathbf{z}_v\right)\right)\right) .
$$
now current epoch end，当前epoch中，training fraud node 和采样出的邻居平均相似度为：
$$
G\left(\mathcal{D}_r^{(l)}\right)^{(e)}=\frac{\sum_{v \in \mathcal{V}_{\text {train }}} \mathcal{D}_r^{(l)}\left(v, v^{\prime}\right)^{(e)}}{\left|\mathcal{V}_{\text {train }}\right|}
$$
第一个epoch后 reward 变为$[1,1,1]$，因为当前的$p_i$使得fraud node采样出了更相似的邻居，那么下一个epoch中就要扩大$p_i$来探索更多相似的邻居，如果下一个epoch在扩大$p_i$后采样到的邻居与中心节点的平均相似度降低，那么reward为-1，下一个epoch要用小一些的$p_i$。通过这种方式，基于每个epoch中中心节点和邻居的相似度，来优化采样概率$p_i$。这个过程和GNN的训练以及Label-aware Similarity Measure的参数训练同时进行，最终的loss func为：
$$
\mathcal{L}_{\mathrm{CARE}}=\mathcal{L}_{\mathrm{GNN}}+\lambda_1 \mathcal{L}_{\mathrm{Simi}}^{(1)}+\lambda_2||\Theta||_2
$$</p>
<h1 id="5-rethinking-graph-neural-networks-for-anomaly-detection-icml-22">5. Rethinking Graph Neural Networks for Anomaly Detection (ICML &lsquo;22)</h1>
<p>由于GNN的neighborhood aggregation机制，位于benign nodes中的anomalies会变得难以区分。现有面向Anomaly Detection的GNN分发可以大概分为3类 （1）采用Attention机制从多个视图聚合不同的邻居；（2）对节点邻居重新采样；（3）设置额外的辅助loss来增强GNN在Fraud graph上的训练能力。但这些方法都是spatial methods，很少从spectral的角度设计模型。然而，选择定制的频谱滤波器是GNN设计的关键组成部分，因为频谱滤波器决定了GNN的表达能力。</p>
<p>因此本文研究如何为图上的异常检测任务设计谱图滤波器。本文首先分析了lens of the graph spectrum (图信号经过图傅里叶变换后的谱域表示)，即图信号在每个频率（特征值）上的响应强度。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/5.png#center" alt=""  />
</p>
<p>图1中（a）（c）：异常节点数量不变，异常节点和正常节点的差别增加，导致图的异常程度增加。（b）（d）：异常节点和正常节点的差别增加，异常节点的数量不变，导致图的异常程度增加。仅关注蓝色柱，表示图的异常程度很低，可以看出图信号在低频部分的energy高，而在高频部分的energy较低，即图信号在低频上的响应更多，在高频上的响应更少。随着异常程度的增加，关注红色柱，可以看出图上信号在低频上的响应降低，高频部分响应增加。可以看出<strong>异常数据会导致频谱能量的 “右移”</strong>。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/6.png#center" alt=""  />
</p>
<p>上图在一个fraud graph amazon上，对比原图，随机删除节点，删除异常节点 三种情况下在普通频率上的能量，可以看出，删除异常节点后低频能量上升，而高频能量（$\lambda = 1.0\sim1.2$）下降。所以信号的高频部分可能carry异常节点的性质，考虑高频部分可以帮助模型区分出异常节点。而保留信号的低频部分可以使得正常节点间平滑。因此设计图上的band-pass filter对于fraud graph很重要。<strong>现有的图神经网络大多属于低通滤波器或者自适应滤波器，它们无法保证带通性质。其中自适应滤波器虽然具有拟合任意函数的能力，但在异常检测中同样可能退化为低通滤波器。这是因为在整个数据集中，异常数据对应的高频信息占比较小（类不平衡），而大部分频谱能量仍然集中在低频。</strong></p>
<p>为了保留图上信号的从低频到高频的部分，本文选择使用Beta distribution作为graph kernel function $g(\Lambda)$。Beta distribution的概率密度函数为：
$$
\beta_{p, q}(w)= \begin{cases}\frac{1}{B(p+1, q+1)} w^p(1-w)^q &amp; \text { if } w \in[0,1] \\ 0 &amp; \text { otherwise }\end{cases}
$$
其中$p, q \in \mathbb{R}^{+}$，$B(p+1, q+1)=p ! q ! /(p+q+1) !$是一个常数。由于normalized graph Laplacian $L$ 的特征值$\lambda \in[0,2]$，所以convolution kernel function（用来给不同频率basis加权的函数）定义为$\beta_{p, q}^*(w)=\frac{1}{2} \beta_{p, q}\left(\frac{w}{2}\right)$使得$\beta_{p, q}^*(\lambda)$可解。将$\beta_{p, q}^*(\Lambda)$ 其中$\Lambda$是$L$的特征值对角阵作为convolution kernel function：
$$
\mathcal{W}_{p, q}=\boldsymbol{U} \beta_{p, q}^*(\boldsymbol{\Lambda}) \boldsymbol{U}^T=\beta_{p, q}^*(\boldsymbol{L})=\frac{\left(\frac{L}{2}\right)^p\left(I-\frac{L}{2}\right)^q}{2 B(p+1, q+1)}
$$
在不同的$p,q$设置下，$\mathcal{W}_{p, q}$可以倾向于不同的频率， 如下图所示，在$p=0,q=4$时，$\boldsymbol{U} \beta_{0, 4}^*(\boldsymbol{\Lambda}) \boldsymbol{U}^T$是一个low-pass filter，graph convolution kernel function $\beta_{p, q}^*(\boldsymbol{\Lambda})$为低频部分赋予更高权重。当$p=1, q=3$，以及$p=2, q=2$时，中频部分被赋予更高的权重，当$p=3,q=1$时，高频部分被赋予更高的权重。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/7.png#center" alt=""  />
</p>
<p>将不同$p,q$取值的graph filter结合起来可以得到一个band pass graph filter，然后将不同filter下提取的信号分量做拼接：
$$
\begin{aligned}
\boldsymbol{Z}_i &amp; =\mathcal{W}_{i, C-i}(\operatorname{MLP}(\boldsymbol{X})) \\
\boldsymbol{H} &amp; =\operatorname{AGG}\left(\left[\boldsymbol{Z}_0, \boldsymbol{Z}_1, \cdots, \boldsymbol{Z}_C\right]\right)
\end{aligned}
$$</p>
<h1 id="6-pick-and-choose-a-gnn-based-imbalanced-learning-approach-for-fraud-detection-www-21">6. Pick and Choose: A GNN-based imbalanced learning approach for fraud detection (WWW &lsquo;21)</h1>
<p><img loading="lazy" src="/posts/2023-05-13-FD/8.png#center" alt=""  />
</p>
<h2 id="pick-label-balanced-sampler">Pick: Label-balanced Sampler</h2>
<p>该方法类似于CAER-GNN。CAER-GNN中每个batch抽取的节点为所有fraud training nodes和一半的benign training nodes，然后每个batch node聚合他们相似度高的一阶邻居。和CARE-GNN固定一个batch中不同类节点数量不同的是，PC-GNN为每个training node 设置采样概率，小类（fraud node）更容易被采样到，大类节点被采样的概率较小，每个训练集节点$v$被采样的概率为：
$$
P(v) \propto \frac{||\hat{A}(:, v)||^2}{\operatorname{LF}(C(v))}
$$
其中$\operatorname{LF}(C(v))$为节点$v$所在类的训练节点数，小类的训练节点更容易被采样。分母为节点的度，表示越重要的节点越容易被采样到，这样一个batch中的训练节点可以避免原图中的类不平衡。</p>
<h2 id="choose-neighborhood-sampler">Choose: Neighborhood Sampler</h2>
<p>第二步为图中的choose过程，为每个节点采样要聚合的邻居。对于每个batch node $v$，它的predicted label probabiity embedding 为 $\mathrm{D}_r^{(\ell)}\left(\mathbf{h}_{v, r}^{(\ell)}\right)=\sigma\left(\mathbf{U}_r^{(\ell)} \mathbf{h}_{v, r}^{(\ell)}\right)$，$v$和它的邻居$u$相似度定义为他们之间的embedding的$\ell_1$距离：
$$
\mathcal{D}_r^{(\ell)}(v, u)=||\mathrm{D}_r^{(\ell)}\left(\mathbf{h}_{v, r}^{(\ell)}\right)-\mathrm{D}_r^{(\ell)}\left(\mathbf{h}_{u, r}^{(\ell)}\right)||_1
$$
对于batch node中的benign node，采样与其label probability embedding相似度最相似的一定数量的邻居来聚合，benign node $v$的采样聚合邻居为：
$$
\underline{\mathcal{N}_r^{(\ell)}}(v)=\{u \in \mathcal{V} \mid A_r(v, u)&gt;0 \text { and } \mathcal{D}_r^{(\ell)}(v, u)&lt;\rho_{-}\}
$$
对于batch中的fraud node，除了聚合上述一阶邻居外，还要将batch中其他和它相似度较高的fraud nodes加入它的邻居集合中：
$$
\overline{\mathcal{N}_r^{(\ell)}}(v)=\{u \in \mathcal{V} \mid C(u)=C(v) \text { and } \mathcal{D}_r^{(\ell)}(v, u)&lt;\rho_{+}\}
$$
所以对于fraud node $v$，他的聚合邻居集合为：$\mathcal{N}_r^{(\ell)}(v)=\underline{\mathcal{N}_r^{(\ell)}}(v) \cup \overline{\mathcal{N}_r^{(\ell)}}(v)$。计算节点何其邻居的距离是基于节点的label probability embeddings，基于cross-entropy loss来优化：
$$
\begin{gathered}
\mathcal{L}_{\text {dist }}=-\sum_{\ell=1}^L \sum_{r=1}^R \sum_{v \in \mathcal{V}}\left[y_v \log p_{v, r}^{(\ell)}+\left(1-y_v\right) \log \left(1-p_{v, r}^{(\ell)}\right)\right] \\
p_{v, r}^{(\ell)}=\mathrm{D}_r^{(\ell)}\left(\mathbf{h}_{v, r}^{(\ell)}\right)
\end{gathered}
$$</p>
<h2 id="aggregate-message-passing-architecture">Aggregate: Message Passing Architecture</h2>
<p>每个节点$v$ concat采样出的邻居：
$$
\mathbf{h}_{v, r}^{(\ell)}=\operatorname{ReLU}\left(W_r^{(\ell)}\left(\mathbf{h}_{v, r}^{(\ell-1)} \oplus \mathrm{AGG}_r^{(\ell)}\left\{\mathbf{h}_{u, r}^{(\ell-1)}, u \in \mathcal{N}_r^{(\ell)}(v)\right\}\right)\right)
$$
每个relation各自计算node embedding，然后拼接MLP后得到节点的embedding:
$$
\mathbf{h}_v^{(\ell)}=\operatorname{ReLU}\left(W^{(\ell)}\left(\mathbf{h}_v^{(\ell-1)} \oplus \mathbf{h}_{v, 1}^{(\ell)} \oplus \cdots \oplus \mathbf{h}_{v, R}^{(\ell)}\right)\right)
$$
GNN的loss为最后一层的输出变换为logits，然后计算cross-entropy：
$$
\begin{gathered}
\mathcal{L}_{\mathrm{gnn}}=-\sum_{v \in \mathcal{V}}\left[y_v \log p_v+\left(1-y_v\right) \log \left(1-p_v\right)\right] \\
p_v=\operatorname{MLP}\left(\mathbf{h}_v^{(L)}\right)
\end{gathered}
$$
模型的loss为：
$$
\mathcal{L}=\mathcal{L}_{\mathrm{gnn}}+\alpha \mathcal{L}_{\mathrm{dist}}
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2022 《Local Augmentation for Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/lagcn/</link>
      <pubDate>Wed, 25 Jan 2023 21:19:40 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/lagcn/</guid>
      <description>ICML2022 &amp;#34;Local Augmentation for Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2109.03856">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>在GNN的neighborhood aggregation中，对于拥有很少邻居的节点，在聚合过程中是否充分从邻居中获得了信息是一个问题。为解决该问题， 本文提出为每个节点做局部增强，即以中心节点为条件，学习邻居节点表示的分布。为了在局部邻域中生成一些样本来提升中心节点的neighborhood aggregation，本文提出一种数据增强框架：LA-GNNs， 以局部结构和中心节点特征为条件，生成neighborhood features。具体来说，在pre-training 阶段，通过一个生成模型，以中心节点的特征为条件来学习邻居特征的条件概率分布。然后利用这个邻居特征分布来生成中心节点的增强邻居特征。另外，通过pre-training来学习邻居增强特征生成器的过程是与下游任务无关的，所以该生成器生成的增强特征可以应用于其他GNN模型。</p>
<p><img loading="lazy" src="/posts/2023-01-28-LAGCN/1.png#center" alt=""  />
</p>
<h1 id="local-augmentation-for-graph-neural-networks-lagnn">Local Augmentation for Graph Neural Networks (LAGNN)</h1>
<h2 id="motivation">Motivation</h2>
<p>GNN在message passing的过程利用局部信息聚合来得到node representations。 但是对于邻居数量较少的节点，从邻居中得到的信息可能会不足。为了为节点$v$的邻域中$\mathcal{N}_v$生成更多样本，就需要知道邻居表示的分布。 由于一个节点邻居分布是与中心节点相关，所以我们要以中心节点$v$的representation为条件，学习它的邻居表示分布。</p>
<h2 id="approach">Approach</h2>
<p>本文利用Conditional Variational Auto-Encoder (CVAE) 来学习<strong>给定中心节点$v$，邻居$u \in \mathcal{N}_v$的节点特征的条件分布</strong>。给定中心节点特征$\boldsymbol{X}_v$，关于中心节点的邻居分布为$p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v)$。定义隐变量$\mathbf{z}$，则先验可以定义为$p_\theta(\mathbf{z}|\boldsymbol{X}_v)$。结合隐变量$\mathbf{z}$，邻居特征$\boldsymbol{X}_u$的分布可以改写为如下形式：
$$
\begin{aligned}
\log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v) &amp;= \log \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v)}{p_\theta( \boldsymbol{X}_v)}= \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}{p_\theta( \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} \\
&amp;=\log \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v, \mathbf{z})}{p_\theta( \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} \\
&amp;= \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}\\
\end{aligned}
$$
假设隐变量$\mathbf{z}$的分布为$q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)$， 左右两边对分布$q_\phi$计算期望，左边：
$$
\int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v) dz = \log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v)
$$
右边：
$$
\begin{aligned}
&amp;\int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)  \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} dz \\
=&amp; \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \left(\frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \cdot \frac{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}\right) dz \\
=&amp; \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} dz +  \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \frac{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}dz \\
=&amp; \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} dz + K L\left(q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) || p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)\right) \\
\geq&amp; \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} = ELBO
\end{aligned}
$$
Evidence Lower Bound (ELBO) 可以写为
$$
\begin{aligned}
L_{ELBO} &amp;= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\
&amp;= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \boldsymbol{X}_v, \mathbf{z}\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) p_\theta\left(\boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\
&amp;= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) p_\theta\left(\boldsymbol{X}_v, \mathbf{z}\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) p_\theta\left(\boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\
&amp;= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\
&amp;= -K L\left(q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) || p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)\right)+\int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) \mathrm{d} \mathbf{z} \\
&amp;=  -K L\left(\underbrace{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}_{Encoder} || \underbrace{ p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)}_{\text{Normal Distribution}}\right) + \mathbb{E}_{\mathbf{z} \sim q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) }\log p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right)
\end{aligned}
$$
在CVAE pre-training的过程中，第一项KL中CVAE Encoder 的一对邻接节点对，对于该节点对，输出一组分布参数均值$\mu$和方差$\sigma$，作为隐变量$z$的分布参数，第一项的优化目标使得编码器输出的分布接近Normal Distribution。然后利用reparameterization trick可微的从生成的$\mathbf{z}$分布中采样一个encoding:</p>
<pre tabindex="0"><code>def reparameterize(means, log_var):
    std = torch.exp(0.5 * log_var)
    eps = torch.randn_like(std)
    return means + eps * std   // z
</code></pre><p>若当前输入节点对为$(\boldsymbol{X}_v, \boldsymbol{X}_u)$，从输出的分布中采样一个encoding $\mathbf{z}$然后将$\mathbf{z}$输入decoding中，使得用$\mathbf{z}$和中心节点$\boldsymbol{X}_v$可以重构邻接节点$\boldsymbol{X}_u$。用所有邻接节点对训练encoder参数$\phi$和generator参数$\phi$。 这样在下游预测任务前，直接从Normal Distribution 随机采样$\mathbf{z}$，拼接中心节点$\boldsymbol{X}_v$输入generator中就可以为节点$v$生成增强邻居。过程如下图所示：</p>
<p><img loading="lazy" src="/posts/2023-01-28-LAGCN/2.png#center" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2022 《ProGCL：Rethinking Hard Negative Mining in Graph Contrastive Learning》 Reading Note</title>
      <link>https://JhuoW.github.io/posts/progcl/</link>
      <pubDate>Sun, 08 Jan 2023 22:28:43 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/progcl/</guid>
      <description>ICML2022 &amp;#34;ProGCL：Rethinking Hard Negative Mining in Graph Contrastive Learning&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2110.02027">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>Contrastive Learning 受益于区分hard negatives (最相似的negative pairs)， 但是其他领域的hard negative mining方法不适用于graph。 对于GCL来说大量embedding之后的hard negatives实际上是false negatives。如左图所示，对于CV上的SimCLR，它所学到的高相似度的negatives中，True negatives 和False negatives数量相当，那么从高相似度的negatives中采样到true negatives的概率更大。然而对于GCL方法GCA来说，是每个anchor节点将其他所有（inter/intra）节点作为negatives，使得在训练过程中与它同类的节点也变成anchor的negatives，这些negatives是false negatives。对于GCA，高相似度的negatives中false negatives的数量远多于true negatives，所以直接采样高相似度的negatives作为hard negatives来针对性的判别他们，会导致同类节点的embedding相互远离。这是传统的hard negatives mining方法在graph domain失效的原因。</p>
<p><img loading="lazy" src="/posts/2023-01-15-ProGCL/1.png#center" alt="你想输入的替代文字"  />
</p>
<p>为了解决这个问题， 本文提出利用Beta mixture model来估计对于一个anchor node，它的一个negatve是true negative的概率，结合相似度，来衡量该negative的hardness。即与anchor node相似度越高，且它是true negative的概率越大，那么该节点的hardness越高。</p>
<h1 id="methodology">Methodology</h1>
<h2 id="gcl">GCL</h2>
<p><img loading="lazy" src="/posts/2023-01-15-ProGCL/gcl.png#center" alt="你想输入的替代文字"  />
</p>
<p>如上图所示，InfoNCE将跨图same node视为positives，其他节点对视为negatives，GCL的目标函数如下：
$$
\begin{aligned}
\ell\left(\boldsymbol{u}_{i},\boldsymbol{v}_{i}\right)=
\log \frac{e^{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right) / \tau}}{\underbrace{e^{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right) / \tau}}_{\text{positive pair }}+\underbrace{\sum_{k\neq i}e^{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{k}\right) / \tau}}_{\text{inter-view negative pairs}}+\underbrace{\sum_{k\neq i}e^{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{u}_{k}\right) / \tau}}_{\text{intra-view negative pairs}}},
\end{aligned}
$$
Overall objective定义在所有跨图same node pairs上：
$$
\mathcal{J}=-\frac{1}{2 N} \sum_{i=1}^N\left[\ell\left(\boldsymbol{u}_{\boldsymbol{i}}, \boldsymbol{v}_{\boldsymbol{i}}\right)+\ell\left(\boldsymbol{v}_{\boldsymbol{i}}, \boldsymbol{u}_{\boldsymbol{i}}\right)\right]
$$
如果将GCA中的2层shared GNN替换为MLP，那么contrastive learning将不存在Message Passing，这样得到的true/false negative分布如(b)所示，可以看出Message Passing是GCL和CL之间产生区别关键因素。直观上，MP将anchor与相邻的negatives拉近，而相邻的negatives大多为False negatives（Homophily），所以GCL得到的高相似度negatives中false negatives要远多于True negatives。</p>
<p><img loading="lazy" src="/posts/2023-01-15-ProGCL/exp.png#center" alt="你想输入的替代文字"  />
</p>
<p><strong>Theorem 3.1</strong>: $\mathcal{G}$ is a non-bipartile and connected graph with $N$ nodes $\mathcal{V}=\left\{v_1, \ldots, v_N\right\}$ and $\boldsymbol{X}_i^{(\tau)}$ is the embedding of node $v_i$ after $\tau$ tims message passing. For large enough $\tau$,
$$
\left|\left|\boldsymbol{X}_i^{(\tau)}-\boldsymbol{X}_j^{(\tau)}\right|\right|_2 \leq\left|\left|\boldsymbol{X}_i^{(0)}-\boldsymbol{X}_j^{(0)}\right|\right|_2
$$
该定理说明了Message passing之后，不同节点间的距离会变小。</p>
<h2 id="progcl">ProGCL</h2>
<p><img loading="lazy" src="/posts/2023-01-15-ProGCL/exp2.png#center" alt="你想输入的替代文字"  />
</p>
<p>Figure 4描述了negatives相似度的直方图分布，即每个相似度下true/false negatives的相对数量。例如Figure 4（a），对于true negatives，高相似度的true negatives较少，中等相似度的true negatives较多；对于False negatives，大多相似度较高。所以直接采样高相似度的negatives作为hard negatives很容易采样到false negatives。那么随机采样一个相似度$s$的概率有该相似度在true negatives中的采样概率和在false negatives采样概率共同决定。因此相似度分布可以建模为mixture model。本文用Beta distribution来建模true/false negatives的相似度分布。beta distribution的pdf为：
$$
p(s \mid \alpha, \beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} s^{\alpha-1}(1-s)^{\beta-1},
$$
其中$\alpha, \beta &gt;0$ 是beta distribution的参数，$\Gamma(\cdot)$ 是gamma function。对于$C$个beta distribution的mixture model，相似度$s$在该mixture model中的概率为$s$在true negatives 相似度分布中的概率和false negatives相似度分布中的概率的加权：
$$
p(s)=\sum_{c=1}^{C} \lambda_{c} p(s \mid \alpha_c, \beta_c),
$$
其中$\lambda_c$是mixture coefficients。下面要做的就是通过EM算法来优化两beta mixture model的参数，包括两个beta distribution的$\alpha, \beta$参数以及mixture coefficient，<strong>使得GCL学习得到的negatives similarities从改mixture model中采样的概率最大</strong>，即优化mixture model这个混合概率分布，使其符合所有negatives similarity分布。例如总共有10000个negatives（true+false），这一万个similarity分布要符合分布$p(s)$。用EM算法来优化mixture model $p(s)$时，E-step要计算后验：
$$
p(c\mid s)=\frac{\lambda_{c} p\left(s \mid \alpha_{c}, \beta_{c}\right)}{\sum_{j=1}^{C} \lambda_{j} p\left(s \mid \alpha_{j}, \beta_{j}\right)}.
$$
其中$c$为latent variable。然而优化Beta Mixture Model使其你和所有negatives的similarity分布是计算量巨大的，为了解决这个问题每次迭代只采样$M$ ($M \ll N^2$)个相似度来的分布来优化BMM，使其你和这$M$个相似度的分布。首先计算$M$个相似度在每个beta distribution上的weighted average  $\bar{s}_c$以及variance $v_c^2$：
$$
\bar{s}_c=\frac{\sum_{i=1}^M p\left(c \mid s_i\right) s_i}{\sum_{i=1}^M p\left(c \mid s_i\right)}, \quad v_c^2=\frac{\sum_{i=1}^M p\left(c \mid s_i\right)\left(s_i-\bar{s}_c\right)^2}{\sum_{i=1}^M p\left(c \mid s_i\right)} .
$$
在M-step，如下优化BMM的参数 $\alpha_c, \beta_c, \lambda_c$使得BMM符合$M$个similarity的分布：
$$
\alpha_c=\bar{s}_c\left(\frac{\bar{s}_c\left(1-\bar{s}_c\right)}{v_c^2}-1\right), \quad \beta_c=\frac{\alpha_c\left(1-\bar{s}_c\right)}{\bar{s}_c}，\quad \lambda_c=\frac{1}{M} \sum_{i=1}^M p\left(c \mid s_i\right)
$$
这样通过E-step计算后验和average, variance，M-step基于average,variance和后验更新BMM的参数，循环迭代$I=10$次后，可以得到拟合输入negatives similarity的BMM分布的参数。最终，得到关于相似度$s$的BMM$p(s)$后，构成$p(s)$的两个分布即给定negative similarity $s$，该negative是true negative/false negative的概率为：
$$
p(c \mid s)=\frac{\lambda_c p\left(s \mid \alpha_c, \beta_c\right)}{p(s)}
$$</p>
<h2 id="progcl-weight">ProGCL-weight</h2>
<p>对于一对negative pair，$\boldsymbol{u}_i$为anchor和他的inter-view $\boldsymbol{v}_k$，他们的相似度$s_{ik}$在true negative分布中的概率为$p(c_t|s_{ik})$，概率越大越可能是true negative，同时$s_{ik}$越大越hardness。所以在Contrastive loss中，越hard的true negative要被赋予越大的权重，使得被判别区分。negative pair权重为：
$$
w(i, k)=\frac{p\left(c_t \mid s_{i k}\right) s_{i k}}{\frac{1}{N-1} \sum_{j \neq i}\left[p\left(c_t \mid s_{i j}\right) s_{i j}\right]}
$$
在Contrastive objective中对negatives加权：
$$
\ell_w\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right)=
\log \frac{e^{\frac{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right)}{ \tau}}}{\underbrace{e^{\frac{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right)}{\tau}}}_{\text{positive pair }}+\underbrace{\sum_{k\neq i}w(i,k)e^{\frac{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{k}\right)}{\tau}}}_{\text{inter-view negative pairs}}+\underbrace{\sum_{k\neq i}w(i,k)e^{\frac{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{u}_{k}\right)} {\tau}}}_{\text{intra-view negative pairs}}},
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/clusterscl/</link>
      <pubDate>Thu, 17 Nov 2022 01:33:20 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/clusterscl/</guid>
      <description>WWW2022 &amp;#34;ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://xiaojingzi.github.io/publications/WWW22-Wang-et-al-ClusterSCL.pdf">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>对于<strong>监督对比学习</strong>（Supervised Contrastive Learning, SupCon）, SupCon loss旨在表示空间中拉近属于同一个class的数据点，分离不同类的数据点。 但是SupCon难以处理高类内方差，类间相似度较大的数据集。为了解决该问题，本文提出了Cluster-aware supervised contrastive learning loss (ClusterSCL)。什么是高类内方差，高跨类相似度问题？如图1(a)所示，节点$u_1$和$u_3$ 是同类节点，$u_2$和$u_4$是同类节点。他们是同类节点但在不同的社区中，所以类内方差较大，即同一个类内的节点跨越了多个community。 另外$u_1$和$u_2$， $u_3$和$u_4$，是不同类的节点对， 但他们处在同一个社区中，导致在MPNN过程中，这些处在同一个community中的不同类节点被拉近，导致跨类相似度较高的问题。</p>
<p>如果对节点$u_2$计算SupCon时，如图1(b)所示，SupCon会使得同类节点被拉近，如$u_2$和$u_4$会被拉近。但是$u_3$和$u_4$处在同一个社区中（structurally similar）那么MPNN会使得$u_3$和$u_4$被拉近，所以SupCon在拉近$u_2$和$u_4$的同时，会间接拉近不同类节点$u_2$和$u_3$。同时，对于构成negative pairs的不同类节点，例如$u_1$和$u_2$，SupCon会推远$u_1$和$u_2$，但是$u_1$和$u_5$ structurally similar, 因此会推远$u_1$和$u_2$会间接导致$u_2$和$u_5$这两个同类节点被推远。因此对于一个cluster内节点不同类，且不同cluster中存在同类节点的情况，会导致复杂的决策边界，即<strong>在拉近同类但不同社区的节点时，也会间接拉近不同类不同社区的节点</strong>。<strong>在推远不同类同社区的节点时，也可能间接推远同类同社区的节点</strong>。</p>
<p><img loading="lazy" src="/posts/2022-11-15-ClusterSCL/1.png#center" alt=""  />
</p>
<p>为了解决上述问题，最直接的方法是对于每个cluster，如图1(a)的Community 1，不考虑其他cluster，只对当前cluster内节点做SupCon。但是这么做忽略了跨cluster的同类节点交互，如$u_1$和$u_3$，$u_2$和$u_4$，这些跨cluster的positive pairs可能包含有益的信息。为了解决这个问题，本文提出<strong>cluster-aware data augmentation (CDA)</strong> 聚类感知的数据增强，来为每个节点生成augmented positives and negatives，如图1(b)中ClusterSCL所示。对于每个节点$u$，为它生成positive 和negative samples, 生成的samples 位于或接近$u$所在的cluster。Recall SupCon存在的问题：</p>
<ul>
<li>SupCon会使得$u_2$和$u_4$被拉的太近，从而间接导致$u_2$和$u_3$被拉近，所以对于high intra-class variances，要求不同cluster的同类节点如$u_2$和$u_4$不要被拉太近；</li>
<li>SupCon会使得$u_1$和$u_2$被推远，从而间接导致$u_2$和$u_5$被推远，所以对于high inter-class similarity，要求同一个cluster内的不同类节点如$u_1$和$u_2$不要被拉的太远。</li>
</ul>
<h1 id="method">Method</h1>
<h2 id="two-stage-training-with-supervised-contrastive-loss">Two stage training with Supervised Contrastive Loss</h2>
<p><strong>SupCon encourages samples of the same class to have similar representations, while pushes apart samples of different classes in the embedding space.</strong></p>
<p>First Stage: 计算node embeddings $H = g_\theta(G)$， 然后用SupCon Loss来训练 $g_\theta$ 。即已经知道训练集中的节点label，基于这些节点label，SupCon在embedding space中把同label的节点拉近，不同label的节点分开。$g_\theta$用于得到node embeddings.</p>
<p>Second Stage：基于学习好的$g_\theta$， 用$\hat{Y}=f_\phi\left(g_\theta(G)\right)$来得到logits/prediction。即用cross-entropy loss来训练$f_\theta$。</p>
<h2 id="supcon">SupCon</h2>
<p>从同一个类中采样的节点构成positive pairs。batch中随机采样的节点对为negative pairs。给定$N$个随机采样的节点，对于每个节点，从其对应的class中随机采样一个不为它的节点作为positive pairs。所以一个batch有$N$对positive pairs，共$2N$个节点。</p>
<p>用$I \equiv\{1,2, \ldots, 2 N\}$ 表示一个batch中的node indices。$s_i \in I$表示这$2N$个节点中与节点$v_i$属于同一类的节点的indices。如下式所示， 在一个batch中，令$S_i \subset I$表示$2N$个节点中可以与节点$v_i$构成positive pairs的节点集合。相比其他节点，SupCon的objective是拉近positive pairs。
$$
\max \sum_{i \in I} \frac{1}{\left|S_i\right|} \sum_{s_i \in S_i} \log \frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_{s_i} / \tau\right)}{\sum_{j \in I \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j / \tau\right)}
$$
但是如果batch中与$v_i$同类的节点$v_j$和它不属于同一个cluster，$v_j$所属的cluster不同类的节点较多，那么拉近他们的距离也会间接拉近$v_i$与不同类节点间的距离。</p>
<h2 id="clusterscl">ClusterSCL</h2>
<p>为了解决上述问题，本文提出ClusterSCL。</p>
<h3 id="cluster-aware-data-augmentation-cda">Cluster-aware Data Augmentation (CDA)</h3>
<p>定义隐变量 $c_i$，该隐变量取值范围为$c_i \in \{1,2, \ldots, M\}$ 表示节点$v_i$属于哪一个cluster。给定两个anchor node $v_i$,$v_j$，CDA使用线性插值法为$v_j$生成augmentation：
$$
\tilde{\mathbf{h}}_j=\alpha \mathbf{h}_j+(1-\alpha) \mathbf{w}_{c_i}     \tag{4}
$$
其中$c_i$指示了节点$v_i$所在的cluster。$\mathbf{w}=\left\{\mathbf{w}_m\right\}_{m=1}^M$ 表示每个cluster的cluster prototypes，即每个cluster的中心，serve to characterize the cluster。$\mathbf{w}_{c_i}$表示节点$v_i$所在<strong>cluster</strong>的中心表示。$\tilde{\mathbf{h}}_j$包含了节点$v_j$的信息。并且，由于$\mathbf{w}_{c_i}$是$v_i$所在cluster的prtotype，所以通过调整$\alpha$，可以使$\tilde{\mathbf{h}}_j$位于$v_i$所在cluster的附近或内部。</p>
<p>（1）如果$(v_i, v_j)$是一个batch中的positive pair，$v_i$是anchor节点，如果$v_j$位于$v_i$所在的cluster $c_i$内，那么就需要学到的$\mathbf{h}_j$与$\mathbf{h}_i$尽可能靠近。在SupCon中，需要设置较大的$\alpha$使得$\tilde{\mathbf{h}}_j$保留更多$\mathbf{h}_j$ 。对于anchor节点$\mathbf{h}_i$，将它与$\tilde{\mathbf{h}}_j$拉近的时候，由于$\tilde{\mathbf{h}}_j$保留了更多$v_j$特征，所以$\mathbf{h}_j$也会被和$\mathbf{h}_i$拉近。如图2(a)所示。</p>
<p>（2）如果$(v_i, v_j)$是positive pair，如果$v_j$位于$v_i$所在的cluster $c_i$外时，如果$v_j$周围有negative samples （不一定在该batch中），那么直接拉近$(v_i, v_j)$会间接导致潜在的negative samples也会被拉近，因此对于位于$v_i$所在cluster外的节点$v_j$，要求它最终的表示$\mathbf{h}_j$不能被拉的太近，此时就需要小一些的$\alpha$，使得$\tilde{\mathbf{h}}_j$保留少一些$v_j$的信息，那么在SupCon拉近$\mathbf{h}_i$和$\tilde{\mathbf{h}}_j$的过程不会导致$\mathbf{h}_j$被拉近太多。因为$ \mathbf{w}_{c_i}$占据了$\tilde{\mathbf{h}}_j$的大部分，且它与$\mathbf{h}_i$已经很接近。如图2(b)所示。</p>
<p><img loading="lazy" src="/posts/2022-11-15-ClusterSCL/2.png#center" alt=""  />
</p>
<p>对于negative pair $(v_i, v_j)$。如果$v_j$位于$v_i$所在的cluster $c_i$内，如果直接推远$\mathbf{h}_i$和$\mathbf{h}_j$，会导致如果$v_j$的邻居有$v_i$的positive sample，那么这个positive sample也会被间接推远。所以$\tilde{\mathbf{h}}_j$应该保留较少的$\mathbf{h}_j$，即$\alpha$应该小。但是本文不考虑negative pairs的这种情况了，直接套用posiive的CDA原则。</p>
<p>综合上面的（1）（2）即对于close positive pairs，要让他们尽可能接近，即$\alpha$要大，对于distant positive pairs，要让他们不要太接近，$\alpha$要小一些。所以$\alpha$应与positive pair之间的相似度相关。所以$\alpha$定义如下：
$$
\alpha=\frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j\right)}{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j\right)+\exp \left(\mathbf{h}_i^{\top} \mathbf{w}_{c_i}\right)}
$$
上式分子越大$\alpha$越大，说明对于anchor node $v_i$， 如果的positive sample $v_j$位于它的cluster内（$v_i$与$v_j$相似）,$\mathbf{h}_j$的augmentation $\tilde{\mathbf{h}}_j$要保留越多自身信息。</p>
<h3 id="integraging-clustering-and-cda-into-supcon-learning">Integraging Clustering and CDA into SupCon Learning</h3>
<p>目标是给定节点$v_i$以及它所在的cluster 隐变量$c_i$，$v_i$的positive samples $s_i$的cluster-aware SupCon定义为条件概率：
$$
\begin{aligned}
p\left(s_i \mid v_i, c_i\right) &amp;=\frac{\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)} \\
&amp;=\frac{\exp \left(\mathbf{h}_i^{\top}\left(\alpha \mathbf{h}_{s_i}+(1-\alpha) \mathbf{w}_{c_i}\right) / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top}\left(\alpha \mathbf{h}_j+(1-\alpha) \mathbf{w}_{c_i}\right) / \tau\right)}
\end{aligned} \tag{6}
$$
即对于positive pair $(v_i, s_i)$，最大化$\mathbf{h}_i$和$s_i$的augmentation $\tilde{\mathbf{h}}_{s_i}$之间的一致性，$\alpha$可以依据$s_i$和$c_i$的关系来调整$\mathbf{h}_{s_i}$对于SupCon的贡献，使得$\mathbf{h}_{i}$与$\mathbf{h}_{s_i}$在位于不同cluster的情况下不会被拉的太近。其中$c_i$是隐变量。</p>
<p>首先定义关于节点$v_i$的cluster分布，即$v_i$属于每个$c_i$的概率。给定anchor node $v_i$，它属于cluster $c_i \in \{1,2, \ldots, M\}$的概率定义为：
$$
p\left(c_i \mid v_i\right)=\frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{w}_{c_i} / \kappa\right)}{\sum_{m=1}^M \exp \left(\mathbf{h}_i^{\top} \mathbf{w}_m / \kappa\right)} \tag{7}
$$
$\mathbf{w}_{c_i}$是cluster $c_i$的prototype表示，$v_i$属于在embedding空间中与它相似的cluster的概率更高。ClusterSCL旨在最大化给定锚节点$v_i$，锚节点与其positive sample $s_i$的likelihood：
$$
p\left(s_i \mid v_i\right)=\int_{c_i} p\left(c_i \mid v_i\right) p\left(s_i \mid v_i, c_i\right) d c_i= \sum^M_{m=1} p(m|v_i)p(s_i|v_i,m)    \tag{8}
$$
即给定anchor $v_i$，$s_i$是$v_i$的postive sample的概率为 当$v_i$属于cluster $m$的情况下，$s_i$是其positive sample的概率， over all clusters $m \in M$。</p>
<p>在likelihood Eq.(8)中，anchor node $v_i$ 为待优化参数，它的positive sample $s_i$为观测数据，$c_i$为隐变量。</p>
<h3 id="maximize-likelihood-eq8-via-em">Maximize likelihood Eq.(8) via EM</h3>
<p>Objective: $\mathrm{maximize} \log p(s_i| v_i)$，即最大化positive pairs的条件概率 given anchor node $v_i$。</p>
<p>E-step：$\mathbb{E}_{p(c_i|s_i, v_i)} \log p(s_i, c_i|v_i)$</p>
<p>M-step: $\widehat{v}_i = \arg \max_{v_i} \mathbb{E}_{p(c_i|s_i, v_i)} \log p(s_i, c_i|v_i)$</p>
<p>可见，如果要通过EM算法来优化得到anchor node $v_i$的表示，需要计算后验 $p(c_i|s_i, v_i)$：
$$
\begin{aligned}
p(c_i|s_i, v_i) &amp; = \frac{p(c_i,v_i, s_i)}{p(s_i,v_i)} \\
&amp; = \frac{p(v_i)p(s_i,c_i | v_i)}{p(s_i |v_i) p(v_i)}\\
&amp; = \frac{p(s_i,c_i|v_i)}{p(s_i |v_i)} \\
&amp; = \frac{p(s_i,c_i|v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}\\
&amp; = \frac{\frac{p(s_i,c_i,v_i)}{p(v_i)} = \frac{p(c_i,v_i)}{p(v_i)} \frac{p(s_i,c_i,v_i)}{p(c_i,v_i)} = p(c_i | v_i)p(s_i|c_i,v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)} \\
&amp; = \frac{p(c_i | v_i)p(s_i|c_i,v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}<br>
\end{aligned}\tag{9}
$$
后验中，$p(c_i | v_i)$，$p(m|v_i)$是$v_i$的cluster 分布，在Eq.(7)中给出定义。但是，对于$p(s_i|c_i,v_i)$和$p(s_i | v_i, m)$，Eq.(6)中给出了它的定义，$p\left(s_i \mid v_i, c_i\right) =\frac{\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)}$， 可以看出分母部分需要计算$\mathbf{h}_i$与所有节点，并且还要over all $M$ cluster，因此后验难以计算。为了解决这个问题，我们可以maximize evidence lower bound (ELBO) of  $\log p(s_i | v_i)$：
$$
\begin{aligned}
\log p(s_i | v_i) &amp;= \log \frac{p(s_i,v_i)}{p(v_i)} = \log  \frac{p(s_i,v_i)p(c_i | v_i,s_i)}{p(v_i)p(c_i | v_i,s_i)} \\
&amp;= \log \frac{p(v_i,s_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} \\
&amp;= \log \frac{p(s_i|v_i,c_i)p(v_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} = \log \frac{p(s_i|v_i,c_i)p(c_i|v_i)p(v_i)}{p(v_i)p(c_i | v_i,s_i)} \\
&amp; = \log \frac{p(s_i|v_i,c_i)p(c_i|v_i)}{p(c_i | v_i,s_i)} \\
&amp; = \log p(s_i|v_i,c_i)-\log p(c_i | v_i,s_i) +  \log p(c_i|v_i) \\
&amp; \text{引进一个关于隐变量$c_i$的分布$q(c_i)$，可以是任意形式，这里定义为$q(c_i|v_i,s_i)$}\\
&amp; = \log p(s_i|v_i,c_i) - \log \frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} + \log \frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} \\
&amp;\text{左右两边分别对$q(c_i|v_i,s_i)$求期望} \\
\text{左边} &amp;= \int q(c_i|v_i,s_i) \log p(s_i | v_i) d c_i = \int q(c_i|v_i,s_i) d c_i \cdot \log p(s_i | v_i) = \log p(s_i | v_i) \\
\text{右边} &amp;= \int q(c_i|v_i,s_i) \log  p(s_i|v_i,c_i) dc_i - \underbrace{\int q(c_i|v_i,s_i) \log \frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} dc_i}_{-\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i))} + \underbrace{\int q(c_i|v_i,s_i) \log \frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} dc_i}_{-\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))} \\
&amp;= \int q(c_i|v_i,s_i) \log  p(s_i|v_i,c_i) dc_i + \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i)) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \\
&amp; \geq \underbrace{\mathbb{E}_{q(c_i|v_i,s_i)} \log  p(s_i|v_i,c_i) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))}_{ELBO}
\end{aligned}  \tag{10}
$$
因此， we have：
$$
\log p(s_i | v_i) \geq ELBO = \mathbb{E}_{q(c_i|v_i,s_i)} \log  p(s_i|v_i,c_i) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \tag{10}
$$
所以目标为找到node embeddings $\mathbf{h}_i, \forall i$， 最大化ELBO。接下来可以定义任意关于隐变量$c_i$的vartational distribution $q(c_i)$。 这里将关于$c_i$的variational distribution定义为用mini-batch近似后验的形式：
$$
q\left(c_i \mid v_i, s_i\right)=\frac{p\left(c_i \mid v_i\right) \tilde{p}\left(s_i \mid v_i, c_i\right)}{\sum_{m=1}^M p\left(m \mid v_i\right) \tilde{p}\left(s_i \mid v_i, m\right)}  \tag{11}
$$
其中$\tilde{p}\left(s_i \mid v_i, c_i\right)=\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right) / \sum_{j \in I \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)$，与Eq.(6)不同的是$\tilde{p}\left(s_i \mid v_i, c_i\right)$的分母只计算mini-batch $I$内的negative samples。并且用$\tilde{p}\left(s_i \mid v_i, c_i\right)$来替换了Eq.10 中的$ p(s_i|v_i,c_i)$，文中证明了这种替代的合理性。</p>
<p>在E-step，定义了variational distribution $q\left(c_i \mid v_i, s_i\right)$和likelihood objective的ELBO，在M-step最大化ELBO。给定一个mini-batch $I$，目标函数为最大化$I$中每对positive pairs 的ELBO：
$$
\max \quad \mathcal{L}_{\mathrm{ELBO}}(\theta, \mathbf{w} ; I) \approx \frac{1}{|I|} \sum_{i \in I} \frac{1}{\left|S_i\right|} \sum_{s_i \in S_i} \mathcal{L}_{\mathrm{ELBO}}\left(\theta, \mathbf{w} ; v_i, s_i\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2022 《Graph Condensation for Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gcond/</link>
      <pubDate>Thu, 01 Sep 2022 10:47:21 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gcond/</guid>
      <description>ICLR2022 &amp;#34;Graph Condensation for Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/forum?id=WLEx3Jo4QaB">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文提出图浓缩技术（Graph Condensation），旨在将大图浓缩为一个小图，使得在小图上训练的GNN可以得到和大图相当的效果。通过优化<strong>gradient matching loss</strong>来模拟GNN在原图上的<strong>训练轨迹</strong>，从而解决图浓缩问题。</p>
<p>通常有两个策略来简化图：Graph Sparsification(图稀疏化)和Graph Coarsening(图粗化)。图稀疏化通过减少边数来近似一个图； 图粗化旨在减少节点数量。（1）当节点具有属性特征时，由于稀疏化不会减少节点数量，因此属性量不会减少。 （2）图粗化的目的是保存一些图属性比如主特征值，这可能对下游任务不是最优的保存属性。</p>
<p>本文提出图浓缩，来学习生成图的结构和节点属性，从这两方面同时进行浓缩。对于Reddit数据集，GCond可以将节点数浓缩至0.1%，并且在浓缩图上可以得到和原图相当的效果。如下图所示：</p>
<p><img loading="lazy" src="/posts/2022-09-01-GCond/1.png#center" alt=""  />
</p>
<p>本文解决了图浓缩面临的两个挑战：1. 构建目标函数， 2. 参数化可学习的节点特征和图结构。为了解决上述挑战，本文使用gradient matching loss来匹配每一个training step上原图与浓缩图的GNN参数梯度，使得GNN在浓缩图上的训练趋势与原图相匹配。为了参数化节点特征和图结构，本文将浓缩图的Feature Matrix设为自由参数矩阵，<strong>将浓缩图结构设为关于Feature matrix 的 函数</strong>（基于结构与特征相关联假设），使得计算开销降低。</p>
<h1 id="methodology">Methodology</h1>
<p>A graph $\mathcal{T}=\{\mathbf{A}, \mathbf{X}, \mathbf{Y}\}$，其中$\mathbf{X} \in \mathbb{R}^{N \times d}$是$d$维节点特征，$\mathbf{Y} \in\{0, \ldots, C-1\}^N$ 表示$N$个节点的labels，共有$C$个class。图浓缩旨在学习一个小的生成图$\mathcal{S}=\left\{\mathbf{A}^{\prime}, \mathbf{X}^{\prime}, \mathbf{Y}^{\prime}\right\}$，其中$\mathbf{A}^{\prime} \in \mathbb{R}^{N^{\prime} \times N^{\prime}}$是浓缩图的邻接矩阵，$\mathbf{X}^{\prime} \in \mathbb{R}^{N^{\prime} \times D}$是浓缩图的特征矩阵，$\mathbf{Y}^{\prime} \in\{0, \ldots, C-1\}^{N^{\prime}}$是浓缩图的node labels 其中$N^{\prime} \ll N$，特征维度从$d$变为$D$。图浓缩的目标是基于原图训练过程<strong>学习浓缩图$\mathcal{S}$</strong>，使得在$\mathcal{S}$上训练的GNN应用在原图上的loss最小：
$$
\min_{\mathcal{S}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_{\mathcal{S}}}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right) \quad \text { s.t } \quad \boldsymbol{\theta}_{\mathcal{S}}=\underset{\boldsymbol{\theta}}{\arg \min } \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}}\left(\mathbf{A}^{\prime}, \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right),
$$
Outer：固定GNN参数，优化小图。 Inner: 固定小图，在小图上训练GNN参数。</p>
<p>由于如果就用一个固定的初始化参数来初始化GNN，小图的训练参数${\boldsymbol{\theta}_{\mathcal{S}}}$可能会过拟合一个特定初始化的GNN。 因此为了使得浓缩data可以泛化到随机初始化的GNN $P_{\boldsymbol{\theta}_0}$，上面的目标函数可以改写为：
$$
\min_{\mathcal{S}} \mathrm{E}_{\boldsymbol{\theta}_0 \sim P_{\theta_0}}\left[\mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_{\mathcal{S}}}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right)\right] \quad \text { s.t. } \quad \boldsymbol{\theta}_{\mathcal{S}}=\underset{\boldsymbol{\theta}}{\arg \min } \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}\left(\boldsymbol{\theta}_0\right)}\left(\mathbf{A}^{\prime}, \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right)
$$
具体实现就是在用一个GNN训练${\boldsymbol{\theta}_{\mathcal{S}}}$后，初始化GNN继续训练${\boldsymbol{\theta}_{\mathcal{S}}}$（${\boldsymbol{\theta}_{\mathcal{S}}}$不用初始化）。也就是在不同的初始化GNN情况下训练${\boldsymbol{\theta}_{\mathcal{S}}}$。</p>
<h2 id="graph-condensation-via-gradient-matching">Graph Condensation via Gradient Matching</h2>
<p>通过优化bi-level问题来求解参数过于困难，因此使用gradient matching方法来匹配在不同数据上每次迭代的参数梯度。通过这种方式，模型在浓缩图$\mathcal{S}$上的训练轨迹可以用来模拟原图$\mathcal{T}$上的训练轨迹。模型的参数匹配可以表示为：
$$
\begin{gathered}
\min_{\mathcal{S}} \mathrm{E}_{\boldsymbol{\theta}_0 \sim P_{\boldsymbol{\theta}_0}}\left[\sum_{t=0}^{T-1} D\left(\boldsymbol{\theta}_t^{\mathcal{S}}, \boldsymbol{\theta}_t^{\mathcal{T}}\right)\right] \quad \text { with } \\
\boldsymbol{\theta}_{t+1}^{\mathcal{S}}=\operatorname{opt}_{\boldsymbol{\theta}}\left(\mathcal{L}\left(\operatorname{GNN}_{\boldsymbol{\theta}_t^{\mathcal{S}}}\left(\mathbf{A}^{\prime}, \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right)\right) \text { and } \boldsymbol{\theta}_{t+1}^{\mathcal{T}}=\operatorname{opt}_{\boldsymbol{\theta}}\left(\mathcal{L}\left(\operatorname{GNN}_{\boldsymbol{\theta}_t^{\mathcal{T}}}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right)\right)
\end{gathered}
$$
表示第$t$次迭代时，原图上训练的GNN参数$\boldsymbol{\theta}_t^{\mathcal{T}}$和小图上训练的GNN参数$\boldsymbol{\theta}_t^{\mathcal{S}}$要接近。由于两个GNN初始化参数一致，如果将同一个GNN应用于两个图，要使他们的每一步训练轨迹一致，那么他们每一步的参数梯度应该一致，对于$\mathrm{GNN}_{\theta_t}$，所有$T$步的梯度匹配可以写为：
$$
\min_{\mathcal{S}}\mathrm{E}_{\boldsymbol{\theta}_0 \sim P_{\theta_0}}\left [\sum_{t=0}^{T-1} D\left(\nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_t}\left(\mathbf{A}^{\prime}, \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right), \nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_t}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right)\right)\right]
$$
其中$D$为参数梯度矩阵之间的距离，若$\mathbf{G}^{\mathcal{S}}, \mathbf{G}^{\mathcal{T}} \in \mathbb{R}^{d_1 \times d_2}$， 那么梯度矩阵的差异定义为：
$$
\operatorname{dis}\left(\mathbf{G}^{\mathcal{S}}, \mathbf{G}^{\mathcal{T}}\right)=\sum_{i=1}^{d_2}\left(1-\frac{\mathbf{G}_{\mathbf{i}}^{\mathcal{S}} \cdot \mathbf{G}_{\mathbf{i}}^{\mathcal{T}}}{\left|\left|\mathbf{G}_{\mathbf{i}}^{\mathcal{S}}\right|\right|\left|\left|\mathbf{G}_{\mathbf{i}}^{\mathcal{T}}\right|\right|}\right)
$$</p>
<h2 id="modeling-condensed-graph-data">Modeling Condensed Graph Data</h2>
<p>要使得浓缩图可学习，直接参数化三个矩阵$\mathbf{A}^{\prime}, \mathbf{X}^{\prime}, \mathbf{Y}^{\prime}$并优化是很困难的。 因此本文先确定浓缩图的label矩阵$\mathbf{Y}^{\prime}$，具体来说，对于每个class的<strong>训练节点</strong>，选取特定比例的节点。例如训练集有3个类，每个类选取一定比例的训练节点，这些节点label作为浓缩图的node labels，features作为浓缩图的初始化node features $\mathbf{X}^{\prime}$。注意，这里$\mathbf{X}^{\prime}$是自由可训练参数。 由于在社交网络中，结构通常与节点特征相关，因此将结构$\mathbf{A}^{\prime}$设置成关于特征$\mathbf{X}^{\prime}$的函数，这样减少了参数量：
$$
\mathbf{A}^{\prime}=g_{\Phi}\left(\mathbf{X}^{\prime}\right), \quad \text { with } \mathbf{A}_{i j}^{\prime}=\operatorname{Sigmoid}\left(\frac{\operatorname{MLP}_{\Phi}\left(\left[\mathbf{x}_i^{\prime} ; \mathbf{x}_j^{\prime}\right]\right)+\operatorname{MLP}_{\Phi}\left(\left[\mathbf{x}_j^{\prime} ; \mathbf{x}_i^{\prime}\right]\right)}{2}\right)
$$
带入gradient matching loss中：
$$
\min_{\mathbf{X}^{\prime}, \Phi} \mathrm{E}_{\boldsymbol{\theta}_0 \sim P_{\theta_0}}\left[\sum_{t=0}^{T-1} D\left(\nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_t}\left(g_{\Phi}\left(\mathbf{X}^{\prime}\right), \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right), \nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_t}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right)\right)\right]
$$
其中：GNN的参数会重复初始化，增强$\theta_\mathcal{S}$的泛化效果。</p>
<p><img loading="lazy" src="/posts/2022-09-01-GCond/2.png#center" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>MLP and GNNs</title>
      <link>https://JhuoW.github.io/posts/glnn/</link>
      <pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/glnn/</guid>
      <description>Scalable GNN with MLP 相关论文总结</description>
      <content:encoded><![CDATA[<p>最近一些工作通过解耦Message-Passing 和 Feature Learning的方式来提升GNN的可拓展性，这里对一小部分相关工作做一个小总结。</p>
<h1 id="1-combining-label-propagation-and-simple-models-out-performs-graph-neural-networks-iclr2021">1. Combining Label Propagation and Simple Models Out-performs Graph Neural Networks （ICLR2021）</h1>
<p><img loading="lazy" src="/posts/2022-07-13/C_S.png#center" alt=""  />
</p>
<p>模型首先忽略图结构，用简单模型（MLP），只使用节点特征预测label：</p>
<p>$$
\min \sum_{i \in L_{t}} \ell\left(f\left(x_{i}\right), y_{i}\right)
$$
考虑一个inductive bias：预测误差与邻近度关系强相关，对图中所有节点的误差做校正。</p>
<p>具体来说，首先计算一个初始的误差矩阵$E$，其中训练集误差如下
$$
E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:}
$$
其他节点的误差未知：$E_{L_{v},:}=0, \quad E_{U,:}=0$。然后通过Label Propagation将误差矩阵在图上做平滑，使得相邻节点的误差相似：</p>
<p>$$
\hat{E}=\underset{W \in \mathbb{R}^{n \times c}}{\arg \min } \operatorname{trace}\left(W^{T}(I-S) W\right)+\mu||W-E||_{F}^{2}
$$
由此得到所有节点的误差矩阵$\hat{E}$。然后用$\hat{E}$对基础MLP预测做校正，这个post-processing过程不涉及训练参数，校正后的预测为：
$$
Z^{(r)} = Z + \hat{E}
$$
考虑homophily：校正的预测label要满足相邻节点label相似。 注意，这里不直接对$Z^{(r)}$做Label Propagation，而是构造了一个label矩阵$H \in \mathbb{R}^{n \times c}$，其中将训练集真实label和验证+测试集校正label加入$H$中，然后对$H$做label propagation：
$$
\begin{aligned}
H_{L_{t},:}&amp;=Y_{L_{t},:} \\
H_{L_{v} \cup U,:}&amp;=Z_{L_{v} \cup U,:}^{(r)}
\end{aligned}
$$
Label Prop:
$$
H^{(t+1)}=(1-\alpha) H+\alpha S H^{(t)}
$$
最后直接用收敛的$H$做预测，即$\hat{Y} = H^{\infty}$，node $i$ 的预测class为：
$$
y_i = \arg \max _{j \in\{1, \ldots, c\}} \hat{Y}_{i j}
$$</p>
<h1 id="2-graph-less-neural-networks-teaching-old-mlps-new-tricks-via-distillation-iclr2022">2. Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation (ICLR2022)</h1>
<p>MLPs相对于GNN更易于部署，并且避开了在线预测过程中的<strong>冷启动问题</strong>，即在线预测时，新节点的加入，它的邻居可能不能立即获得，MLP无需依赖于图结构，相比于GNN，MLP的延迟低。而GNN依赖于节点上下文，准确性更高。如何融合GNN和MLP的有点是本文解决的挑战。</p>
<p>本文的核心发现为：可以在不显著损失性能的情况下将知识从 GNN 提取到 MLP，从而大大减少节点分类的<strong>推理时间</strong>。 知识蒸馏（Knowledge Distillation, KD）可以离线完成，并且与模型训练相耦合，即<strong>将推理阶段的耗时迁移到训练阶段，因为训练阶段可以容忍高耗时，而推理通常需要较大的时间减少。</strong>  也就是 训练阶段不仅要训练GNN模型，还要将模型的知识迁移到MLP上，而推理阶段直接用MLP做预测。</p>
<p><strong>Motivation：</strong> GNN虽然可以取得较好的性能，但是由于它依赖于图结构，所以有较大的推理延迟。 每增加一层GNN，就要为每个节点多捕获一跳邻居。对于一个平均度为$R$的图，在用$L$层GNN预测一个节点时，需要先获取的邻居数量为$\mathcal{O}(R^L)$。 同时，由于层是sequential，所以要逐层获取邻居，总的延迟会随层数加深而增大，每层需要融合的邻居数也成指数上升趋势，是的层数越深，预测延迟越高。</p>
<p>相反，MLP不利用图结构，使得推理时间远小于GNN，但是损害了节点分类的预测性能，因此 如何同时兼顾GNN和MLP的优势，是的模型以获得高精度和低延迟是一个待解决问题，因此本文提出了GNN和MLP的跨模型Knowledge Distillation。</p>
<p><img loading="lazy" src="/posts/2022-07-13/GLNN.png#center" alt=""  />
</p>
<p>本文提出了GLNN，如上图所示，训练一个“Boosted” MLP，他的Knowledge来自于一个Teacher GNN。如上图所示，首先训练好一个GNN模型（这里用GraphSAGE+GCN Aggregation）作为Teacher，GNN的为节点集$v \in V$的预测输出为$\boldsymbol{z}_{v}$。 然后训练一个student MLP，predictions为$\hat{\boldsymbol{y}}_{v}$。loss由两部分组成，第一部分直接用MLP做半监督预测的损失；第二部分为KD，使得MLP对所有节点的预测与GNN的预测接近：
$$
\mathcal{L}=\lambda \Sigma_{v \in \mathcal{V}^{L}} \mathcal{L}_{\text {label }}\left(\hat{\boldsymbol{y}}_{v}, \boldsymbol{y}_{v}\right)+(1-\lambda) \Sigma_{v \in \mathcal{V}} \mathcal{L}_{\text {teacher }}\left(\hat{\boldsymbol{y}}_{v}, \boldsymbol{z}_{v}\right)
$$
其中$\mathcal{L}_{\text {label }}$为cross-entropy loss， $\mathcal{L}_{\text {teacher }}$为KL-divergence。在推理阶段直接使用MLP来预测测试集节点label，无需依赖图结构。预测阶段，直接用MLP做预测。实做中直接去掉了第一部分，只保留KD部分。</p>
<h1 id="3-node-dependent-local-smoothing-for-scalable-graph-learning-neurips2021">3. Node Dependent Local Smoothing for Scalable Graph Learning （NeurIPS2021）</h1>
<p>以往的工作已经证明了，简单的MLP+Label Smoothing的性能可以超过vanilla GCN。但是如何控制模型的平滑程度（extent of smoothness）依然是个问题。 太少的平滑迭代会造成欠平滑（under-smoothing）问题，而太多的迭代会造成过平滑（oversmoothing）问题。另外，不同节点应有特定的平滑程度。大多数现有的GNN使用一个统一的迭代次数$k$，即每个节点都聚合它的$k$阶邻居。 这种统一的聚合方式存在问题，因为迭代次数应与每个节点的度和局部结构相关。如下图所示，两个红色节点有完全不同的局部结构。 左边的红色节点位于Dense region中，因此它的传播速度更快，即很少的step就可以扩散到很多节点，因此，对于这类节点，需要较少次的propagation，因为小的iteration足以聚合足够多的节点。而右边红色节点需要更多次的propagation来聚合足够的信息。</p>
<p><img loading="lazy" src="/posts/2022-07-13/NDLS_1.png#center" alt=""  />
</p>
<p>本文提出了Node-dependent Local Smoothing (NDLS)， 计算每个节点的特定迭代次数（LSI），使得节点只会聚合它特定LSI以内的节点。</p>
<p><strong>Over-Smoothing issue:</strong> <em>The convolution matrix is defined as $\widetilde{\mathbf{D}}^{r-1} \tilde{\mathbf{A}} \widetilde{\mathbf{D}}^{-r}$.  By continually smoothing the node feature with infinite number of propagation in SGC, the final smoothed feature $\mathbf{X}^{(\infty)}$ is：</em>
$$
\mathbf{X}^{(\infty)}=\hat{\mathbf{A}}^{\infty} \mathbf{X}, \quad \hat{\mathbf{A}}_{i, j}^{\infty}=\frac{\left(d_{i}+1\right)^{r}\left(d_{j}+1\right)^{1-r}}{2 m+n}
$$
可以看到，无限多层的SGC，卷积矩阵$\hat{\mathbf{A}}_{i, j}^{\infty}$每个元素只和两个节点的度，节点数$n$以及边数$m$有关，即$\hat{\mathbf{A}}^{\infty} \mathbf{X}$表示节点聚合图中所有节点，聚合权重只与两个节点的度有关，与节点位置，距离根节点距离无关。因此度越大的节点被赋予更大的聚合权重，无论两个节点的相对位置如何。</p>
<p><strong>Local Smoothing Iteration (LSI)：</strong> 对于SGC，第$k$次迭代为$\mathbf{X}^{(k)}=\hat{\mathbf{A}}^{k} \mathbf{X}$。对于第$h$个feature，定义Influence matrix $I_{h}(k)$：
$$
I_{h}(k)_{i j}=\frac{\partial \hat{\mathbf{X}}_{i h}^{(k)}}{\partial \hat{\mathbf{X}}_{j h}^{(0)}}
$$
$I_{h}(k)_{i j}$ 表示在第$h$个feature处，节点$j$的变化对于<strong>节点$i$第$k$层输出的影响</strong>。因为$\mathbf{X}^{(k)}_{ih}=\hat{\mathbf{A}}^{k}_i \mathbf{X}_{:,h}$， 所以$\frac{\partial \hat{\mathbf{X}}_{i h}^{(k)}}{\partial \hat{\mathbf{X}}_{j h}^{(0)}} = \hat{\mathbf{A}}^{k}_{ij}$， 与特征$h$无关，因此节点$j$的输入特征对于节点$i$的第$k$层表示的影响为$\hat{\mathbf{A}}^{k}_{ij}$。那么第$k$次迭代的影响力矩阵可以写为：
$$
I(k)=\hat{\mathbf{A}}^{k}
$$</p>
<p>$$
\tilde{I} = I(\infty)=\hat{\mathbf{A}}^{\infty}
$$</p>
<p>影响力矩阵$I(\infty)$收敛于稳态分布，即无限多层GNN时，节点$j$对$i$的representation的影响只与两个节点的度有关，与他们之间的结构关系无关。</p>
<p>$\tilde{I}_i$为节点$v_i$的over-smoothing stationarity，LSI衡量了其他节点对$v_i$的影响力到达over-smoothing所需最少的迭代次数：
$$
K(i, \epsilon)=\min \left\{k:\left|\left|\tilde{I}_{i}-I(k)_{i}\right|\right|_{2}&lt;\epsilon\right\}
$$
上式可以通过迭代次数来控制节点$v_i$的平滑程度，是node-specific的。</p>
<p><strong>NDLS Pipeline</strong> 1). 节点依赖的局部平滑 （NDLS-F） 2). 基于平滑特征的base prediction 3). 节点依赖的标签平滑（NDLS-L）。其中，第一步为pre-processing，第三部为post-processing。图结构仅用于第一部和第三部，参数的训练过程不涉及图结构，因此更加scalable。</p>
<p>LSI和参数$\epsilon$ 使得每个节点可以与oversmoothing保持一个合适的距离。NDLS-F和NDLS-L利用label smoothing和node smoothing, 具体如下:</p>
<ul>
<li>
<p>NDLS-F</p>
<p>对于节点$i$， 计算它的LSI$K(i,\epsilon)$，对节点$i$做$K(i,\epsilon)$次propagation，然后做multi-scale features residual connection:
$$
\widetilde{\mathbf{X}}_{i}(\epsilon)=\frac{1}{K(i, \epsilon)+1} \sum_{k=0}^{K(i, \epsilon)} \mathbf{X}_{i}^{(k)}
$$
上式的矩阵形式可写为：
$$
\tilde{\mathbf{X}}(\epsilon)=\sum_{k=0}^{\max_{i} K(i, \epsilon)} \mathbf{M}^{(k)} \mathbf{X}^{(k)}, \quad \mathbf{M}^{(\mathbf{k})}{ }_{i j}=\left\{\begin{array}{l}
\frac{1}{K(i, \epsilon)+1}, \quad i=j \quad \text { and } \quad k \leq K(i, \epsilon) \\
0, \quad \text { otherwise }
\end{array}\right.
$$</p>
</li>
<li>
<p>Base Prediction</p>
<p>基于NDLS-F的得到的smoothed features $\tilde{\mathbf{X}}$训练一个base predictor，$\hat{\mathbf{Y}}=f(\widetilde{\mathbf{X}})$。 $\hat{\mathbf{Y}}$ 为模型预测的soft label (softmax output)。</p>
</li>
<li>
<p>NDLS-L</p>
<p>将预测的soft label $\hat{\mathbf{Y}}$ 再做Label Propagation，得到最终的预测结果。依然先计算在做Label Prop时，每个节点的LSI。 $\hat{\mathbf{Y}}^{(k)}=\hat{\mathbf{A}}^{k} \hat{\mathbf{Y}}$， 同理，影响力矩阵为$J_h(k) = I_h(k)$，LP for node $i$：
$$
\tilde{\mathbf{Y}}_{i}(\epsilon)=\frac{1}{K(i, \epsilon)+1} \sum_{k=0}^{K(i, \epsilon)} \hat{\mathbf{Y}}_{i}^{(k)}
$$</p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2021 《Combining Label Propagation and Simple Models Out-performs Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/c_and_s/</link>
      <pubDate>Mon, 11 Jul 2022 09:42:15 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/c_and_s/</guid>
      <description>ICLR2021 &amp;#34;Combining Label Propagation and Simple Models Out-performs Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=8E1-f3VhX1o">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文研究了结合更简单的模型来处理transductive node classification任务。 主要包括1个预测模块和两个后处理（post-processing）模块：</p>
<ul>
<li>Base predictor：忽略图结构，用简单模型（如MLP或线性模型）使用节点特征预测label</li>
<li>Error correction：校正步骤，将训练数据中的不确定性（误差）传播到图上，来校正Base predictor的预测</li>
<li>Smoothing：在图上平滑预测</li>
</ul>
<p>其中只有第一步base predictor的参数是可学习的，即涉及图结构的操作（Correction和Smoothing）无需参数学习，这种简单的模型使得参数数量减少了几个数量级，训练时间也减少了几个数量级，并且可以轻松扩展到大规模图。</p>
<p>相比于过去的GNN+LP的方法，C&amp;S更加高效：1）C&amp;S首先只使用节点特征进行低成本的base prediction；2）然后再使用标签传播对基础预测进行校正 ；3）最后对最终预测进行平滑。 第一步是预测操作，后两部是后处理操作，也就是第一步为一个独立的端到端模型，后两部基于一个inductive bias来调整节点的表示。即homophily假设：相连节点的误差和label是相似的（正相关）。训练节点的误差和它相连节点的误差应相似，那么就用训练节点的误差来校正邻居节点。</p>
<p>因此，将标签更加直接的整合到GNN的学习算法中是本文性能的关键，并且发现LP与node features是相互互补的信号。实验表明，在OGB-Products上，参数量比GNN少了2个数量级，训练时间也减少2个数量级。</p>
<h1 id="correct-and-smooth-cs-model">Correct and Smooth (C&amp;S) Model</h1>
<p>给定无向图$G=(V,E)$，$A$为邻接矩阵，$S=D^{-1 / 2} A D^{-1 / 2}$为归一化邻接矩阵。节点集划分为labeled nodes $V_L$和unlabeled nodes $V_U$，其中$V = V_L \cup V_U$。进一步，labeled nodes可以分为训练节点集$V_{L_t}$和验证节点集$V_{L_v}$。训练集和验证集的label分别为$Y_{L_t:}$和$Y_{L_v:}$， 每行为label的one-hot向量。</p>
<p><img loading="lazy" src="/posts/2022-07-11-CS/1.png#center" alt=""  />
</p>
<h2 id="simple-base-predictor">Simple Base Predictor</h2>
<p>$$
\min \sum_{i \in L_{t}} \ell\left(f\left(x_{i}\right), y_{i}\right)
$$</p>
<p>$f(\cdot)$为简单的训练模型+softmax，如浅层MLP， $\ell$为cross-entropy loss。 基于训练节点$V_{L_t}$特征的模型$f$可以得到输出预测$Z \in \mathbb{R}^{n\times c}$， 其中$Z$的每行是softmax得到的分类概率分布。Simple Base Predictor是一个独立训练的端到端模型。</p>
<h2 id="correcting-base-prediction-with-error-correlation-使用邻居误差关联来纠正基础预测">Correcting Base Prediction with Error Correlation (使用邻居误差关联来纠正基础预测）</h2>
<p>通过融合标签信息来提高base prediction $Z$的准确率。 本文期望base prediction中的误差沿着图中的边正相关，即节点$i$出的预测误差在它的邻居处也会出现相似的误差。为了实现这个目的，首先定义一个误差矩阵$E \in \mathbb{R}^{n \times c}$用来保存每个节点的预测误差，其中误差为训练数据集上的残差（只有训练节点由误差）其他没有训练过程中不知道label的节点误差设为0：
$$
E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:}  \quad 为训练集节点 V_{L_t}的误差
$$</p>
<p>$$
E_{L_{v},:}=0, \quad E_{U,:}=0  \quad 验证集和测试集节点的误差设为0
$$</p>
<p>若base predictor做出perfect prediction时，$E$将是一个全0矩阵。</p>
<p>然后要在$E$中填补图中其他节点（验证集和测试集节点）的误差。依据homophily假设，相邻接节点的误差相似，因此使用标签扩散技术来平滑误差，即优化一下函数：
$$
\hat{E}=\underset{W \in \mathbb{R}^{n \times c}}{\arg \min } \operatorname{trace}\left(W^{T}(I-S) W\right)+\mu||W-E||_{F}^{2}
$$
实际上就是Laplacian Smoothing，$W \in \mathbb{R}^{n \times c}$表示$c$个信号，最小化第一项用来保证$W$每一列在图上平滑，即相邻的节点的误差向量$W_i \in \mathbb{R}^c$相似。第二项要求$W$要尽量接近$E$。最优的$W$表示为$\hat{E}$，上式可以通过：
$$
E^{(t+1)}=(1-\alpha) E+\alpha S E^{(t)}
$$
迭代求解，其中$\alpha = 1/(1+\mu)$。得到的$\hat{E}$称为smoothed errors。Base predictor中得到的$E_{L_{t},:}$只包含训练节点的误差，而通过在图上的误差平滑后，基于homophily 假设可以得到图中所有的误差（平滑误差）。已知图中所有节点在base predictor中的预测为$Z$，它的误差矩阵为$\hat{E}$，然后用误差矩阵来校正预测结果：
$$
Z^{(r)} = Z + \hat{E}
$$
通过这种方式对图中所有节点的误差做校正会存在一个问题，已知训练集的总误差为$||E||_2$， 通过迭代计算得到的总误差为$||E^{(t)}||_2$，且$||S||_2 = 1$, 所以下式成立：
$$
||E^{(t+1)}||_2 = ||(1-\alpha)E + \alpha S E^{(t)}||_2 \leq (1-\alpha)||E||_2+\alpha ||S||_2||E^{(t)}||_2 = (1-\alpha)||E||_2+\alpha ||E^{(t)}||_2
$$
因为 $||E^{(1)}||_2 \leq (1-\alpha)||E||_2 + \alpha ||E||_2 = ||E||_2$， 可以推出$||E^{(2)}||_2\leq (1-\alpha)||E||_2 + \alpha ||E^{(1)}||_2 \leq (1-\alpha)||E||_2 + \alpha ||E||_2 = ||E||_2$。因此，可以得到：
$$
||E^{(t)}||_2 \leq ||E||_2
$$
可以看出，传播之后的总error小了，因此不能完全纠正所有节点上的error。并且实验发现，对残差做放缩可以取得实质上的帮助。因此，本文提出两种方式对误差（残差）做放缩。</p>
<ul>
<li>
<p><strong>Autoscale.</strong> 希望平滑后的总误差$\hat{E}$可以放缩到和训练集误差$E$差不多大小。 由于我们只知道训练节点上的真实误差，所以用训练集节点上的平均误差来缩放。形式上，令$e^T_j \in \mathbb{R}^c$表示$E$的第$j$行，即节点$j$的误差，用$\hat{e}^T_j \in \mathbb{R}^c$表示平滑之后的节点$j$误差，即$\hat{E}$的第$j$行。 定义：
$$
\sigma=\frac{1}{\left|L_{t}\right|} \sum_{j \in L_{t}}\left|\left|e_{j}\right|\right|_{1}
$$
$\sigma$为训练集节点的平均误差，对于每个unlabeled node $i \in  V_U$，它的校正prediction为：
$$
Z_{i,:}^{(r)}=Z_{i,:}+\frac{\sigma}{\left|\left|\hat{e}_{i}\right|\right|_{1}} \cdot \hat{e}_{i}^{T}
$$
其中校正误差为 $\frac{\hat{e}_{i}^{T}}{\left|\left|\hat{e}_{i}\right|\right|_{1}} \cdot \sigma$， 表示对校正的误差做放缩，使得unlabeled node每个节点的校正误差为训练集节点的平均校正误差。</p>
</li>
<li>
<p><strong>Scaled Fixed Diffusion (FDiff-scale).</strong> 每次传播完，把training node的误差设为真实误差再进行下一次传播。另外，本文发现用超参数来放缩误差校正也是有效的：$Z^{(r)}=Z+s \hat{E}$。</p>
</li>
</ul>
<h2 id="smoothing-final-predictions-with-prediction-correlation">Smoothing Final Predictions with Prediction Correlation</h2>
<p>在用$\hat{E}$校正base prediction $Z$后得到校正预测矩阵$Z^{(r)}$。为了得到最后的预测，本文进一步对校正预测做平滑处理。 动机是：图中相邻的顶点可能具有相似的标签，即homophily假设。而在对base prediction做校正后，仅是的相邻的节点具有相似的误差（误差正相关），为了使其进一步满足homophily假设（即标签正相关），本文通过另一个LP来使得label在图上是平滑的：定义一个预测矩阵$H \in \mathbb{R}^{n \times c}$, 将训练集ground-truth label赋值给对应位置：
$$
H_{L_{t},:}=Y_{L_{t},:}
$$
然后将验证集和测试集节点的位置赋值为校正预测 （平滑误差后的预测）：
$$
H_{L_{v} \cup U,:}=Z_{L_{v} \cup U,:}^{(r)}
$$
然后对矩阵$H$做LP:
$$
H^{(t+1)}=(1-\alpha) H+\alpha S H^{(t)}
$$
其中 $H^{(0)} = H$，直到收敛，即收敛的$H^{(T)}$会尽可能保持平滑，并且和$H^{(0)}$接近，即最优$H$会依据训练集label使得相邻节点的label尽可能一样的同时，对图中节点做校正。</p>
<h3 id="与appnp的关系">与APPNP的关系</h3>
<p>APPNP也可以视为先特征变换，再平滑的过程，但APPNP是端到端的过程，label信息没有被加入平滑过程中。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2022 《GLASS：GNN with Labeling Tricks for Subgraph Representation Learning》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/glass/</link>
      <pubDate>Thu, 09 Jun 2022 23:01:07 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/glass/</guid>
      <description>ICLR2022 &amp;#34;GLASS：GNN with Labeling Tricks for Subgraph Representation Learning&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/forum?id=XLxhEjKNbXj">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>SubGNN在学习子图representation时保留子图的三种属性：Position，Neighborhood，Structure，每种属性包含Internal 和Border两方面，并且要精心设计不同的anchor patch，所以过于复杂。通过分析SubGNN和普通GNN，作者发现子图表示的核心可能是区分子图内部和外部节点。基于此发现，本文提出了一种labeling trick, 即max-zero-one，来提升子图GNN的表达能力和可拓展性。</p>
<p><img loading="lazy" src="/posts/2022-06-09-GLASS/1.png#center" alt=""  />
</p>
<p>Subgraph representation task 如上图所示，目标子图$\mathbb{S}$被嵌入在整个图中，并且可能拥有多个连通分量，目标是学习子图的表示向量，使其可以预测子图的属性。NeurIPS2020文章SubGNN提出子图级message-passing来代替节点级的message passing，并且设计了三个message passing通道，每个通道分为内部和边界模块，分别捕获子图分量间的交互，以及子图与图的其他部分之间的交互。尽管取得了比普通GNN更好的效果，但是SubGNN需要繁琐的预计算，因为SubGNN通过不同采样规则的anchor patch来传递子图分量之间，以及子图分量与图其他部分之间的相关性，而三个通道共6个aspects需要不同的采样规则，以及各自的message passing，计算十分冗长（<a href="https://jhuow.fun/posts/subgnn/">这里</a>有解读）。另外 SubGNN对每个aspects需要使用不同的anchor patch随机采样策略，无法保证采样的anchor patch是最优的，因此效果的方差较大，使得鲁棒性堪忧。</p>
<p>通过对比SubGNN相较于普通GNN的优势，作者发现对于<strong>子图任务来说，区分子图内部节点和外部节点非常重要</strong>。基于这个发现，本文提出了一种<em>labeling trick</em>，即max-zero-one labeling trick，来标注每个节点是否在子图外或者子图内。</p>
<p><strong>Labeling Trick [1]:</strong> 使用GNN生成multi-node representations （即，为一组节点，例如子图生成表示向量），该方法说明了为高阶结构生成有表达能力的representation，需要捕获结构内不同节点间的交互。在实现上，labeling trick通过一个专门设计的label来表示节点的结构特征，这个label与node feature 结合作为新的feature输入GNN中。</p>
<p>注： 本文只考虑诱导子图，即每个子图的每个连通分量保留原图中的所有边。</p>
<h1 id="plain-gnn-and-subgnn">Plain GNN and SubGNN</h1>
<p><img loading="lazy" src="/posts/2022-06-09-GLASS/2.png#center" alt=""  />
</p>
<p>如上图所示，$G$是一个regular graph, 所以在没有节点feature的情况下，每个节点的embedding相同，所以GNN无法区分子图$\mathcal{S}$和$\mathcal{S}^\prime$。如下图所示，Plain GNN 在message passing中子图$\mathcal{S}$内部节点1同时接收来自子图内和子图外的邻居信息，并不会加以区分。同样$\mathcal{S}^\prime$中节点3也同时接收子图内外节点，因此对于Plain GNN ，它无法区分节点1和3，因此无法区分两个子图。</p>
<p><img loading="lazy" src="/posts/2022-06-09-GLASS/3.png#center" alt=""  />
</p>
<p>而SubGNN引入了3个通道：position (P)，neighborhood (N), 和structure (S) 每个通道分别学习Internal 和Border两方面，共6个属性融入子图表示学习中。对于子图$\mathcal{S}$，为了捕获某个通道$i$的属性，SubGNN首先随机采样$n_A$个anchor patches： $\mathbb{A}_{i}=\left\{\mathcal{A}_{i}^{(1)}, \ldots, \mathcal{A}_{i}^{\left(n_{A}\right)}\right\}$，然后学习$\mathcal{S}$中的每个连通分量在这个属性$i$下的表示向量，通过子图内部连通分量和anchor patches之间的消息传递，来捕获子图内部连通分量的相对位置/邻域/结构信息，以及子图连通分量相对于子图外部分的位置/邻域/结构信息。如图2右边所示。对于通道$i$，它的Internal和border两方面采样的anchor patches表示为$\mathbb{A}_{i}=\left\{\mathcal{A}_{i}^{(1)}, \ldots, \mathcal{A}_{i}^{\left(n_{A}\right)}\right\}$，对于子图$\mathcal{S}$的一个连通分量$\mathcal{S}^{(c)}$，要学习该连通分量的表示，可使用一下subgraph-level message passing layer:
$$
\begin{aligned}
&amp;\boldsymbol{a}_{i, \mathcal{S}^{(c)}}=\sum_{\mathcal{A}_{i} \in \mathbb{A}_{i}} \gamma_{i}\left(\mathcal{S}^{(c)}, \mathcal{A}_{i}\right) \boldsymbol{g}_{\mathcal{A}_{i}}, \\
&amp;\boldsymbol{h}_{i, \mathcal{S}^{(c)}}^{(k)}=\sigma\left(W_{i} \cdot\left[\boldsymbol{a}_{i, \mathcal{S}^{(c)}}, \boldsymbol{h}_{i, \mathcal{S}^{(c)}}^{(k-1)}\right]\right)
\end{aligned}
$$
其中$\gamma_{i}\left(\mathcal{S}^{(c)}, \mathcal{A}_{i}\right)$是子图分量$\mathcal{S}^{(c)}$和一个anchor patch $\mathcal{A}_{i}$的相似度。即每个子图分量依照与anchor patch 的相似度聚合来自anchor的信息。由于相似度函数的存在，SubGNN实际上是使用与子图分量$\mathcal{S}^{(c)}$接近或结构相似的anchor patch对$\mathcal{S}^{(c)}$的representation做平滑，即$\mathcal{S}^{(c)}$聚合更多与它结构相似的anchor patches的信息。通过精心设计的anchor和subgraph-level message passing，6个属性可以被各自保留，然后在融合。</p>
<p>Plain GNN 存在的问题在于不能很好的表示内部结构和外部结构，即Plain GNN在message passing过程中不能为子图中的节点判断它的邻居是在子图内还是子图外。 而SubGNN如Figure 2右边所示， 子图内节点1接收Internal消息和border消息在两个独立的message passing中，回味每个节点生成2个表示向量，分别表示内部MP和外部MP，因为节点1和3内外部节点不一样，所以SubGNN可以为这两个节点生成不同的representations。</p>
<h1 id="glassgnns-with-labeling-tricks-for-subgraph">GLASS：Gnns with LAbeling trickS for Subgraph</h1>
<p>首先介绍zero-one label trick：</p>
<p><strong>Definition 1 （zero-one label trick）:</strong> 给定一个图$\mathcal{G}$和它的一个子图$\mathcal{S}$，对与子图$\mathcal{S}$， 图$\mathcal{G}$中的任意一个节点$v$的zero-one标签为：
$$
l_{v}^{(\mathcal{S})}= \begin{cases}1 &amp; \text { if } v \in \mathbb{V}_{\mathcal{S}} \\ 0 &amp; \text { if } v \notin \mathbb{V}_{\mathcal{S}}\end{cases}
$$
即对于一个子图$\mathcal{S}$，对图中所有节点赋予一个node label，用来区分节点在$\mathcal{S}$内外。</p>
<h2 id="max-zero-one-labeling-trick">Max-Zero-One Labeling Trick</h2>
<p>对每个节点做zero-one labeling trick可以区分<strong>一个</strong>子图的内外节点，因此zero-one labeling trick难以做batch training。因为为一个子图标记内部节点和外部节点，可以得到一个<strong>labeled graph</strong> （子图内节点为1，子图外节点为0），也就是对于一个graph $\mathcal{G}$，每个子图都需要专门生成一个该子图的labeled graph, 不同子图的labeled graph 也不同。如果要为每个子图都构造labeled graph, 再各自在每个labeled graph上做独立的message passing，得到每个labeled graph对应的子图embedding，这样过于耗时。</p>
<p>为了减轻上述每个组图对应一个特定的labeled graph 问题，本文认为可以为一个batch 子图生成一个labeled graph，这样的话，通过一次message passing就可以计算一个batch subgraphs的representations。为了结合一个batch 子图的zero-one labels，从而生成一个公共的labeled graph， 本文进一步提出了<strong>Max-Zero-One</strong> Labeling Trick。具体来说，一个batch的所有子图只生成一个labeled graph，其中，该batch内所有子图内节点都被赋予label 1, 所有子图外节点都被赋予label 0， 然后在labeled graph 上做GNN，就可以一次性学习一个batch子图的representations。</p>
<p>作者认为如果目标子图稀疏的分布在图中的话，一个子图外有其他节点被赋予1标签的影响是微不足道的，因为浅层GNN也不会聚合到远距离的节点。另外这么做还可以避免对一个子图的过拟合。</p>
<h2 id="implementation">Implementation</h2>
<p>Input: Graph $\mathcal{G}$ ,  所有子图<code>subG_node = [[subgraph 1], [subgraph 2], [subgraph 3], ...]</code>， $z=[0,0,1,0,1,1,0,0,1, \cdots]$ 为batch subgraphs对应的labeled graph, 在batch subgraphs中的节点为1，不在的为0。</p>
<p>以下为一层GLASS：</p>
<ul>
<li>
<p>对于每个节点特征，分别做两个线性变换</p>
<pre tabindex="0"><code>x1 = MLP_1(x)  # 节点充当batch subgraphs内节点时的embedding
x0 = MLP_0(x)  # 节点充当batch subgraphs外节点时的embedding
</code></pre></li>
<li>
<p><strong>对于label=1的节点 （batch 子图内的节点）</strong></p>
<p>特征 $x = \alpha x_1 + (1-\alpha)x_0$， 对于ppi_bp数据集，$\alpha = 0.95$为超参数，若节点是batch子图内的节点，保留更多$x_1$。</p>
<p><strong>对于label=0的节点 （batch 子图外的节点）</strong></p>
<p>$x = (1-\alpha)x_1 + \alpha x_0$，对于不在batch子图中的节点，保留更多$x_0$。</p>
<p>通过这种方式，子图内外的节点得以区分</p>
</li>
<li>
<p>Message Passing:</p>
<p>$x = (D^{-1}A)X$</p>
<p>GraphNorm:</p>
<p>$x = \mathrm{GraphNorm}(x)$</p>
<p>Residual:</p>
<p>$x = \mathrm{cat}(x_, x)$    //和初始特征拼接</p>
</li>
<li>
<p>再次区分batch subgraph 内外节点：</p>
<pre tabindex="0"><code>x1 = MLP_2(x)  
x0 = MLP_3(x) 
</code></pre></li>
<li>
<p>再对子图内外节点做不同的组合</p>
<p>$x = \alpha x_1 + (1-\alpha)x_0$： 子图内节点</p>
<p>$x = (1-\alpha)x_1 + \alpha x_0$： 子图外节点</p>
</li>
</ul>
<p>可以发现，如果$\alpha = 1$，那么相当于子图内节点用$x_1$， 子图外节点用$x_0$，这样就彻底区别了子图内外的节点。即， 对于邻居聚合操作来说，如果聚合到了子图外邻居，那么子图外邻居使用$\mathrm{MLP}_0$变换过的特征，如果聚合到子图内的节点，使用$\mathrm{MLP}_1$变换过的特征。</p>
<h2 id="一点理论">一点理论</h2>
<p><strong>Proposition 1:</strong>  <em>给定图$G$，$\mathcal{S}$和$\mathcal{S}^\prime$，如果Plain GNN可以区分的子图，GLASS也一定可以区分。但是存在Plain GNN不能区分但GLASS可以区分的子图。</em></p>
<p><strong>Proof:</strong> 首先证明给定任意Plain GNN  model $m_1$，存在一个GLASS模型$m_2$，使得对于目标子图$\mathcal{S}$，$m_1$和$m_2$的输出相同，也就是GLASS至少可以和Plain GNN一样expressive。</p>
<p>假设Plain GNN  $m_1$ 的第$k$层 $\mathrm{AGGREGATE}$函数为$f^{(k)}_1$，$\mathrm{COMBINE}$函数为$g^{(k)}_1$， 第$k$层$\mathrm{READOUT}$函数为$\phi_1$，那么Plain GNN可以表达为：
$$
\begin{aligned}
&amp;\boldsymbol{a}_{v}^{(k)}=\operatorname{AGGREGATE}^{(k)}\left(\left\{\boldsymbol{h}_{u}^{(k-1)} \mid u \in N(v)\right\}\right) = f^{(k)}_1 \left(\left\{\boldsymbol{h}_{u}^{(k-1)} \mid u \in N(v)\right\}\right) \\
&amp;\boldsymbol{h}_{v}^{(k)}=\operatorname{COMBINE}^{(k)}\left(\boldsymbol{h}_{v}^{(k-1)}, \boldsymbol{a}_{v}^{(k)}\right) = g^{(k)}_1\left(\boldsymbol{h}_{v}^{(k-1)}, \boldsymbol{a}_{v}^{(k)}\right) \\
&amp;\boldsymbol{h}_{\mathcal{S}}=\operatorname{READOUT}\left(\left\{\boldsymbol{h}_{u} \mid u \in \mathbb{V}_{\mathcal{S}}\right\}\right) = \phi_1\left(\left\{\boldsymbol{h}_{u} \mid u \in \mathbb{V}_{\mathcal{S}}\right\}\right)
\end{aligned}
$$
接下来设计GLASS，将每层节点特征定义为$\operatorname{CONCATENATE}\left(\boldsymbol{h}_{u}^{(k-1)}, \boldsymbol{l}^{(\mathcal{S})}\right)$，即每层拼接该节点的label （是否在子图中），基于universal approximation theorem，一定存在一个函数$\theta$, 使得$\theta\left(\operatorname{CONCATENATE}\left(\boldsymbol{h}_{u}^{(k-1)}, \boldsymbol{l}^{(\mathcal{S})}\right)\right)=\boldsymbol{h}_{u}^{(k-1)}$，那么GLASS可以定义为：
$$
\begin{aligned}
\boldsymbol{h}_{u}^{\prime(k-1)} &amp;=\operatorname{CONCATENATE}\left(\boldsymbol{h}_{u}^{(k-1)}, \boldsymbol{l}^{(\mathcal{S})}\right), \\
\boldsymbol{a}_{v}^{(k)} &amp;=f_{1}^{(k)}\left(\left\{\theta \left(\boldsymbol{h}_{u}^{\prime (k-1)}\right) \mid u \in N(v)\right\}\right) \\
\boldsymbol{h}_{v}^{(k)} &amp;=g_{1}^{(k)}\left(\boldsymbol{h}_{v}^{(k-1)}, \boldsymbol{a}_{v}^{(k)}\right)
\end{aligned}
$$
因为$\theta \left(\boldsymbol{h}_{u}^{\prime (k-1)}\right) = \theta\left(\operatorname{CONCATENATE}\left(\boldsymbol{h}_{u}^{(k-1)}, \boldsymbol{l}^{(\mathcal{S})}\right)\right)=\boldsymbol{h}_{u}^{(k-1)}$， Plain GNN 是上述特定形式GLASS的特例，所以GLASS使得至少与Plain GNN 表达能力相同。</p>
<p>Figure 2给出了Plain GNN不能区分但GLASS可以区分的子图实例。</p>
<p><strong>Theorem 1：</strong> <em>给定任意图$\mathcal{G}$，存在一个GLASS model，可以准确预测$\mathcal{G}$中任意子图的density 和cut ratio。</em></p>
<p><strong>Proof:</strong> 一个图的Density定义为：
$$
D = \frac{2 |E|}{|V| \cdot|V-1|}
$$
即图中实际存在的边数，占左右节点对可能构成的总边数的比例</p>
<p>一个子图$\mathcal{S}$的cut ratio定义为：
$$
\mathrm{CR}(\mathcal{S}) = \frac{|B_{\mathcal{S}}|}{|\mathcal{S}| \cdot |\mathcal{G} \backslash \mathcal{S}|}
$$
为子图和其他部分之间的边数$|B_{\mathcal{S}}|$, 占子图和其他部分可能存在的总边数的比例。其中$B_{S}=\{(u, v) \in E \mid u \in S, v \in G \backslash S\}$。
$$
\begin{aligned}
\boldsymbol{a}_{v}^{(1)} &amp;=\sum_{u \in N(v)}\left(\boldsymbol{l}_{u}^{(\mathcal{S})}\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]+\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]\right)  = \left[\begin{array}{l}
N(v)中存在于\mathcal{S}的节点数量\quad m \\
N(v)中\mathcal{S}以外节点数量 \quad n\\
0
\end{array}\right]  \\
\boldsymbol{h}_{v}^{(1)} &amp;=\left[\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
-1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{array}\right] \boldsymbol{a}_{v}^{(1)}+\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right] = \left[\begin{array}{l}
m \\
n-m \\
1
\end{array}\right]  \\
\boldsymbol{h}_{\mathcal{S}} &amp;=\sum_{v \in \mathbb{V}_{\mathcal{S}}} \boldsymbol{h}_{v}^{(1)} = \left[\begin{array}{l}
子图内边数 \\
边界边数-子图内边数\\
子图内节点数
\end{array}\right]
\end{aligned}
$$
其中$l_{u}^{(\mathcal{S})}$为节点$u$的zero-one label，如果$u$在子图$\mathcal{S}$中，那么为1不在为0。 因此子图$\mathcal{S}$的Density $d$和 cut ratio $c$可以由上述定义的GLASS模型推导出：
$$
\begin{aligned}
&amp;d\left(\boldsymbol{h}_{\mathcal{S}}\right)=\left(\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}\right) /\left[\left(\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}\right) \cdot\left(\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}-1\right)\right] \\
&amp;c\left(\boldsymbol{h}_{\mathcal{S}}\right)=\left(\left[\begin{array}{c}
0 \\
0.5 \\
0
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}\right) /\left[\left(\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}\right) \cdot\left(n-\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}-1\right)\right] .
\end{aligned}
$$</p>
<h1 id="reference">Reference</h1>
<p>[1] Labeling trick: A theory of using graph neural networks for multi-node representation learning. NeurIPS2021</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2020 《Subgraph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/subgnn/</link>
      <pubDate>Fri, 27 May 2022 17:13:33 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/subgnn/</guid>
      <description>NeurIPS2020 &amp;#34;Subgraph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2006.10538">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>GNN通常关注节点级任务和图级任务，缺少针对子图级预测任务的方法。针对这个问题，本文提出SubGNNs用于解耦子图在不同结构aspect的表示。为了学习准确的子图表示，SubGNN在子图的连通分量和随机采样的anchor patches之间进行消息传递，从而学习高准确度的子图表示。SubGNN指定了三个通道，每个通道捕获子图不同的拓扑结构属性。</p>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/1.png#center" alt="图1"  title="123"  />
</p>
<p>从拓扑的角度来看，子图是非常具有挑战性的结构，对子图的预测存在以下挑战：</p>
<ul>
<li>如要对更大且size不同的子图做联合预测，挑战在于<strong>如何表征含有多个分量，甚至分量间间隔较远的子图</strong>。</li>
<li>子图包含了高阶连通模式（connectivity patterns），这些连通模式不仅存在于子图内节点之间，也存在与子图内节点与子图外部节点之间， 挑战在于<strong>如何将子图边界信息和子图外部信息注入GNN中</strong>。</li>
<li>子图可能存在于图中的一个特定区域，也可能它的连通分量分布于多个局部邻域，挑战在于<strong>如何学习子图在图中的位置</strong>。</li>
<li>子图间共享边（sharing edges）和非边（non-edges）存在相关性，挑战在于<strong>如何将这种子图间的依赖融合进模型中，同时任然能够将特征信息考虑在内进行辅助归纳推理</strong>。</li>
</ul>
<p>本文提出SubGNN以解决上述挑战， SubGNN的核心原则是子图级的消息传递，可以捕获子图位置、邻域、结构三种特征</p>
<h1 id="formulating-subgraph-prediction">Formulating Subgraph Prediction</h1>
<p>给定无向图$G=(V,E)$，它的一个子图表示为$S=\left(V^{\prime}, E^{\prime}\right)$，每个子图$S$有一个label $y_{S}$，并且子图$S$可能包含多个连通分量，连通分量表示为$S^{(C)}$。</p>
<p><strong>Problem (Subgraph Representations and Property Prediction)</strong> 给定子图集合 $\mathcal{S} = \left\{S_{1}, S_{2}, \ldots, S_{n}\right\}$，SubGNN $E_S$为每个子图$S\in \mathcal{S}$生成一个$d_s$维的表示向量$\mathbf{Z}_S \in \mathbb{R}^{d_{s}}$， 然后用这些子图的表示向量学习一个子图分类器 $f: \mathcal{S} \rightarrow\{1,2, \ldots, C\}$，使得输入子图得到预测label: $f(S)=\hat{y}_{S}$。</p>
<p>本文针对子图分类任务，所提出的模型为一个可学习的embedding函数$E_{S}: S \rightarrow \mathbb{R}^{d_{s}}$， 将每个子图映射为低维表示向量，这些表示向量可以捕获子图拓扑对预测重要的aspects。具体来说，对于一个子图，message再它的连通分量之间传递，这使得我们可以对多个连通分量的子图学习有意义的表示。</p>
<h2 id="subgnn-properties-of-subgraph-topology">SUBGNN: Properties of subgraph topology</h2>
<p>子图拥有独特的内部结构，边界连通性，邻域概念，以及和图其他部分的相对位置。直觉上，我们的目标是以最大的似然保存保存特定的图属性。本文设计模型以考虑<strong>6</strong>种特定的图结构属性：</p>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/2.png#center" alt="图1"  title="123"  />
</p>
<p>具体来说：</p>
<p><strong>(1) Position.</strong></p>
<p>Border Position: 该属性保留子图和图的其他部分之间的距离，通过这种距离关系，可以区分两个同构但处于不同位置的子图。</p>
<p>Internal Position：子图自己连通分量之间的距离。</p>
<p><strong>(2) Neighborhood.</strong></p>
<p>Border Neighborhood：为子图的边界邻域，表示子图$S$中任意节点的$k$跳邻域中（不属于子图$S$）的节点集合。</p>
<p>Internal Neighborhood：子图内每个连通分量的边界邻域，每个连通分量$S^{(c)}$中任意节点的$k$跳邻域中（不属于子图$S^{(c)}$）的节点集合。</p>
<p><strong>(3) Structure.</strong></p>
<p>Border Structure：子图内部节点和边界邻居之间的连通性。</p>
<p>Internal Structure：每个连通分量的内部连通性。</p>
<p>本文旨在将上述属性学习到子图表示向量中。</p>
<h1 id="subgnn-subgraph-neural-network">SubGNN： Subgraph Neural Network</h1>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/3.png#center" alt="图1"  title="123"  />
</p>
<p>SubGNN以层次的方式学习子图表示，将神经消息从anchor patch传递到子图分量中，并将所有子图分量的表示聚合为最终的子图表示。如图2（a）所示，独立考虑子图的每个连通分量的每个属性，从anchor patch中获得相应的属性信息，对于每个分量，聚合它的所以属性表示，得到该分量的表示，然后聚合子图所有分量的表示，得到最后的子图表示。</p>
<h2 id="property-aware-routing">Property-Aware Routing</h2>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/4.png#center" alt="图1"  title="123"  />
</p>
<p>通过anchor patch向子图分量传递消息，使得子图分量可以实现结构属性感知，如图2（b）所示，每个通道$\mathbf{z}_{i}$表示子图的第$i$个分量的输出表示，它由3部分构成，分别为该分量的位置属性（绿色），邻居属性（蓝色），结构属性（橙色）。将所有通道（分量）聚合，可以得到最终的子图表示。</p>
<p>对于每个属性，我们定义一个anchor patch 采样函数$\phi_{\mathrm{X}}:\left(G, S^{(c)}\right) \rightarrow A_{\mathrm{X}}$，即对于子图$S$的一个连通分量$S^{(c)}$，$\phi_{\mathrm{X}}$为该子图分量输出属性$X$的anchor patch。</p>
<h3 id="position">Position</h3>
<p>为了捕获Internal Position, $\phi_{\mathrm{P}_{\mathrm{I}}}$返回anchor patch $A_{P_{I}}$，每个anchor patch为 子图内的单个节点，所有的针对Internal Position属性的anchor patch集合为$\mathcal{A}_{P_{I}} = \{A^{(1)}_{P_{I}},A^{(2)}_{P_{I}}, \cdots\}$， 为子图$S$内随机采样的节点集合，即$A^{(i)}_{P_{I}}$为一个子图内的节点。由于Internal Position的目的是捕获子图连通分量之间的距离，因此不同的连通分量共享anchor，通过共享的anchor和不同连通分量之间的相似度，来将anchor的信息聚合到不同的连通分量中，这将允许子图中不同连通分量相互定位。 例如$S$有两个连通分量，有一个anchor $A^{(i)}_{P_{I}}$存在于其中一个分量中，那么两个连通分量分别依据和anchor的相似度来聚合anchor的表示，那么如果有很多anchor的情况下，可以较为准确的为这两个分量区分相对位置。子图$S$的anchor patch集合$\mathcal{A}_{P_{I}}$对$S$的<strong>所有分量</strong>共享。</p>
<p>为了捕获Border Position， 由于Border Position是子图整体和图的其他部分的距离，所以$\phi_{\mathrm{P}_{\mathrm{B}}}$采样的节点在所有<strong>子图</strong>间共享，anchor集$\mathcal{A}_{P_{B}} = \{A^{(1)}_{P_{B}},A^{(2)}_{P_{B}}, \cdots\}$中每个anchor patch $A^{(i)}_{P_{B}}$是随机采样的节点，并且集合$\mathcal{A}_{P_{B}}$所有子图都共享，即所有子图的所有分量都聚合来自$\mathcal{A}_{P_{B}}$的消息，例如子图$S_1$和$S_2$， $S_1$的所有分量依据和anchor的相似度聚合所有anchor的信息，同样$S_2$的所有分量依据和anchor的相似度聚合所有anchor的信息，那么如果anchor数量足够多，并且在子图间共享，所以为子图$S_1$和$S_2$学习到的emb可以分别反映两个子图和原图中其他部分的相似度，从而区分两个子图的position。</p>
<p>综上所述，对于Internal Positon，anchor在子图中采样，且在同一个子图的不同分量间共享，从而捕获同一个子图不同分量间的位置距离。</p>
<p>对于Border Position， anchor在整个图中采样，且在所有子图间共享，从而捕获不同子图在原图中的位置距离。</p>
<h3 id="neighborhood">Neighborhood</h3>
<p>为了捕获Internal Neighborhood，$\phi_{\mathrm{N}_{\mathrm{I}}}$是从子图分量$S^{(c)}$中采样anchor patch，对于每个子图分量，它的anchor patch来自自己内部节点，聚合来自自己内部节点的消息，从而捕获内部邻域信息。</p>
<p>而$\phi_{\mathrm{N}_{\mathrm{B}}}$是从子图分量$S^{(c)}$的border neighborhood中采样anchor，即子图分量$S^{(c)}$中任意节点$k$-hop以内邻居（不在$S^{(c)}$）中的节点采样anchor，子图$S^{(c)}$聚合这些$k$-hop border neighborhood从而为每个子图分量捕获边界邻域信息。</p>
<h3 id="structure">Structure</h3>
<p>$\phi_{\mathrm{S}}$采样anchor patch用于捕获子图的内部结构信息和边界结构信息，针对内部结构信息采样的anchor集合$\mathcal{A}_{\mathrm{S}_{1}}$以及针对边界结构信息采样的anchor集合$\mathcal{A}_{\mathrm{S}_{B}}$对所有子图也是共享的，具体来说，$\phi_{\mathrm{S}}$返回的是根据三角随机游走从图中抽取的连通部分，通过计算与这些anchor<strong>图</strong>的相似度来聚合这些anchor图，从而区分不同子图在结构上的相似度。具体来说，每个子图根据与共享anchor图之间的相似度，聚合anchor图，那么不同子图如果与共享的anchor子图集相似度越高，那么这这些子图的结构相似度就越高。例如，$S_1$和$S_2$为$G$中的两个子图，要使两个子图的embedding可以反映两个子图结构上的相似性，那么给定一大堆anchor子图，$S_1$基于它和这些anchor的相似性聚合所有anchor，$S_2$同样基于它和anchor的相似性聚合所有anchor，anchor set相当于一个相似度中介，两个子图的embedding分别反映了两个子图和共享anchor set之间的相似度 $Sim_1$和$Sim_2$，那么如果，如果$Sim_1$和$Sim_2$的差别较大，那么说明两个子图与anchor set的相似度相差太大，所以两个子图的结构差别较大。</p>
<h2 id="neural-encoding-of-anchor-patches">Neural Encoding of Anchor Patches</h2>
<p>对提取出的anchor进行编码，对于Position anchor patches 和 Neighbor anchor patches，由于每个anchor patch是单一节点，所以节点特征就是anchor的representation。而对于 Structure属性的anchor patches，它是一个个子图，为了将他们编码，本文首先在每个anchor patch上进行长度为$w$参数为$\beta$的三角随机游走，得到节点序列$\left(u_{\pi_{w}(1)}, \ldots, u_{\pi_{w}(n)}\right)$，然后将节点序列输入双向LSTM中得到最终的patch 表示$\mathbf{a}_{S}$。</p>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/7.png#center" alt="图1"  title="123"  />
</p>
<h2 id="subgraph-level-message-passing">Subgraph-Level Message Passing</h2>
<p>SubGNN的消息传递定义在<strong>子图分量级</strong>。首先为每个结构属性采样一对anchor patches：$\mathcal{A}_{\mathrm{X}}=\left\{A_{\mathrm{X}}^{(1)}, \ldots, A_{\mathrm{X}}^{\left(n_{A}\right)}\right\}$，每个属性的采样规则如Property-Aware Routing 中所示，即针对Position和Neighborhood属性，采样的anchor patch是节点，针对Structure，采样的anchor patch是子图。$\mathcal{A}_{\mathrm{P}}$、$\mathcal{A}_{\mathrm{N}}$和$\mathcal{A}_{\mathrm{S}}$分别为三个属性的anchor patch 集合。对于子图$S$的第$c$个分量$S^{(c)}$，定义从anchor patch $A_{\mathrm{X}}$到$S^{(c)}$的消息：
$$
\mathrm{MSG}_{\mathrm{X}}^{A \rightarrow S}=\gamma_{\mathrm{X}}\left(S^{(c)}, A_{\mathrm{X}}\right) \cdot \mathbf{a}_{\mathrm{X}}
$$
其中$X$是结构属性，$\gamma_{\mathrm{X}}$是该结构属性的相似度函数，用于衡量anchor patch 和子图embedding之间的相似度值。将一个属性下所有anchor patches的消息聚合，然后与子图分量$S^{(c)}$结合成该子图分量在$X$属性上的表示：
$$
\begin{aligned}
&amp;\mathbf{g}_{\mathrm{X}, c}=\mathrm{AGG}_{M}\left(\left\{\mathrm{MSG}_{\mathrm{X}}^{A_{\mathrm{X}} \rightarrow S^{(c)}} \forall A_{\mathrm{X}} \in \mathcal{A}_{\mathrm{X}}\right\}\right) \\
&amp;\mathbf{h}_{\mathrm{X}, c} \leftarrow \sigma\left(\mathbf{W}_{\mathrm{X}} \cdot\left[\mathbf{g}_{\mathrm{X}, c} ; \mathbf{h}_{\mathrm{X}, c}\right]\right),
\end{aligned}
$$
<img loading="lazy" src="/posts/2022-05-29-SubGNN/5.png#center" alt="图1"  title="123"  />
</p>
<p>以上图为例，给定一个子图$S$的第一个连通分量$S^{(1)}$在Position属性下的输入embedding为$\mathbf{h}_{P,1}$， Position属性在Internal和Border方面一共有4个anchor patches，每个anchor patches将消息聚合到$S^{(1)}$中，得到4个Messages,即 $\mathbf{m}_{P,1,1}$，$\mathbf{m}_{P,1,2}$，$\mathbf{m}_{P,1,3}$，$\mathbf{m}_{P,1,4}$，分别表示4个Position属性的anchor patch聚合到子图$S$第1个连通分量的消息。</p>
<p>$\mathbf{h}_{\mathrm{X}, c}$的顺序不变性是层到层消息传递的重要属性，但是它会限制捕捉子图结构和位置的能力。因此这里构造了property-aware的输出表征$\mathbf{Z}_{X, c}$，通过将属性$X$在分量$c$上的所有anchor message拼接，得到anchor-set message 矩阵$\mathbf{M}_{\mathrm{X}}$， 如图二所示，$\mathbf{M}_{\mathrm{X}}$的每行都是anchor-set message （集合消息），然后传递给非线性激活函数（如算法1所示）。输出的表示向量的每一维都编码了anchor patch 的结构信息和位置信息，对于邻域通道，设定$\mathbf{Z}_{\mathrm{N}, c}=\mathbf{h}_{\mathrm{N}, c}$。最后SubGNN为连通分量拼接每个属性的消息，得到分量表示$\mathbf{z}_c$。最后所有的分量表示通过$\mathrm{READOUT}$聚合成最后的子图表示$\mathbf{z}_{S}$。</p>
<p>概括来说，先得到子图每个分量$S^{(c)}$在属性$X$上的表示$\mathbf{z}_{X,c}$，然后1）对于每层SubGNN，将分量$S^{(c)}$的所有属性表示向量聚合，得到该层$c$分量的表示。2）将所有层的$S^{(c)}$分量的表示向量聚合，得到子图分量$S^{(c)}$的最终表示$\mathbf{z}_c$。3）子图$S$所有分量的表示聚合，得到最终的子图表示$\mathbf{z}_S$。</p>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/6.png#center" alt="图1"  title="123"  />
</p>
<h1 id="conclusion">Conclusion</h1>
<p>有点复杂~</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Decoupling the Depth and Scope of Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/decouplinggcn/</link>
      <pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/decouplinggcn/</guid>
      <description>NeurIPS2021 &amp;#34;Decoupling the Depth and Scope of Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2201.07858">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>现有的GNN在图和模型的size方面可拓展性有限。 对于大图，增加模型的深度对导致scope(感受野)大小成指数放大。深层model主要面临两个基本挑战： 1. oversmoothing导致表达能力下降，2. 邻域爆炸导致计算成本高昂。</p>
<p>本文旨在结构GNN的depth 和 scope，首先提取子图作为有限大小（bounded-size）的scope, 然后将任意深度的GNN用于子图上。 由于提取出的局部子图是由少量<strong>关键</strong>邻居组成，且排除了不相关的邻居，所以深层GNN也可以学到informative representations。</p>
<p>增加GNN层数会造成以下基本障碍：</p>
<ul>
<li>Expresivity (oversmoothing): 邻居的迭代混合导致不同节点的切入向量收敛到一个固定的低维子空间</li>
<li>Scalability (neighbor explosion): 多跳邻居递归导致感受野大小呈指数级增长</li>
</ul>
<p>为了研究导致表达能力和可拓展性缺陷的根本原因，本文提出了以下insight:</p>
<p><strong>Two views on the graph:</strong>  如果从全局视角来看两个节点，如果两个节点在同一个图的同一个连通分量中，那么这两个节点在随机游走中存在到达概率，无论他们间隔多远。而本文给出了图的局部视角，具体来说， 给定节点$v$的局部子图$\mathcal{G}_{[v]}$，将$\mathcal{G}_{[v]}$仅包含节点$v$的特性，整个图看一看做所有子图$\mathcal{G}_{[v]}$的集合。那么$v$的邻域不在是所有节点$\mathcal{V}$，而它的邻域只存在于$\mathcal{V}_{[v]}$中。 如果节点$u$不在$\mathcal{V}_{[v]}$中，$u$将永远不会被考虑为$v$的邻居，无论GNN有多深。</p>
<p><strong>Scope of GNNs:</strong> 加深GNN层次所造成的的表达能力和可拓展性问题都和GNN不断扩大的感受野（scope）有关。随着层次变深，<strong>感受野不断变大</strong>，使得每个节点包含的信息重叠越多，最终收敛到同一个子空间，导致oversmoothing; 另外 <strong>感受野变大</strong>，导致每个节点的邻居数呈指数级上升，导致邻居爆炸。所以GNN的层数加深会导致感受野变大（耦合），即$L$层GNN的感受野为全部$L$-hop以内的邻居，层数深度（depth）和感受野大小(scope)的强耦合限制了GNN的设计。</p>
<p><strong>Decoupling the GNN depth and scope:</strong> 为了解耦GNN的深度（depth）与感受野(scope)，使得加层数与感受野无关。对于节点$v$，首先为它提取一个小的子图$\mathcal{G}_{[v]}$，然后在小的子图上应用任意层数的GNN。若GNN的层数$L^\prime$大于感受野的跳数，那么子图中的每对节点会交换多次信息，额外的消息传递有助于GNN更好的融合scope内的信息，从而增强表达能力。</p>
<h1 id="decoupling-the-depth-and-scope-of-gnns">Decoupling the Depth and Scope of GNNs</h1>
<p><strong>Definition (Depth of subgraph)</strong> ：假设子图$\mathcal{G}_{[v]}$是连通的，$\mathcal{G}_{[v]}$的depth定义为$\max _{u \in \mathcal{V}_{[v]}} d(u, v)$, 其中$d(u, v)$表示$u$到$v$的最短路径。</p>
<p>本文提出shaDow-GNN，它包含了一个子图提取器$\text { EXTRACT}$。 shaDow-GNN的过程如下：</p>
<ol>
<li>用子图提取器$\operatorname{EXTRACT}(v, \mathcal{G})$为节点$v$提取一个连通子图$\mathcal{G}_{[v]}$，子图的深度（距离$v$最远的节点和$v$之间的跳数）为$L$。</li>
<li>构建一个$L^\prime$层的GNN并应用在$\mathcal{G}_{[v]}$上。 如果 $L^\prime &gt; L$那么可以反映decoupling，因为GNN层数此时与scope无关，层数加深不会影响感受野。</li>
</ol>
<p>本文从三个不同的角度理论证明了shaDow-GNN可以提升GNN的表达能力。</p>
<h2 id="graph-signal-processing-perspective">Graph Signal Processing Perspective</h2>
<ol>
<li>oversmoothing by <strong>deep GCN</strong>。 2. oversmoothing by <strong>repeated GCN-style propagation</strong>。</li>
</ol>
<p>对于deep GCN, 包含了非线性激活，权重和bias。而带有bias参数的deep GCN 不会导致oversmoothing[1]， 但任然存在准确率下降的问题，这说明GCN的这种传播形式是导致学习困难的根本原因，而不是来自于激活函数或者bias。</p>
<p>即，repeat-GCN-style propagation表示为$\boldsymbol{M}=\lim _{L \rightarrow \infty} \widetilde{\boldsymbol{A}}^{L} \boldsymbol{X}$这种邻居迭代聚合的传播形式会导致学习困难。因此，这里在忽略激活函数和bias的情况下分析聚合矩阵的渐近性。</p>
<p>对于子图$\mathcal{G}_{[v]}$， 无限次特征聚合表示为：$\boldsymbol{M}_{[v]}=\lim_{L \rightarrow \infty} \widetilde{\boldsymbol{A}}^{L}_{[v]} \boldsymbol{X}_{[v]}$。$\boldsymbol{M}_{[v]}$为节点$v$的子图$\mathcal{G}_{[v]}$的embedding矩阵。 由于$\widetilde{\boldsymbol{A}}_{[v]} = (\boldsymbol{D}_{[v]} + \boldsymbol{I}_{[v]})^{-\frac{1}{2}} (\boldsymbol{A}_{[v]} + \boldsymbol{I}_{[v]}) (\boldsymbol{D}_{[v]} + \boldsymbol{I}_{[v]})^{-\frac{1}{2}}$，$\widetilde{\boldsymbol{A}}_{[v]}$是实对称阵， 所以可以做特征分解， 即$\widetilde{\boldsymbol{A}}_{[v]}=\boldsymbol{E}_{[v]} \boldsymbol{\Lambda} \boldsymbol{E}_{[v]}^{-1}=\boldsymbol{E}_{[v]} \boldsymbol{\Lambda} \boldsymbol{E}_{[v]}^{\top}$。那么:
$$
\widetilde{\boldsymbol{A}}_{[v]}^{L \to \infty} =\boldsymbol{E}_{[v]} \boldsymbol{\Lambda}^L \boldsymbol{E}_{[v]}^{\top} = \boldsymbol{E}_{[v]} 	\begin{bmatrix}
\lambda_1^L &amp;   &amp; &amp;  \\
&amp;  \ddots &amp;  &amp; \\
&amp;   &amp; \lambda_N^L&amp;
\end{bmatrix}\boldsymbol{E}_{[v]}^{\top} = e_{[v]}e_{[v]}^\top
$$
因为归一化邻接矩阵$\widetilde{\boldsymbol{A}}_{[v]}$的最大特征值一定为1，且半正定，那么当$L \to \infty$时， 若$1 = \lambda_1 &gt; \cdots &gt; \lambda_N$, 所以$\lambda_1^L = 1$，$\lambda_2^L,\cdots,\lambda_N^L \to 0$。所以上式成立，其中$e_{[v]}$为$\widetilde{\boldsymbol{A}}_{[v]}$最大特征值对应特征向量。所以：
$$
\lim_{L\to \infty} \widetilde{\boldsymbol{A}}_{[v]}^{L} X_{[v]} = e_{[v]}e_{[v]}^\top X_{[v]} = e_{[v]}(e_{[v]}^\top X_{[v]}) = \boldsymbol{M}
$$
因此在子图$\mathcal{G}_{[v]}$上使用无限多层GNN的 shaDow-GNN得到的节点emb矩阵为$\boldsymbol{M} = e_{[v]}(e_{[v]}^\top X_{[v]})$, 那么 shaDow-GNN为节点$v$学习到的embedding为 节点$v$在其子图$\mathcal{G}_{[v]}$中的对应embedding。即：
$$
\boldsymbol{M}_{[v]}=\left[e_{[v]}\right]_{v} \cdot\left(\boldsymbol{e}_{[v]}^{\top} \boldsymbol{X}_{[v]}\right)  \tag{1}
$$
考虑到这是无向图，$\widetilde{\boldsymbol{A}}_{[v]}$的最大特征值对应的特征向量表示中的每个元素表示$\mathcal{G}_{[v]}$中对应节点的度。所以$\left[e_{[v]}\right]_{u}$是$\mathcal{G}_{[v]}$中节点$u$的度$\delta_{[v]}(u)$。所以$e_{[v]} = [\delta_{[v]}(u)]$是$\mathcal{G}_{[v]}$中节点的度向量。由于这里的粗体$e_{[v]}$是对$e_{[v]}$的normalization:<br>
$$
\left[e_{[v]}\right]_{u}=\sqrt{\frac{\delta_{[v]}(u)}{\sum_{w \in \mathcal{V}_{[v]}} \delta_{[v]}(w)}}
$$
上面的$\boldsymbol{M}_{[v]}$为 shaDow-GNN为节点$v$学到的embedding。</p>
<p>对于普通的GCN propagation，可以看做是一个有足够大的hop $L$的$\mathcal{G}_{[v]}$ 和一个有足够大hop $L$的$\mathcal{G}_{[u]}$，使得在无限多层GCN时，  $\mathcal{G}_{[v]} = \mathcal{G}_{[u]}$。即对于无限多层的普通GCN，两个不同节点的scope是一样的，这就说明  $e_{[v]} = e_{[u]}$，并且$\boldsymbol{X}_{[u]}=\boldsymbol{X}_{[v]}=\boldsymbol{X}$。即对于普通的无限多层GNN来说，两个不同节点的局部子图就是整个图，无法保存节点特有的特征信息，根据公式一，由于$e_{[v]} = e_{[u]}$只和两个节点的度有关，特征信息又是一样的，所以在整个图上使用无限多层GNN的到的两个节点的embedding $\boldsymbol{M}_{[v]}$和$\boldsymbol{M}_{[u]}$只和两个节点的度有关。</p>
<p>而对于shaDow-GNN，它实现一种局部平滑（local-smoothing）,由于是在子图上做特征聚合，那么无论多少层，都不会聚合到子图范围以外的邻居。从公式（1）可以看出，在子图内，目标节点的embedding实际上就是子图内所有节点embedding的线性组合，无限增加层数值只会使得线性组合的系数$e_{[v]}$收敛到一个固定值，即$\widetilde{\boldsymbol{A}}_{[v]}$的稳态分布向量。而稳态分布向量$e_{[v]}$的具体取值只与对应子图的度有关，与GNN层数无关。直观来看，子图提取器$\text { EXTRACT}$会为不同的节点提取不同的子图，若$u$,$v$的子图不同，那么对应的$e_{[v]}$与$e_{[v]}$也不同，$\boldsymbol{X}_{[u]}$和$\boldsymbol{X}_{[v]}$也不同，所以shaDow-GNN在无限多层下依旧捕获局部特征信息。</p>
<h2 id="function-approximation-perspective">Function Approximation Perspective</h2>
<p>GraphSAGE的每层定义为：
$$
\boldsymbol{h}_{v}^{(\ell)}=\sigma\left(\left(\boldsymbol{W}_{1}^{(\ell)}\right)^{\top} \boldsymbol{h}_{v}^{(\ell-1)}+\left(\boldsymbol{W}_{2}^{(\ell)}\right)^{\top}\left(\frac{1}{\mid \mathcal{N}_{v}\mid } \sum_{u \in \mathcal{N}_{v}} \boldsymbol{h}_{u}^{(\ell-1)}\right)\right)
$$
对于$L^\prime$层的shaDow-SAGE，，若$\text { EXTRACT}$为节点提取$L$-hop的邻居，并且，对于shaDow-SAGE的$L+1 \leq \ell \leq L^{\prime}$层，令$\boldsymbol{W}_{1}^{(\ell)}=\boldsymbol{I}$，$\boldsymbol{W}_{2}^{(\ell)}=\mathbf{0}$， 此时$L^\prime$层的shaDow-SAGE等价于$L$层的GraphSAGE。所以$L^\prime$层的shaDow-SAGE可以表达GraphSAGE能表达的所有函数。</p>
<p>要证明shaDow-SAGE可以表达一些GraphSAGE无法表达的函数，首先对于每个邻域子图$\mathcal{G}_{[v]}$考虑一个目标函数：$\tau\left(\boldsymbol{X}, \mathcal{G}_{[v]}\right)=C \cdot \sum_{u \in \mathcal{V}_{[v]}} \delta_{[v]}(u) \cdot \boldsymbol{x}_{u}$，其中$C$是一个scaling常数，$\delta_{[v]}(u)$是节点$u$在子图$\mathcal{G}_{[u]}$中的度。</p>
<p>GraphSAGE准确的学习函数$\tau$，而shaDow-SAGE可以。令子图$\mathcal{G}_{[v]}$的深度为$L$，对于GraphSAGE，只有它在原图上做$L$层的Message Passing，或者它做$L^\prime$次MP的同时，$L^{\prime}-L$层的$\boldsymbol{W}_{2}$为0时，GraphSAGE才会遍历$\mathcal{G}_{[v]}$中的节点，否则，$L^\prime$层GraphSAGE将受到$v^{\prime} \notin \mathcal{V}_{[v]}$的影响，那么它无法近似$\tau$。那么问题就变为比较$L^\prime$层shaDow-SAGE在$\mathcal{G}_{[v]}$上的表达能力 和 $L$层GraphSAGE在原图上的表达能力， 因为他们的感受野都是$\mathcal{G}_{[v]}$。接下来，假设GraphSAGE可以在一个局部子图$\mathcal{G}_{[v]}^{\prime}$学习一个函数$\zeta$， 使得$\zeta\left(\mathcal{G}_{[v]}^{\prime}\right)=\tau\left(\mathcal{G}_{[v]}^{\prime}\right)$， 那么如果我们为子图$\mathcal{G}_{[v]}^{\prime}$添加一条边$e$来连接子图第$L$层的两个节点。 那么边$e$将会改变子图的度分布$\delta_{[v]}(\cdot)$，因此$\tau\left(\mathcal{G}_{[v]}^{\prime}\right) \neq \tau\left(\mathcal{G}_{[v]}^{\prime \prime}\right)$一定成立。对于在原图上$L$层GraphSAGE来说，第$L$-hop节点没有可能进行互相之间的message passing，除非GraphSAGE的层数增加，也就是GraphSAGE至少需要$L+1$层才有可能区分关于两个不同图的函数$\tau$, $\tau\left(\mathcal{G}_{-}[v]^{\prime}\right)$ 和$\tau\left(\mathcal{G}_{-}[v]^{\prime \prime}\right)$。所以原图上的$L$层GraphSAGE难以区分函数$\tau$的不同输入，会将不同的输入图输出同一个值，导致表达能力下降。而对于子图上的$L^\prime$层shaDow-SAGE， 直观上来看，它在最后一层后还有Message Passing，所以可以区分不同的输入。因此表达能力强于GraphSAGE。</p>
<h2 id="topological-learning-perspective">Topological Learning Perspective</h2>
<p><img loading="lazy" src="/posts/2022-05-21-DGCN/1.png#center" alt=""  />
</p>
<p>对于Regular Graph, 总所周知它无法被1-WL test 区分， 因为“regular” property describes a global topological symmetry among nodes, 节点之间是全局拓扑对称的，任意两个节点的任意阶邻居数量完全一样，所以1-WL test无法区分regular graph 中的任意两个节点， 如图一中$\mathcal{G}$所示。而对于shaDow-GNN来说，每个节点的局部子图不一定regular，如上图中$\mathcal{G}_{[v]}^{1}$ 和$\mathcal{G}_{[u]}^{1}$所示，使用$\text { EXTRACT}$为节点提取一阶邻居后，可以区分两个节点。</p>
<p>另外$L^\prime$层shaDow-GNN 可以近似$L$层GIN。基于Universal Approximation Theorem, 先做$L^\prime-L$次MLP，使得输入等于输出：
$$
\definecolor{energy}{RGB}{114,0,172}
\definecolor{freq}{RGB}{45,177,93}
\definecolor{spin}{RGB}{251,0,29}
\definecolor{signal}{RGB}{18,110,213}
\definecolor{circle}{RGB}{217,86,16}
\definecolor{average}{RGB}{203,23,206}
\definecolor{red}{RGB}{255,0,0}
\definecolor{black}{RGB}{0,0,0}
\begin{aligned}
\boldsymbol{h}_{v}^{(\ell)} &amp;=f_{1}^{(\ell)}\left(\color{red}\boldsymbol{h}_{v}^{(\ell-1)}, \color{black}\sum_{u \in \mathcal{N}_{v}} f_{2}^{(\ell)}\left(\boldsymbol{h}_{v}^{(\ell-1)}, \boldsymbol{h}_{u}^{(\ell-1)}\right)\right) \\
&amp;=\color{red}\boldsymbol{h}_{v}^{(\ell-1)}\color{black}, \quad \forall 1 \leq \ell \leq L^{\prime}-L
\end{aligned}
$$</p>
<p>在此基础上在做$L$层GIN。 所以$L^\prime$层shaDow-GNN 可以表达$L$层GIN。</p>
<h2 id="子图提取算法">子图提取算法</h2>
<p>启发式$\text { EXTRACT}$： 1. 根据最短路径，随机选取或者选取全部$L$-hop内的邻居作为$\mathcal{G}_{[v]}$。 2. 基于PPR score为每个节点选取top-K 节点，构造诱导子图。同理 Katz index， SimRank都可以作为分数指标。</p>
<h2 id="architecture">Architecture</h2>
<h3 id="subgraph-pooling">Subgraph Pooling</h3>
<p>对于shaDow-GNN，通过READOUT子图中的所有节点embedding来作为target node的embedding、对于normal GNN, 则是直接将第$L$层的输出作为embedding。考虑一个问题，节点的$L$-hop邻域内的节点也可能存在交互，比如对于节点$v$来说，它的邻域内有两个节点$u$和$w$，这两个节点之间的距离可能有$2L$ hop。 但是$L$层GNN无法捕获$u$和$w$之间的交互，除非将层数扩大到$2L$，但这将引入相当多无关节点。 而shaDow-GNN在$L$-hop子图$\mathcal{G}_{[v]}$中直接应用深层（$&gt;2L$）GNN，此时可以捕获邻域内节点互相之间的交互，获得更加有意义的embedding。</p>
<h3 id="subgraph-ensemble">Subgraph Ensemble</h3>
<p>单一的$\text { EXTRACT}$不一定能够选取有意义的邻居， 所以可以使用多个候选$\text { EXTRACT}$，若有$R$个候选$\text { EXTRACT}$，最后每个节点将得到$R$个embedding，然后可以用一个基于attention的聚合函数将每个$\text { EXTRACT}$输出的node embedding组合目标节点最终的node embedding。</p>
<h1 id="reference">Reference</h1>
<p>[1]Tackling Over-Smoothing for General Graph Convolutional Networks.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《From Canonical Correlation Analysis to Self-supervised Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/cca-ssg/</link>
      <pubDate>Thu, 14 Apr 2022 22:54:10 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/cca-ssg/</guid>
      <description>NeurIPS2021 &amp;#34;From Canonical Correlation Analysis to Self-supervised Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2106.12484">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文提出了一种新型的Graph Contrastive Learning构造Contrastive pairs的方式，即将跨图的同维度特征作为positive pairs， 不同维度特征作为negative pairs。 和过去的GCL方法相比，本文无需互信息估计器（MI Estimator），映射头（Projector），不对称结构（asymmetric structures）。 并且理论证明了该方法可以看做Information Bottleneck 原则在自监督设置下的实例。</p>
<p>具体来说，受典型相关分析（From Canonical Correlation Analysis）的启发，本文提出了一种简单有效的GCL框架，从而是模型避免复杂难以理解多模块设计。 和过去的方法<strong>相同</strong>的是，为输入图以随机增强的方式生成两个view， 目的是为两个view学习共享的 node representations 通过共享的GNN Encoder。<strong>不同</strong>在于，本文利用了典型相关分析（CCA），具体来说，新目标旨在最大化同一输入的两个增强views之间的相关性，同时对单个视图表示的不同（特征）维度进行去相关（避免不同维度捕获相同信息，即同一个view内的不同维度channel互为negative pairs）。 这么做的目的是 1）本质上追求的是丢弃增强后变化的信息，同时保留增强后不变的信息，以及 2）防止维度崩溃（即不同维度捕获相同的信息）。</p>
<p><img loading="lazy" src="/posts/2022-04-15-CCA-SSG/1.png#center" alt=""  />
</p>
<p>和其他方法的对比如上图所示， 本文提出的CCA-SSG无需negative pairs， 参数化的互信息估计器， projection head或者不对称结构。对比对的数量仅为$O(D^2)$, 其中$D$为输出维度。</p>
<h1 id="canonical-correlation-analysis">Canonical Correlation Analysis</h1>
<p>CCA: Identify and Quantify the associations between  two sets of variables， 即CCA用来衡量两组随机变量的相关性，每组可能有很多Random Variables.</p>
<p>从相关系数引入：</p>
<p>Pearson 相关系数： 给定两组数据集$X$， $Y$。 其中$X \in \mathbb{R}^{N \times 1}$ 表示只有一个随机变量（属性），样本数为$N$。 $Y \in  \mathbb{R}^{M \times 1}$: 一个随机变量，样本量为$M$。那么Pearson 相关系数$\rho$定义为：
$$
\rho(X,Y)=  \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}
$$
其中$\sigma_X$，$\sigma_Y$分别为$X$和$Y$的标准差。$\mathrm{Cov}(X,Y)$为$X$, $Y$的协方差。$\rho \in [-1,1]$。 $\rho$越接近1， $X$和$Y$的线性相关性越高。$\rho$越接近0，$X$和$Y$的线性相关性月底。</p>
<p><strong>相关系数存在问题</strong>：相关系数不适用于高维数据。 如果$X$是2维的（2个属性，例如身高和体重）， $Y$也是2维的，属性为(跑步，跳远)， $X \in \mathbb{R}^{N \times 2}$, $Y \in  \mathbb{R}^{M \times 2}$。此时，相关系数$\rho$無法計算2維隨機變量的相關程度。</p>
<h2 id="cca-基本思想">CCA 基本思想</h2>
<p>$X$和$Y$ 为两个变量集合， 例如$X$中有两个随机变量（2维）， $Y$中也有两个随机变量。 要衡量变量间的相关性： 现将高维随机变量（即多个随机变量）降到一维（一个随机变量），再用相关系数计算相关性。</p>
<p>令$X = \{\boldsymbol{x}_1,\boldsymbol{x}_2\} \in \mathbb{R}^{n_1\times m}$， 表示$n_1=2$个随机变量，$m$个样本。 $Y = \{\boldsymbol{y}_1,\boldsymbol{y}_2\} \in \mathbb{R}^{n_2\times m}$表示$n_2=2$个随机变量，$m$个样本。</p>
<p>$U$为随机变量集合$X$的线性组合：
$$
U = a_1 \boldsymbol{x}_1 + a_2 \boldsymbol{x}_2 = [a_1, a_2]\begin{bmatrix} \boldsymbol{x}_1 \\ \boldsymbol{x}_2\end{bmatrix} = a^\top X
$$
$V$为随机变量集合$Y$的线性组合：
$$
V = b_1 \boldsymbol{y}_1 + b_2\boldsymbol{x}_2 = b^\top Y
$$
<strong>CCA</strong>的优化目标： 找到一组最优解$a$和$b$， 使得$\rho_{U,V}$最大：
$$
\arg \max_{a,b} \rho_{U,V} = \frac{\mathrm{Cov}(U,V)}{\sigma_U \sigma_V}
$$
得到的$a$, $b$是使得$X$与$Y$有最大关联的权重。</p>
<h2 id="cca的表示与求解">CCA的表示与求解</h2>
<p>输入：两个随机变量集合$X = \{\boldsymbol{x}_1 , \cdots, \boldsymbol{x}_n\}$, $Y= \{\boldsymbol{y}_1 , \cdots, \boldsymbol{y}_m\}$。 分别有$n$个和$m$个随机变量。</p>
<p>$X$是一个$n \times L$的矩阵， 即有$L$个样本， $n$个属性（$n$个随机变量）。</p>
<p>$Y$是一个$m \times L$的矩阵， $L$个样本， $m$个属性。</p>
<p>$U = a^\top X \in \mathbb{R}^{1 \times L}$, $V= b^\top Y \in \mathbb{R}^{1\times L}$, 分别将组高维随机变量转为一维。 目标函数为
$$
\arg \max_{a,b} \rho_{U,V} =\arg \max_{a,b}  \frac{\mathrm{Cov}(U,V)}{\sigma_U \sigma_V}
$$
设 $\Sigma_{XX} = \mathrm{Cov}(X,X) = \mathrm{Var}(X)$， $\Sigma_{YY} = \mathrm{Cov}(Y,Y) = \mathrm{Var}(Y)$， $\Sigma_{XY} = \mathrm{Cov}(X,Y)$， $E[X] = \mu_X \in \mathbb{R}^{n \times 1}$ （样本均值）， $E[Y] = \mu_Y \in \mathbb{R}^{m \times 1}$。</p>
<p>定义$X$ 为一个$n$个随机变量stack成的列向量：
$$
X= \begin{bmatrix} \boldsymbol{x}_1 \\ \cdots \\ \boldsymbol{x}_n\end{bmatrix} \in \mathbb{R}^{n \times L}
$$
$C$ 为$n$个scalars $c_1, \cdots, c_n$ stack成的列向量：
$$
C= \begin{bmatrix} \boldsymbol{c}_1 \\ \cdots \\ \boldsymbol{c}_n\end{bmatrix}
$$
$C^\top X$是这$n$个Random Variables的线性组合。 $C^\top X$的方差为：
$$
\mathrm{Var}(C^\top X) = C^\top \Sigma_{XX} C = C^\top \mathrm{Var}(X) C
$$
那么$\mathrm{Var}(U) = \mathrm{Var}(a^\top X) = a^\top \mathrm{Var}(X) a$。</p>
<p>每个随机变量$\boldsymbol{x}_i$为数据的第$i$个特征，每列为一个样本$X \in \mathbb{R}^{n \times L}$。 有$L$个样本， 对特征维度做标准化，也就是对每个维度$\boldsymbol{x}_i$做标准化， 可得$E(\boldsymbol{x}_i) = 0$, $\mathrm{Var}(\boldsymbol{x}_i) = 1$。
$$
\begin{aligned}
\mathrm{Var}(X) &amp;= E(X-E(X))^2 \\
&amp;= E(\begin{bmatrix} \boldsymbol{x}_1 \\ \cdots \\ \boldsymbol{x}_n\end{bmatrix} -\begin{bmatrix} \boldsymbol{\mu}_1 \\ \cdots \\ \boldsymbol{\mu}_n\end{bmatrix} )^2   \\
&amp;= E (\begin{bmatrix} \boldsymbol{x}_1 \\ \cdots \\ \boldsymbol{x}_n\end{bmatrix}^2) \\
&amp;= E(XX^\top)
\end{aligned}
$$
所以$\mathrm{Var}(U) = a^\top E(XX^\top) a$， 同理$\mathrm{Var}(V) = b^\top E(YY^\top) b$。另外：
$$
E(a^\top X) = E(a_1\boldsymbol{x}_1 + \cdots + a_n\boldsymbol{x}_n) = a_1E(\boldsymbol{x}_1) + \cdots + a_n E(\boldsymbol{x}_n) = 0
$$
那么：
$$
\begin{aligned}
\mathrm{Cov}(U,V) &amp;= \mathrm{Cov}(a^\top X, b^\top Y) \\
&amp;= E\left[ \langle a^\top X - E(a^\top X), b^\top Y- E(b^\top Y) \rangle  \right] \\
&amp;= E[\langle a^\top X,  b^\top Y \rangle] \\
&amp;= E[(a^\top X)(b^\top Y)^\top] \\
&amp;= E[a^\top X Y^\top b] \\
&amp;= a^\top E[XY^\top]b
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathrm{Var}(X) &amp;= \mathrm{Cov}(X,X) = E[XX^\top] \\
\mathrm{Var}(Y) &amp;= \mathrm{Cov}(Y,Y) = E[YY^\top] \\
\mathrm{Cov}(X,Y) &amp;= E[\langle X-\mu_X, Y-\mu_Y \rangle] = E[XY^\top] = \Sigma_{XY}\\
\mathrm{Cov}(Y,X) &amp;=E[YX^\top]
\end{aligned}
$$</p>
<p>优化目标转化为：
$$
\begin{aligned}
\arg \max_{a,b} \rho_{U,V} &amp;=\arg \max_{a,b}  \frac{\mathrm{Cov}(U,V)}{\sigma_U \sigma_V}  \\
&amp;=\arg \max_{a,b} \frac{a^\top \Sigma_{XY}b}{\sqrt{a^\top \Sigma_{XX} a} \sqrt{b^\top \Sigma_{YY}b}}
\end{aligned}
$$
若对$a$， $b$同时放缩， 即$a$放缩$k$倍， $b$放缩$l$倍， 公式的值不会改变：
$$
\frac{ka^\top \Sigma_{XY}lb}{\sqrt{ka^\top \Sigma_{XX} ka} \sqrt{lb^\top \Sigma_{YY}lb}} =  \frac{a^\top \Sigma_{XY}b}{\sqrt{a^\top \Sigma_{XX} a} \sqrt{b^\top \Sigma_{YY}b}}
$$
所以， 可以直接对$a$做放缩，使得$a^\top \Sigma_{XX} a=1$, 对$b$做放缩，使得$b^\top \Sigma_{YY}b=1$（类似于SVM）。 那么优化目标转化为：
$$
\begin{aligned}
&amp;\max_{a, b} a^{\top} \Sigma_{X Y} b, \\ &amp;\text{ s.t. } a^{\top} \Sigma_{X X} a=b^{\top} \Sigma_{Y Y} b=1
\end{aligned}
$$
对于两个向量集合$X_1$和$X_2$， CCA 寻求两组向量最大化它们的相关性，并受到它们彼此不相关的约束。 后来的研究通过用神经网络代替线性变换，将 CCA 应用于具有深度模型的多视图学习。 具体来说，假设 $X_1$和$X_2$作为输入数据的两个视图，CCA的优化目标为：
$$
\max_{\theta_{1}, \theta_{2}} \operatorname{Tr}\left(P_{\theta_{1}}^{\top}\left(X_{1}\right) P_{\theta_{2}}\left(X_{2}\right)\right) \quad \text { s.t. } P_{\theta_{1}}^{\top}\left(X_{1}\right) P_{\theta_{1}}\left(X_{1}\right)=P_{\theta_{2}}^{\top}\left(X_{2}\right) P_{\theta_{2}}\left(X_{2}\right)=I \text {. }  \tag{1}
$$
其中， $P_{\theta_{1}}$和$P_{\theta_{2}}$为两个Neural Network。尽管上式很精确，但这种计算确实很昂贵。Soft CCA 通过采用以下拉格朗日松弛, 消除了hard decorrelation constraint：
$$
\min_{\theta_{1}, \theta_{2}} \mathcal{L}_{\text {dist }}\left(P_{\theta_{1}}\left(X_{1}\right), P_{\theta_{2}}\left(X_{2}\right)\right)+\lambda\left(\mathcal{L}_{S D L}\left(P_{\theta_{1}}\left(X_{1}\right)\right)+\mathcal{L}_{S D L}\left(P_{\theta_{2}}\left(X_{2}\right)\right)\right)
$$
其中$\mathcal{L}_{\text {dist }}$用于衡量两个view的representations之间的相关性，$\mathcal{L}_{S D L}$ (stochastic decorrelation loss)计算$P_{\theta_{i}}\left(X_{i}\right)$和identity matrix之间的$L_1$距离。</p>
<h1 id="approach">Approach</h1>
<p><img loading="lazy" src="/posts/2022-04-15-CCA-SSG/2.png#center" alt=""  />
</p>
<p>模型包含3个模块 1. 随机图增强器$\mathcal{T}$，2. GNN encoder $f_\theta$, 3. 基于CCA的feature-level对比损失。</p>
<h2 id="graph-augmentations">Graph Augmentations</h2>
<p>本文利用 edge droping和 node feature masking两种graph corruption方式来对输入图做增强。 $\mathcal{T}$是所有可能的转换操作，$t \sim \mathcal{T}$表示图$G$的一种特定的转换。比如删除一条边的操作$t_r$就是$\mathcal{T}$中的一个变换。</p>
<h2 id="training">Training</h2>
<p>从$\mathcal{T}$随机采样两种图变换 $t_A$和$t_B$。 生成两个View: $\tilde{\mathbf{G}}_{A}=\left(\tilde{\mathbf{X}}_{A}, \tilde{\mathbf{A}}_{A}\right)$和$\tilde{\mathbf{G}}_{B}=\left(\tilde{\mathbf{X}}_{B}, \tilde{\mathbf{A}}_{B}\right)$，经过共享的GNN后，得到输出$\mathbf{Z}_{A}=f_{\theta}\left(\tilde{\mathbf{X}}_{A}, \tilde{\mathbf{A}}_{A}\right)$，$\mathbf{Z}_{B}=f_{\theta}\left(\tilde{\mathbf{X}}_{B}, \tilde{\mathbf{A}}_{B}\right)$。然后对feature dimensionzuo normalization (列标准化)， 是的每个特征维度均值为0， 标准差为$1 / \sqrt{N}$：</p>
<p>$$
\tilde{\mathbf{Z}}=\frac{\mathbf{Z}-\mu(\mathbf{Z})}{\sigma(\mathbf{Z}) * \sqrt{N}}
$$</p>
<h1 id="inference">Inference</h1>
<p>基于公式（1）,使用公式(1)中的CCA目标函数，将向量集定义为输出$\tilde{\mathbf{Z}}$的列向量， 最终CCA-SSG的目标函数定义如下：
$$
\mathcal{L}=\underbrace{\left|\left|\tilde{\mathbf{Z}}_{A}-\tilde{\mathbf{Z}}_{B}\right|\right|_{F}^{2}}_{\text {invariance term }}+\lambda \underbrace{\left(\left|\left|\tilde{\mathbf{Z}}_{A}^{\top} \tilde{\mathbf{Z}}_{A}-\mathbf{I}\right|\right|_{F}^{2}+\left|\left|\tilde{\mathbf{Z}}_{B}^{\top} \tilde{\mathbf{Z}}_{B}-\mathbf{I}\right|\right|_{F}^{2}\right)}_{\text {decorrelation term }}
$$
第二项中，要求不同特征之间的相似度尽可能低， 从而使得不同特征捕获不同的语义信息。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2020 《Contrastive Multi-View Representation Learning on Graphs》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/mvgrl/</link>
      <pubDate>Tue, 12 Apr 2022 22:21:29 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/mvgrl/</guid>
      <description>ICML2020 &amp;#34;Contrastive Multi-View Representation Learning on Graphs&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2006.05582">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文旨在通过多视图Contrastive Learning 来学习节点表示和图表示。其中对比视图为结构视图（structural view）。本文发现，两个以上的对比视图不会提升性能（我觉得仅是针对本文的Diffusion-based view吧~）。 本文实验性的表明了基于一阶邻居和图扩散视图做CL可以达到最好的效果。</p>
<p>为了将对比学习应用到图表示学习任务，本文提出通过最大化图的不同结构视角的互信息来作为监督信号。通过对提出框架的系统研究，本文展示了一些GCL和visual CL上的不同： （1）将view数量（即增强）增加到两个以上不会提高性能，最好的性能是通过对比来自一阶邻居view的embedding和graph diffusion的embedding，(2) 与对比图编码或多尺度编码相比，跨视图对比节点和图编码在node classification 和 graph classification上都能获得更好的结果。 (3) 与分层图池化方法（例如DiffPool相比）一个简单的Readout在这node classification 和 graph classification上实现了更好的性能，以及 (4) 应用正则化（early stopping除外） 或归一化层对性能有负面影响。</p>
<h1 id="method">Method</h1>
<p><img loading="lazy" src="/posts/2022-04-13-MVGRL/1.png" alt=""  />
</p>
<p>MVGRL通过最大化一个view的node embedding和另一个view的graph embedding之间的 互信息来学习节点和图表示。如上图所示，MVGRL由以下几个部分构成</p>
<ul>
<li><strong>增强机制</strong>：将样本图转化为同一个图的相关view， 这个view只是structural view， 不会改变原图中的node feature，然后对两个增强图中的相同节点（identical node）进行子采样，类似于CV中的域剪裁。</li>
<li><strong>两个专用的GNNs</strong>， 每个view一个GNN，再接一个共享的MLP作为projection head，来为两个view学习representation。</li>
<li><strong>图池化层</strong>， 在MLP后学习两个图的graph-level representation。</li>
<li><strong>判别器</strong> 来对比一个图的embedding和另一个图的节点embedding,并对他们的一致性（agreement）评分。</li>
</ul>
<h2 id="augmentations">Augmentations</h2>
<p>考虑两种类型的图增强：(1) 对初始节点特征进行操作的特征空间增强，例如，mask或添加高斯噪声，以及 (2) 通过添加或删除连通性、子采样或使用最短路径或diffusion matrix生成全局视图来对做图结构增强。 前一种增强可能是有问题的，因为许多数据集不带有初始节点特征。 此外，观察到在任一空间上屏蔽或添加噪声都会降低性能。 因此，本文选择生成全局视图，然后进行子采样。</p>
<p>实验表明，在大多数情况下，最好的结果是通过将邻接矩阵转化为扩散矩阵，并将这两个矩阵视为同一图的结构的两个一致view。因为<strong>邻接矩阵和扩散矩阵分别提供了图结构的局部和全局视图</strong>，从这两种view中学习到的表示之间最大一致性，从而鼓励模型同时编码的局部和全局信息。</p>
<p>Diffusion matrix从全局角度提供了任意节点对之间的相关性，其中$\mathbf{T} \in \mathbb{R}^{n \times n}$是通用转移矩阵，$\Theta$是权重系数，决定了全局和局部信息的比例，即对于每个节点，不同层次信息的比重， $\Theta_{k}$越大，表示全局信息权重越大。 令$\sum_{k=0}^{\infty} \theta_{k}=1, \theta_{k} \in[0,1]$，$\lambda_{i} \in[0,1]$,其中$\lambda$是$\mathbf{T}$的特征向量，  这样来保证$\mathbf{S}$可以收敛到一个固定矩阵。扩散用快速近似值和稀疏化方法计算：
$$
\mathbf{S}=\sum_{k=0}^{\infty} \Theta_{k} \mathbf{T}^{k} \in \mathbb{R}^{n \times n}
$$
给定一个邻接矩阵$\mathbf{A} \in \mathbb{R}^{n \times n}$和一个对角度矩阵$\mathbf{D} \in \mathbb{R}^{n \times n}$, Personalized PageRank (PPR)和Heat Kernel分别为两种不同的Diffusion matrix实例。对于PPR和HK，转移概率矩阵定义为$\mathbf{T}=\mathbf{A} \mathbf{D}^{-1}$。PPR将第$k$层的权重系数设置为$\theta_{k}=\alpha(1-\alpha)^{k}$, 而HK将第$k$层的权重系数设置为$\theta_{k}=e^{-t} t^{k} / k !$。</p>
<p>PPR的封闭阶如下所示：
$$
\mathbf{S}^{\mathrm{PPR}}=\alpha\left(\mathbf{I}_{n}-(1-\alpha) \mathbf{D}^{-1 / 2} \mathbf{A} \mathbf{D}^{-1 / 2}\right)^{-1}
$$
HK的封闭解如下所示：
$$
\mathbf{S}^{\text {heat }}=\exp \left(t \mathbf{A} \mathbf{D}^{-1}-t\right)
$$</p>
<h2 id="sub-sampling">Sub-Sampling</h2>
<p>从一个view中<strong>随机采样节点及其边</strong>，并从另一个view中<strong>选择exact的的节点和边</strong> (如示意图所示， 从第一个图中采样节点和边的子图作为一个view，从第二个图中采样相同节点以及这些节点之间的边作为另一个view，来做对比学习)。这个过程允许MVGRL应用于具有图数据不适合GPU内存的inductive任务，也可以通过将子样本视为独立的图来考虑transductive任务。</p>
<h2 id="encoder">Encoder</h2>
<p>和其他GCL方法不同的是，这里不同视图使用的是各自的GNN编码器， 邻接矩阵和Diffusion matrix是同一个图的两个一致视角，分别反映了局部和全局性质。首先，为两种view采样之后的子图分别定义GNN encoder：$g_{\theta}(.), g_{\omega}(.): \mathbb{R}^{n \times d_{x}} \times \mathbb{R}^{n \times n} \longmapsto \mathbb{R}^{n \times d_{h}}$， 使用最简单的GCN，传播矩阵分别为normalized adjacency matrix $\sigma(\tilde{\mathbf{A} }X \boldsymbol{\Theta})$ 和 Diffusion Matrix:  $\sigma(\mathbf{S} X \boldsymbol{\Theta})$。学习到的embedding输入projection head （MLP）$f_{\psi}(.): \mathbb{R}^{n \times d_{h}} \longmapsto \mathbb{R}^{n \times d_{h}}$中， 得到两个view的输出node embedding matrix: $\mathbf{H}^{\alpha}, \mathbf{H}^{\beta} \in \mathbb{R}^{n \times d_{h}}$。</p>
<p>接下来使用pooling $\mathcal{P}(.): \mathbb{R}^{n \times d_{h}} \longmapsto \mathbb{R}^{d_{h}}$ 输出两个view的graph representations。 本文采用JKnet中的跳连机制，即GNN的每层输出做sum pooling, 然后将所有层拼起来做特征变换：
$$
\vec{h}_{g}=\sigma\left(||_{l=1}^{L}\left[\sum_{i=1}^{n} \vec{h}_{i}^{(l)}\right] \mathbf{W}\right) \in \mathbb{R}^{h_{d}}
$$
其中$\vec{h}_{i}^{(l)}$是节点$i$的第$l$层输出，$||$是concatenation， $\mathbf{W} \in \mathbb{R}^{\left(L \times d_{h}\right) \times d_{h}}$是特征变换参数，$\sigma$是PReLU非线性激活。最终，将图表示输入到一个projection head $f_{\phi}(.): \mathbb{R}^{d_{h}} \longmapsto \mathbb{R}^{d_{h}}$ 中，得到最终的图表示：$\vec{h}_{g}^{\alpha}, \vec{h}_{g}^{\beta} \in \mathbb{R}^{d_{h}}$。</p>
<p>在推理阶段， 由于两个view来自同一个图，可以把两个view的表示结合起来作为原图的表示：两个view的graph embedding直接相加，作为原图 embedding.。 两个view的node embedding 直接相加，作为原图的node embedding $\vec{h}=\vec{h}_{g}^{\alpha}+\vec{h}_{g}^{\beta} \in \mathbb{R}^{n}$ 。 $\mathbf{H}=\mathbf{H}^{\alpha}+\mathbf{H}^{\beta} \in \mathbb{R}^{n \times d_{h}}$。 这里得到的原图embedding可以应用于下游任务。</p>
<h2 id="training">Training</h2>
<p>为了端到端训练encoder并学习与下游任务无关的丰富节点和图级表示，本文利用 Deep InfoMax 方法并通过对比一个视图的节点表示与图表示来最大化两个视图之间的 互信息。 实验表明，这种方法在节点和图分类上始终优于对比图-图或多尺度编码。 目标定义如下：
$$
\max_{\theta, \omega, \phi, \psi} \frac{1}{|\mathcal{G}|} \sum_{g \in \mathcal{G}}\left[\frac{1}{|g|} \sum_{i=1}^{|g|}\left[\operatorname{MI}\left(\vec{h}_{i}^{\alpha}, \vec{h}_{g}^{\beta}\right)+\operatorname{MI}\left(\vec{h}_{i}^{\beta}, \vec{h}_{g}^{\alpha}\right)\right]\right]
$$
其中$\theta, \omega, \phi, \psi$为是GNN encoder和projection head的参数, $|\mathcal{G}|$是图数量，$|\mathcal{g}|$是图中节点数， $\vec{h}_{i}^{\alpha}, \vec{h}_{g}^{\beta}$分别表示view $\alpha$中的节点$i$的representation， 和view $\beta$的 graph representation。</p>
<p>互信息判别器： $\mathcal{D}(., .): \mathbb{R}^{d_{h}} \times \mathbb{R}^{d_{h}} \longmapsto \mathbb{R}$简单的设置为表示向量间的内积相似度：
$$
\mathcal{D}\left(\vec{h}_{n}, \vec{h}_{g}\right)=\vec{h}_{n} \cdot \vec{h}_{g}^{T}
$$
作者发现当判别器和projection head集成到双线性层中时，节点分类基准略有改进。 为了确定 MI 估计器，实验中调查了四个估计器并为每个基准选择了最好的一个。</p>
<p>正样本采样自联合分布$x_{p} \sim p\left(\left[\mathbf{X}, \tau_{\alpha}(\mathbf{A})\right],\left[\mathbf{X}, \tau_{\beta}(\mathbf{A})\right]\right)$， 从边际乘积中采样负样本 $x_{p} \sim p\left(\left[\mathbf{X}, \tau_{\alpha}(\mathbf{A})\right]\right) p\left(\left[\mathbf{X}, \tau_{\beta}(\mathbf{A})\right]\right)$。利用小批量随机梯度下降法对模型参数进行优化。 MVGRL算法如下：</p>
<p><img loading="lazy" src="/posts/2022-04-13-MVGRL/2.png" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>WWW2022 《SimGRACE:A Simple Framework for Graph Contrastive Learning without Data Augmentation》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/simgrace/</link>
      <pubDate>Sat, 09 Apr 2022 14:47:57 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/simgrace/</guid>
      <description>WWW2022 &amp;#34;SimGRACE:A Simple Framework for Graph Contrastive Learning without Data Augmentation&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2202.03104">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>图对比学习（GCL）已经成为图表示学习的主要技术，它最大化了共享相同语义的成对图增强之间的互信息。鉴于图数据的多样性，在增强过程中很难很好地保留语义。目前，GCL 中选择图增强方式的途径通常有以下三种。 1.  适用于不同数据集的图增强方式可能是不同的，需要在每个数据集上做验证，手动选择最适用于每个数据集的增强。2. 通过繁琐的搜索来选择增强方式。3. 通过邻域只是来选择增强方式。所有这些都限制了现有 GCL 方法的效率和通用性。为了解决该问题，本文提出了一种不需要对图做编辑， 而是对GNN编码器做扰动的增强方式： <strong>SimGRACE</strong>。并且对SimGRACE设计了对抗训练的方案：<strong>AT-SimGRACE</strong>。</p>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/1.png#center" alt="图1"  title="123"  />
</p>
<p>上图的实验中，两类图用不同的颜色标出，三种GCL模型分别在三个数据集上训练，训练完成后的分类效果如第一行所示。 对于GraphCL, 对边做扰动后再输入GraphCL 训练好的encoder,可以看出GraphCL的encoder对于扰动后的图数据集无法很好的保留分类语义。而对于SIMGRACE，不对图做扰动，而对训练好的encoder做扰动，扰动后的encoder对数据集的分类效果可以很好地保留语义信息。由此实验性表明了对encoder扰动可以保留比直接对图扰动更多的语义信息。</p>
<p>GraphCL 表明 GNN 可以使用他们提出的框架获得鲁棒性。 但是，（1）他们没有解释为什么 GraphCL 可以增强鲁棒性； (2) GraphCL 似乎对随机攻击具有很好的免疫力，而对对抗性攻击的表现却不尽如人意。为了弥补这些缺陷，本文基于SimGRACE提出了一种新的算法 AT-SimGRACE通过对抗的方式来扰动编码器，从而是实现对抗训练的效果，它引入了更少的计算开销，同时显示出更好的鲁棒性。</p>
<h1 id="method">Method</h1>
<h2 id="simgrace">SimGRACE</h2>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/2.png#center" alt="图1"  />
</p>
<h3 id="编码器扰动encoder-perturbation">编码器扰动（Encoder perturbation）</h3>
<p>给定一个GNN编码器$f(\cdot;\theta)$,它的参数扰动版本表示为$f(\cdot;\theta^\prime)$。如图中所示，参数扰动版本的编码器不需要梯度反传训练参数，每次训练过程更新$f(\cdot;\theta)$，而$f(\cdot;\theta^\prime)$的参数$\theta^\prime$只通过对$\theta$扰动得到。第$l$层GNN的参数表示为$\theta_l$，那么它的扰动后参数$\theta^\prime_l$有下式得到：
$$
\theta_{l}^{\prime}=\theta_{l}+\eta \cdot \Delta \theta_{l} ; \quad \Delta \theta_{l} \sim \mathcal{N}\left(0, \sigma_{l}^{2}\right)
$$
其中$\eta$用来控制扰动的缩放，$\Delta \theta_{l}$是扰动项，扰动值采样自0均值$\sigma_{l}^{2}$的Gaussian Distribution。$f(\cdot;\theta)$和$f(\cdot;\theta^\prime)$的输出分别为$\mathbf{h}$和$\mathbf{h}^{\prime}$：
$$
\mathbf{h}=f(\mathcal{G} ; \boldsymbol{\theta}), \mathbf{h}^{\prime}=f\left(\mathcal{G} ; \boldsymbol{\theta}^{\prime}\right)
$$
从下图可以看出，如果不对编码器施加扰动，即超参数$\eta=0$，效果会很差，扰动太多效果也会很差。</p>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/3.png#center" alt="图1"  />
</p>
<h3 id="映射头-projection-head">映射头 （Projection Head）</h3>
<p>和其他大多数GCL方法一样，该方法也要一个projection head来对GNN的output representation做一次变换，通常就是个MLP，得到输出$z$和$z^\prime$：
$$
z=g(\mathbf{h}), z^{\prime}=g\left(\mathbf{h}^{\prime}\right)
$$</p>
<h3 id="对比损失contrastive-loss">对比损失（Contrastive loss）</h3>
<p>和GraphCL一样，使用NT-Xent作为损失函数。具体来说，用$z_n$和$z_n^\prime$分别表示表示图$n$在$f(\cdot;\theta)$和$f(\cdot;\theta^\prime)$两个编码器下的输出， 用$z_n$和$z_{n^\prime}$表示一个batch中两个不同图$n$和图$n^\prime$在未扰动编码器$f(\cdot;\theta)$下的输出。在一个batch内，最大化同一个图的两个编码器（$f(\cdot;\theta)$和$f(\cdot;\theta^\prime)$）输出间的相似度，同时最小化不同图在未扰动编码器$f(\cdot;\theta)$下输出的相似度：
$$
\ell_{n}=-\log \frac{\left.\exp \left(\operatorname{sim}\left(z_{n}, z_{n}^{\prime}\right)\right) / \tau\right)}{\sum_{n^{\prime}=1, n^{\prime} \neq n}^{N} \exp \left(\operatorname{sim}\left(z_{n}, z_{n^{\prime}}\right) / \tau\right)}
$$
即同一个图的两个输出为positive pair, 不同图的$f(\cdot;\theta)$输出为negative pair.</p>
<h2 id="why-can-simgrace-work-well">Why can SimGRACE work well?</h2>
<p>[1] 提供了两个属性来衡量对比学习学到的representation的质量： <em>Alignment和Uniformity</em>。其中Alignment metric直接定义为positive pairs之间的距离：
$$
\ell_{\text {align }}(f ; \alpha) \triangleq \underset{(x, y) \sim p_{\text {pos }}}{\mathbb{E}}\left[||f(x)-f(y)||_{2}^{\alpha}\right], \quad \alpha&gt;0
$$
其中$p_{\text {pos }}$为positive pairs的分布，也就是positive pairs之间的距离越小，说明CL越好。 基于SimGRACE构造contrastive pairs的方式，alignment metric 可以定义为如下形式：
$$
\ell_{\text {align }}(f ; \alpha) \triangleq \underset{x \sim p_{\text {data }}}{\mathbb{E}}\left[\left|\left|f(x ; \theta)-f\left(x ; \theta^{\prime}\right)\right|\right|_{2}^{\alpha}\right], \quad \alpha&gt;0
$$
另一个衡量指标是Uniformity， 定义为成对高斯势函数（Gaussian Potential）：
$$
\ell_{\text {uniform }}(f ; \alpha) \triangleq \log \underset{x, y_{\sim}^{i . i . d .} p_{\text {data }}}{\mathbb{E}}\left[e^{-t||f(x ; \theta)-f(y ; \theta)||_{2}^{2}}\right] . \quad t&gt;0
$$
它要求随机样本的embedding应尽可能分散在hypersphere上， 即随机采样两个图在未扰动编码器输出的embedding距离要尽可能大。从下图可以看出，随着training epoch的增加，三种方法都呈现出正确的趋势。</p>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/4.png#center" alt="图1"  />
</p>
<h2 id="at-simgrace">AT-SimGRACE</h2>
<p>通过对抗训练（Adversarial Training， AT）来提升SimGRACE的鲁棒性。 对抗训练的优化问题定义如下：
$$
\min_{\theta} \mathcal{L}^{\prime}(\theta), \quad \text { where } \quad \mathcal{L}^{\prime}(\theta)=\frac{1}{n} \sum_{i=1}^{n} \max_{ | |\mathrm{x}_{i}^{\prime}-\mathrm{x}_{i} | |_{p} \leq \epsilon} \ell_{i}^{\prime}\left(f\left(\mathrm{x}_{i}^{\prime} ; \theta\right), y_{i}\right)
$$
其中$n$是训练样本数，$\mathrm{x}_{i}^{\prime}$是对抗样本， 其中对抗样本在训练样本的$\epsilon$-ball中，即$| |\mathrm{x}_{i}^{\prime}-\mathrm{x}_{i} | |_{p} \leq \epsilon$, 表示对抗样本和原样本的变化不能超过$\epsilon$。Adversarial Training: 优化$\theta$，使得$f$可以在$\mathrm{x}_{i}$的对抗样本$\mathrm{x}_{i}^{\prime}$上可以预测准确。其中$\ell^{\prime}(\cdot)$为监督分类损失，$\mathcal{L}^{\prime}(\theta)$为对抗损失。AT不能直接应用于CL上，因为（1）CL任务无标签，（2）对数据集中的每个样本扰动计算量太大。 为了解决这个问题，本文将AT loss中的损失函数部分换成NT-Xent对比学习损失，然后用对抗的方式来扰动encoder，从而无需对数据集中的所有样本扰动。</p>
<p>假设$\Theta$为GNN的权重空间(weight space)， 对于任意$\mathbf{w}$任意正实数$\epsilon$, 为$\theta$定义半径为$\epsilon$,中心为$\mathbf{w}$的norm ball:
$$
\mathbf{R}(\mathbf{w} ; \epsilon):=\{\boldsymbol{\theta} \in \boldsymbol{\Theta}:||\boldsymbol{\theta}-\mathbf{w}|| \leq \epsilon\}
$$
$\theta \in \Theta$表示权重空间$\Theta$中任意一组可能的GNN权重$\theta$, $\mathbf{R}(\mathbf{w} ; \epsilon)$表示GNN所有与$\mathbf{w}$相似的权重，即所有与$\mathbf{w}$的差距小于$\epsilon$的权重。</p>
<p>那么AT-SimGRACE的优化问题定义如下：
$$
\begin{gathered}
\min_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}+\Delta) \\
\text { where } \mathcal{L}(\boldsymbol{\theta}+\Delta)=\frac{1}{M} \sum_{i=1}^{M} \max_{\Delta \in \mathrm{R}(0 ; \epsilon)} \ell_{i}\left(f\left(\mathcal{G}_{i} ; \boldsymbol{\theta}+\Delta\right), f\left(\mathcal{G}_{i} ; \boldsymbol{\theta}\right)\right)
\end{gathered}
$$
这里$\mathrm{R}(0 ; \epsilon)=\{\Delta \in \Theta: ||\Delta|| \leq \epsilon\}$ ，$\mathcal{L}(\boldsymbol{\theta}+\Delta)$表示在对GNN参数施加扰动$\Delta$，使得GNN的效果最差，换句话说，找到一个扰动$\Delta$，使得GNN的参数在被$\Delta$扰动后（变为$\theta+\Delta$）两个图最不匹配（对比学习损失达到最大）。 $min_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}+\Delta)$表示训练GNN参数，使得对比学习可以适应该扰动。算法如下：</p>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/5.png#center" alt="图1"  />
</p>
<p>对抗训练：</p>
<p>内层： 固定GNN参数，训练扰动参数$\Delta$，使得GNN的对比学习loss上升</p>
<p>外层： 固定扰动参数$\Delta$， 训练GNN参数$\theta$， 使得$\theta$加上扰动$\Delta$后的对比学习loss最小化。</p>
<h1 id="reference">Reference</h1>
<p>[1] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. ICML (2020)</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Graph Neural Networks with Adaptive Residual》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/airgnn/</link>
      <pubDate>Fri, 08 Apr 2022 21:23:41 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/airgnn/</guid>
      <description>NeurIPS2021 &amp;#34;Graph Neural Networks with Adaptive Residual&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=hfkER_KJiNw">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>在Deeper GNN中，residual connections通常可以缓解oversmoothing问题，但是，若图中的存在abnormal node features, 那么residual connections会放大abnormal features的影响。本文旨在设计AirGNN， 在自适应调整残差连接的权重，是的可以弹性适应存在abnormal node features的图。太多聚合（deep layers）会导致oversmoothing，但residual 对深层GNN有益，但是对于abnormal features是脆弱的。</p>
<h1 id="preliminary">Preliminary</h1>
<p>Frobenius norm: $||\mathbf{X}||_{F}=\sqrt{\sum_{i j} \mathbf{X}_{i j}^{2}}$</p>
<p>$\ell_{21}$ norm: $||\mathbf{X}||_{21}= \sum_{i}\left|\left|\mathbf{X}_{i}\right|\right|_{2}=\sum_{i} \sqrt{\sum_{j} \mathbf{X}_{i j}^{2}}$  表示对每行算$\ell_2$ norm 再对所有行算$\ell_1$ norm。</p>
<h2 id="study">Study</h2>
<p><img loading="lazy" src="/posts/2022-04-05-AirGNN/1.png#center" alt="你想输入的替代文字"  />
</p>
<p>如图Figure 1所示， 对于具有abnormal node feature 的图，添加residual（蓝线）会导致性能巨大下降，因为abnormal node feature是与任务无关的，residual相对于无residual 保留了更多original abnormal features。</p>
<p><img loading="lazy" src="/posts/2022-04-05-AirGNN/2.png#center" alt="你想输入的替代文字"  />
</p>
<p>对于node feature 都是normal的图， 没有residual的话，随着层次加深，GNN的性能会下降。</p>
<p>综上，residual可以是GNN层次加深（容忍更多聚合），但是对abnormal features的鲁棒性较差。</p>
<h2 id="understandings-i-feature-aggregation-as-laplacian-smoothing">Understandings I: Feature aggregation as Laplacian smoothing</h2>
<p>对于Laplacian Smoothing problem:
$$
\underset{\mathbf{X} \in \mathbb{R}^{n \times d}}{\arg \min } \mathcal{L}_{1}(\mathbf{X}):=\frac{1}{2} \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)=\frac{1}{2} \sum_{\left(v_{i}, v_{j}\right) \in \mathcal{E}}\left|\left|\frac{\mathbf{X}_{i}}{\sqrt{d_{i}+1}}-\frac{\mathbf{X}_{j}}{\sqrt{d_{j}+1}}\right|\right|_{2}^{2}  \tag{1}
$$
其目标是找到最佳的$X$,使得$X$在图上最平滑。而对于GCN, GCNII (w/o residual)和APPNP (w/o residual), 他们的每一层都可以看做是如下的特征聚合方式：
$$
\mathbf{X}^{(k+1)}=\tilde{\mathbf{A}} \mathbf{X}^{(k)} = \tilde{\mathbf{A}}=(\hat{\mathbf{D}}^{-\frac{1}{2}} \hat{\mathbf{A}} \hat{\mathbf{D}}^{-\frac{1}{2}})\mathbf{X}^{(k)} \tag{2}
$$
实际上，迭代多层GNN可以看做是以 step size =1的条件下，以梯度下降的方式求解Laplacian Smoothing问题，即以梯度下降的方式找到在图上最平滑的信号：
$$
\begin{equation}
\begin{aligned}
\mathbf{X}^{(k+1)} &amp;= \mathbf{X}^{(k)}-\left.\gamma \frac{\partial \mathcal{L}_{1}}{\partial \mathbf{X}} \right|_{\mathbf{X}=\mathbf{X}^{(k)}}\\
&amp;= \mathbf{X}^{(k)}-\left.\gamma \frac{\partial \frac{1}{2} \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)}{\partial \mathbf{X}} \right|_{\mathbf{X}=\mathbf{X}^{(k)}} \\
&amp;= \mathbf{X}^{(k)} - (\mathbf{I}-\tilde{\mathbf{A}})\mathbf{X}^{(k)} \\
&amp;= \tilde{\mathbf{A}} \mathbf{X}^{(k)}<br>
\end{aligned}\tag{3}
\end{equation}
$$
其中令$\gamma = 1$, 所以，迭代多层GCN相当于以step size=1的方式迭代求解Laplacian Smoothing 问题。GCNII (w/o residual)和APPNP (w/o residual) 同理。</p>
<p>堆叠GCN层来求解Laplacian smoothing问题可以被解释为图上信号的低通filter，即对于相邻节点，保留邻居节点间相似的特征(低频信号)，remove相邻节点间不同的特征（高频信号）。abnormal feature会导致图上的信号不平滑， 也就是若$v_i$的feature是abnormal，$v_j$的feature是normal, 且$v_i$与$v_j$相邻， 那么可能导致信号$k$的两个分量$X_{ik}$和$X_{jk}$ 差异较大， 所以abnormal feature可以看做图上的高频信号。 作为高频信号，它会被GCN等低筒滤波器过滤，即随着层数的加深，网络上的信号会越来越平滑，$X_{ik}^{out}$和$X_{jk}^{out}$的差距会变小，所以加深GCN可以缓解abnormal带来的问题。</p>
<h2 id="understandings-ii-residual-connection-maintains-feature-proximity">Understandings II: Residual connection maintains feature proximity</h2>
<p>含有residual的APPNP形式如下：
$$
\mathbf{X}^{k+1}=(1-\alpha) \tilde{\mathbf{A}} \mathbf{X}^{k}+\alpha \mathbf{X}_{\text {in }}  \tag{4}
$$
它也可以看做是对Laplaican Smoothing问题的求解，只不过加上的正则化项。 APPNP可以看做迭代求解如下regularized Laplacian smoothing problem:
$$
\underset{\mathbf{X} \in \mathbb{R}^{n \times d}}{\arg \min} \mathcal{L}_{2}(\mathbf{X}):=\frac{1}{2} \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right) + \frac{\alpha}{2(1-\alpha)}\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{F}^{2} \tag{5}
$$
其中 step size $\gamma = 1-\alpha$:
$$
\mathbf{X}^{k+1}=\mathbf{X}^{k}-(1-\alpha)\left(\frac{\alpha}{1-\alpha}\left(\mathbf{X}^{k}-\mathbf{X}_{\text {in }}\right)+(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}^{k}\right)=(1-\alpha) \tilde{\mathbf{A}} \mathbf{X}^{k}+\alpha \mathbf{X}_{\text {in }} \tag{6}
$$
上式的求解过程和公式(3)差不多。 其中$\frac{\alpha}{2(1-\alpha)}\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{F}^{2}$为正则化项， 上式要求 求得的$X$在图上尽可能平滑的同时， 要与输入尽可能接近。这样对平滑加以限制后，可以缓解深层GNN产生的oversmoothing 问题，因为保留了一些必要的高频信号。但是这些残差连接也携带了有害的异常特征，导致在含有abnormal feature的图上性能较差。</p>
<h1 id="the-proposed-model">The Proposed Model</h1>
<h2 id="design-motivation">Design Motivation</h2>
<p>更多的feature aggregation 可以缓解abnormal feature但是oversmoothing。residual 可以缓解深层GNN，但是受abnormal feature影响。 <strong>如何设计MPNN使得node 可以自适应的特征聚合和residual?</strong></p>
<p>公式（5）中的正则化项$\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{F}^{2} $决定了GNN的更新中含有residual，这样可以保持深层GNN的稳定性。虽然保持输入与每层输出之间的proximity对于加深层次很重要，但是，用Frobenius norm来惩罚偏差可能过于激进，即会使得输入和输出过于接近，从而削弱了Laplacian Smoothing项$\operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)$去除abnormal feature的能力。因此本文提出用$\ell_{21}$ norm 来替换Frobenius norm作为输入输出之间proximity的保留项。 相比于Frobenius norm，不那么激进，即最小化$\ell_{21}$ norm 不会让输出过于接近输入。It also allows large deviations because the penalty on large values is less aggressive, leading to the potential removal of abnormal features：意思是$\mathbf{X}-\mathbf{X}_{\text {in}}$ 如果很大的话，它的$\ell_{21}$ norm在目标函数中不会占据过于大的分量，即 $\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{F}^{2} &gt; \left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{21} $。这样，最小化regularized Laplacian smoothing problem的目标函数时，不会过度倾向于最小化邻近度项。 所以regularized Laplacian smoothing 问题定义如下：
$$
\underset{\mathbf{X} \in \mathbb{R}^{n \times d}}{\arg \min } \mathcal{L}(\mathbf{X}):=\underbrace{(1-\lambda) \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)}_{可微 g(X)} + \underbrace{\lambda\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{21}}_{不可微 h(X)}   \tag{7}
$$
其中$\lambda \in [0,1]$。</p>
<h2 id="adaptive-massage-passing">Adaptive Massage Passing</h2>
<p>公式（7）中，$ \mathcal{L}(\mathbf{X})$由可微和不可微凸函数组成，可以使用Proximal Gradient Descent（PGD）来优化 （PGD可以见<a href="https://jhuow.fun/posts/pgd/">这篇文章</a>）。公式（7）中可微部分 记为$g(X) = (1-\lambda) \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)$ ， 不可微部分记为 $h(X) = \lambda\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{21}$。 根据PGD, 是的(7)最小的$\mathbf{X}$可以通过迭代方式求解：
$$
\definecolor{energy}{RGB}{114,0,172}
\definecolor{freq}{RGB}{45,177,93}
\definecolor{spin}{RGB}{251,0,29}
\definecolor{signal}{RGB}{18,110,213}
\definecolor{circle}{RGB}{217,86,16}
\definecolor{average}{RGB}{203,23,206}
\definecolor{red}{RGB}{255,0,0}
\boldsymbol{X}^{k+1}=\operatorname{prox}_{\color{signal}h\color{energy}\gamma}\left(\boldsymbol{X}^{k}-\gamma \nabla g\left(\boldsymbol{X}^{k}\right)\right) \tag{8}
$$
其中，
$$
\begin{equation}
\begin{aligned}
\nabla g\left(\boldsymbol{X}^{k}\right) &amp;= \nabla \left[ (1-\lambda) \operatorname{tr}\left((\boldsymbol{X}^{k})^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \boldsymbol{X}^{k}\right)\right] \\
&amp;=(1-\lambda)(2(I-\tilde{\mathbf{A}})\boldsymbol{X}^{k}) \\
&amp;= 2\boldsymbol{X}^{k} - 2 \tilde{\mathbf{A}}\boldsymbol{X}^{k} - 2\lambda \boldsymbol{X}^{k}+ 2\lambda \tilde{\mathbf{A}}\boldsymbol{X}^{k} \\
&amp;=2(1-\lambda) (\mathbf{I}-\tilde{\mathbf{A}}) \boldsymbol{X}^{k}
\end{aligned} \tag{9}
\end{equation}
$$
令
$$
\begin{aligned}
\boldsymbol{Y}^{k} = \boldsymbol{X}^{k}-\gamma \nabla g\left(\boldsymbol{X}^{k}\right) &amp;= \boldsymbol{X}^{k} - 2\gamma(1-\lambda) (\mathbf{I}-\tilde{\mathbf{A}}) \boldsymbol{X}^{k} \\ &amp;= (1-2 \gamma(1-\lambda)) \mathbf{X}^{k}+2 \gamma(1-\lambda) \tilde{\mathbf{A}} \mathbf{X}^{k}<br>
\end{aligned}\tag{10}
$$
那么：
$$
\begin{equation}
\begin{aligned}
\boldsymbol{X}^{k+1}&amp;=\operatorname{prox}_{\color{signal}h\color{energy}\gamma}\left(\boldsymbol{Y}^{k}\right) \\
&amp;= \underset{\mathbf{X}}{\arg \min }\left\{\lambda\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{21}+\frac{1}{2 \gamma}\left|\left|\mathbf{X}-\mathbf{Y}^{k}\right|\right|_{F}^{2}\right\}
\end{aligned} \tag{11}
\end{equation}
$$
这样，<strong>迭代公式（11）只和不可微函数$||\cdot||_{21}$有关</strong>， 上式的第二项$\left|\left|\mathbf{X}-\mathbf{Y}^{k}\right|\right|_{F}^{2}$是proximity operator的固定计算，优化目标中的$g(\cdot)$无关。</p>
<p>接下来，对公式（11）做一个换元， 令$\mathbf{Z}=\mathbf{X}-\mathbf{X}_{\mathrm{in}}$， 那么公式（11）可以重写为：
$$
\begin{aligned}
\mathbf{Z}^{k+1} &amp;=\underset{\mathbf{Z}}{\arg \min }\left\{\lambda||\mathbf{Z}||_{21}+\frac{1}{2 \gamma}\left|\left|\mathbf{Z}-\left(\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}\right)\right|\right|_{F}^{2}\right\} \\
&amp;=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}\right) \\
\mathbf{X}^{k+1} &amp;=\mathbf{X}_{\text {in }}+\mathbf{Z}^{k+1}
\end{aligned}
$$
下面就是求解$\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}\right)$。</p>
<p>先看看$\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{X}\right)$怎么算的, 参考[1,2]:
$$
\left[\operatorname{prox}_{\color{signal}||\cdot||_{2}}\left(\mathbf{X}\right)\right]_i =
\begin{cases}\mathbf{X}_i-\frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}} &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2}&gt;1 \\ 0 &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} \leq 1\end{cases}
$$</p>
<p>$$
\begin{equation}
\begin{aligned}
\left[\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{X}\right)\right]_i &amp; = \operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left(\operatorname{prox}_{\color{signal}\lambda||\cdot||_{2}\color{energy}\gamma }\left(\mathbf{X}\right)_i\right) \\
&amp;= \begin{cases}
\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left(\mathbf{X}_i-\frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}}\right) &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2}&gt;1 \\
\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left(0\right)=\color{red}0 &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} \leq 1\end{cases}
\end{aligned}
\end{equation}
$$
其中：
$$
\begin{equation}
\begin{aligned}
\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left(\mathbf{X}_i-\frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}}\right) &amp;= \frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}} \operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left( \left|\left|\mathbf{X}_i\right|\right|_{2}-\underbrace{1}_{constant, 不影响结果}\right) \\
&amp;= \frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}} \operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left( \left|\left|\mathbf{X}_i\right|\right|_{2}\right) \\
&amp;= \frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}} \cdot \underbrace{\boxed{\begin{cases}
\left|\left|\mathbf{X}_i\right|\right|_{2} - \lambda\gamma &amp;
\text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} &gt; \lambda\gamma \\
0 &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} = \lambda\gamma \\
\left|\left|\mathbf{X}_i\right|\right|_{2} + \lambda\gamma &amp;
\text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} \leq -\lambda\gamma
\end{cases}}}_{\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left( \left|\left|\mathbf{X}_i\right|\right|_{2}\right)}
\end{aligned} \tag{12}
\end{equation}
$$
公式（12）中，第三种情况不存在，所以
$$
\begin{equation}
\begin{aligned}
\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left( \left|\left|\mathbf{X}_i\right|\right|_{2}\right) = \max \left(\left|\left|\mathbf{X}_{i}\right|\right|_{2}-\gamma \lambda, 0\right)
\end{aligned}
\end{equation}
$$
所以：
$$
\left(\operatorname{prox}_{\gamma \lambda||\cdot||_{21}}(\mathbf{X})\right)_{i}=\frac{\mathbf{X}_{i}}{\left|\left|\mathbf{X}_{i}\right|\right|_{2}} \max \left(\left|\left|\mathbf{X}_{i}\right|\right|_{2}-\gamma \lambda, 0\right)=\max \left(1-\frac{\gamma \lambda}{\left|\left|\mathbf{X}_{i}\right|\right|_{2}}, 0\right) \cdot \mathbf{X}_{i} \tag{13}
$$
将公式（13）中的$\mathbf{X}$替换为$\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}$, 计算$\mathbf{Z}^{k+1}=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}\right)$ , 然后计算$\mathbf{X}^{k+1} =\mathbf{X}_{\text {in }}+\mathbf{Z}^{k+1}$ 可得：
$$
\mathbf{X}_{i}^{k+1}=\left(\mathbf{X}_{\text {in }}\right)_{i}+\beta_{i}\left(\mathbf{Y}_{i}^{k}-\left(\mathbf{X}_{\text{in }}\right)_{i}\right)=\left(1-\beta_{i}\right)\left(\mathbf{X}_{\text {in }}\right)_{i}+\beta_{i} \mathbf{Y}_{i}^{k}, \quad \forall i \in [n]
$$
其中 $\beta_{i}:=\max \left(1-\frac{\gamma \lambda}{\left|\left|\mathbf{Y}_{i}^{k}-\left(\mathbf{X}_{\text{in }}\right)_{i}\right|\right|_{2}}, 0\right)$。</p>
<p>综上，最终Adaptive Message Passing（AMP）总结如下：
$$
\left\{\begin{aligned}
\mathbf{Y}^{k} &amp;=(1-2 \gamma(1-\lambda)) \mathbf{X}^{k}+2 \gamma(1-\lambda) \tilde{\mathbf{A}} \mathbf{X}^{k} \\
\beta_{i} &amp;=\max \left(1-\frac{\gamma \lambda}{\left|\left|\mathbf{Y}_{i}^{k}-\left(\mathbf{X}_{\text {in }}\right)_{i}\right|\right|_{2}}, 0\right) \quad \forall i \in[n] \\
\mathbf{X}_{i}^{k+1} &amp;=\left(1-\beta_{i}\right)\left(\mathbf{X}_{\text {in }}\right)_{i}+\beta_{i} \mathbf{Y}_{i}^{k} \quad \forall i \in[n]
\end{aligned}\right.
$$</p>
<p><img loading="lazy" src="/posts/2022-04-05-AirGNN/3.png#center" alt="你想输入的替代文字"  />
</p>
<p>文章证明了$\gamma=\frac{1}{4(1-\lambda)}$或$\gamma=\frac{1}{2(1-\lambda)}$可以保证收敛 （具体过程没看了）。这样AMP简化为$\mathbf{Y}^{k}=\frac{1}{2} \mathbf{X}^{k}+\frac{1}{2} \tilde{\mathbf{A}} \mathbf{X}^{k}$和$\mathbf{Y}^{k}=\tilde{\mathbf{A}} \mathbf{X}^{k}$。</p>
<h1 id="reference">Reference</h1>
<p>[1] <a href="https://math.stackexchange.com/questions/2665254/proximal-operator-of-summation-of-l-1-norm-and-l-2-1-norm">https://math.stackexchange.com/questions/2665254/proximal-operator-of-summation-of-l-1-norm-and-l-2-1-norm</a></p>
<p>[2] <a href="https://math.stackexchange.com/questions/2190885/proximal-operator-of-the-euclidean-norm-l-2-norm">https://math.stackexchange.com/questions/2190885/proximal-operator-of-the-euclidean-norm-l-2-norm</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2020 《When Does Self-Supervision Help Graph Convolutional Networks?》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2020-04-03-ssgcns/</link>
      <pubDate>Fri, 08 Apr 2022 14:04:49 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2020-04-03-ssgcns/</guid>
      <description>ICML2020 &amp;#34;When Does Self-Supervision Help Graph Convolutional Networks?&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2006.09136v4">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文是自监督方法在GCNs上首次系统的探索，设计了3种自监督任务来将分析自监督在GCN中起到的作用。自监督旨在充分利用unlabeled数据中的知识来设计前置任务（pretext task），来帮助模型学习更具迁移性和泛化能力的表示。前置任务可以认为是对目标任务有帮助的辅助正则化网络，设计用于帮助原任务学习到更多下游任务相关的语义信息。</p>
<p>GCN任务通常是直推半监督的（transductive semi-supervised）,含有大量unlabeled数据，而self-supervision(SSL)可以充分利用unlabeled data， 那么就产生了一个值得探索的问题：<strong>将自监督学习应用到GCN上是否也可以达到提升泛化能力和鲁棒能力的效果？</strong></p>
<p>先给结论</p>
<p>Q1: 自监督学习可否在分类任务中提升GCN？ 如果可以，如何将其合并到 GCN 中以最大化增益？</p>
<p>A1: 本文证明了通过多任务学习将自监督学习融入 GCN 是有效的，即多任务损失作为 GCN 训练中的正则化项。 这种作为自监督作为正则化项的方法，强于用自监督来预训练或者self-training。</p>
<p>Q2: 前置任务的设计重要吗？ GCN 有哪些有用的自监督前置任务？</p>
<p>A2: 本文研究了三个基于图属性的自监督任务。 分别是节点聚类node clustering, 图划分graph partitioning 和图补全graph completion。 并且进一步说明不同的模型和数据集倾向于不同的自监督任务。</p>
<p>Q3: 自监督也会影响 GCN 的对抗鲁棒性吗？ 如果是，如何设计前置任务？</p>
<p>A3: 本文进一步将上述发现推广到对抗性训练环境中。提供了广泛的结果，以表明自监督还可以提高 GCN 在各种攻击下的鲁棒性，而不需要更大的模型或额外的数据。</p>
<h1 id="method">Method</h1>
<p>GCNs $\boldsymbol{Z}=\hat{\boldsymbol{A}} \operatorname{ReLU}\left(\hat{\boldsymbol{A}} \boldsymbol{X} \boldsymbol{W}_{0}\right) \boldsymbol{W}_{1}$可以分为两块来看 (1) 特征提取模块$f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) = \hat{\boldsymbol{A}} \operatorname{ReLU}\left(\hat{\boldsymbol{A}} \boldsymbol{X} \boldsymbol{W}_{0}\right)$ 参数为$\theta = \{\boldsymbol{W}_{0}\}$和（2）线性变换模块$\boldsymbol{Z}=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta}$ 其中 参数$ \boldsymbol{\Theta} = \boldsymbol{W}_{1}$。 半监督GCN优化任务的目标函数为：
$$
\begin{aligned}
\boldsymbol{Z} &amp;=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta} \\
\theta^{*}, \boldsymbol{\Theta}^{} &amp;=\arg \min_{\theta, \boldsymbol{\Theta}} \mathcal{L}_{\mathrm{sup}}(\theta, \boldsymbol{\Theta}) \\
&amp;=\arg \min_{\theta, \boldsymbol{\Theta}} \frac{1}{\left|\mathcal{V}_{\text {label }}\right|} \sum_{v_{n} \in \mathcal{V}_{\text {label }}} L\left(\boldsymbol{z}_{n}, \boldsymbol{y}_{n}\right)
\end{aligned} \tag{1}
$$
其中$L(\cdot, \cdot)$是每个labeled node的损失函数。</p>
<h2 id="three-schemes-self-supervision-meets-gcns">Three Schemes: Self-Supervision Meets GCNs</h2>
<p>研究三种将SSL配置到GCNs的方式。 其中 给定输入$\boldsymbol{X}_{ss}$, $\hat{\boldsymbol{A}}_{\mathrm{ss}}$, label $\boldsymbol{Y}_{ss}$和节点集$\mathcal{V}_{ss}$。</p>
<h3 id="pretraining--fintuning">Pretraining &amp; Fintuning</h3>
<p>预训练过程：
$$
\begin{aligned}
\boldsymbol{Z}_{\mathrm{ss}} &amp;=f_{\theta}\left(\boldsymbol{X}_{\mathrm{ss}}, \hat{\boldsymbol{A}}_{\mathrm{ss}}\right) \boldsymbol{\Theta}_{\mathrm{Ss}} \\
\theta_{\mathrm{ss}}^{*}, \boldsymbol{\Theta}_{\mathrm{ss}}^{*} &amp;=\arg \min_{\theta, \boldsymbol{\Theta}_{\mathrm{ss}}} \mathcal{L}_{\mathrm{ss}}\left(\theta, \boldsymbol{\Theta}_{\mathrm{ss}}\right) \\
&amp;=\arg \min_{\theta, \boldsymbol{\Theta}} \frac{1}{\left|\mathcal{V}_{\mathrm{ss}}\right|} \sum_{v_{n} \in \mathcal{V}_{\mathrm{ss}}} \underbrace{L_{\mathrm{ss}}\left(\boldsymbol{z}_{\mathrm{ss}, n}, \boldsymbol{y}_{\mathrm{ss}, n}\right)}_{\text{loss of other task}}<br>
\end{aligned} \tag{2}
$$
也就是在另一个任务训练好的模型参数$\theta_{\mathrm{ss}}^{*}, \boldsymbol{\Theta}_{\mathrm{ss}}^{*}$迁移到新任务（如半监督节点分类任务）上作为初始化参数训练新模型。</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/1.png#center" alt=""  />
</p>
<p>上表中，可以看出用graph partitioning作为预训练任务，得到的模型fine-tuning到节点分类任务上之后，效果仅从79.10变成了79.19,是非常微小的。 本文推测可能原因有两个（1）.两个不同的任务的Loss function不一样，从$\mathcal{L}_{\mathrm{ss}}$变为$\mathcal{L}_{\mathrm{sup}}$会影响实验效果。（2）参数迁移前一句是在多层GCN上的训练结果了，迁移后再训练，相当于深层，易oversmoothing。</p>
<h3 id="self-training">Self-Training</h3>
<p>每次迭代为unlabeled samples分配高度可信的为标签，然后将这些分配了伪标签的节点纳入到下一次迭代的监督训练中，随迭代过程不断更新标签。</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/2.png#center" alt=""  />
</p>
<p>表2可以看出Self-training的方式带来的提升有限</p>
<h3 id="multi-task-learning">Multi-task Learning</h3>
<p>考虑一个目标task和一个自监督task. GCN的目标为公式（1）。该多任务的训练过程如下：
$$
\begin{aligned}
\boldsymbol{Z} &amp;=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta}, \quad \boldsymbol{Z}_{\mathrm{ss}}=f_{\theta}\left(\boldsymbol{X}_{\mathrm{ss}}, \hat{\boldsymbol{A}}_{\mathrm{ss}}\right) \boldsymbol{\Theta}_{\mathrm{ss}} \\
\theta^{*}, \boldsymbol{\Theta}^{*}, \boldsymbol{\Theta}_{\mathrm{ss}}^{*} &amp;=\arg \min_{\theta, \boldsymbol{\Theta}, \boldsymbol{\Theta}_{\mathrm{ss}}} \alpha_{1} \mathcal{L}_{\mathrm{sup}}(\theta, \boldsymbol{\Theta})+\alpha_{2} \mathcal{L}_{\mathrm{ss}}\left(\theta, \boldsymbol{\Theta}_{\mathrm{ss}}\right)
\end{aligned} \tag{3}
$$
其中任务的权重参数$\alpha_{1}, \alpha_{2} \in \mathbb{R}_{&gt;0}$, 半监督目标任务的损失$\mathcal{L}_{\mathrm{sup}}$定义为公式（1）， 辅助自监督损失$\mathcal{L}_{\mathrm{ss}}$定义为公式（2）.其中特征提取器$f_{\theta}(\cdot, \cdot)$对于自监督任务和目标任务是参数共享的，而线性变换参数$\boldsymbol{\Theta}, \boldsymbol{\Theta}_{\mathrm{ss}}$是各自任务的。</p>
<p>在公式(3)中，自监督任务的loss作为一个<strong>regularization term</strong> 与目标任务一同训练。正则化项在图信号处理中是广泛应用的， 常见的有Graph Laplaician Regularization（GLR）， 它用于惩罚相邻节点间的不平滑，用于在学习目标任务的同时保持特征在图结构上的smoothing。虽然GLR可以作为一个自监督任务，但是它是给予不涉及具体数据情况下的平滑先验，SSL的regularization term不用的是，SSL是给予unlabeled data,是一种引入数据驱动的先验知识。综上所述， 多任务学习是3种自监督方式中最通用的。</p>
<h2 id="gcn-specific-self-supervised-tasks">GCN-Specific Self-Supervised Tasks</h2>
<p>本文为 GCN 扩展了一个自监督任务的“工具包”。 通过利用图中的丰富节点和边信息，可以定义各种GCN特定的自监督任务（如表 3 所示），并且进一步证明了不同的自监督任务对不同类型的监督/下游任务有益。这些自监督任务会为节点分配伪标签来构造自监督损失$\mathcal{L}_{ss}$, 如公式（3）所示。</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/4.png#center" alt=""  />
</p>
<h3 id="node-clustering">Node clustering</h3>
<p>第一个任务为节点聚类， 给定节点集$\mathcal{V}$以及feature set $\boldsymbol{X}$, 一个预设值的簇数量$K \in\{1, \ldots,|\mathcal{V}|\}$（是一个超参数）一定要小于等于节点数$|\mathcal{V}|$。 聚类算法输出一个节点集合的集合$\left\{\mathcal{V}_{\text {clu }, 1}, \ldots, \mathcal{V}_{\text {clu }, K} \mid \mathcal{V}_{\text {clu }, n} \subseteq \mathcal{V}, n=1, \ldots, K\right\}$， 其中$\mathcal{V}_{\text {clu }, i}$是一个集合表示在簇$i$中的节点集。
$$
\begin{aligned}
&amp;\mathcal{V}_{\text {clu }, n} \neq \emptyset \quad(n=1, \ldots, K), \quad \cup_{n=1}^{K} \mathcal{V}_{\text {clu }, n}=\mathcal{V} \\
&amp;\mathcal{V}_{\text {clu }, i} \cap \mathcal{V}_{\text {clu }, j}=\emptyset \quad(\forall i, j=1, \ldots, K \text { and } i \neq j)
\end{aligned}
$$
这$K$个簇互相之间没有公共的节点，SSL任务将每个节点所在的簇的index作为伪标签来构造自监督损失$\mathcal{L}_{ss}$：
$$
y_{\mathrm{ss}, n}=k \text { if } v_{n} \in \mathcal{V}_{\mathrm{clu}, k}(\forall n=1, \ldots,|\mathcal{V}|, \forall k=1, \ldots, K)
$$</p>
<h3 id="graph-partitioning">Graph partitioning</h3>
<p>上面的节点聚类任务，是基于特征的，与拓扑无关。 而这里的图划分任务，与feature无关，只与拓扑有关。 具体来说，通过“强”边连接的两个节点很可能属于同一标签类别。 因此，本文提出了一种使用图划分的基于拓扑的自监督任务。</p>
<p>图划分是将图的节点划分为大致相等的子集，使得跨子集间的边数最小化（高聚类，低耦合，同时簇中节点数不能差别太大）。先预定义一个簇数量，$K \in\{1, \ldots,|\mathcal{V}|\}$（超参数）。 和节点聚类任务类似，图划分算法也会输出一个节点集合的集合，用来标识每个节点属于哪个partition: $\left\{\mathcal{V}_{\text {par }, 1}, \ldots, \mathcal{V}_{\text {par }, K} \mid \mathcal{V}_{\text {par }, n} \subseteq \mathcal{V}, n=1, \ldots, K\right\}$, 使得：
$$
\begin{aligned}
&amp;\mathcal{V}_{\text {par }, n} \neq \emptyset \quad(\forall n=1, \ldots, K), \quad \cup_{n=1}^{K} \mathcal{V}_{\text {par }, n}=\mathcal{V} \\
&amp;\mathcal{V}_{\text {par }, i} \cap \mathcal{V}_{\text {par }, j}=\emptyset \quad(\forall i, j=1, \ldots, K \text { and } i \neq j)
\end{aligned}
$$
上面的约束其实和node clustering任务差不多，Graph partitioning任务还需要两个约束，一个是平衡约束来保证簇不要太大：
$$
K \frac{\max_{k}\left|\mathcal{V}_{\text {par }, k}\right|}{|\mathcal{V}|} \leqslant 1+\epsilon, \text { where } \epsilon \in(0,1)
$$
其中$\max_{k}\left|\mathcal{V}_{\text {par }, k}\right|$是节点数最多的簇中的节点数。 另一个约束要保证簇间边要尽可能少，即最小化edgecut:
$$
\text { edgecut }=\frac{1}{2} \sum_{k=1}^{K} \sum_{v_{i} \in \mathcal{V}_{\text {par }, k}} \quad\sum_{\left(v_{i}, v_{j}\right) \in \mathcal{E} ,\text {and } v_{j} \notin \mathcal{V}_{\text {par }, k}} \quad a_{i j}
$$
将每个节点所在的partition index作为label。</p>
<h3 id="graph-completion">Graph completion</h3>
<p>图补全任务如下图所示。</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/5.png#center" alt=""  />
</p>
<p>图补全首先通过删除目标节点的特征来mask目标节点。 然后，通过向 GCN 提供未掩蔽的节点特征（目前仅限于 2 层 GCN 的每个目标节点的二阶邻居）来恢复/预测被mask的节点特征。设计该自监督任务的原因如下：1）标签可以自由获取，也就是节点特征本身； 2）图补全可以帮助网络获得更好的特征表示，这可以教会网络从上下文中提取特征。</p>
<p>最终多任务自监督GCN模型的框架如下图所示：</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/3.png#center" alt=""  />
</p>
<h2 id="self-supervision-in-graph-adversarial-defense">Self-Supervision in Graph Adversarial Defense</h2>
<p>本文专注于Evasion Attack，在模型训练好后对目标节点$v_n$扰动， 实际上对于Evasion Attack，对扰动图重新训练或许可以纠正扰动的影响，但是本文这里不考虑重新训练。一个attacker $g$生成新的特征和邻接矩阵：
$$
\boldsymbol{X}^{\prime}, \boldsymbol{A}^{\prime}=g\left(\boldsymbol{X}, \boldsymbol{A}, \boldsymbol{Y}, v_{n}, \theta^{*}, \boldsymbol{\Theta}^{*}\right)
$$
其中$ \theta^{*}, \boldsymbol{\Theta}^{*}$是在clean 图上训练好的模型参数。</p>
<p>对抗训练的目标函数定义为:
$$
\begin{aligned}
\boldsymbol{Z} &amp;=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta}, \quad \boldsymbol{Z}^{\prime}=f_{\theta}\left(\boldsymbol{X}^{\prime}, \boldsymbol{A}^{\prime}\right) \boldsymbol{\Theta} \\
\theta^{*}, \boldsymbol{\Theta}^{*} &amp;=\arg \min_{\theta, \boldsymbol{\Theta}}\left(\mathcal{L}_{\text {sup }}(\theta, \boldsymbol{\Theta})+\alpha_{3} \mathcal{L}_{\mathrm{adv}}(\theta, \boldsymbol{\Theta})\right)
\end{aligned}
$$
表示模型要同时在扰动图和训练图上都保持较好的效果。 本文将基于自监督的对抗训练定义为：
$$
\begin{aligned}
\boldsymbol{Z} &amp;=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta}, \quad \boldsymbol{Z}^{\prime}=f_{\theta}\left(\boldsymbol{X}^{\prime}, \boldsymbol{A}^{\prime}\right) \boldsymbol{\Theta} \\
\boldsymbol{Z}_{\mathrm{ss}}=&amp; f_{\theta}\left(\boldsymbol{X}_ \mathrm{ss}, \boldsymbol{A}_{\mathrm{ss}}\right) \\
\theta^{*}, \boldsymbol{\Theta}^{*}, \boldsymbol{\Theta}_{\mathrm{ss}}^{*}=&amp; \arg \min_{\theta, \boldsymbol{\Theta}, \boldsymbol{\Theta}_{\mathrm{ss}}}\left(\alpha_{1} \mathcal{L}_{\mathrm{sup}}(\theta, \boldsymbol{\Theta})\right.\\
&amp;\left.+\alpha_{2} \mathcal{L}_{\mathrm{ss}}\left(\theta, \boldsymbol{\Theta}_{\mathrm{ss}}\right)+\alpha_{3} \mathcal{L}_{\mathrm{adv}}(\theta, \boldsymbol{\Theta})\right)
\end{aligned}
$$
其中自监督损失被引入到以扰动图数据作为输入的训练中（自监督标签矩阵 $\boldsymbol{Y}_{ss}$ 也是从扰动输入生成的）。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Proximal Gradient Descent</title>
      <link>https://JhuoW.github.io/posts/pgd/</link>
      <pubDate>Mon, 04 Apr 2022 16:41:42 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/pgd/</guid>
      <description>&lt;p&gt;当目标函数中有不可微部分时，可使用近端梯度下降来优化（Proximal Gradient Descent）&lt;/p&gt;
&lt;p&gt;假设目标函数如下：
$$
\definecolor{energy}{RGB}{114,0,172}
\definecolor{freq}{RGB}{45,177,93}
\definecolor{spin}{RGB}{251,0,29}
\definecolor{signal}{RGB}{18,110,213}
\definecolor{circle}{RGB}{217,86,16}
\definecolor{average}{RGB}{203,23,206}
\definecolor{red}{RGB}{255,0,0}
f(w) = g(w) + h(w)
$$
其中$g(w)$是可微凸函数，$h(w)$是不可微（或局部不可微）凸函数。 以线性回归为例，&lt;/p&gt;
&lt;p&gt;给定$X \in \mathbb{R}^{m \times n}$, $y \in \mathbb{R}^m$， Ridge Regression的目标函数为&lt;/p&gt;
&lt;p&gt;$$
f(\boldsymbol{w})=\underbrace{\frac{1}{2}||\boldsymbol{y}-\boldsymbol{X} \boldsymbol{w}||_{2}^{2}}_{g(\boldsymbol{w})}+\underbrace{\lambda||\boldsymbol{w}||_{2}}_{h(\boldsymbol{w})}
$$
因为$\ell_2$ norm处处可导，所以Ridge可以用SGD或GD来直接优化。但是若目标函数为Lasso，即正则化项定义为$\ell_1$ norm:
$$
f(\boldsymbol{w})=\underbrace{\frac{1}{2}||\boldsymbol{y}-\boldsymbol{X} \boldsymbol{w}||_{2}^{2}}_{g(\boldsymbol{w})}+\underbrace{\lambda||\boldsymbol{w}||_{1}}_{h(\boldsymbol{w})}
$$
这里$h(w)=\lambda||\boldsymbol{w}||_{1}$在$w=0$处不可导，那么可用PGD来优化。&lt;/p&gt;
&lt;h1 id=&#34;proximity-operator&#34;&gt;Proximity Operator&lt;/h1&gt;
&lt;p&gt;近端算子： 对于不可微函数$h(w)$, $h(w)$的proximity operator定义为：&lt;/p&gt;
&lt;p&gt;$$
u^* = \operatorname{prox}_{\color{signal}h}(w)=\underset{u}{\arg \min }\left(h(u)+\frac{1}{2}||u-w||_{2}^{2}\right)
$$
近端算子$\operatorname{prox}_{\color{signal}h}(w)$只和不可微凸函数$h(\cdot)$有关。 上式含义，给定一个不可微凸函数$h(\cdot)$, 给定向量$w \in \mathbb{R}^n$, 找到向量$u = u^*$, 使得公式$h(u)+\frac{1}{2}||u-w||_{2}^{2}$最小。 这个$u^* = \operatorname{prox}_{\color{signal}h}(w)$就是$h(\cdot)$在给定$w$条件下的近端算子（Proximity Operator）。$u^* = \operatorname{prox}_{\color{signal}h}(w)$要求最佳的$u^* $可以使得函数值$h(u^*)$尽可能小，同时$u^*$要尽可能接近给定的$w$。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>当目标函数中有不可微部分时，可使用近端梯度下降来优化（Proximal Gradient Descent）</p>
<p>假设目标函数如下：
$$
\definecolor{energy}{RGB}{114,0,172}
\definecolor{freq}{RGB}{45,177,93}
\definecolor{spin}{RGB}{251,0,29}
\definecolor{signal}{RGB}{18,110,213}
\definecolor{circle}{RGB}{217,86,16}
\definecolor{average}{RGB}{203,23,206}
\definecolor{red}{RGB}{255,0,0}
f(w) = g(w) + h(w)
$$
其中$g(w)$是可微凸函数，$h(w)$是不可微（或局部不可微）凸函数。 以线性回归为例，</p>
<p>给定$X \in \mathbb{R}^{m \times n}$, $y \in \mathbb{R}^m$， Ridge Regression的目标函数为</p>
<p>$$
f(\boldsymbol{w})=\underbrace{\frac{1}{2}||\boldsymbol{y}-\boldsymbol{X} \boldsymbol{w}||_{2}^{2}}_{g(\boldsymbol{w})}+\underbrace{\lambda||\boldsymbol{w}||_{2}}_{h(\boldsymbol{w})}
$$
因为$\ell_2$ norm处处可导，所以Ridge可以用SGD或GD来直接优化。但是若目标函数为Lasso，即正则化项定义为$\ell_1$ norm:
$$
f(\boldsymbol{w})=\underbrace{\frac{1}{2}||\boldsymbol{y}-\boldsymbol{X} \boldsymbol{w}||_{2}^{2}}_{g(\boldsymbol{w})}+\underbrace{\lambda||\boldsymbol{w}||_{1}}_{h(\boldsymbol{w})}
$$
这里$h(w)=\lambda||\boldsymbol{w}||_{1}$在$w=0$处不可导，那么可用PGD来优化。</p>
<h1 id="proximity-operator">Proximity Operator</h1>
<p>近端算子： 对于不可微函数$h(w)$, $h(w)$的proximity operator定义为：</p>
<p>$$
u^* = \operatorname{prox}_{\color{signal}h}(w)=\underset{u}{\arg \min }\left(h(u)+\frac{1}{2}||u-w||_{2}^{2}\right)
$$
近端算子$\operatorname{prox}_{\color{signal}h}(w)$只和不可微凸函数$h(\cdot)$有关。 上式含义，给定一个不可微凸函数$h(\cdot)$, 给定向量$w \in \mathbb{R}^n$, 找到向量$u = u^*$, 使得公式$h(u)+\frac{1}{2}||u-w||_{2}^{2}$最小。 这个$u^* = \operatorname{prox}_{\color{signal}h}(w)$就是$h(\cdot)$在给定$w$条件下的近端算子（Proximity Operator）。$u^* = \operatorname{prox}_{\color{signal}h}(w)$要求最佳的$u^* $可以使得函数值$h(u^*)$尽可能小，同时$u^*$要尽可能接近给定的$w$。</p>
<p>基于后面的公式推导，我们给$\operatorname{prox}_{\color{signal}h}(w)$添加一个参数$\color{energy}\gamma$:
$$
u^* = \operatorname{prox}_{\color{signal}h\color{energy}\gamma}(w)=\underset{u}{\arg \min }\left(h(u)+\frac{1}{2\color{energy}\gamma}||u-w||_{2}^{2}\right)
$$
上式表示，给定一个不可微凸函数$h(\cdot)$， 一个给定的点$w$, 一个参数$\gamma$, 要找到一个$u = u^*$, 使得$u^*$带入公式$h(u)+\frac{1}{2\color{energy}\gamma}||u-w||_{2}^{2}$的到的结果最小。 $\operatorname{prox}_{\color{signal}h\color{energy}\gamma}(w)$是使得$h(u)+\frac{1}{2\color{energy}\gamma}||u-w||_{2}^{2}$最小的输入$u$。</p>
<p>因为$h(u)$和$||u-w||_{2}^{2}$都为凸函数，所以一定存在$u^*$使得函数值最小， 这个$u^* = \operatorname{prox}_{\color{signal}h\color{energy}\gamma}(w)$要求使得$h(u^*)$尽可能小（第一项），同时$u^*$要尽可能接近给定的$w$（第二项）。</p>
<p>例子：</p>
<ul>
<li>
<p>若$h(w)=0$, $\operatorname{prox}_{\color{signal}h\color{energy}\gamma}(w) = u^* = w$。</p>
</li>
<li>
<p>当$h(w) = ||w||_{1}$时，$\operatorname{prox}_{\color{signal}h\color{energy}\gamma}(w) = \operatorname{prox}_{\color{signal}||\cdot||_{1}\color{energy}\gamma}(w)$是软阈值操作</p>
</li>
</ul>
<p>$$
u^* = \left(\operatorname{prox}_{\color{signal}h\color{energy}\gamma}(w)\right)_{i}= \begin{cases}w_{i}-\gamma &amp; w_{i} \geq \gamma \\ 0 &amp; \left|\mathrm{w}_{i}\right| \leq \gamma \\ w_{i}+ \gamma &amp; w_{i} \leq-\gamma\end{cases}
$$</p>
<p>如果在$\ell_1$ norm前加上参数$\lambda$， 即$h(w) = \lambda||w||_{1}$， 那么近端算子为：
$$
u^* = \left(\operatorname{prox}_{\color{signal}h\color{energy}\gamma}(w)\right)_{i}= \begin{cases}w_{i}-\lambda\gamma &amp; w_{i} \geq \lambda\gamma \\ 0 &amp; \left|\mathrm{w}_{i}\right| \leq \lambda\gamma \\ w_{i}+ \lambda\gamma &amp; w_{i} \leq-\lambda\gamma\end{cases}
$$</p>
<h1 id="近端梯度算法">近端梯度算法</h1>
<p>回到Lasso 回归，要求解：
$$
\min_{w}(g(w)+h(w))
$$
$w$可以通过递推式求出：
$$
\boldsymbol{w}^{k}=\operatorname{prox}_{\color{signal}h\color{energy}\gamma}\left(\boldsymbol{w}^{k-1}-\gamma \nabla g\left(\boldsymbol{w}^{k-1}\right)\right)
$$
为什么可以通过不断迭代迭代上式来求解最佳的$w^{K}$, 使得$g(w)+h(w)$收敛到最小？下面先给出证明
$$
\begin{equation}
\begin{aligned}
\boldsymbol{w}^{k} &amp; =\operatorname{prox}_{\color{signal}h\color{energy}\gamma}\left(\boldsymbol{w}^{k-1}-\gamma \nabla g\left(\boldsymbol{w}^{k-1}\right)\right)\\
&amp;=\underset{u}{\operatorname{argmin}}\left(\underbrace{h(u)}_{\color{red}h(u)尽可能小}+\frac{1}{2 \gamma}\left|\left|\underbrace{u-\left(w^{k-1}-\gamma \nabla g\left(w^{k-1}\right)\right)}_{\color{red}u尽可能接近w^{k-1}-\gamma \nabla g\left(w^{k-1}\right)}\right|\right|_{2}^{2}\right)\\
&amp;= \underset{u}{\operatorname{argmin}}\left(h(u)+\frac{1}{2 \gamma}\left|\left|(u-w^{k-1} )+\gamma \nabla g\left(w^{k-1}\right)\right|\right|_{2}^{2}\right)\\
&amp; = \underset{u}{\operatorname{argmin}}\left(h(u)+\underbrace{\frac{\gamma}{2}\left|\left|\nabla g\left(w^{k-1}\right)\right|\right|_{2}^{2}}_{\color{red}\gamma和w^{k-1}给定， 所以该项与u无关，视为常数，可省略}+\left(u-w^{k-1}\right)^{T}\nabla g\left(w^{k-1}\right)+\frac{1}{2 \gamma}\left|\left|u-w^{k-1}\right|\right|_{2}^{2}\right) \\
&amp;= \underset{u}{\operatorname{argmin}}\left(h(u)+\underbrace{g\left(w^{k-1}\right)}_{\color{red}添加与u无关的项，不影响结果}+\left(u-w^{k-1}\right)^T\nabla g\left(w^{k-1}\right)+\frac{1}{2 \gamma}\left|\left|u-w^{k-1}\right|\right|_{2}^{2}\right) \quad \quad (5)\\ &amp;
\approx \underset{u}{\arg \min }(g(u)+h(u))
\end{aligned}
\end{equation}
$$
<strong>整个过程不涉及对$h(u)$求梯度</strong></p>
<p>最后两步怎么来的？</p>
<blockquote>
<p>泰勒展开式：</p>
<p>$$f(x)=\frac{f\left(a\right)}{0 !}+\frac{f^{\prime}\left(a\right)}{1 !}\left(x-a\right)+\frac{f^{\prime \prime}\left(a\right)}{2 !}\left(x-a\right)^{2}+\ldots+\frac{f^{(n)}\left(a\right)}{n !}\left(x-a\right)^{n}$$</p></blockquote>
<p>对$g(u)$做泰勒展开， 令$a=w^{k-1}$:
$$
\begin{equation}
\begin{aligned}
g(u) &amp;= g(w^{k-1}) + (u-w^{k-1})^{T}\nabla g\left(w^{k-1}\right) + \langle u-w^{k-1}, u-w^{k-1}\rangle \nabla^2 g\left(w^{k-1}\right) \\
&amp; \approx (5)式最后三项
\end{aligned}
\end{equation}
$$
综上：
$$
\begin{equation}
\begin{aligned}
\boldsymbol{w}^{k} &amp; =\operatorname{prox}_{\color{signal}h\color{energy}\gamma}\left(\boldsymbol{w}^{k-1}-\gamma \nabla g\left(\boldsymbol{w}^{k-1}\right)\right) \\
&amp; \approx \underset{u}{\arg \min }(g(u)+h(u))
\end{aligned}
\end{equation}
$$
<strong>所以通过迭代的方式求$w^k = \operatorname{prox}_{\color{signal}h\color{energy}\gamma}\left(\boldsymbol{w}^{k-1}-\gamma \nabla g\left(\boldsymbol{w}^{k-1}\right)\right)$ 就是$\min_{w}(g(w)+h(w))$的迭代递推求解过程。</strong></p>
<h2 id="求解ell_1-范数">求解$\ell_1$ 范数</h2>
<p><strong>将求解问题转为递推式</strong>。</p>
<p>现在我们有问题，形式为：
$$
\min_{w}(\underbrace{g(w)}_{\color{red}凸可微}+\underbrace{\lambda \left|\left| w \right|\right|_1}_{\color{red}h(w)凸不可微})
$$
找到最佳$w$使得上式最小的过程可以迭代递推为：
$$
\begin{equation}
\begin{aligned}
w^{k+1}&amp;=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma}\left(w^{k}-\gamma \nabla g\left(w^{k}\right)\right)\\
&amp;= \underset{u}{\arg \min }\left(\lambda \left|\left| u \right|\right|_1+\frac{1}{2\color{energy}\gamma}||u-\left(w^{k}-\gamma \nabla g\left(w^{k}\right)\right)||_{2}^{2}\right)
\end{aligned}
\end{equation}
$$</p>
<h2 id="求解lasso回归">求解Lasso回归</h2>
<p>待求解问题形如：
$$
\min_{w}\left(\frac{1}{2}||X w-y||_{2}^{2}+r||w||_1\right)
$$
可见第一项可微，第二项为$\ell_1$ norm 在$w=0$处不可微。</p>
<p>根据递推式：
$$
w^{k+1}=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma}\left(\underbrace{w^{k}-\gamma \nabla g\left(w^{k}\right)}_{\color{red}z^k}\right)
$$
令$z^k = w^{k}-\gamma \nabla g\left(w^{k}\right)$, 上式可改写为</p>
<p>因为learning step size $\gamma$ 与$w$无关，所以上式可以改写为：
$$
\begin{equation}
\begin{aligned}
w^{k+1}&amp;=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma}\left(z^k\right)\\
&amp;= \underset{w}{\arg \min } \left(  \lambda \left|\left| w \right|\right|_1 + \frac{1}{2\color{energy}\gamma}||w-z^k||_{2}^{2}\right) \\
&amp;= \underset{w}{\arg \min } \left(  \lambda \gamma\left|\left| w \right|\right|_1 + \frac{1}{2}||w-z^k||_{2}^{2}\right)
\end{aligned}
\end{equation}
$$</p>
<p>$\because g(w^k) = \frac{1}{2}||X w-y||_{2}^{2}$</p>
<p>$\therefore \nabla g(w^k) = X^{T}(Xw^k-y) = X^TXw^k-X^Ty$</p>
<p>$\therefore z^k = w^k-\gamma (X^TXw^k-X^Ty)$</p>
<p>把$z^k$带入$w^{k+1}$中：
$$
\begin{equation}
\begin{aligned}
w^{k+1}&amp;=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma}\left(z^k\right)\\
&amp;=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma}\left( w^k-\gamma X^TXw^k+ \gamma X^Ty\right)
\end{aligned}
\end{equation}
$$
因为$\left(\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma}(w)\right)_{i}= \begin{cases}w_{i}-\lambda\gamma &amp; w_{i} \geq \lambda\gamma \\ 0 &amp; \left|w_{i}\right| \leq \lambda\gamma \\ w_{i}+ \lambda\gamma &amp; w_{i} \leq-\lambda\gamma\end{cases}$,</p>
<p>所以$w^k$到$w^{k+1}$的迭代优化方式如下：
$$
w_i^{k+1} = \left(\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma}\left( z^k\right)\right)_i = \begin{cases} z^k_{i}-\lambda\gamma &amp; z^k_{i} \geq \lambda\gamma \\ 0 &amp; \left|z^k_{i}\right| \leq \lambda\gamma \\ z^k_{i}+ \lambda\gamma &amp; z^k_{i} \leq-\lambda\gamma \end{cases}
$$
其中 $z^k_{i}$是$z^k$的第$i$行。</p>
<h1 id="reference">Reference</h1>
<p><a href="https://blog.csdn.net/Chaolei3/article/details/81320940">https://blog.csdn.net/Chaolei3/article/details/81320940</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/82622940">https://zhuanlan.zhihu.com/p/82622940</a></p>
<p><a href="http://roachsinai.github.io/2016/08/03/1Proximal_Method/">http://roachsinai.github.io/2016/08/03/1Proximal_Method/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Everything about Graph Laplacian</title>
      <link>https://JhuoW.github.io/posts/laplacian/</link>
      <pubDate>Sat, 02 Apr 2022 15:00:20 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/laplacian/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds.  The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem. Another application is spectral matching that solves for graph matching.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds.  The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem. Another application is spectral matching that solves for graph matching.</p>
<h1 id="basic-notations">Basic notations</h1>
<p>We consider simple graphs (no multiple edges or loops), $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ :</p>
<ul>
<li>
<p>$\mathcal{V}(\mathcal{G})=\left\{v_{1}, \ldots, v_{n}\right\}$ is called the vertex set with $n=|\mathcal{V}|$;</p>
</li>
<li>
<p>$\mathcal{E}(\mathcal{G})=\left\{e_{i j}\right\}$ is called the edge set with $m=|\mathcal{E}|$;</p>
</li>
<li>
<p>An edge $e_{i j}$ connects vertices $v_{i}$ and $v_{j}$ if they are adjacent or neighbors. One possible notation for adjacency is $v_{i} \sim v_{j}$;</p>
</li>
<li>
<p>The number of neighbors of a node $v$ is called the degree of $v$ and is denoted by $d(v), d\left(v_{i}\right)=\sum_{v_{i} \sim v_{j}} e_{i j}$. If all the nodes of a graph have the same degree, the graph is regular; The nodes of an Eulerian graph have even degree.</p>
</li>
<li>
<p>A graph is complete if there is an edge between every pair of vertices.</p>
</li>
</ul>
<h1 id="subgraph-of-a-graph">Subgraph of a graph</h1>
<ul>
<li>
<p>$\mathcal{H}$ is a subgraph of $\mathcal{G}$ if $\mathcal{V}(\mathcal{H}) \subseteq \mathcal{V}(\mathcal{G})$ and $\mathcal{E}(\mathcal{H}) \subseteq \mathcal{E}(\mathcal{G})$;</p>
</li>
<li>
<p>a subgraph $\mathcal{H}$ is an induced subgraph of $\mathcal{G}$ if two vertices of $\mathcal{V}(\mathcal{H})$ are adjacent if and only if they are adjacent in $\mathcal{G}$.</p>
</li>
<li>
<p>A clique is a complete subgraph of a graph.</p>
</li>
<li>
<p>A path of $k$ vertices is a sequence of $k$ distinct vertices such that consecutive vertices are adjacent.</p>
</li>
<li>
<p>A cycle is a connected subgraph where every vertex has exactly two neighbors.</p>
</li>
<li>
<p>A graph containing no cycles is a forest. A connected forest is a tree.</p>
</li>
</ul>
<h1 id="a-k-partite-graph">A k-partite graph</h1>
<ul>
<li>A graph is called k-partite if its set of vertices admits a partition into $k$ classes such that the vertices of the same class are not adjacent.</li>
<li>An example of a bipartite graph.</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/1.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-adjacency-matrix-of-a-graph">The adjacency matrix of a graph</h1>
<ul>
<li>For a graph with $n$ vertices, the entries of the $n \times n$ adjacency matrix are defined by:</li>
</ul>
<p>$$
\mathbf{A}:= \begin{cases}A_{i j}=1 &amp; \text { if there is an edge } e_{i j} \\ A_{i j}=0 &amp; \text { if there is no edge } \\ A_{i i}=0 &amp; \end{cases}
$$</p>
<p>$$
\begin{aligned}
&amp; \mathbf{A}=\left[\begin{array}{llll}0 &amp; 1 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 1 &amp; 1 \\1 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 &amp; 0\end{array}\right]
\end{aligned}
$$</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="eigenvalues-and-eigenvectors">Eigenvalues and eigenvectors</h1>
<ul>
<li>
<p>A is a real-symmetric matrix: it has $n$ real eigenvalues and its $n$ real eigenvectors form an orthonormal basis.</p>
</li>
<li>
<p>Let $\left\{\lambda_{1}, \ldots, \lambda_{i}, \ldots, \lambda_{r}\right\}$ be the set of distinct eigenvalues.</p>
</li>
<li>
<p>The eigenspace $S_{i}$ contains the eigenvectors associated with $\lambda_{i}$ :</p>
</li>
</ul>
<p>$$
S_{i}=\left\{\boldsymbol{x} \in \mathbb{R}^{n} \mid \mathbf{A} \boldsymbol{x}=\lambda_{i} \boldsymbol{x}\right\}
$$</p>
<ul>
<li>
<p>For real-symmetric matrices, the algebraic multiplicity is equal to the geometric multiplicity, for all the eigenvalues.</p>
</li>
<li>
<p>The dimension of $S_{i}$ (geometric multiplicity) is equal to the multiplicity of $\lambda_{i}$.</p>
</li>
<li>
<p>If $\lambda_{i} \neq \lambda_{j}$ then $S_{i}$ and $S_{j}$ are mutually orthogonal.</p>
</li>
</ul>
<h1 id="real-valued-functions-on-graphs">Real-valued functions on graphs</h1>
<ul>
<li>
<p>We consider real-valued functions on the set of the graph&rsquo;s vertices, $\boldsymbol{f}: \mathcal{V} \longrightarrow \mathbb{R}$. Such a function assigns a real number to each graph node.</p>
</li>
<li>
<p>$\boldsymbol{f}$ is a vector indexed by the graph&rsquo;s vertices, hence $\boldsymbol{f} \in \mathbb{R}^{n}$.</p>
</li>
<li>
<p>Notation: $\boldsymbol{f}=\left(f\left(v_{1}\right), \ldots, f\left(v_{n}\right)\right)=(f(1), \ldots, f(n))$.</p>
</li>
<li>
<p>The eigenvectors of the adjacency matrix, $\mathbf{A} \boldsymbol{x}=\lambda \boldsymbol{x}$, can be viewed as eigenfunctions.</p>
</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/3.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="matrix-a-as-an-operator-and-quadratic-form">Matrix A as an operator and quadratic form</h1>
<ul>
<li>The adjacency matrix can be viewed as an operator</li>
</ul>
<p>$$
\boldsymbol{g}=\mathbf{A} \boldsymbol{f} ; g(i)=\sum_{i \sim j} f(j)
$$</p>
<ul>
<li>It can also be viewed as a quadratic form:</li>
</ul>
<p>$$
\boldsymbol{f}^{\top} \mathbf{A} \boldsymbol{f}=\sum_{e_{i j}} f(i) f(j)
$$</p>
<h1 id="the-incidence-matrix-of-a-graph">The incidence matrix of a graph</h1>
<ul>
<li>
<p>Let each edge in the graph have an arbitrary but fixed orientation;</p>
</li>
<li>
<p>The incidence matrix of a graph is a $|\mathcal{E}| \times|\mathcal{V}|(m \times n)$ matrix defined as follows:</p>
</li>
</ul>
<p>$$
\nabla:= \begin{cases}\nabla_{e v}=-1 &amp; \text { if } v \text { is the initial vertex of edge } e \\ \nabla_{e v}=1 &amp; \text { if } v \text { is the terminal vertex of edge } e \\ \nabla_{e v}=0 &amp; \text { if } v \text { is not in } e\end{cases}
$$</p>
<p>$$
\begin{aligned}
&amp; \nabla=\left[\begin{array}{cccc}-1 &amp; 1 &amp; 0 &amp; 0 \\1 &amp; 0 &amp; -1 &amp; 0 \\0 &amp; -1 &amp; 1 &amp; 0 \\0 &amp; -1 &amp; 0 &amp; +1\end{array}\right]
\end{aligned}
$$</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-incidence-matrix-a-discrete-differential-operator">The incidence matrix: A discrete differential operator</h1>
<ul>
<li>
<p>The mapping $\boldsymbol{f} \longrightarrow \nabla \boldsymbol{f}$ is known as the co-boundary mapping of the graph.</p>
</li>
<li>
<p>$(\nabla \boldsymbol{f})\left(e_{i j}\right)=f\left(v_{j}\right)-f\left(v_{i}\right)$</p>
</li>
</ul>
<p>$$
\left(\begin{array}{c}
f(2)-f(1) \\
f(1)-f(3) \\
f(3)-f(2) \\
f(4)-f(2)
\end{array}\right)=\left[\begin{array}{cccc}
-1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; -1 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; +1
\end{array}\right]\left(\begin{array}{c}
f(1) \\
f(2) \\
f(3) \\
f(4)
\end{array}\right)
$$</p>
<h1 id="the-laplacian-matrix-of-a-graph">The Laplacian matrix of a graph</h1>
<ul>
<li>
<p>$\mathbf{L}=\nabla^{\top} \nabla$</p>
</li>
<li>
<p>$(\mathbf{L} \boldsymbol{f})\left(v_{i}\right)=\sum_{v_{j} \sim v_{i}}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)$</p>
</li>
<li>
<p>Connection between the Laplacian and the adjacency matrices:</p>
</li>
</ul>
<p>$$
\mathbf{L}=\mathbf{D}-\mathbf{A}
$$</p>
<ul>
<li>The degree matrix: $\mathbf{D}:=D_{i i}=d\left(v_{i}\right)$.</li>
</ul>
<p>$$
\mathbf{L}=\left[\begin{array}{cccc}
2 &amp; -1 &amp; -1 &amp; 0 \\
-1 &amp; 3 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 2 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 1
\end{array}\right]
$$</p>
<h1 id="the-laplacian-matrix-of-an-undirected-weighted-graph">The Laplacian matrix of an undirected weighted graph</h1>
<ul>
<li>
<p>We consider undirected weighted graphs: Each edge $e_{i j}$ is weighted by $w_{i j}&gt;0$.</p>
</li>
<li>
<p>The Laplacian as an operator:</p>
</li>
</ul>
<p>$$
(\mathbf{L} \boldsymbol{f})\left(v_{i}\right)=\sum_{v_{j} \sim v_{i}} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)
$$</p>
<ul>
<li>As a quadratic form:</li>
</ul>
<p>$$
\boldsymbol{f}^{\top} \mathbf{L} \boldsymbol{f}=\frac{1}{2} \sum_{e_{i j}} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)^{2}
$$</p>
<ul>
<li>
<p>L is symmetric and positive semi-definite.</p>
</li>
<li>
<p>L has $n$ non-negative, real-valued eigenvalues: $0=\lambda_{1} \leq \lambda_{2} \leq \ldots \leq \lambda_{n} .$</p>
</li>
</ul>
<h1 id="the-laplacian-of-a-3d-discrete-surface-mesh">The Laplacian of a 3D discrete surface (mesh)</h1>
<ul>
<li>
<p>A graph vertex $v_{i}$ is associated with a 3D point $\boldsymbol{v}_{i}$.</p>
</li>
<li>
<p>The weight of an edge $e_{i j}$ is defined by the Gaussian kernel:</p>
</li>
</ul>
<p>$$
w_{i j}=\exp \left(-\left|\boldsymbol{v}_{i}-\boldsymbol{v}_{j}\right|^{2} / \sigma^{2}\right)
$$</p>
<ul>
<li>
<p>$0 \leq w_{\min } \leq w_{i j} \leq w_{\max } \leq 1$</p>
</li>
<li>
<p>Hence, the geometric structure of the mesh is encoded in the weights.</p>
</li>
<li>
<p>Other weighting functions were proposed in the literature.</p>
</li>
</ul>
<h1 id="the-laplacian-of-a-cloud-of-points">The Laplacian of a cloud of points</h1>
<ul>
<li>
<p>3-nearest neighbor graph</p>
</li>
<li>
<p>$\varepsilon$-radius graph</p>
</li>
<li>
<p>KNN may guarantee that the graph is connected (depends on the implementation)</p>
</li>
<li>
<p>$\varepsilon$-radius does not guarantee that the graph has one connected component</p>
</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/4.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-laplacian-of-a-graph-with-one-connected-component">The Laplacian of a graph with one connected component</h1>
<ul>
<li>
<p>$Lu =\lambda \boldsymbol{u}$.</p>
</li>
<li>
<p>$\mathbf{L} \mathbf{1}_{n}=\mathbf{0}, \lambda_{1}=0$ is the smallest eigenvalue.</p>
</li>
<li>
<p>The one vector: $\mathbf{1}_{n}=(1 \ldots 1)^{\top}$.</p>
</li>
<li>
<p>$0=\boldsymbol{u}^{\top} \mathbf{L} \boldsymbol{u}=\sum_{i, j=1}^{n} w_{i j}(u(i)-u(j))^{2}$.</p>
</li>
<li>
<p>If any two vertices are connected by a path, then $\boldsymbol{u}=(u(1), \ldots, u(n))$ needs to be constant at all vertices such that the quadratic form vanishes. Therefore, a graph with one connected component has the constant vector $\boldsymbol{u}_{1}=\mathbf{1}_{n}$ as the only eigenvector with eigenvalue 0 .</p>
</li>
</ul>
<h1 id="a-graph-with-k1-connected-components">A graph with $k&gt;1$ connected components</h1>
<ul>
<li>Each connected component has an associated Laplacian. Therefore, we can write matrix $\mathbf{L}$ as a block diagonal matrix:</li>
</ul>
<p>$$
\mathbf{L}=\left[\begin{array}{lll}
\mathbf{L}_{1} &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \mathbf{L}_{k}
\end{array}\right]
$$</p>
<ul>
<li>
<p>The spectrum of $\mathbf{L}$ is given by the union of the spectra of $\mathbf{L}_{i}$.</p>
</li>
<li>
<p>Each block corresponds to a connected component, hence each matrix $\mathbf{L}_{i}$ has an eigenvalue 0 with multiplicity 1 .</p>
</li>
<li>
<p>The spectrum of $\mathbf{L}$ is given by the union of the spectra of $\mathbf{L}_{i}$.</p>
</li>
<li>
<p>The eigenvalue $\lambda_{1}=0$ has multiplicity $k$.</p>
</li>
</ul>
<h1 id="the-eigenspace-of-lambda_10-with-multiplicity-k">The eigenspace of $\lambda_{1}=0$ with multiplicity $k$</h1>
<ul>
<li>The eigenspace corresponding to $\lambda_{1}=\ldots=\lambda_{k}=0$ is spanned by the $k$ mutually orthogonal vectors:</li>
</ul>
<p>$$
\begin{aligned}
\boldsymbol{u}_{1} &amp;=\mathbf{1}_{L_{1}} \\
&amp; \cdots \\
\boldsymbol{u}_{k} &amp;=\mathbf{1}_{L_{k}}
\end{aligned}
$$</p>
<ul>
<li>
<p>with $\mathbf{1}_{L_{i}}=(0000111110000)^{\top} \in \mathbb{R}^{n}$</p>
</li>
<li>
<p>These vectors are the indicator vectors of the graph&rsquo;s connected components.</p>
</li>
<li>
<p>Notice that $\mathbf{1}_{L_{1}}+\ldots+\mathbf{1}_{L_{k}}=\mathbf{1}_{n}$</p>
</li>
</ul>
<h1 id="the-fiedler-vector-of-the-graph-laplacian">The Fiedler vector of the graph Laplacian</h1>
<ul>
<li>
<p>The first non-null eigenvalue $\lambda_{k+1}$ is called the Fiedler value.</p>
</li>
<li>
<p>The corresponding eigenvector $\boldsymbol{u}_{k+1}$ is called the Fiedler vector.</p>
</li>
<li>
<p>The multiplicity of the Fiedler eigenvalue is always equal to $1 .$</p>
</li>
<li>
<p>The Fiedler value is the algebraic connectivity of a graph, the further from 0 , the more connected.</p>
</li>
<li>
<p>The Fidler vector has been extensively used for spectral bi-partioning</p>
</li>
<li>
<p>Theoretical results are summarized in Spielman &amp; Teng 2007: <a href="http://cs-www.cs.yale.edu/homes/spielman/">http://cs-www.cs.yale.edu/homes/spielman/</a></p>
</li>
</ul>
<h1 id="eigenvectors-of-the-laplacian-of-connected-graphs">Eigenvectors of the Laplacian of connected graphs</h1>
<ul>
<li>
<p>$\boldsymbol{u}_{1}=\mathbf{1}_{n}, \mathbf{L} \mathbf{1}_{n}=\mathbf{0}$.</p>
</li>
<li>
<p>$\boldsymbol{u}_{2}$ is the the Fiedler vector with multiplicity 1 .</p>
</li>
<li>
<p>The eigenvectors form an orthonormal basis: $\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{j}=\delta_{i j}$.</p>
</li>
<li>
<p>For any eigenvector $\boldsymbol{u}_{i}=\left(\boldsymbol{u}_{i}\left(v_{1}\right) \ldots \boldsymbol{u}_{i}\left(v_{n}\right)\right)^{\top}, 2 \leq i \leq n$ :</p>
</li>
</ul>
<p>$$
\boldsymbol{u}_{i}^{\top} \mathbf{1}_{n}=0
$$</p>
<ul>
<li>Hence the components of $\boldsymbol{u}_{i}, 2 \leq i \leq n$ satisfy:</li>
</ul>
<p>$$
\sum_{j=1}^{n} \boldsymbol{u}_{i}\left(v_{j}\right)=0
$$</p>
<ul>
<li>Each component is bounded by:</li>
</ul>
<p>$$
-1&lt;\boldsymbol{u}_{i}\left(v_{j}\right)&lt;1
$$</p>
<h1 id="laplacian-embedding-mapping-a-graph-on-a-line">Laplacian embedding: Mapping a graph on a line</h1>
<ul>
<li>Map a weighted graph onto a line such that connected nodes stay as close as possible, i.e., minimize $\sum_{i, j=1}^{n} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)^{2}$, or:</li>
</ul>
<p>$$
\arg \min _{\boldsymbol{f}} \boldsymbol{f}^{\top} \mathbf{L} \boldsymbol{f} \text { with: } \boldsymbol{f}^{\top} \boldsymbol{f}=1 \text { and } \boldsymbol{f}^{\top} \mathbf{1}=0
$$</p>
<ul>
<li>
<p>The solution is the eigenvector associated with the smallest nonzero eigenvalue of the eigenvalue problem: $\mathbf{L} \boldsymbol{f}=\lambda \boldsymbol{f}$, namely the Fiedler vector $\boldsymbol{u}_{2}$.</p>
</li>
<li>
<p>For more details on this minimization see Golub &amp; Van Loan Matrix Computations, chapter 8 (The symmetric eigenvalue problem).</p>
</li>
</ul>
<p><em><strong>Example of mapping a graph on the Fiedler vector</strong></em>:</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/5.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="laplacian-embedding">Laplacian embedding</h1>
<ul>
<li>
<p>Embed the graph in a $k$-dimensional Euclidean space. The embedding is given by the $n \times k$ matrix $\mathbf{F}=\left[\boldsymbol{f}_{1} \boldsymbol{f}_{2} \ldots \boldsymbol{f}_{k}\right]$ where the $i$-th row of this matrix $-\boldsymbol{f}^{(i)}-$ corresponds to the Euclidean coordinates of the $i$-th graph node $v_{i}$.</p>
</li>
<li>
<p>We need to minimize:</p>
</li>
</ul>
<p>$$
\arg \min_{\boldsymbol{f}_{1} \ldots} \sum_{k}^{n} \sum_{i, j=1}^{n} w_{i j}\left|\left|\boldsymbol{f}^{(i)}-\boldsymbol{f}^{(j)}\right|\right|^{2} \text { with: } \mathbf{F}^{\top} \mathbf{F}=\mathbf{I}
$$</p>
<ul>
<li>The solution is provided by the matrix of eigenvectors corresponding to the $k$ lowest nonzero eigenvalues of the eigenvalue problem $\mathbf{L} \boldsymbol{f}=\lambda \boldsymbol{f}$.</li>
</ul>
<h1 id="spectral-embedding-using-the-unnormalized-laplacian">Spectral embedding using the unnormalized Laplacian</h1>
<ul>
<li>
<p>Compute the eigendecomposition $\mathbf{L}=\mathbf{D}-\mathbf{A}$.</p>
</li>
<li>
<p>Select the $k$ smallest non-null eigenvalues $\lambda_{2} \leq \ldots \leq \lambda_{k+1}$</p>
</li>
<li>
<p>$\lambda_{k+2}-\lambda_{k+1}=$ eigengap.</p>
</li>
<li>
<p>We obtain the $n \times k$ matrix $\mathbf{U}=\left[\boldsymbol{u}_{2} \ldots \boldsymbol{u}_{k+1}\right]$ :</p>
</li>
</ul>
<p>$$
\mathbf{U}=\left[\begin{array}{ccc}
\boldsymbol{u}_{2}\left(v_{1}\right) &amp; \ldots &amp; \boldsymbol{u}_{k+1}\left(v_{1}\right) \\
\vdots &amp; &amp; \vdots \\
\boldsymbol{u}_{2}\left(v_{n}\right) &amp; \ldots &amp; \boldsymbol{u}_{k+1}\left(v_{n}\right)
\end{array}\right]
$$</p>
<ul>
<li>
<p>$\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{j}=\delta_{i j}$ (orthonormal vectors), hence $\mathbf{U}^{\top} \mathbf{U}=\mathbf{I}_{k}$.</p>
</li>
<li>
<p>Column $i(2 \leq i \leq k+1)$ of this matrix is a mapping on the eigenvector $\boldsymbol{u}_{i}$.</p>
</li>
</ul>
<h1 id="euclidean-l-embedding-of-the-graphs-vertices">Euclidean L-embedding of the graph&rsquo;s vertices</h1>
<ul>
<li>(Euclidean) L-embedding of a graph:</li>
</ul>
<p>$$
\mathbf{X}=\boldsymbol{\Lambda}_{k}^{-\frac{1}{2}} \mathbf{U}^{\top}=\left[\begin{array}{llll}
\boldsymbol{x}_{1} &amp; \ldots &amp; \boldsymbol{x}_{j} \ldots &amp; \boldsymbol{x}_{n}
\end{array}\right]
$$</p>
<p>The coordinates of a vertex $v_{j}$ are:</p>
<p>$$
\boldsymbol{x}_{j}=\left(\begin{array}{c}
\frac{\boldsymbol{u}_{2}\left(v_{j}\right)}{\sqrt{\lambda_{2}}} \\
\vdots \\
\frac{\boldsymbol{u}_{k+1}\left(v_{j}\right)}{\sqrt{\lambda_{k+1}}}
\end{array}\right)
$$</p>
<h1 id="justification-for-choosing-the-l-embedding">Justification for choosing the L-embedding</h1>
<p>Both</p>
<ul>
<li>
<p>the commute-time distance (CTD) and</p>
</li>
<li>
<p>the principal-component analysis of a graph (graph PCA)</p>
</li>
</ul>
<p>are two important concepts; They allow to reason &ldquo;statistically&rdquo; on a graph. They are both associated with the unnormalized Laplacian matrix.</p>
<h1 id="the-commute-time-distance">The commute-time distance</h1>
<ul>
<li>
<p>The CTD is a well known quantity in Markov chains;</p>
</li>
<li>
<p>It is the average number of (weighted) edges that it takes, starting at vertex $v_{i}$, to randomly reach vertex $v_{j}$ for the first time and go back;</p>
</li>
<li>
<p>The CTD decreases as the number of connections between the two nodes increases;</p>
</li>
<li>
<p>It captures the connectivity structure of a small graph volume rather than a single path between the two vertices - such as the shortest-path geodesic distance.</p>
</li>
<li>
<p>The CTD can be computed in closed form:</p>
</li>
</ul>
<p>$$
\operatorname{CTD}^{2}\left(v_{i}, v_{j}\right)=\operatorname{vol}(\mathcal{G})\left|\left|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right|\right|^{2}
$$</p>
<h1 id="the-graph-pca">The graph PCA</h1>
<ul>
<li>The mean (remember that $\sum_{j=1}^{n} \boldsymbol{u}_{i}\left(v_{j}\right)=0$ ):</li>
</ul>
<p>$$
\overline{\boldsymbol{x}}=\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{x}_{j}=\boldsymbol{\Lambda}_{k}^{-\frac{1}{2}}\left(\begin{array}{c}
\sum_{j=1}^{n} \boldsymbol{u}_{2}\left(v_{j}\right) \\
\vdots \\
\sum_{j=1}^{n} \boldsymbol{u}_{k+1}\left(v_{j}\right)
\end{array}\right)=\left(\begin{array}{c}
0 \\
\vdots \\
0
\end{array}\right)
$$</p>
<ul>
<li>The covariance matrix:</li>
</ul>
<p>$$
\mathbf{S}=\frac{1}{n} \sum_{j=1}^{n} \boldsymbol{x}_{j} \boldsymbol{x}_{j}^{\top}=\frac{1}{n} \mathbf{X} \mathbf{X}^{\top}=\frac{1}{n} \boldsymbol{\Lambda}_{k}^{-\frac{1}{2}} \mathbf{U}^{\top} \mathbf{U} \boldsymbol{\Lambda}_{k}^{-\frac{1}{2}}=\frac{1}{n} \boldsymbol{\Lambda}_{k}^{-1}
$$</p>
<ul>
<li>The vectors $\boldsymbol{u}_{2}, \ldots, \boldsymbol{u}_{k+1}$ are the directions of maximum variance of the graph embedding, with $\lambda_{2}^{-1} \geq \ldots \geq \lambda_{k+1}^{-1}$.</li>
</ul>
<h1 id="other-laplacian-matrices">Other Laplacian matrices</h1>
<ul>
<li>The normalized graph Laplacian (symmetric and semi-definite positive):</li>
</ul>
<p>$$
\mathbf{L}_{n}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}}=\mathbf{I}-\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}
$$</p>
<ul>
<li>The transition matrix (allows an analogy with Markov chains):</li>
</ul>
<p>$$
\mathbf{L}_{t}=\mathbf{D}^{-1} \mathbf{A}
$$</p>
<ul>
<li>The random-walk graph Laplacian:</li>
</ul>
<p>$$
\mathbf{L}_{r}=\mathbf{D}^{-1} \mathbf{L}=\mathbf{I}-\mathbf{L}_{t}
$$</p>
<ul>
<li>These matrices are similar:</li>
</ul>
<p>$$
\mathbf{L}_{r}=\mathbf{D}^{-\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} \mathbf{D}^{\frac{1}{2}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L}_{n} \mathbf{D}^{\frac{1}{2}}
$$</p>
<h1 id="eigenvalues-and-eigenvectors-of-mathrml_n-and-mathrml_r">Eigenvalues and eigenvectors of $\mathrm{L}_{n}$ and $\mathrm{L}_{r}$</h1>
<ul>
<li>$\mathbf{L}_{r} \boldsymbol{w}=\lambda \boldsymbol{w} \Longleftrightarrow \mathbf{L} \boldsymbol{w}=\lambda \mathbf{D} \boldsymbol{w}$, hence:</li>
</ul>
<p>$$
\mathbf{L}_{r}: \quad \lambda_{1}=0 ; \quad \boldsymbol{w}_{1}=\mathbf{1}
$$</p>
<ul>
<li>$\mathbf{L}_{n} \boldsymbol{v}=\lambda \boldsymbol{v}$. By virtue of the similarity transformation between the two matrices:</li>
</ul>
<p>$$
\mathbf{L}_{n}: \quad \lambda_{1}=0 \quad \boldsymbol{v}_{1}=\mathbf{D}^{\frac{1}{2}} \mathbf{1}
$$</p>
<ul>
<li>More generally, the two matrices have the same eigenvalues:</li>
</ul>
<p>$$
0=\lambda_{1} \leq \ldots \leq \lambda_{i} \ldots \leq \lambda_{n}
$$</p>
<ul>
<li>Their eigenvectors are related by:</li>
</ul>
<p>$$
\boldsymbol{v}_{i}=\mathbf{D}^{\frac{1}{2}} \boldsymbol{w}_{i}, \forall i=1 \ldots n
$$</p>
<h1 id="spectral-embedding-using-the-random-walk-laplacian-mathbfl_r">Spectral embedding using the random-walk Laplacian $\mathbf{L}_{r}$</h1>
<ul>
<li>The $n \times k$ matrix contains the first $k$ eigenvectors of $\mathbf{L}_{r}$ :</li>
</ul>
<p>$$
\mathbf{W}=\left[\begin{array}{lll}
\boldsymbol{w}_{2} &amp; \ldots &amp; \boldsymbol{w}_{k+1}
\end{array}\right]
$$</p>
<ul>
<li>It is straightforward to obtain the following expressions, where $\boldsymbol{d}$ and $\mathbf{D}$ are the degree-vector and the degree-matrix:</li>
</ul>
<p>$$
\begin{gathered}
\boldsymbol{w}_{i}^{\top} \boldsymbol{d}=0, \forall i, 2 \leq i \leq n \\
\mathbf{W}^{\top} \mathbf{D W}=\mathbf{I}_{k}
\end{gathered}
$$</p>
<ul>
<li>The isometric embedding using the random-walk Laplacian:</li>
</ul>
<p>$$
\mathbf{Y}=\mathbf{W}^{\top}=\left[\begin{array}{lll}
\boldsymbol{y}_{1} &amp; \ldots &amp; \boldsymbol{y}_{n}
\end{array}\right]
$$</p>
<h1 id="the-normalized-additive-laplacian">The normalized additive Laplacian</h1>
<ul>
<li>Some authors use the following matrix:</li>
</ul>
<p>$$
\mathbf{L}_{a}=\frac{1}{d_{\max }}\left(\mathbf{A}+d_{\max } \mathbf{I}-\mathbf{D}\right)
$$</p>
<ul>
<li>This matrix is closely related to L:</li>
</ul>
<p>$$
\mathbf{L}_{a}=\frac{1}{d_{\max }}\left(d_{\max } \mathbf{I}-\mathbf{L}\right)
$$</p>
<ul>
<li>and we have:</li>
</ul>
<p>$$
\mathbf{L}_{a} \boldsymbol{u}=\mu \boldsymbol{u} \Longleftrightarrow \mathbf{L} \boldsymbol{u}=\lambda \boldsymbol{u}, \mu=1-\frac{\lambda}{d_{\max }}
$$</p>
<h1 id="the-graph-partitioning-problem">The graph partitioning problem</h1>
<ul>
<li>The graph-cut problem: Partition the graph such that:</li>
</ul>
<p>(1) Edges between groups have very low weight, and</p>
<p>(2) Edges within a group have high weight.</p>
<p>$\operatorname{cut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} W\left(A_{i}, \bar{A}_{i}\right)$ with $W(A, B)=\sum_{i \in A, j \in B} w_{i j}$</p>
<ul>
<li>Ratio cut:</li>
</ul>
<p>$$
\operatorname{RatioCut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\left|A_{i}\right|}
$$</p>
<ul>
<li>Normalized cut:</li>
</ul>
<p>$$
\operatorname{NCut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\operatorname{vol}\left(A_{i}\right)}
$$</p>
<h1 id="what-is-spectral-clustering">What is spectral clustering?</h1>
<p>See my <a href="https://jhuow.fun/posts/2019-09-07-spectral-clustering/">Blog</a> of Spectral Clustering (in Chinese).</p>
<ul>
<li>
<p>Both ratio-cut and normalized-cut minimizations are NP-hard problems</p>
</li>
<li>
<p>Spectral clustering is a way to solve relaxed versions of these problems:</p>
</li>
</ul>
<p>(1) The smallest non-null eigenvectors of the unnormalized Laplacian approximate the RatioCut minimization criterion, and</p>
<p>(2) The smallest non-null eigenvectors of the random-walk Laplacian approximate the NCut criterion.</p>
<h1 id="spectral-clustering-using-the-random-walk-laplacian">Spectral clustering using the random-walk Laplacian</h1>
<ul>
<li>
<p>For details see (von Luxburg &lsquo;07)</p>
</li>
<li>
<p>Input: Laplacian $\mathbf{L}_{r}$ and the number $k$ of clusters to compute.</p>
</li>
<li>
<p>Output: Cluster $C_{1}, \ldots, C_{k}$.</p>
</li>
</ul>
<p>(3) Compute W formed with the first $k$ eigenvectors of the random-walk Laplacian.</p>
<p>(2) Determine the spectral embedding $\mathbf{Y}=\mathbf{W}^{\top}$</p>
<p>(3) Cluster the columns $\boldsymbol{y}_{j}, j=1, \ldots, n$ into $k$ clusters using the K-means algorithm.</p>
<h1 id="k-means-clustering">K-means clustering</h1>
<p>See Bishop'2006 (pages 424-428) for more details.</p>
<ul>
<li>
<p>What is a cluster: a group of points whose inter-point distance are small compared to distances to points outside the cluster.</p>
</li>
<li>
<p>Cluster centers: $\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k}$.</p>
</li>
<li>
<p>Goal: find an assignment of points to clusters as well as a set of vectors $\mu_{i}$.</p>
</li>
<li>
<p>Notations: For each point $\boldsymbol{y}_{j}$ there is a binary indicator variable $r_{j i} \in{0,1}$.</p>
</li>
<li>
<p>Objective: minimize the following distorsion measure:</p>
</li>
</ul>
<p>$$
J=\sum_{j=1}^{n} \sum_{i=1}^{k} r_{j i}\left|\left|\boldsymbol{y}_{j}-\boldsymbol{\mu}_{i}\right|\right|^{2}
$$</p>
<h1 id="the-k-means-algorithm">The K-means algorithm</h1>
<p>(1) Initialization: Choose initial values for $\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k}$.</p>
<p>(2) First step: Assign the $j$-th point to the closest cluster center:</p>
<p>$$
r_{j i}= \begin{cases}1 &amp; \text { if } i=\arg \min_{l}\left|\left|\boldsymbol{y}_{j}-\mu_{l}\right|\right|^{2} \\ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>(3) Second Step: Minimize $J$ to estimate the cluster centers:</p>
<p>$$
\boldsymbol{\mu}_{i}=\frac{\sum_{j=1}^{n} r_{j i} \boldsymbol{y}_{j}}{\sum_{j=1}^{n} r_{j i}}
$$</p>
<p>(4) Convergence: Repeat until no more change in the assignments.</p>
<h1 id="the-laplacian-and-the-rayleigh-quotient">The Laplacian and the Rayleigh quotient</h1>
<p>As usual, for a graph $G=(V, E)$, let $A$ be its adjacency matrix and $D$ be the diagonal matrix with $D(v, v)=d_{v}$. Then, the random walk on $G$ will be taken according to the transition matrix $P=D^{-1} A$. We also define the stationary distribution $\pi$ with $\pi(x)=d_{x} / \operatorname{vol} G$.</p>
<p>Our discussion of random walks on $G$ left off with the result</p>
<p>$$
\left|\left|f P^{t}-\pi\right|\right|_{2} \leq \max_{i \neq 0}\left|\rho_{i}\right|^{t} \frac{\max_{x} \sqrt{d_{x}}}{\min_{y} \sqrt{d_{y}}}
$$</p>
<p>where $f$ is a probability distribution (i.e. $f \geq 0$ and $\sum_{x} f(x)=1$ ) and $1=\rho_{0} \geq \rho_{1} \geq \ldots \geq \rho_{n-1}$ are the eigenvalues of $P$. This inequality implies that convergence to the stationary distribution $\pi$ will follow if $\max \left\{\left|\rho_{1}\right|,\left|\rho_{n-1}\right|\right\}&lt;1$.</p>
<p>The transition probability matrix $P$ is similar to the matrix $M=D^{\frac{1}{2}} P D^{-\frac{1}{2}}$, so $P$ and $M$ have the same eigenvalues. We previously introduced the Laplacian of the graph as $\mathcal{L}=I-M$, so it has eigenvalues $0=\lambda_{0} \leq \lambda_{1} \leq \ldots \leq \lambda_{n-1}$ (where $\lambda_{i}=1-\rho_{i}$ ).</p>
<p>The main tool we&rsquo;ll use to study the spectrum of $\mathcal{L}$ is the Rayleigh quotient $R(f)$ of $\mathcal{L}$, defined (for our purposes) as</p>
<p>$$
R(f)=\frac{f L f^{*}}{f D f^{*}}
$$</p>
<p>where $L=D-A$ is the combinatorial Laplacian. This is the same as the usual sense of the Rayleigh quotient $g \mathcal{L} g^{*} / g g^{*}$ with the subtitution $f=g D^{-\frac{1}{2}}$. Following this equivalence, if the $\phi_{i}$ are the eigenvectors of $\mathcal{L}$, we&rsquo;ll call the $\psi_{i}=\phi_{i} D^{-\frac{1}{2}}$ the harmonic eigenvectors of $\mathcal{L}$.</p>
<p>Employing the Rayleigh quotient, we see that the eigenvalue $\lambda_{1}$ can be written as</p>
<p>$$
\lambda_{1}=\inf_{\substack{f \\ \sum_{x} f(x) d_{x}=0}} R(f) .
$$</p>
<p>Since the eigenvector associated with $\lambda_{0}$ is $\phi_{0}=1 D^{\frac{1}{2}}$, the condition $\sum_{x} f(x) d_{x}=0$ is an orthogonality condition. Such variational characterizations can also be made for the other eigenvalues:</p>
<p>$$
\lambda_{n-1}=\sup _{f} R(f)
$$</p>
<p>and, in general,
$$
\lambda_{i}=\sup_{h_{0}, h_{1}, \ldots, h_{i-1}}
\inf_{\substack{f: \\ \sum_{x} f(x) h_{j}(x) d_{x}=0 \\ \forall j \in{0, \ldots, i-1}}}  R(f)
$$
The following characterization of the Rayleigh quotient (demonstrated last time) will be useful later:
$$
R(f)=\frac{\sum_{x \sim y}(f(x)-f(y))^{2}}{\sum_{x} f^{2}(x) d_{x}} .
$$</p>
<p>To this point, we have done a lot of linear algebra. We are not here to teach linear algebra; we are here to take linear algebra one step further to understand what is happening in the graph.</p>
<h1 id="the-cheeger-ratio-and-the-cheeger-constant">The Cheeger Ratio and The Cheeger Constant</h1>
<p>In many areas of mathematics the questions of &ldquo;best&rdquo; comes into play. What is the best bound for a given constant? What is the best way of row reducing a certain matrix? In this section, we will describe a way to make the &ldquo;best possible cut&rdquo; of a graph $G=(V, E)$, where a cut may be either an edge-cut or a vertex-cut, and this cut will split $G$ into two disconnected pieces.</p>
<p>We would like a way to measure the quality of a cut that is made to $G$. That is, would it be better to cut 4 edges which cause us to lose 20 vertices, or is it better to cut 10 edges which would result in the removal of 120 vetices?</p>
<p>Suppose we are given a graph $G=(V, E)$ and a subset $S \subseteq V$. We wish to define the folling two sets:</p>
<p>$$
\partial S={{u, v} \mid u \in S, v \notin S}
$$</p>
<p>and</p>
<p>$$
\delta S={v \notin S \mid v \sim u, u \in S} .
$$</p>
<p>Definition 1 For any vertex set $W$, the volume of $W$ is given by</p>
<p>$$
\operatorname{vol}(W)=\sum_{x W} d_{x},
$$</p>
<p>where $d_{x}$ is the degree of $\mathrm{x}$ in $W$.</p>
<p>Definition 2 The Cheerger Ratio for a vertex set $S$ is</p>
<p>$$
h(S)=\frac{|\partial S|}{\min {\operatorname{vol}(S), \operatorname{vol}(\bar{S})}},
$$</p>
<p>where $\bar{S}=V-S$.</p>
<p>It is first worth noting that in terms of this defintion of the Cheeger ratio, we are gauging the quality of our cut by taking a measure of what&rsquo;s been cut off of $G$. There are other forms of the Cheeger ratio as well. For example, we can use $|\delta S|$ instead of $|\partial S|,|S|($ or $\bar{S})$ instead of $\operatorname{vol}(S)$ (or $\operatorname{vol}(\bar{S}))$, or $|S||\bar{S}|$ instead of $\min {\operatorname{vol}(S), \operatorname{vol}(\bar{S})}$.</p>
<p>Definition 3 For any graph $G=(V, E)$, the Cheeger Constant of $G$ is given by</p>
<p>$$
h_{G}=\min_S h(S) .
$$</p>
<p>Now, if we consider the case where $\operatorname{vol}(S) \leq \frac{1}{2} \operatorname{vol}(G)$, then we can see that</p>
<p>$$
|\partial S| \geq h_{G}(\operatorname{vol}(S)) .
$$</p>
<h1 id="the-cheeger-inequality">The Cheeger Inequality</h1>
<p>Given a graph $G$, we can define $\lambda_{1}$ to be the first nontrivial eignevalue of the Laplacian, $\mathcal{L}$, of $G$.</p>
<p>For any graph $G$,</p>
<p>$$
2 h_{G} \geq \lambda_{1} \geq \frac{h_{G}^{2}}{2}
$$</p>
<h1 id="reference">Reference</h1>
<p><a href="https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf">https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf</a></p>
<p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
<p><a href="https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf">https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf</a></p>
<p><a href="https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf">https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Topology-Imbalance Learning for Semi-Supervised Node Classification》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2022-04-02-tinl/</link>
      <pubDate>Sat, 02 Apr 2022 10:34:01 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2022-04-02-tinl/</guid>
      <description>NeurIPS2021 &amp;#34;Topology-Imbalance Learning for Semi-Supervised Node Classification&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2110.04099v1">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>类别不均衡（Class Imbalance）是真实场景中非常常见的问题。一般在我们提及类别不均衡时，默认指的是<strong>数量不均衡</strong>：即不同类中训练样本数量的不一致带来的模型于不同类别学习能力的差异，由此引起的一个严重问题是模型的决策边界会主要由数量多的类来决定 。</p>
<p>但是在图结构中，<strong>不同类别的训练样本不仅有在数量上的差异，也有在位置结构上的差异</strong>.这就使得图上的类别不均衡问题有了一个独特的来源：<strong>拓扑不均衡</strong>。这个工作最主要的动机就是研究拓扑不均衡的特点，危害以及解决方法，希望能够引起社区对拓扑不均衡问题的重视。</p>
<p><img loading="lazy" src="/posts/2022-04-02-tnil/1.png" alt="1"  />
</p>
<p>本文提出Topology-Imbalance Node Representation Learning （TINL）, 主要关注拓扑不平衡导致的决策边界漂移。所谓拓扑不平衡值得是， labeled nodes的位置如果位于拓扑中的决策边界，那么会传播错误的影响。 如上图所示，颜色和色调分别表示节点从labeled node接收到的influence类型和强度，节点R1位于两类节点的拓扑边界，第一张图可以看出，两个$\mathbf{x}$节点面临influence conflict问题，两个$\mathbf{Y}$节点由于远离R2，面临影响力不足的问题。也就是，如果决策便捷有labeled node（如R1）, 那么他的影响力很容易传播给另一个类的边界unlabeled节点，导致影响力冲突，从而分类错误。 而冲突较小的labeled node更可能位于类的拓扑中心（如R2）,顾增加其权重，是的它在训练过程中发挥更大作用。</p>
<h1 id="understanding-topology-imbalance-via-label-propagation">Understanding Topology Imbalance via Label Propagation</h1>
<p><img loading="lazy" src="/posts/2022-04-02-tnil/2.png" alt="1"  />
</p>
<p>Label Propagation中，labels从labeled node延边传播， 看做label从labeled node开始的随机游走过程。LP最终收敛状态可以认为每个节点的soft-labels:
$$
\boldsymbol{Y}=\alpha\left(\boldsymbol{I}-(1-\alpha) \boldsymbol{A}^{\prime}\right)^{-1} \boldsymbol{Y}^{0}
$$
其中$\boldsymbol{A}^{\prime}=\boldsymbol{D}^{-\frac{1}{2}} A D^{-\frac{1}{2}}$，其实就是PageRank的极限分布， $\boldsymbol{Y}^{0}$为每个节点的初始one-hot label。 第$i$个节点的预测结果为$\boldsymbol{q}_{i}=\arg \max _{j} \boldsymbol{Y}_{i j}$，每个节点的预测向量反映了每个节点主要受哪个类的影响。图(a)反映了GCN与LP的预测一致性，所以LP的节点影响力边界可以作为GNN的决策边界。理想状态下，labeled node的影响力边界应与真实类边界一致，例如红色的labeled node 在LP下所传播的影响力范围，应与所有红色node的范围一致。但是如图(b)所示，蓝色的labeled node如果较多位于真实类边界，这些位于边界的节点也会传播影响力，从而导致位于边界的真是红色节点被预测为蓝色，预测边界向红色类偏移。</p>
<h1 id="measuring-topology-imbalance-by-influence-conflict">Measuring Topology Imbalance by Influence Conflict</h1>
<p>可以看出，位于决策边界的labeled node 会不可避免的将影响力传播到其他类节点，因此需要衡量labeled node与其所属类的相对拓扑位置（位于类边缘还是中心）。由于Homophily， 位于类边缘的节点也具有和其邻居相似的性质，因此利用邻域特征差别来判断labeled node是否位于边缘是不可靠的。因此本文利用整个图中的节点影响力冲突，提出基于冲突检测的拓扑相对位置Conflict Detection-based Topology Relative Location metric (Totoro).</p>
<p>Personalized PageRank矩阵定义为：
$$
\boldsymbol{P} = \alpha\left(\boldsymbol{I}-(1-\alpha) \boldsymbol{A}^{\prime}\right)^{-1}
$$
$\boldsymbol{P}_{ij} = \boldsymbol{P}(j \to^\infty i)$， 可以用来反映拓扑中节点$i$对节点$j$的影响力（随机游走越有可能到达的两个节点，在拓扑中的越能相互影响）。</p>
<p><strong>Node influence conflict denotes topological position.</strong> $\boldsymbol{P}$可以看做每个节点向外施加影响力的分布。 如果一个labeled node $v$ 在周围子图中受到了来自其他类中的labeled node的异质影响，而$v$本身也具有较大的影响力，那么可以认为$v$具有较大影响力冲突，他更可能位于所在类的拓扑边界。</p>
<p>基于上述假设，本文将 从节点$v$开始在图上随机游走时， 节点$v$与其他类的labeled nodes之间的影响力冲突的期望作为节点$v$与其所在类的类中心的接近程度的度量。labeled node $v$ 的Totoro值定义如下：
$$
\boldsymbol{T}_{v}=\mathbb{E}_{x \sim \boldsymbol{P}_{v, :}}\left[\sum_{j \in[1, k], j \neq \boldsymbol{y}_{v}} \frac{1}{\left|\mathcal{C}_{j}\right|} \sum_{i \in \mathcal{C}_{j}} \boldsymbol{P}_{i, x}\right]
$$
其中， $\mathbb{E}_{x \sim \boldsymbol{P}_{v, :}}$： $x$节点受$v$的影响程度，$\sum_{j \in[1, k], j \neq \boldsymbol{y}_{v}}$表示其他所有类（不包括$v$所在的类）。 $\frac{1}{\left|\mathcal{C}_{j}\right|} \sum_{i \in \mathcal{C}_{j}} \boldsymbol{P}_{i, x}$表示类$\mathcal{C}_{j}$中的labeled node对$x$的平均影响。 $\boldsymbol{T}_{v}$越大，表示labeled node $v$对$x$的影响力很大，而且其他类的labeled node 对$x$的影响也很大，那么可以认为$v$越接近类边界。</p>
<p>整个数据集的conflict可以表示为所有labeled node 的Totoro value之和：$\sum_{v \in \mathcal{L}} \boldsymbol{T}_{v}$</p>
<h1 id="node-re-weighting">Node Re-weighting</h1>
<h2 id="preliminary">Preliminary</h2>
<p>余弦退火：
$$
\eta_{t}=\eta_{\min }^{i}+\frac{1}{2}\left(\eta_{\max }^{i}-\eta_{\min }^{i}\right)\left(1+\cos \left(\frac{T_{\text {cur }}}{T_{i}} \pi\right)\right)
$$
$\eta_{\min }$: 最小学习率</p>
<p>$\eta_{\max }$: 最大学习率</p>
<p>$T_{\text {cur }}$: 当前执行多少个epoch</p>
<p>$i$: 第$i$次迭代</p>
<p><img loading="lazy" src="/posts/2022-04-02-tnil/3.png#center" alt="1"  />
</p>
<h2 id="renode">ReNode</h2>
<p>本文提出模型无关的训练权重re-weight 机制：<strong>ReNode</strong>.</p>
<p>本文基于余弦退货算法来为训练节点（labeled nodes）加权：
$$
\boldsymbol{w}_{v}=w_{\min }+\frac{1}{2}\left(w_{\max }-w_{\min }\right)\left(1+\cos \left(\frac{\operatorname{Rank}\left(\boldsymbol{T}_{v}\right)}{|\mathcal{L}|} \pi\right)\right), \quad v \in \mathcal{L}
$$
上式中$\boldsymbol{T}_{v}$越大(越接近决策边界)，在所有labeled node $v \in \mathcal{L}$的排名越高，$\operatorname{Rank}\left(\boldsymbol{T}_{v}\right)$越大，$\boldsymbol{w}_{v}$越小，越接近$w_{\min }$。</p>
<p>最终，对于一个quantity-balanced，topology-imbalanced (class labeled node 数量是平衡的，但拓扑不平衡) node classification task, the training loss $L_T$ is computed by:
$$
L_{T}=-\frac{1}{|\mathcal{L}|} \sum_{v \in \mathcal{L}} \boldsymbol{w}_{v} \sum_{c=1}^{k} \boldsymbol{y}_{v}^{* c} \log \boldsymbol{g}_{v}^{c}, \quad \boldsymbol{g}=\operatorname{softmax}(\mathcal{F}(\boldsymbol{X}, \boldsymbol{A}, \boldsymbol{\theta}))
$$
其中$\mathcal{F}$是任意GNN encoder,$g_i$为GNN对第$i$个节点的output。$\boldsymbol{y}_{v}^{* c} \log \boldsymbol{g}_{v}^{c}$为cross-entropy。 对于每个training labeled node，计算它的CE loss时，用这个节点的权重为loss加权，说明越靠近决策边界的节点，他的损失权重尽可能小，意味着model倾向于把它当做一个unlabeled node，它的损失对于总损失贡献较小。</p>
<h2 id="renode-to-jointly-handle-tinl-and-qinl">ReNode to Jointly Handle TINL and QINL</h2>
<p>若要同时处理数量不平衡且拓扑不平衡问题， loss定义如下：
$$
L_{Q}=-\frac{1}{|\mathcal{L}|} \sum_{v \in \mathcal{L}} \boldsymbol{w}_{v} \frac{|\overline{\mathcal{C}}|}{\left|\mathcal{C}_{j}\right|} \sum_{c=1}^{k} \boldsymbol{y}_{v}^{* c} \log \boldsymbol{g}_{v}^{c}
$$
与$L_T$的不同就是多了对类的加权，若labeled node所在的类 training node较少，那么增加权重。同时，接近拓扑边界的节点权重降低。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Expectation Maximization</title>
      <link>https://JhuoW.github.io/posts/em-algo/</link>
      <pubDate>Fri, 01 Apr 2022 19:45:48 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/em-algo/</guid>
      <description>EM算法笔记</description>
      <content:encoded><![CDATA[<h1 id="最大似然估计mle">最大似然估计MLE</h1>
<p>数据 $X = \{x_1, \cdots x_N\}$, 模型参数为$\theta$，Likelihood 定义为 $P(X | \theta)$：当参数为$\theta$时，观测到给定数据$X$的概率。
$$
P(X|\theta) = L(\theta | X) = P_\theta(X) \tag{1}
$$
最大似然估计 （Maximum Likelihood Estimation, MLE）:
$$
\theta_{\mathrm{MLE}} = \arg \max_\theta  P(X|\theta)  \tag{2}
$$</p>
<blockquote>
<p>最大似然估计：给定一组样本$X$，模型的参数$\theta$是研究对象。若能找到参数$\theta_{\mathrm{MLE}}$，使得样本发生的可能性最大，则此估计值$\theta_{\mathrm{MLE}}$为参数$\theta$的最大似然估计。</p></blockquote>
<p>举例来说，如果模型是单个Gaussian Distribution下，参数为Gaussian Distribution的参数（均值$\mu$, 标准差$\Sigma$， $\theta = {\mu, \Sigma}$）.
给定一组数据$X$, 要计算$X$来自什么样的Gaussian，即：$P(\cdot | \theta) = f_\theta(\cdot) = \mathcal{N}(\cdot | \mu,\Sigma)$是一个Gaussian Distribution函数，目标为：
$$
\theta_{\mathrm{MLE}} = \mu^\star, \Sigma^\star = \arg \max_{\mu,\Sigma} \sum^N_{i = 1} \log \mathcal{N}(x_i|\mu,\Sigma)  \tag{3}
$$
即MLE的目标是找到最佳的高斯分布，是的从该分布中采样出数据$X$的概率最高。</p>
<p>如果只需要用一个Gaussian来拟合$X$的分布的话，这个Gaussian可以很容易用求导的方式获得$\theta_{\mathrm{MLE}}$的解析解：  对$\mu$求导：$\frac{\partial P(X|\mu,\Sigma)}{\partial \mu}$；对$\Sigma$求导：$\frac{\partial P(X|\mu,\Sigma)}{\partial \Sigma}$，令导数为0，即可求得最佳的$\mu$，$\Sigma$，使得对应的高斯分布符合数据$X = {x_1, \cdots x_N}$的分布。</p>
<p>但是，要用更复杂的模型（更多参数）来更准确的拟合$X$的分布，例如Gaussian Mixture Model，即多个Gaussian的组合，其模型参数为：
$$
\theta = \{\underbrace{\mu_1, \cdots,\mu_K}_{\text{每个Gaussian的 mean参数}}, \underbrace{\Sigma_1,\cdots, \Sigma_K}_{\text{每个Gaussian的 std参数}}, \underbrace{\alpha_1, \cdots, \alpha_{K-1}}_{\text{每个Gaussian的权重}} \}  \tag{4}
$$
假设是一个$K$个Gaussian的Gaussian Mixture Model，那么$\sum^K_{k = 1}\alpha_k = 1$。</p>
<p>给定数据$X = \{x_1, \cdots x_N\}$，若要用$K$维Gaussian Mixture Model来拟合该数据，就要优化所有$K$个Gaussian的均值参数，标准差参数，和权重参数，使得混合高斯分布采样出$X$的概率最大，即：
$$
\begin{aligned}
\theta_{\mathrm{MLE}} &amp;= \mu_1^\star,\cdots  \mu_K^\star,\Sigma_1^\star, \cdots, \Sigma_K^\star, \alpha^\star_1,\cdots,\alpha^\star_{K-1} \\
&amp;=\underset{\theta}{\arg\max} \sum^N_{i = 1} \log \sum^K_{k=1} \alpha_k \mathcal{N}(x_i|\mu_k,\Sigma_k)
\end{aligned} \tag{5}
$$
如果要得到上式的解析解，要对$\mu_1,\cdots,\mu_K, \Sigma_1, \cdots,\Sigma_K, \alpha_1, \cdots, \alpha_{K-1}$求导，再令导数为0来求解，这非常困难，由此引出EM算法。</p>
<h1 id="期望最大算法">期望最大算法</h1>
<p>求解MLE问题时，在最大化log-likelihood:
$$
\theta_{\mathrm{MLE}} = \arg \max_\theta \log  P(X|\theta)  \tag{6}
$$
难以直接对$\theta$求导来得到解析解时（如高斯混合模型情况），可以使用EM算法来迭代求解：
$$
\theta^{(t+1)}=\underset{\theta}{\arg \max} \int_z \log P(X,z | \theta) \cdot P(z|X,\theta^{(t)}) dz   \tag{7}
$$
$X$为观测数据， $z$为latent variables（隐变量），隐变量必须不会影响$X$的边缘分布,即 $P(X) = \int_z P(X|z) P(z) dz$。</p>
<p>而公式(7)中
$$
\begin{aligned}
&amp;\int_z \underbrace{\log P(X,z | \theta)}_{\text{每个z对应的值}} \cdot \underbrace{P(z|X,\theta^{(t)})}_{\text{z的分布}} dz\\
=&amp;\mathbb{E}_{P(z|X,\theta^{(t)})} \left[\log P(X,z | \theta)\right]
\end{aligned} \tag{8}
$$
所以，期望最大化算法求参数$\theta$的迭代公式可改写为:
$$
\theta^{(t+1)}=\arg \max_\theta \mathbb{E}_{P(z|X,\theta^{(t)})} \left[\log P(X,z | \theta)\right]     \quad \text{期望最大化}  \tag{9}
$$</p>
<p>其中$P(z|X,\theta^{(t)})$为后验分布posterior。</p>
<h1 id="em算法收敛性证明">EM算法收敛性证明</h1>
<p>因为EM算法通过迭代的方式优化模型参数$\theta$，使得对数似然$\log P(X|\theta)$最大。通过公式(7)，可以保证在$\theta^{(t+1)}$参数下的模型比$\theta^{(t)}$参数下的模型更拟合数据分布。通过公式(7)迭代更新参数$\theta^{(t)} \to \theta^{(t+1)}$，可以使得$\log P(X|\theta)$变大。收敛性即证明：
$$
\log P(X|\theta^{(t)}) \leq \log P(X|\theta^{(t+1)})  \tag{10}
$$
证明.
$$
\begin{aligned}
&amp;\because P(X,z) = P(z|X) P(X)\quad \text{always true}, \text{then}\quad P(X) = \frac{P(X,z)}{P(z|X)} \\
&amp;\therefore P(X|\theta) = \frac{P(X,z|\theta)}{P(z|X,\theta)} \\
&amp;\therefore \log P(X|\theta) = \log P(X,z|\theta) - \log P(z|X,\theta)
\end{aligned} \tag{11}
$$
上式左右两边对分布$P(z|X,\theta^{(t)})$求期望：
$$
\mathbb{E}_{z \sim P(z|X,\theta^{(t)})} \underbrace{\left[\log P(X|\theta)\right]}_{\text{与z无关}} = \mathbb{E}_{z \sim P(z|X,\theta^{(t)})} \left[\log P(X,z|\theta) - \log P(z|X,\theta)\right]  \tag{12}
$$
上式左边$=\log P(X|\theta)$，右边：
$$
\begin{aligned}
&amp;\mathbb{E}_{P(z|X,\theta^{(t)})} \left[\log P(X,z|\theta) - \log P(z|X,\theta)\right] \\
=&amp; \underbrace{\int_{z} P(z|X,\theta^{(t)}) \log P(X,z|\theta) dz}_{Q(\theta,\theta^{(t)})} - \underbrace{\int_z P(z|X,\theta^{(t)}) \log P(z|X,\theta) dz}_{H(\theta,\theta^{(t)})}
\end{aligned} \tag{13}
$$
注意到$Q(\theta,\theta^{(t)}) = \int_{z} P(z|X,\theta^{(t)}) \log P(X,z|\theta) dz$ 就是EM算法的迭代更新函数，即$\theta^{(t+1)} = \arg \max_\theta Q(\theta,\theta^{(t)})$。结合公式(12)和公式(13)：
$$
\log P(X|\theta) = Q(\theta,\theta^{(t)}) - H(\theta,\theta^{(t)}) \tag{14}
$$</p>
<blockquote>
<p>因此log-likelihood under $\theta^{(t)}$ and $\theta^{(t+1)}$：
$$
\begin{aligned}
\log P(X|\theta^{(t+1)}) &amp;= Q(\theta^{(t+1)},\theta^{(t)}) - H(\theta^{(t+1)},\theta^{(t)}) \\
\log P(X|\theta^{(t)}) &amp;= Q(\theta^{(t)},\theta^{(t)}) - H(\theta^{(t)},\theta^{(t)})
\end{aligned}  \tag{15}
$$</p></blockquote>
<p>首先，根据EM的迭代求解公式，$\theta^{(t+1)}$由
$$
\theta^{(t+1)} = \arg \max_\theta Q(\theta,\theta^{(t)})  \tag{16}
$$
得到，所以$Q(\theta^{(t+1)},\theta^{(t)}) \geq Q(\theta,\theta^{(t)})$一定成立。所以下式成立：
$$
Q(\theta^{(t+1)},\theta^{(t)}) \geq Q(\theta^{(t)},\theta^{(t)})  \tag{17}
$$
对于$H(\theta,\theta^{(t+1)})$，首先介绍<strong>Jensen Inequality</strong>:</p>
<blockquote>
<p>Jensen Inequality：</p>
<p>If $g(x)$ is a convex function on $R_X$, and $\mathbb{E}[g(x)]$ and $g(\mathbb{E}[X])$ are finite, then $\mathbb{E}[g(x)] \geq g(\mathbb{E}[X])$。</p>
<p><img loading="lazy" src="/posts/EM_Algo/Convex_b.png" alt=""  />
</p>
<p>显然$\log$是concave，所以$\mathbb{E}[\log(\cdot)]\leq \log(\mathbb{E}[\cdot])$。同理$-\log$是convex，所以$\mathbb{E}[-\log(\cdot)]\geq -\log(\mathbb{E}[\cdot])$。</p></blockquote>
<p>下面，计算$H(\theta^{(t)},\theta^{(t)})-H(\theta,\theta^{(t)})$：
$$
\begin{aligned}
&amp;H(\theta,\theta^{(t)}) = \int_z P(z|X,\theta^{(t)}) \log P(z|X,\theta) dz \\
&amp;H(\theta^{(t)},\theta^{(t)})-H(\theta,\theta^{(t)})\\
=&amp;\int_z P(z|X,\theta^{(t)}) \log P(z|X,\theta^{(t)}) dz - \int_z P(z|X,\theta^{(t)}) \log P(z|X,\theta) dz \\
=&amp; \underbrace{\int_z P(z|X,\theta^{(t)}) \log \frac{P(z|X,\theta^{(t)})}{P(z|X,\theta)} dz}_{\mathrm{KL}(P(z|X,\theta^{(t)})|P(z|X,\theta))}\\
=&amp; -\int_z P(z|X,\theta^{(t)}) \log \frac{P(z|X,\theta)}{P(z|X,\theta^{(t)})} dz \\
=&amp; \mathbb{E}_{P(z|X,\theta^{(t)})} \left[-\log \frac{P(z|X,\theta)}{P(z|X,\theta^{(t)})}\right]\\
\geq &amp; -\log \mathbb{E}_{P(z|X,\theta^{(t)})} \left[\frac{P(z|X,\theta)}{P(z|X,\theta^{(t)})}\right] \\
=&amp; -\log \int_z \frac{P(z|X,\theta)}{P(z|X,\theta^{(t)})} \cdot P(z|X,\theta^{(t)}) dz \\
=&amp; -\log \int_z P(z|X,\theta) dz \\
=&amp; - \log 1  \\
=&amp; 0
\end{aligned}\tag{18}
$$
因此，下式成立：
$$
\begin{aligned}
&amp;\therefore H(\theta^{(t)},\theta^{(t)})\geq H(\theta,\theta^{(t)})  \\
&amp;\therefore H(\theta^{(t+1)},\theta^{(t)}) \leq  H(\theta^{(t)},\theta^{(t)}) \\
&amp;\because  Q(\theta^{(t+1)},\theta^{(t)}) \geq Q(\theta^{(t)},\theta^{(t)})\\
&amp;\therefore Q(\theta^{(t+1)},\theta^{(t)}) - H(\theta^{(t+1)},\theta^{(t)}) \geq Q(\theta^{(t)},\theta^{(t)}) - H(\theta^{(t)},\theta^{(t)}) \\
&amp;\therefore \log P(X|\theta^{(t+1)}) \geq \log P(X|\theta^{(t)})
\end{aligned} \tag{19}
$$
所以通过EM算法的迭代得到新的$\theta^{(t+1)}$增大likelihood，使得模型更加拟合数据。</p>
<h1 id="em算法公式推导">EM算法公式推导</h1>
<p>EM算法Maximize Likelihood Estimation迭代公式：
$$
\begin{aligned}
\theta^{(t+1)}&amp;=\underset{\theta}{\arg \max} \int_z \log P(X,z | \theta) \cdot P(z|X,\theta^{(t)}) dz\\
&amp;=\arg \max_\theta \mathbb{E}_{P(z|X,\theta^{(t)})} \left[\log P(X,z | \theta)\right]<br>
\end{aligned} \tag{20}
$$</p>
<ul>
<li>E-Step： $\mathbb{E}_{P(z|X,\theta^{(t)})} \left[\log P(X,z | \theta)\right]$</li>
<li>M-Step：$\arg \max_\theta \mathbb{E}_{P(z|X,\theta^{(t)})} \left[\log P(X,z | \theta)\right]$</li>
</ul>
<p>其中，$X$是观测数据，$z$是隐变量，$(X,z)$为完整数据，$\theta$为待优化模型参数，$P(\cdot|X)$为后验。</p>
<p>上一节通过收敛性证明，验证了上式每次迭代都朝着最大化log-likelihood的方向。本节推导EM的迭代公式。</p>
<p>公式(11)中得到：
$$
\log P(X|\theta) = \log P(X,z|\theta) - \log P(z|X,\theta) \tag{21}
$$
引入一个关于隐变量$z$的分布$q(z)$，可以定义为任意关于$z$的非0分布。上式可以改写为：
$$
\log P(X|\theta) = \log \frac{P(X,z|\theta)}{q(z)} - \log\frac{ P(z|X,\theta)}{q(z)} \tag{22}
$$
左右两边对$q(z)$求期望：
$$
\text{左边} = \mathbb{E}_{q(z)} \log P(X|\theta) = \int_z q(z) \log P(X|\theta) dz = \log P(X|\theta)  \underbrace{\int_z q(z) dz}_{=1} =  \log P(X|\theta) \tag{23}
$$</p>
<p>$$
\begin{aligned}
\text{右边}&amp;=\mathbb{E}_{q(z)}  \left[\log \frac{P(X,z|\theta)}{q(z)} - \log\frac{ P(z|X,\theta)}{q(z)}\right]    \\
&amp;= \underbrace{\int_z q(z) \log \frac{P(X,z|\theta)}{q(z)} dz}_{ELBO=\text{Evidence Lower Bound}} \underbrace{- \int_z q(z)  \log\frac{ P(z|X,\theta)}{q(z)} dz}_{\mathrm{KL}(q(z)||P(z|X,\theta))}
\end{aligned} \tag{24}
$$</p>
<p>所以
$$
\log P(X|\theta) = ELBO + \mathrm{KL}(q(z)||P(z|X,\theta)) \tag{25}
$$
其中$P(z|X,\theta)$为后验（posterior）。而$\mathrm{KL}(q(z)||P(z|X,\theta)) \geq 0$, 当分布$q(z) = P(z|X,\theta)$时，等号成立。所以
$$
\log P(X|\theta) \geq ELBO = \int_z q(z) \log \frac{P(X,z|\theta)}{q(z)} dz \tag{26}
$$
因此，最大化log-likelihood $\log P(X|\theta)$问题可以转化为最大化$\log P(X|\theta)$的下界ELBO，即：
$$
\hat{\theta} = \arg \max_\theta \log P(X|\theta) \Longleftrightarrow \hat{\theta} =  \arg \max_\theta ELBO \tag{27}
$$</p>
<p>$$
\begin{aligned}
\hat{\theta} &amp;=  \arg \max_\theta ELBO \\
&amp; = \arg \max_\theta \int_z q(z) \log \frac{P(X,z|\theta)}{q(z)} dz  \quad \text{令关于}z\text{的分布}q(z) = P(z|X,\theta^{(t)})\\
&amp;= \arg \max_\theta \int_z P(z|X,\theta^{(t)}) \left[\log P(X,z|\theta) -  \underbrace{P(z|X,\theta^{(t)})}_{\text{与}\theta \text{无关，去掉不影响结果}}\right] dz \\
&amp;= \arg \max_\theta \int_z P(z|X,\theta^{(t)}) \log P(X,z|\theta) dz\\
&amp;= \text{公式(7)}
\end{aligned} \tag{28}
$$</p>
<p>我把本文整理成了<a href="/posts/EM_Algo/EM.pdf">PDF</a></p>
<h1 id="参考">参考</h1>
<p><a href="https://youtube.com/playlist?list=PLOxMGJ_8X74bhcPbpiX642NIfPlkpD1BC">https://youtube.com/playlist?list=PLOxMGJ_8X74bhcPbpiX642NIfPlkpD1BC</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/78311644">https://zhuanlan.zhihu.com/p/78311644</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2020 《Robust Graph Representation Learning via Neural Sparsification》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/neuralsparse/</link>
      <pubDate>Fri, 01 Apr 2022 10:55:44 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/neuralsparse/</guid>
      <description>ICML2020 &amp;#34;Robust Graph Representation Learning via Neural Sparsification&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://proceedings.mlr.press/v119/zheng20d.html">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>GNN的neighborhood aggregation中会引入邻域中任务无关的邻居，所以要移除图中有大量任务无关的边，顾本文提出NeuralSparse来解决该问题。</p>
<p>在构造数据集时，两个节点连接的动机可能与拿到这张图后要进行的下游任务无关，例如下游任务是节点分类，那么和这个任务相关的连接应是同类节点间产生边。但是构造图数据集是的两个节点连接的动机可能是特征相似的节点（不一定同类）。</p>
<p>下图给出一个例子，Blue 和 Red节点分别采样自两个独立的二维Gaussian distribution, 图1(a)显示两类节点的原始features有大量重合。对于每个节点，随机抽取10个其他节点作为他的邻居，这样生成的图（图1(b) )中的边和node label没有任何关系，即所有边都与节点分类任务无关。在这个图上做GCN得到(图1(b)下方)的node embedding，可以看到GCN学到的node embedding在任务无关的边上，无法区分两类节点。 图1(c)是DropEdge学到的node embedding，图1(d)为本文的NeuralSparse学到的node embedding。</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/1.png#center" alt=""  />
</p>
<p><strong>Present work</strong>：根据监督信号来选择任务相关的边。 由sparsification network 和GNN两个模块组成. sparsification network旨在参数化稀疏过程，即在给定预算下， 为每个节点选择任务相关的一节邻居。在训练阶段，优化稀疏策略，即训练一个可以将图稀疏化的网络。在测试阶段，将测试图输入网络，得到一个稀疏化的图。</p>
<h1 id="neuralsparse">NeuralSparse</h1>
<h2 id="theoretical-justification">Theoretical justification</h2>
<p>首先从统计学习角度建模问题。将预测任务定义为$P(Y|G)$, 其中$Y$是预测目标，$G$是输入图。本文利用稀疏化子图来移除任务无关的边, 问题形式化为:
$$
P(Y \mid G) \approx \sum_{g \in \mathbb{S}_{G}} P(Y \mid g) P(g \mid G)
$$
$g$是一个稀疏化子图，$\mathbb{S}_{G}$为$G$的稀疏化子图集合。 $P(Y \mid g)$ 为给定一个稀疏化子图$g$，用$g$过GNN后预测为$Y$的概率。
$$
\sum_{g \in \mathbb{S}_{G}} P(Y \mid g) P(g \mid G) \approx \sum_{g \in \mathbb{S}_{G}} Q_{\theta}(Y \mid g) Q_{\phi}(g \mid G)
$$
用函数来近似(计算) 分布， 即给定$g$ 预测为$Y$的概率$ P(Y \mid g)$定义为一个参数为$\theta$的函数$Q_{\theta}(Y \mid g)$, 从$G$中获得子图$g$的概率$P(g \mid G)$定义为一个参数为$\phi$的函数$Q_{\phi}(g \mid G)$。</p>
<p>$Q_{\phi}(g \mid G)$表示输入$G$, 生成一个子图分布，从分布中采样得到子图$g$的概率， 为了使得分布中采样这个过程可微，本文采用reparameterization tricks,使得：
$$
\sum_{g \in \mathbb{S}_{G}} Q_{\theta}(Y \mid g) Q_{\phi}(g \mid G) \propto \sum_{g^{\prime} \sim Q_{\phi}(g \mid G)} Q_{\theta}\left(Y \mid g^{\prime}\right)
$$
$g^{\prime} \sim Q_{\phi}(g \mid G)$表示给定图$G$，生成一个子图分布（每种子图的采样概率）。$\sum_{g^{\prime} \sim Q_{\phi}(g \mid G)} Q_{\theta}\left(Y \mid g^{\prime}\right)$: 表示从子图分布中采样的子图来预测label $Y$的概率。</p>
<p><strong>Goal</strong>: 1. 找到合适的$Q_{\phi}(g \mid G)$， 使得它生成的分布可以采样到最佳的稀疏化子图 ， 即通过优化$\phi$使得$Q_{\phi}(g \mid G)$生成的子图分布中采样到最佳子图的概率是最大的。 2. 找到合适的$Q_{\theta}(Y \mid g)$表示优化GNN，使得采样出的$g$可以最好的预测label。</p>
<h2 id="architecture">Architecture</h2>
<p>包含两个模块： sparsification network 和GNNs.</p>
<h3 id="sparsification-network">Sparsification Network</h3>
<p>目的为输入图生成稀疏化子图，即为每个节点的边生成一个分布，表示边被采样的概率，然后为节点采样边，从而实现采样的系数子图。首先定义所有候选子图。</p>
<p><strong>k-neighbor subgraphs</strong>: 给定输入图$G$，一个$k$-neighbor subgraph和图$G$有相同的节点集，每个节点可以从他的邻居中选择不多于$k$条边。</p>
<p>理由： 超参数$k$可以用来调整任务相关的图数据量。如果$k$是低估的，那么GNN处理的任务相关数据不足，如果$k$被高估，那么下游GNN会拟合更多无关数据。</p>
<p><strong>Sampling k-neighbor subgraphs</strong>：给定$k$和一个图$G=(V, E, \mathbf{A})$, 以节点$u$为例，令$\mathbb{N}_u$为$u$的一阶邻居。</p>
<ol>
<li>$v \sim f_{\phi}\left(V(u), V\left(\mathbb{N}_{u}\right), \mathbf{A}(u)\right)$, 其中，$f_{\phi}(\cdot)$是一个函数，输入为节点$u$的节点属性$V(u)$，节点$u$的邻居属性$V\left(\mathbb{N}_{u}\right)$, 和$u$的边属性$\mathbf{A}(u)$。输出为$u$的邻居分布，$v$从该邻居分布中采样。 比如当前$u$有3个节点，$f_\phi$生成这三个节点的采样分布[0.1, 0.3, 0.6], 那么从这个分布中随机采样一个节点$v$作为$u$的重构邻居。</li>
<li>采样出的节点$v$作为$u$的重构邻居，即$E(u,v)$作为边保留下来。</li>
<li>重复上述过程$k$次，得到$u$的$k$个重构邻居。</li>
</ol>
<p>注意，上述采样过程为不放回过程（sampling without replacement），即邻居只能被选择一次， $f_{\phi}(\cdot)$对所有节点共享，即一个$f_{\phi}(\cdot)$，每个节点都输入它来获得邻居采样分布。</p>
<p><strong>Making samples differentiable</strong> 为了使样本的采样过程可微，本文采用基于Gumbel-Softmax的NN来实现采样函数$f_{\phi}(\cdot)$。</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/2.png#center" alt=""  />
</p>
<p>Gumbel-Softmax [1,2] 是一种reparameterization trick，用于以可微的方式生成离散样本。参数$\tau$越小，生成的连续向量越sharp，越接近one-hot。</p>
<p>以节点$u$为例，$f_\phi(\cdot)$如下：</p>
<ol>
<li>
<p>$\forall v \in \mathbb{N}_u$：
$$
z_{u, v}=\operatorname{MLP}_{\phi}(V(u), V(v), \mathbf{A}(u, v))
$$</p>
</li>
<li>
<p>$\forall v \in \mathbb{N}_u$，使用softmax来计算边被采样的概率：
$$
\pi_{u, v}=\frac{\exp \left(z_{u, v}\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(z_{u, w}\right)}
$$</p>
</li>
<li>
<p>使用Gumbel-Softmax来生成可微样本：
$$
x_{u, v}=\frac{\exp \left(\left(\log \left(\pi_{u, v}\right)+\epsilon_{v}\right) / \tau\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(\left(\log \left(\pi_{u, w}\right)+\epsilon_{w}\right) / \tau\right)}
$$</p>
</li>
</ol>
<p>其中， $x_{u, v}$是一个scalar，$\epsilon_{v}=-\log (-\log (s))$，$s$从$\mathrm{Uniform}(0,1)$中采样， $\tau$是一个temperature超参数，$\tau$越小，分布$x_u$越接近one-hot。</p>
<h3 id="algorithm">Algorithm</h3>
<p>算法如下：</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/3.png#center" alt=""  />
</p>
<p>对所有节点$u \in \mathbb{V}$逐个稀疏化： 先遍历$u$的每个邻居$v$, 对于每个$v$ 通过公式$z_{u, v}=\operatorname{MLP}_{\phi}(V(u), V(v), \mathbf{A}(u, v))$ 计算它对于$u$的分数， 然后将$u$的所有邻居$v$的分数用softmax变成概率。</p>
<p>为$u$做$k$次采样， 每次采样过程如下： 每次采样遍历$u$的所有邻居$v$，根据$x_{u, v}=\frac{\exp \left(\left(\log \left(\pi_{u, v}\right)+\epsilon_{v}\right) / \tau\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(\left(\log \left(\pi_{u, w}\right)+\epsilon_{w}\right) / \tau\right)}$计算$u$到每个邻居的$x_{u,v}$, 每次迭代产生一个向量$\left[x_{u, v}\right]$,用来表示采样出来的边，经过$k$次迭代，产生$k$个表示边的向量，$\tau$越小，每个向量越接近one-hot。 每个向量$\left[x_{u, v}\right]_{v \in \mathbb{N}_u}$表示$u$的一个采样邻居，每个$u$有$k$个这样的邻居表示向量，那么网络中的所有边$\mathbb{H}$就有$|\mathbb{V}|k$个这样的向量，每个向量表示要保留的一条边，得到稀疏化子图，反向传播时，先更新GNN参数，然后直接对$f_\phi$的参数求梯度, 如上图所示。</p>
<h1 id="reference">Reference</h1>
<p>[1] Jang, E., Gu, S., and Poole, B. Categorical reparameteriza- tion with gumbel-softmax. In ICLR, 2017.</p>
<p>[2] Maddison, C. J., Mnih, A., and Teh, Y. W. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Neo-GNNs:Neighborhood Overlap-aware Graph Neural Networks for Link Prediction》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/neo-gnns/</link>
      <pubDate>Wed, 30 Mar 2022 13:51:57 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/neo-gnns/</guid>
      <description>NeurIPS2021 &amp;#34;Neo-GNNs:Neighborhood Overlap-aware Graph Neural Networks for Link Prediction&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=Ic9vRN3VpZ">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>由于GNNs过度强调平滑的节点特征而不是图结构，使得在Link Prediction任务上的表现甚至弱于heuristic方法。平滑邻居难以反映邻域结构的相关性以及其他拓扑特征。 <strong>Structural information, (e.g., overlapped neighborhoods, degrees, and shortest path), is crucial for link prediction whereas GNNs heavily rely on smoothed node features rather than graph structure</strong>。</p>
<blockquote>
<p><strong>Link prediction heuristics:</strong>  基于预定义的假设的链路预测。举几个例子[1]：</p></blockquote>
<ol>
<li>Common Neighbors (CN)： 公共邻居较多的节点存在边（heuristic），则需要计算节点对间的公共邻居。</li>
<li>Preferential Attachment (PA): 一个节点当前的连接越多，那么它越有可能接受到新的连接（heuristic）,这需要统计每个节点的度, i.e., $P A(x, y)=|N(x)| *|N(y)|$</li>
<li>Katz Index heuristic: $\sum^{\infty}_{\ell=1} \beta^{\ell}|walks(x,y)=\ell|$ 表示从$x$到$y$的所有路径数， $0&lt;\beta&lt;1$， 表示越长的路径权重越低。 katz Index作为Link prediction heuristic假设来作为边是否存在的预测。</li>
</ol>
<p>本文提出了Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs)来从邻接矩阵中学习结构信息，并且估计重叠多跳邻域用于link prediction。</p>
<h1 id="preliminaries">Preliminaries</h1>
<h2 id="gnns-for-link-prediction">GNNs for Link Prediction</h2>
<p>$$
\hat{y}_{i j}=\sigma\left(s\left(h_{i}^{(L)}, h_{j}^{(L)}\right)\right)
$$</p>
<p>其中$s(\cdot, \cdot)$ 是一个相似度计算函数 e.g., inner product or MLP. $h_{i}^{(L)}$为 $v_i$的 node embedding.</p>
<h2 id="neighborhood-overlap-based-heuristic-methods">Neighborhood Overlap-based Heuristic Methods</h2>
<p>就是上面提到的CN heuristic。Common Neighbors 通过count节点的公共邻居来衡量两个节点之间的链路存在分数$\mathrm{link}(u,v)$：
$$
S_{C N}(u, v)=|\mathcal{N}(u) \cap \mathcal{N}(v)|=\sum_{k \in \mathcal{N}(u) \cap \mathcal{N}(v)} 1
$$
CN的缺点在于不能衡量公共节点的权重。</p>
<p>Resource Allocation (RA) 认为度叫小的节点因更加重要， 所以用度的倒数来加权公共节点：
$$
S_{R A}(u, v)=\sum_{k \in \mathcal{N}(u) \cap \mathcal{N}(v)} \frac{1}{d_{k}}
$$</p>
<p>Adamic-Adar：通过使用节点 $u$ 和$v$之间的共同邻居度的倒数对数，与 RA 相比，Adamic-Adar 对更高度的惩罚相对减少：
$$
S_{A A}(u, v)=\sum_{k \in \mathcal{N}(u) \cap \mathcal{N}(v)} \frac{1}{\log d_{k}}
$$
上述基于公共邻居的方法存在两个局限，1. 需要手动设计邻居结构特征，比如CN的公共邻居结构特征为1， RA的结构特征为$\frac{1}{d}$, AA 的邻居结构特征为$\frac{1}{\log d}$。 2. 忽略了node features</p>
<p>本文提出的Neo-GNN从邻接矩阵中学习结构特征，并且结合了node feature信息来做Link prediction。</p>
<h1 id="model-neo-gnns">Model: Neo-GNNs</h1>
<p>定义structural feature generator $\mathcal{F}_{\theta}$:
$$
x_{i}^{\text {struct }}=\mathcal{F}_{\theta}\left(A_{i}\right)=f_{\theta_{n o d e}}\left(\sum_{j \in \mathcal{N}_{i}} f_{\theta_{e d g e}}\left(A_{i j}\right)\right)
$$
输入节点$i$的邻居$A_i$，提取自邻接矩阵$A$, Neo-GNNs 只是用$A$作为输入来获得节点的结构特征。 其中，$f_{\theta_{e d g e}}(A_{ij})$生成节点$i$的局部边特征，然后聚合起来用$f_{\theta_{n o d e}}$生成节点$i$的总体结构特征$x_{i}^{\text {struct }}$， 作为节点$i$的structural feature，表示反映了节点$i$的局部结构。其中$f_{\theta_{n o d e}}$和$f_{\theta_{e d g e}}$是两个MLP。 也可以把上面的$A$替换成$A$的幂的组合，那就是$k$跳以内邻域的结构特征。</p>
<p>得到了节点的邻居结构特征$x_{i}^{\text {struct }}$后， 要用<strong>重叠邻居的结构特征</strong>来计算两个节点的相似度分数。 传统的GNN无法计算重叠邻域的结构特征的原因有两个：1. normalized adjacency matrix: 归一化邻接矩阵阻止了GNN计数邻居数量（我的理解是因为Norm adj上的元素为小数）2. 远低于节点数的hidden representation维度$d \ll N$：低维度的节点表示向量使得在neighborhood aggregration后 节点邻域特征难以区分。</p>
<p><img loading="lazy" src="/posts/2022-03-30-NeoGNN/frameworks.png#center" alt=""  />
</p>
<p>本文提出了邻域重叠感知的聚合模式。 注意，上面的节点邻域特征是一个scale, 即$x_{i}^{\text {struct }} \in \mathbb{R}^1$, 整个图的节点邻域结构特征可以表示为$X^{struct} \in \mathbb{R}^{N \times N}$, 为一个对角阵，对角线元素为每个节点的邻域<strong>结构</strong>特征，如Figure 1所示。也就是$X^{struct}$的每一行为一个节点的局部结构特征表示向量，作为这个节点的结构特征。</p>
<p>那么$Z = AX^{struct}$就可以为节点聚合结构特征。 因为$X^{struct}_i$表示节点$v_i$的structural feature (neighborhood structural), 所以$Z_i$表示节点$i$的1-st neighborhood structural feature, 所以$z_{i}^{T} z_{j}=\sum_{k \in \mathcal{N}(i) \cap \mathcal{N}(j)}\left(x_{k}^{s t r u c t}\right)^{2}$表可以表示节点$i$和节点$j$的重叠邻域。</p>
<p><strong>注意</strong> $X_{i}^{\text {struct }}$表示节点$i$自身的结构特征。 而$Z_i$表示节点$i$的邻居的结构特征聚合, 所以$z_{i}^{T} z_{j}$表示节点$i$邻居的结构特征和节点$j$邻居的结构特征的相似度。 $x_i^T x_j$表示节点$i$自身的结构特征和节点$j$自身的结构特征的相似度。</p>
<p>进一步，考虑多跳邻居：
$$
Z=g_{\Phi}\left(\sum_{l=1}^{L} \beta^{l-1} A^{l} X^{\text {struct }}\right)
$$</p>
<p>$A^lX^{struct}$的第$i$行表示节点$i$ 的$l$跳邻居特征。 $Z_i$表示节点$i$在$L$跳以内的邻居结构特征总和。</p>
<p>除了考虑结构特征来预测链接外，还应考虑node features，直接用GNN：
$$
H=\operatorname{GNN}\left(X, \tilde{A}_{G N N} ; W\right)
$$
最终节点$i$和节点$j$的相似度分数表示为：
$$
\left.\hat{y}_{i j}=\alpha \cdot \sigma\left(z_{i}^{T} z_{j}\right)+(1-\alpha) \cdot \sigma\left(s\left(h_{i}, h_{j}\right)\right)\right)
$$
即为邻域结构相似度 与 特征相似度 的加权平均。</p>
<p>最终损失函数要求 3种相似度衡量标准（基于邻域结构，基于节点feature, 两者加权平均）都可以你和真实的相似度，即：
$$
\mathcal{L}=\sum_{(i, j) \in D}\left(\lambda_{1} B C E\left(\hat{y}_{i j}, y_{i j}\right)+\lambda_{2} B C E\left(\sigma\left(z_{i}^{T} z_{j}\right), y_{i j}\right)+\lambda_{3} B C E\left(\sigma\left(s\left(h_{i}, h_{j}\right)\right), y_{i j}\right)\right)
$$
其中$BCE(\cdot, \cdot)$为 binary cross entropy loss。</p>
<h1 id="reference">Reference</h1>
<p>[1] Link Prediction Based on Graph Neural Networks. NeurIPS 2018.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Representing Long-Range Context for Graph Neural Networks with Global Attention》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/graphtrans/</link>
      <pubDate>Wed, 30 Mar 2022 10:37:41 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/graphtrans/</guid>
      <description>NeurIPS2021 &amp;#34;Representing Long-Range Context for Graph Neural Networks with Global Attention&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/pdf/2201.08821.pdf">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>加深GNN层数来增加感受野会导致优化不稳定性，比如梯度消失和oversmoothing。 因此本文采用Transformer-based self-attention来学习成对节点间的长距离关系，并且提出一种新型的readout池化机制来学习global graph embedding。即在一个GNN模块后接一个置换不变（permutation-invariant）Transformer, GNN模块捕获local信息，Transformer捕获global信息。</p>
<p>GNN作为一种专门的架构医学系节点<strong>直接邻域结构的局部表示</strong>， 而Transformer作为全局推理模块以位置无关的方式计算所有成对节点的交互。作者认为，一个没有positional encoding的Transformer是置换不变的，因此很适合图。</p>
<h1 id="motivation">Motivation</h1>
<p>强关系Inductive bias(我的理解是Homophily假设) 鼓励学习局部短距离的关联性。 而对于长距离相关性，结构化较低的模块（不需要过于考虑图的结构信息）更受欢迎。</p>
<p><strong>GraphTrans leaves learning long-range dependencies to Transformer</strong>, 通过Transformer来学习图中所有节点对的依赖关系而不是只关注局部邻居。</p>
<p>下图中展示了一个子图的attention map。一共有17个节点，横坐标表示目标节点，纵坐标表示源节点，第$i$行第$j$列表示节点$i$在Transformer中聚合$j$的attention权重。第18行为一个特殊的$&lt;CLS&gt;$token 作为图的readout embedding。结合本文的SOTA效果，表面在学习长距离依赖时不考虑图结构先验（spatial priors）对Graph summarization（graph-level representation）是有必要的</p>
<p><img loading="lazy" src="/posts/2022-03-30-GraphTrans/pic2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="model">Model</h1>
<p><img loading="lazy" src="/posts/2022-03-30-GraphTrans/pic1.png#center" alt="你想输入的替代文字"  />
</p>
<h2 id="gnn-module">GNN Module</h2>
<p>一个通用的GNN模块：
$$
\boldsymbol{h}_{v}^{\ell}=f_{\ell}\left(\boldsymbol{h}_{v}^{\ell-1},\left\{\boldsymbol{h}_{u}^{\ell-1} \mid u \in \mathcal{N}(v)\right\}\right), \quad \ell=1, \ldots, L_{\mathrm{GNN}}
$$</p>
<h2 id="transformer-module">Transformer Module</h2>
<p>通过上面的GNN模块，我们可以得到每个节点的embedding $h_{v}^{L_{\mathrm{GNN}}}$, 将所有节点作为Transformer的Input。传统Transformer中输入先计算Self-attention（当前输入的$Q$向量和所有节点的$K$向量做内积得到输入节点和其他节点的att值，再用这个att值来为当前输入节点加权聚合所有节点的$V$向量），聚合后再和自身相加做residual,然后在做Layer Norm, 即对节点$i$的表示做Layer Norm 为$x_i^\prime = \frac{x_i-m}{\sigma}$, 其中$m$为$x_i$的均值， $\sigma$为$x_i$的标准差。</p>
<p>这里的Transformer不同的是， 先对所有节点做一次MLP，然后直接计算Layer Norm:
$$
\overline{\boldsymbol{h}}_{v}^{0}=\operatorname{LayerNorm}\left(\boldsymbol{W}^{\text {Proj }} \boldsymbol{h}_{v}^{L_{\mathrm{GNN}}}\right)
$$
其中$\boldsymbol{W}^{\text {Proj }} \in \mathbb{R}^{d_{\mathrm{TF}} \times d_{L_{\mathrm{GNN}}}}$， 把GNN的输出维度转为TF的输入维度$d_{\mathrm{TF}}$。将所有节点的GNN node embeddings作为Transformer的输入（无positional encoding）。 每个节点的$Q$, $K$和$V$向量分别用$\boldsymbol{W}_{\ell}^{Q}, \boldsymbol{W}_{\ell}^{K}, \boldsymbol{W}_{\ell}^{V} \in \mathbb{R}^{d_{\mathrm{TF}} / n_{\text {head }} \times d_{\mathrm{TF}} / n_{\text {head }}}$计算， 对于第$\ell$层 Transformer,  节点$v$ 的$Q$向量$Q_v = \boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}$和节点$u$的$K$向量$K_u = \boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}$做内积，得到两个节点之间的attention。然后用$\alpha_{v, u}^{\ell}$来为节点$v$聚合其他所有节点的$V$向量$V_u = \boldsymbol{W}_{\ell}^{V} \overline{\boldsymbol{h}}_{u}^{\ell-1}$, 如下所示:
$$
a_{v, u}^{\ell}=\left(\boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}\right)^{\top}\left(\boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}\right) / \sqrt{d_{\mathrm{TF}}} \tag{1}
$$</p>
<p>$$
\alpha_{v, u}^{\ell}=\operatorname{softmax}_{u \in \mathcal{V}}\left(a_{v, u}^{\ell}\right) \tag{2}
$$</p>
<p>$$
\overline{\boldsymbol{h}}_{v}^{\prime \ell}=\sum_{w \in \mathcal{V}} \alpha_{v, u}^{\ell} \boldsymbol{W}_{\ell}^{V} \overline{\boldsymbol{h}}_{u}^{\ell-1} \tag{3}
$$</p>
<h2 id="cls--embedding-as-a-gnn-readout-method">&lt;CLS&gt;  embedding as a GNN “readout” method</h2>
<p>Graph Pooling 部分旨在基于node embedding，得到整个图的一个global embedding. 大多数pooling方法为简单的mean,sum, 或者构造一个virtual node连接到所有节点并参与训练，这个virtual node聚合所有节点的信息作为global embedding。</p>
<p>本文提出special-token readout module。具体来说，对Transformer的输入$[\overline{\boldsymbol{h}}_{v}^{0}]_{v\in V}$, where $\overline{\boldsymbol{h}}_{v}^{0} \in \mathcal{R}^{d_{TF}}$我们添加一个额外的可学习embedding （可以被认为是一个额外virtual node）$\bar{h}_{\langle\mathrm{CLS}\rangle} \in \mathbb{R}^{d_{\mathrm{TF}}}$, 这样 Transformer 的输入就变为$[\overline{\boldsymbol{h}}_{v}^{0}]_{v \in V} \cup \bar{h}_{\langle\mathrm{CLS}\rangle}$, 因为训练过程中$\overline{\boldsymbol{h}}_{v}^{0}$回聚合来自所有节点的信息，所以用它来作为readout embedding。 最终Transformer输出的token embedding $\overline{\boldsymbol{h}}_{&lt;\mathrm{CLS}&gt;}^{L_{\mathrm{TF}}}$ 再过一层MLP后用Softmax输出图的prediction:
$$
y=\operatorname{softmax}\left(\boldsymbol{W}^{\mathrm{out}} \overline{\boldsymbol{h}}_{&lt;\mathrm{CLS}&gt;}^{L_{\mathrm{TF}}}\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Not All Low-Pass Filters are Robust in Graph Convolutional Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gcn-lfr/</link>
      <pubDate>Tue, 29 Mar 2022 21:20:31 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gcn-lfr/</guid>
      <description>NeurIPS2021 &amp;#34;Not All Low-Pass Filters are Robust in Graph Convolutional Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/forum?id=bDdfxLQITtu">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>很多GNN易受图结构攻击的影响，本文首先证明了symmetric normalized laplacian的低频分量作为GCN的filter，在某个特征值区间内对于结构扰动更加robust。基于该理论提出GCN-LFR，通过一个辅助神经网络迁移低频分量的robustness。</p>
<p>Q: 对抗扰动边是否会对graph spectrum产生同等的影响？过去的研究显示来自结构攻击的扰动在图谱上表达了一种隐含的趋势。如下图所示，结构扰动后，小的特征值（低频）变化较小， 高频变化较大，即高频对扰动更加敏感。</p>
<p><img loading="lazy" src="/posts/2022-04-08-GraphLFR/1.png#center" alt=""  />
</p>
<p>本文证明了当normalized symmetric laplacian的特征值落于某个特定区间时，低频分量会更加robust。</p>
<h1 id="methodology">Methodology</h1>
<p>Poisoning Attack是指训练前扰动：</p>
<p><strong>Problem 1 （Poisoning Attack）:</strong> 给定一个扰动图 $\mathcal{G}^\prime$, 要对目标集合$\mathcal{T}$做对抗防御的目的是设计一个更加鲁棒的模型，使得模型在扰动图上训练后对$\mathcal{T}$中节点的预测结果和在原图上训练得到的预测结果相似：
$$
\min_{\boldsymbol{\Theta}^{r *}} \sum_{u \in \mathcal{T}}\left|\left|\mathcal{M}_{u}^{r}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}^{r *}\right)-\mathcal{M}_{u}\left(\boldsymbol{A}, \boldsymbol{X} ; \boldsymbol{\Theta}^{*}\right)\right|\right|
$$
即模型可以尽可能避免扰动对预测带来的影响。 其中$\mathcal{M}_{u}^{r}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}^{r *}\right)=\hat{\boldsymbol{y}}_{u}^{r}$模型在扰动图上训练过后对节点$u$的预测，$\mathcal{M}_{u}^{r}$是在扰动图上训练过后的模型，最佳参数为$\boldsymbol{\Theta}^{r *}$, $\mathcal{M}$是在原图上训练过后的模型。</p>
<p>观察$\hat{\boldsymbol{A}}$的特征值， 因为$\hat{\boldsymbol{A}} = I - L$, 所以$\hat{\boldsymbol{A}}$的大特征值对应于低频分量，小特征值对应于高频分量。 从图1可以看出$\hat{\boldsymbol{A}}$的大特征值对于结构扰动更加鲁棒, 因为扰动之后大特征值的变化较小。所以GCN-SVD只是用最低频的分量来做defense.</p>
<p>接下来本文证明了只有一条边被扰动的情况下，一定存在低频$\lambda_b$，比所有高频都robust, 鲁棒区间为：
$$
\max \left(0, \frac{d_{b}-d_{a}+c_{a} \lambda_{a}}{c_{b}}\right)&lt;\lambda_{b}&lt;\min \left(\frac{d_{b}+d_{a}-c_{a} \lambda_{a}}{c_{b}}, 1\right)
$$
即，当特征值落于这个区间中时，它一定比高频更加鲁棒。</p>
<p>对于Non-targeted Perturbation,  图中有多条边被扰动，那么特征值的鲁棒区间为：
$$
\max_{v \in \mathcal{P}_{u}, u \in \mathcal{T}}\left(0, \frac{d_{b u v}-d_{a u v}+c_{a u v} \lambda_{a}}{c_{b u v}}\right)&lt;\lambda_{b}&lt;\min_{v \in \mathcal{P}_{u}, u \in \mathcal{T}}\left(\frac{d_{b u v}+d_{a u v}-c_{a u v} \lambda_{a}}{c_{b u v}}, 1\right)
$$
在得到不同扰动情况下的鲁棒区间后，我们可以基于鲁棒区间来增强GCN的鲁棒性。</p>
<h2 id="gcn-lfr">GCN-LFR</h2>
<p>基于鲁棒区间，利用区间内频率分量可以设计更加鲁棒的GCN。</p>
<p><img loading="lazy" src="/posts/2022-04-08-GraphLFR/2.png#center" alt=""  />
</p>
<p>给定一个结构扰动图$\mathcal{G}^\prime$,邻接矩阵为$A^\prime$, GCN-LFR使用一个辅助正则化网络$\mathcal{M}_{\mathrm{LFR}}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}\right)$来计算robust 区间， 但是鲁棒区间时基于原图的，在只给定扰动图的情况下无法计算。 为了解决该问题， 本文用可学习的参数$\mathbf{F}$作为filters来学习鲁棒区间。其中，$\mathbf{F} = \left(\begin{array}{cccc}
f_{1} &amp; &amp; &amp; \\
&amp; f_{2} &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; f_{k}
\end{array}\right)$。</p>
<p>解释：假设$U$是$\hat{\boldsymbol{A}}$的特征向量，则 $\hat{\boldsymbol{A}}$可以分解为:</p>
<p>$$\hat{\boldsymbol{A}} = [u_1, \cdots, u_n] F \left[\begin{array}{l}
u_1^\top \\
\cdots \\
u_n^\top
\end{array}\right] = f_1 u_1 u_1^\top + \cdots + f_n u_n u_n^\top$$</p>
<p>其中$u_1$对应拉普拉斯矩阵的最小特征值的特征向量（$\hat{\boldsymbol{A}}$最大特征值的特征向量），所以$[u_1, \cdots, u_k]$表示最低频的$k$个特征向量。$f_i$是第$i$个频率滤波器的权重。 我们选择$k$个低频滤波器，并自适应的学习他们的权重，即 $U^\prime_{low} = [u_1, \cdots, u_k]$, $\mathbf{F}$是$k$个低频filter的参数， 所以图卷积层$\mathcal{M}_{\text {LFR }}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}, \boldsymbol{F}\right)$可以写作:
$$
\boldsymbol{H}^{(l+1)}=\sigma\left(\boldsymbol{U}_{\text {low }}^{\prime} \boldsymbol{F} \boldsymbol{U}_{\text {low }}^{\prime \top} \boldsymbol{H}^{\prime(l)}\Theta\right)
$$
另外，本文提出交替训练策略，对于在原本扰动图上的模型$\mathcal{M}_{\mathrm{GCN}}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}\right)$以及低频自适应学习的模型$\mathcal{M}_{\mathrm{LFR}}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}, \boldsymbol{F}\right)$, 这两个模型交替训练，损失函数分别用$\alpha$和$1-\alpha$加权：
$$
\mathcal{L}_{\text {total }}=(1-\alpha) \mathcal{L}_{\mathrm{GCN}}+\alpha \mathcal{L}_{\mathrm{LFR}}
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Blog, Tools and Survey</title>
      <link>https://JhuoW.github.io/posts/2019-04-28-paper-unscramble/</link>
      <pubDate>Tue, 29 Mar 2022 11:03:50 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-04-28-paper-unscramble/</guid>
      <description>&lt;p&gt;&lt;strong&gt;这篇笔记用于收藏别人的博客&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;tech-blog&#34;&gt;Tech Blog&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Blog&lt;/th&gt;
          &lt;th&gt;Author&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://michael-bronstein.medium.com/&#34;&gt;https://michael-bronstein.medium.com/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Michael Bronstein&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://geometricdeeplearning.com/&#34;&gt;https://geometricdeeplearning.com/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Michael Bronstein&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c&#34;&gt;https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Vitaly Kurin (Many Paper Notes)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html&#34;&gt;https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;UvA DL Notebooks&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://graph-neural-networks.github.io/index.html&#34;&gt;https://graph-neural-networks.github.io/index.html&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;GNN Books&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;http://prob140.org/sp17/textbook/&#34;&gt;http://prob140.org/sp17/textbook/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;http://prob140.org/&#34;&gt;Probability for Data Science&lt;/a&gt; class at UC Berkeley&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://graphreason.github.io/schedule.html&#34;&gt;https://graphreason.github.io/schedule.html&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://graphreason.github.io/index.html&#34;&gt;Learning and Reasoning with Graph-Structured Representations&lt;/a&gt; ICML 2019 Workshop&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://chuxuzhang.github.io/KDD21_Tutorial.html&#34;&gt;https://chuxuzhang.github.io/KDD21_Tutorial.html&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;KDD2021 Tutorial: Data Efficient Learning on Graphs&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;http://songcy.net/posts/&#34;&gt;http://songcy.net/posts/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Changyue Song (Kernel)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://www.cs.mcgill.ca/~wlh/grl_book/&#34;&gt;https://www.cs.mcgill.ca/~wlh/grl_book/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;William L. Hamilton&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://kexue.fm/&#34;&gt;https://kexue.fm/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;BoJone&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://danielegrattarola.github.io/blog/&#34;&gt;https://danielegrattarola.github.io/blog/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Daniele Grattarola (EPFL)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html&#34;&gt;https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Google AI Blog&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://zhiyuchen.com/blogs/&#34;&gt;https://zhiyuchen.com/blogs/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Zhiyu Chen&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://andreasloukas.blog/&#34;&gt;https://andreasloukas.blog/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Andreas Loukas (EPFL)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://irhum.pubpub.org/pub/gnn/release/4&#34;&gt;https://irhum.pubpub.org/pub/gnn/release/4&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Understanding Graph Neural Networks&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://lilianweng.github.io/&#34;&gt;https://lilianweng.github.io/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Lilian Weng&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://www.zhihu.com/column/marlin&#34;&gt;https://www.zhihu.com/column/marlin&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;深度学习与图网络&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/roboticcam/machine-learning-notes&#34;&gt;https://github.com/roboticcam/machine-learning-notes&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Yida Xu&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://www.dgl.ai/pages/index.html&#34;&gt;https://www.dgl.ai/pages/index.html&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;DGL&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://www.kexinhuang.com/tech-blog&#34;&gt;https://www.kexinhuang.com/tech-blog&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Kexin Huang&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8&#34;&gt;https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Rishabh Anand&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://saashanair.com/blog&#34;&gt;https://saashanair.com/blog&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Saasha Nair&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;http://www.huaxiaozhuan.com/&#34;&gt;http://www.huaxiaozhuan.com/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;华校专&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/dglai/WWW20-Hands-on-Tutorial&#34;&gt;https://github.com/dglai/WWW20-Hands-on-Tutorial&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;DGL&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://blog.csdn.net/CSDNTianJi/article/details/104195306&#34;&gt;https://blog.csdn.net/CSDNTianJi/article/details/104195306&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Meng Liu&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://www.chaitjo.com/post/&#34;&gt;https://www.chaitjo.com/post/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Chaitanya K. Joshi&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://scottfreitas.medium.com/&#34;&gt;https://scottfreitas.medium.com/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Scott Freitas&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://fabianfuchsml.github.io/&#34;&gt;https://fabianfuchsml.github.io/&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Fabian Fuchs&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://medium.com/@pantelis.elinas&#34;&gt;https://medium.com/@pantelis.elinas&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Pantelis Elinas&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/tianyicui/pack&#34;&gt;https://github.com/tianyicui/pack&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;背包9講&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://www.fenghz.xyz/&#34;&gt;https://www.fenghz.xyz/&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://sakigami-yang.me/2017/08/13/about-kernel-01/&#34;&gt;https://sakigami-yang.me/2017/08/13/about-kernel-01/&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;kernel&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://davidham3.github.io/blog&#34;&gt;https://davidham3.github.io/blog&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://fenghz.github.io/index.html&#34;&gt;https://fenghz.github.io/index.html&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://archwalker.github.io/&#34;&gt;https://archwalker.github.io/&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;awesome-awesomes&#34;&gt;Awesome-Awesomes&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Repo&lt;/th&gt;
          &lt;th&gt;Name&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/naganandy/graph-based-deep-learning-literature&#34;&gt;https://github.com/naganandy/graph-based-deep-learning-literature&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;links to conference publications in graph-based deep learning&lt;/strong&gt; (Very, Very, Very Important)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/SherylHYX/pytorch_geometric_signed_directed&#34;&gt;https://github.com/SherylHYX/pytorch_geometric_signed_directed&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Geometric Signed Directed is a signed/directed graph neural network extension library for PyTorch Geometric.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning&#34;&gt;https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Paper Lists for Fair Graph Learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/PromptPapers&#34;&gt;https://github.com/thunlp/PromptPapers&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Must-read papers on prompt-based tuning for pre-trained language models.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/zhao-tong/graph-data-augmentation-papers&#34;&gt;https://github.com/zhao-tong/graph-data-augmentation-papers&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A curated list of graph data augmentation papers.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Thinklab-SJTU/ThinkMatch&#34;&gt;https://github.com/Thinklab-SJTU/ThinkMatch&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Code &amp;amp; pretrained models of novel deep graph matching methods.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/FLHonker/Awesome-Knowledge-Distillation&#34;&gt;https://github.com/FLHonker/Awesome-Knowledge-Distillation&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/zlpure/awesome-graph-representation-learning&#34;&gt;https://github.com/zlpure/awesome-graph-representation-learning&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A curated list for awesome graph representation learning resources.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/basiralab/GNNs-in-Network-Neuroscience&#34;&gt;https://github.com/basiralab/GNNs-in-Network-Neuroscience&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A review of papers proposing novel GNN methods with application to brain connectivity published in 2017-2020.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/flyingdoog/awesome-graph-explainability-papers&#34;&gt;https://github.com/flyingdoog/awesome-graph-explainability-papers&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Papers about explainability of GNNs&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/yuanqidu/awesome-graph-generation&#34;&gt;https://github.com/yuanqidu/awesome-graph-generation&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A curated list of graph generation papers and resources.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/benedekrozemberczki/awesome-decision-tree-papers&#34;&gt;https://github.com/benedekrozemberczki/awesome-decision-tree-papers&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A collection of research papers on decision, classification and regression trees with implementations.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/AstraZeneca/awesome-explainable-graph-reasoning&#34;&gt;https://github.com/AstraZeneca/awesome-explainable-graph-reasoning&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A collection of research papers and software related to explainability in graph machine learning.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/LirongWu/awesome-graph-self-supervised-learning&#34;&gt;https://github.com/LirongWu/awesome-graph-self-supervised-learning&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Awesome Graph Self-Supervised Learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Chen-Cai-OSU/awesome-equivariant-network&#34;&gt;https://github.com/Chen-Cai-OSU/awesome-equivariant-network&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Paper list for equivariant neural network&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/mengliu1998/DL4DisassortativeGraphs&#34;&gt;https://github.com/mengliu1998/DL4DisassortativeGraphs&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Papers about developing DL methods on disassortative graphs&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers&#34;&gt;https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A curated list of graph reinforcement learning papers.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ChandlerBang/awesome-self-supervised-gnn&#34;&gt;https://github.com/ChandlerBang/awesome-self-supervised-gnn&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Papers about pretraining and self-supervised learning on Graph Neural Networks (GNN).&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks&#34;&gt;https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Paper Lists for Graph Neural Networks&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/jwzhanggy/IFMLab_GNN&#34;&gt;https://github.com/jwzhanggy/IFMLab_GNN&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Graph Neural Network Models from IFM Lab&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ChandlerBang/awesome-graph-attack-papers&#34;&gt;https://github.com/ChandlerBang/awesome-graph-attack-papers&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Adversarial attacks and defenses on Graph Neural Networks.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/safe-graph/graph-adversarial-learning-literature&#34;&gt;https://github.com/safe-graph/graph-adversarial-learning-literature&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A curated list of adversarial attacks and defenses papers on graph-structured data.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/benedekrozemberczki/awesome-graph-classification&#34;&gt;https://github.com/benedekrozemberczki/awesome-graph-classification&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A collection of important graph embedding, classification and representation learning papers with implementations.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers&#34;&gt;https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A curated list of gradient boosting research papers with implementations.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/benedekrozemberczki/awesome-community-detection&#34;&gt;https://github.com/benedekrozemberczki/awesome-community-detection&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A curated list of community detection research papers with implementations.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/giannifranchi/awesome-uncertainty-deeplearning&#34;&gt;https://github.com/giannifranchi/awesome-uncertainty-deeplearning&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;This repository contains a collection of surveys, datasets, papers, and codes, for predictive uncertainty estimation in deep learning models.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://sites.google.com/site/graphmatchingmethods/&#34;&gt;https://sites.google.com/site/graphmatchingmethods/&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Efficient Methods for Graph Matching and MAP Inference&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering&#34;&gt;https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Awesome Deep Graph Clustering is a collection of SOTA, novel deep graph clustering methods (papers, codes, and datasets).&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/jwwthu/GNN4Traffic&#34;&gt;https://github.com/jwwthu/GNN4Traffic&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;This is the repository for the collection of Graph Neural Network for Traffic Forecasting.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/zwt233/Awesome-Auto-GNNs&#34;&gt;https://github.com/zwt233/Awesome-Auto-GNNs&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A paper collection about automated graph learning&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/chaitjo/awesome-efficient-gnn&#34;&gt;https://github.com/chaitjo/awesome-efficient-gnn&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Efficient Graph Neural Networks - a curated list of papers and projects&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/Radical3-HeZhang/Awesome-Trustworthy-GNNs&#34;&gt;https://github.com/Radical3-HeZhang/Awesome-Trustworthy-GNNs&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Awesome Resources on Trustworthy Graph Neural Networks&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/EdisonLeeeee/Awesome-Masked-Autoencoders&#34;&gt;https://github.com/EdisonLeeeee/Awesome-Masked-Autoencoders&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A collection of literature after or concurrent with Masked Autoencoder (MAE) (Kaiming He el al.).&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/XiaoxiaoMa-MQ/Awesome-Deep-Graph-Anomaly-Detection&#34;&gt;https://github.com/XiaoxiaoMa-MQ/Awesome-Deep-Graph-Anomaly-Detection&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Awesome graph anomaly detection techniques built based on deep learning frameworks.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/mengliu1998/awesome-expressive-gnn&#34;&gt;https://github.com/mengliu1998/awesome-expressive-gnn&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A collection of papers studying/improving the expressiveness of graph neural networks (GNNs)&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;useful-repotools&#34;&gt;Useful Repo/Tools&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Name&lt;/th&gt;
          &lt;th&gt;Info&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;http://acronymify.com/&#34;&gt;http://acronymify.com/&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Model Name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://csacademy.com/app/graph_editor/&#34;&gt;https://csacademy.com/app/graph_editor/&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Graph Editor&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/guanyingc/python_plot_utils&#34;&gt;https://github.com/guanyingc/python_plot_utils&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;A simple code for plotting figure, colorbar, and cropping with python&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/guanyingc/latex_paper_writing_tips&#34;&gt;https://github.com/guanyingc/latex_paper_writing_tips&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;Tips for Writing a Research Paper using LaTeX&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/JhuoW/Pytorch_Program_Templete&#34;&gt;https://github.com/JhuoW/Pytorch_Program_Templete&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Pytorch Program Templete GNN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/graph4ai/graph4nlp&#34;&gt;https://github.com/graph4ai/graph4nlp&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Graph4nlp is the library for the easy use of Graph Neural Networks for NLP. Welcome to visit our DLG4NLP website (&lt;a href=&#34;https://dlg4nlp.github.io/index.html&#34;&gt;https://dlg4nlp.github.io/index.html&lt;/a&gt;) for various learning resources!&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/benedekrozemberczki/pytorch_geometric_temporal&#34;&gt;https://github.com/benedekrozemberczki/pytorch_geometric_temporal&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models (CIKM 2021)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/ysig/GraKeL&#34;&gt;https://github.com/ysig/GraKeL&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A scikit-learn compatible library for graph kernels&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/jajupmochi/graphkit-learn&#34;&gt;https://github.com/jajupmochi/graphkit-learn&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A python package for graph kernels, graph edit distances, and graph pre-image problem.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/pliang279/awesome-phd-advice&#34;&gt;https://github.com/pliang279/awesome-phd-advice&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Collection of advice for prospective and current PhD students&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/MLEveryday/100-Days-Of-ML-Code&#34;&gt;https://github.com/MLEveryday/100-Days-Of-ML-Code&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;100-Days-Of-ML-Code中文版&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/d2l-ai/d2l-zh&#34;&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;《动手学深度学习》&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR&#34;&gt;https://github.com/lukas-blecher/LaTeX-OCR&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;pix2tex: Using a ViT to convert images of equations into LaTeX code.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/OpenPrompt&#34;&gt;https://github.com/thunlp/OpenPrompt&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;An Open-Source Framework for Prompt-Learning.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/snap-stanford/GraphGym&#34;&gt;https://github.com/snap-stanford/GraphGym&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Platform for designing and evaluating Graph Neural Networks (GNN)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/pygod-team/pygod&#34;&gt;https://github.com/pygod-team/pygod&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A Python Library for Graph Outlier Detection (Anomaly Detection)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/MLNLP-World/Paper_Writing_Tips&#34;&gt;https://github.com/MLNLP-World/Paper_Writing_Tips&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;latex写作建议&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/dair-ai/ML-YouTube-Courses&#34;&gt;https://github.com/dair-ai/ML-YouTube-Courses&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;A place to discover the latest machine learning courses on YouTube.&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;miscellaneous&#34;&gt;Miscellaneous&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Name&lt;/th&gt;
          &lt;th&gt;Desc&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://github.com/The-Run-Philosophy-Organization/run&#34;&gt;https://github.com/The-Run-Philosophy-Organization/run&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;run学指南&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://10beasts.net/&#34;&gt;https://10beasts.net/&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;测评&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</description>
      <content:encoded><![CDATA[<p><strong>这篇笔记用于收藏别人的博客</strong></p>
<h1 id="tech-blog">Tech Blog</h1>
<table>
  <thead>
      <tr>
          <th>Blog</th>
          <th>Author</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://michael-bronstein.medium.com/">https://michael-bronstein.medium.com/</a></strong></td>
          <td>Michael Bronstein</td>
      </tr>
      <tr>
          <td><strong><a href="https://geometricdeeplearning.com/">https://geometricdeeplearning.com/</a></strong></td>
          <td>Michael Bronstein</td>
      </tr>
      <tr>
          <td><strong><a href="https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c">https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c</a></strong></td>
          <td>Vitaly Kurin (Many Paper Notes)</td>
      </tr>
      <tr>
          <td><strong><a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html</a></strong></td>
          <td>UvA DL Notebooks</td>
      </tr>
      <tr>
          <td><strong><a href="https://graph-neural-networks.github.io/index.html">https://graph-neural-networks.github.io/index.html</a></strong></td>
          <td>GNN Books</td>
      </tr>
      <tr>
          <td><strong><a href="http://prob140.org/sp17/textbook/">http://prob140.org/sp17/textbook/</a></strong></td>
          <td><a href="http://prob140.org/">Probability for Data Science</a> class at UC Berkeley</td>
      </tr>
      <tr>
          <td><strong><a href="https://graphreason.github.io/schedule.html">https://graphreason.github.io/schedule.html</a></strong></td>
          <td><a href="https://graphreason.github.io/index.html">Learning and Reasoning with Graph-Structured Representations</a> ICML 2019 Workshop</td>
      </tr>
      <tr>
          <td><strong><a href="https://chuxuzhang.github.io/KDD21_Tutorial.html">https://chuxuzhang.github.io/KDD21_Tutorial.html</a></strong></td>
          <td>KDD2021 Tutorial: Data Efficient Learning on Graphs</td>
      </tr>
      <tr>
          <td><strong><a href="http://songcy.net/posts/">http://songcy.net/posts/</a></strong></td>
          <td>Changyue Song (Kernel)</td>
      </tr>
      <tr>
          <td><strong><a href="https://www.cs.mcgill.ca/~wlh/grl_book/">https://www.cs.mcgill.ca/~wlh/grl_book/</a></strong></td>
          <td>William L. Hamilton</td>
      </tr>
      <tr>
          <td><strong><a href="https://kexue.fm/">https://kexue.fm/</a></strong></td>
          <td>BoJone</td>
      </tr>
      <tr>
          <td><strong><a href="https://danielegrattarola.github.io/blog/">https://danielegrattarola.github.io/blog/</a></strong></td>
          <td>Daniele Grattarola (EPFL)</td>
      </tr>
      <tr>
          <td><strong><a href="https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html">https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html</a></strong></td>
          <td>Google AI Blog</td>
      </tr>
      <tr>
          <td><strong><a href="https://zhiyuchen.com/blogs/">https://zhiyuchen.com/blogs/</a></strong></td>
          <td>Zhiyu Chen</td>
      </tr>
      <tr>
          <td><strong><a href="https://andreasloukas.blog/">https://andreasloukas.blog/</a></strong></td>
          <td>Andreas Loukas (EPFL)</td>
      </tr>
      <tr>
          <td><strong><a href="https://irhum.pubpub.org/pub/gnn/release/4">https://irhum.pubpub.org/pub/gnn/release/4</a></strong></td>
          <td>Understanding Graph Neural Networks</td>
      </tr>
      <tr>
          <td><strong><a href="https://lilianweng.github.io/">https://lilianweng.github.io/</a></strong></td>
          <td>Lilian Weng</td>
      </tr>
      <tr>
          <td><strong><a href="https://www.zhihu.com/column/marlin">https://www.zhihu.com/column/marlin</a></strong></td>
          <td>深度学习与图网络</td>
      </tr>
      <tr>
          <td><strong><a href="https://github.com/roboticcam/machine-learning-notes">https://github.com/roboticcam/machine-learning-notes</a></strong></td>
          <td>Yida Xu</td>
      </tr>
      <tr>
          <td><strong><a href="https://www.dgl.ai/pages/index.html">https://www.dgl.ai/pages/index.html</a></strong></td>
          <td>DGL</td>
      </tr>
      <tr>
          <td><strong><a href="https://www.kexinhuang.com/tech-blog">https://www.kexinhuang.com/tech-blog</a></strong></td>
          <td>Kexin Huang</td>
      </tr>
      <tr>
          <td><strong><a href="https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8">https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8</a></strong></td>
          <td>Rishabh Anand</td>
      </tr>
      <tr>
          <td><strong><a href="https://saashanair.com/blog">https://saashanair.com/blog</a></strong></td>
          <td>Saasha Nair</td>
      </tr>
      <tr>
          <td><strong><a href="http://www.huaxiaozhuan.com/">http://www.huaxiaozhuan.com/</a></strong></td>
          <td>华校专</td>
      </tr>
      <tr>
          <td><strong><a href="https://github.com/dglai/WWW20-Hands-on-Tutorial">https://github.com/dglai/WWW20-Hands-on-Tutorial</a></strong></td>
          <td>DGL</td>
      </tr>
      <tr>
          <td><strong><a href="https://blog.csdn.net/CSDNTianJi/article/details/104195306">https://blog.csdn.net/CSDNTianJi/article/details/104195306</a></strong></td>
          <td>Meng Liu</td>
      </tr>
      <tr>
          <td><strong><a href="https://www.chaitjo.com/post/">https://www.chaitjo.com/post/</a></strong></td>
          <td>Chaitanya K. Joshi</td>
      </tr>
      <tr>
          <td><strong><a href="https://scottfreitas.medium.com/">https://scottfreitas.medium.com/</a></strong></td>
          <td>Scott Freitas</td>
      </tr>
      <tr>
          <td><strong><a href="https://fabianfuchsml.github.io/">https://fabianfuchsml.github.io/</a></strong></td>
          <td>Fabian Fuchs</td>
      </tr>
      <tr>
          <td><strong><a href="https://medium.com/@pantelis.elinas">https://medium.com/@pantelis.elinas</a></strong></td>
          <td>Pantelis Elinas</td>
      </tr>
      <tr>
          <td><a href="https://github.com/tianyicui/pack">https://github.com/tianyicui/pack</a></td>
          <td>背包9講</td>
      </tr>
      <tr>
          <td><a href="https://www.fenghz.xyz/">https://www.fenghz.xyz/</a></td>
          <td></td>
      </tr>
      <tr>
          <td><a href="https://sakigami-yang.me/2017/08/13/about-kernel-01/">https://sakigami-yang.me/2017/08/13/about-kernel-01/</a></td>
          <td>kernel</td>
      </tr>
      <tr>
          <td><a href="https://davidham3.github.io/blog">https://davidham3.github.io/blog</a></td>
          <td></td>
      </tr>
      <tr>
          <td><a href="https://fenghz.github.io/index.html">https://fenghz.github.io/index.html</a></td>
          <td></td>
      </tr>
      <tr>
          <td><a href="https://archwalker.github.io/">https://archwalker.github.io/</a></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h1 id="awesome-awesomes">Awesome-Awesomes</h1>
<table>
  <thead>
      <tr>
          <th>Repo</th>
          <th>Name</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong><a href="https://github.com/naganandy/graph-based-deep-learning-literature">https://github.com/naganandy/graph-based-deep-learning-literature</a></strong></td>
          <td><strong>links to conference publications in graph-based deep learning</strong> (Very, Very, Very Important)</td>
      </tr>
      <tr>
          <td><a href="https://github.com/SherylHYX/pytorch_geometric_signed_directed">https://github.com/SherylHYX/pytorch_geometric_signed_directed</a></td>
          <td>PyTorch Geometric Signed Directed is a signed/directed graph neural network extension library for PyTorch Geometric.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning">https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning</a></td>
          <td>Paper Lists for Fair Graph Learning</td>
      </tr>
      <tr>
          <td><a href="https://github.com/thunlp/PromptPapers">https://github.com/thunlp/PromptPapers</a></td>
          <td>Must-read papers on prompt-based tuning for pre-trained language models.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/zhao-tong/graph-data-augmentation-papers">https://github.com/zhao-tong/graph-data-augmentation-papers</a></td>
          <td>A curated list of graph data augmentation papers.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/Thinklab-SJTU/ThinkMatch">https://github.com/Thinklab-SJTU/ThinkMatch</a></td>
          <td>Code &amp; pretrained models of novel deep graph matching methods.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/FLHonker/Awesome-Knowledge-Distillation">https://github.com/FLHonker/Awesome-Knowledge-Distillation</a></td>
          <td>Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)。</td>
      </tr>
      <tr>
          <td><a href="https://github.com/zlpure/awesome-graph-representation-learning">https://github.com/zlpure/awesome-graph-representation-learning</a></td>
          <td>A curated list for awesome graph representation learning resources.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/basiralab/GNNs-in-Network-Neuroscience">https://github.com/basiralab/GNNs-in-Network-Neuroscience</a></td>
          <td>A review of papers proposing novel GNN methods with application to brain connectivity published in 2017-2020.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/flyingdoog/awesome-graph-explainability-papers">https://github.com/flyingdoog/awesome-graph-explainability-papers</a></td>
          <td>Papers about explainability of GNNs</td>
      </tr>
      <tr>
          <td><a href="https://github.com/yuanqidu/awesome-graph-generation">https://github.com/yuanqidu/awesome-graph-generation</a></td>
          <td>A curated list of graph generation papers and resources.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/benedekrozemberczki/awesome-decision-tree-papers">https://github.com/benedekrozemberczki/awesome-decision-tree-papers</a></td>
          <td>A collection of research papers on decision, classification and regression trees with implementations.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/AstraZeneca/awesome-explainable-graph-reasoning">https://github.com/AstraZeneca/awesome-explainable-graph-reasoning</a></td>
          <td>A collection of research papers and software related to explainability in graph machine learning.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/LirongWu/awesome-graph-self-supervised-learning">https://github.com/LirongWu/awesome-graph-self-supervised-learning</a></td>
          <td>Awesome Graph Self-Supervised Learning</td>
      </tr>
      <tr>
          <td><a href="https://github.com/Chen-Cai-OSU/awesome-equivariant-network">https://github.com/Chen-Cai-OSU/awesome-equivariant-network</a></td>
          <td>Paper list for equivariant neural network</td>
      </tr>
      <tr>
          <td><a href="https://github.com/mengliu1998/DL4DisassortativeGraphs">https://github.com/mengliu1998/DL4DisassortativeGraphs</a></td>
          <td>Papers about developing DL methods on disassortative graphs</td>
      </tr>
      <tr>
          <td><a href="https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers">https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers</a></td>
          <td>A curated list of graph reinforcement learning papers.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ChandlerBang/awesome-self-supervised-gnn">https://github.com/ChandlerBang/awesome-self-supervised-gnn</a></td>
          <td>Papers about pretraining and self-supervised learning on Graph Neural Networks (GNN).</td>
      </tr>
      <tr>
          <td><a href="https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks">https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks</a></td>
          <td>Paper Lists for Graph Neural Networks</td>
      </tr>
      <tr>
          <td><a href="https://github.com/jwzhanggy/IFMLab_GNN">https://github.com/jwzhanggy/IFMLab_GNN</a></td>
          <td>Graph Neural Network Models from IFM Lab</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ChandlerBang/awesome-graph-attack-papers">https://github.com/ChandlerBang/awesome-graph-attack-papers</a></td>
          <td>Adversarial attacks and defenses on Graph Neural Networks.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/safe-graph/graph-adversarial-learning-literature">https://github.com/safe-graph/graph-adversarial-learning-literature</a></td>
          <td>A curated list of adversarial attacks and defenses papers on graph-structured data.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/benedekrozemberczki/awesome-graph-classification">https://github.com/benedekrozemberczki/awesome-graph-classification</a></td>
          <td>A collection of important graph embedding, classification and representation learning papers with implementations.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers">https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers</a></td>
          <td>A curated list of gradient boosting research papers with implementations.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/benedekrozemberczki/awesome-community-detection">https://github.com/benedekrozemberczki/awesome-community-detection</a></td>
          <td>A curated list of community detection research papers with implementations.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/giannifranchi/awesome-uncertainty-deeplearning">https://github.com/giannifranchi/awesome-uncertainty-deeplearning</a></td>
          <td>This repository contains a collection of surveys, datasets, papers, and codes, for predictive uncertainty estimation in deep learning models.</td>
      </tr>
      <tr>
          <td><a href="https://sites.google.com/site/graphmatchingmethods/">https://sites.google.com/site/graphmatchingmethods/</a></td>
          <td>Efficient Methods for Graph Matching and MAP Inference</td>
      </tr>
      <tr>
          <td><a href="https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering">https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering</a></td>
          <td>Awesome Deep Graph Clustering is a collection of SOTA, novel deep graph clustering methods (papers, codes, and datasets).</td>
      </tr>
      <tr>
          <td><a href="https://github.com/jwwthu/GNN4Traffic">https://github.com/jwwthu/GNN4Traffic</a></td>
          <td>This is the repository for the collection of Graph Neural Network for Traffic Forecasting.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/zwt233/Awesome-Auto-GNNs">https://github.com/zwt233/Awesome-Auto-GNNs</a></td>
          <td>A paper collection about automated graph learning</td>
      </tr>
      <tr>
          <td><a href="https://github.com/chaitjo/awesome-efficient-gnn">https://github.com/chaitjo/awesome-efficient-gnn</a></td>
          <td>Efficient Graph Neural Networks - a curated list of papers and projects</td>
      </tr>
      <tr>
          <td><a href="https://github.com/Radical3-HeZhang/Awesome-Trustworthy-GNNs">https://github.com/Radical3-HeZhang/Awesome-Trustworthy-GNNs</a></td>
          <td>Awesome Resources on Trustworthy Graph Neural Networks</td>
      </tr>
      <tr>
          <td><a href="https://github.com/EdisonLeeeee/Awesome-Masked-Autoencoders">https://github.com/EdisonLeeeee/Awesome-Masked-Autoencoders</a></td>
          <td>A collection of literature after or concurrent with Masked Autoencoder (MAE) (Kaiming He el al.).</td>
      </tr>
      <tr>
          <td><a href="https://github.com/XiaoxiaoMa-MQ/Awesome-Deep-Graph-Anomaly-Detection">https://github.com/XiaoxiaoMa-MQ/Awesome-Deep-Graph-Anomaly-Detection</a></td>
          <td>Awesome graph anomaly detection techniques built based on deep learning frameworks.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/mengliu1998/awesome-expressive-gnn">https://github.com/mengliu1998/awesome-expressive-gnn</a></td>
          <td>A collection of papers studying/improving the expressiveness of graph neural networks (GNNs)</td>
      </tr>
  </tbody>
</table>
<h1 id="useful-repotools">Useful Repo/Tools</h1>
<table>
  <thead>
      <tr>
          <th>Name</th>
          <th>Info</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="http://acronymify.com/">http://acronymify.com/</a></td>
          <td>Model Name</td>
      </tr>
      <tr>
          <td><a href="https://csacademy.com/app/graph_editor/">https://csacademy.com/app/graph_editor/</a></td>
          <td>Graph Editor</td>
      </tr>
      <tr>
          <td><strong><a href="https://github.com/guanyingc/python_plot_utils">https://github.com/guanyingc/python_plot_utils</a></strong></td>
          <td>A simple code for plotting figure, colorbar, and cropping with python</td>
      </tr>
      <tr>
          <td><strong><a href="https://github.com/guanyingc/latex_paper_writing_tips">https://github.com/guanyingc/latex_paper_writing_tips</a></strong></td>
          <td>Tips for Writing a Research Paper using LaTeX</td>
      </tr>
      <tr>
          <td><a href="https://github.com/JhuoW/Pytorch_Program_Templete">https://github.com/JhuoW/Pytorch_Program_Templete</a></td>
          <td>Pytorch Program Templete GNN</td>
      </tr>
      <tr>
          <td><a href="https://github.com/graph4ai/graph4nlp">https://github.com/graph4ai/graph4nlp</a></td>
          <td>Graph4nlp is the library for the easy use of Graph Neural Networks for NLP. Welcome to visit our DLG4NLP website (<a href="https://dlg4nlp.github.io/index.html">https://dlg4nlp.github.io/index.html</a>) for various learning resources!</td>
      </tr>
      <tr>
          <td><a href="https://github.com/benedekrozemberczki/pytorch_geometric_temporal">https://github.com/benedekrozemberczki/pytorch_geometric_temporal</a></td>
          <td>PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models (CIKM 2021)</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ysig/GraKeL">https://github.com/ysig/GraKeL</a></td>
          <td>A scikit-learn compatible library for graph kernels</td>
      </tr>
      <tr>
          <td><a href="https://github.com/jajupmochi/graphkit-learn">https://github.com/jajupmochi/graphkit-learn</a></td>
          <td>A python package for graph kernels, graph edit distances, and graph pre-image problem.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/pliang279/awesome-phd-advice">https://github.com/pliang279/awesome-phd-advice</a></td>
          <td>Collection of advice for prospective and current PhD students</td>
      </tr>
      <tr>
          <td><a href="https://github.com/MLEveryday/100-Days-Of-ML-Code">https://github.com/MLEveryday/100-Days-Of-ML-Code</a></td>
          <td>100-Days-Of-ML-Code中文版</td>
      </tr>
      <tr>
          <td><a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></td>
          <td>《动手学深度学习》</td>
      </tr>
      <tr>
          <td><a href="https://github.com/lukas-blecher/LaTeX-OCR">https://github.com/lukas-blecher/LaTeX-OCR</a></td>
          <td>pix2tex: Using a ViT to convert images of equations into LaTeX code.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/thunlp/OpenPrompt">https://github.com/thunlp/OpenPrompt</a></td>
          <td>An Open-Source Framework for Prompt-Learning.</td>
      </tr>
      <tr>
          <td><a href="https://github.com/snap-stanford/GraphGym">https://github.com/snap-stanford/GraphGym</a></td>
          <td>Platform for designing and evaluating Graph Neural Networks (GNN)</td>
      </tr>
      <tr>
          <td><a href="https://github.com/pygod-team/pygod">https://github.com/pygod-team/pygod</a></td>
          <td>A Python Library for Graph Outlier Detection (Anomaly Detection)</td>
      </tr>
      <tr>
          <td><a href="https://github.com/MLNLP-World/Paper_Writing_Tips">https://github.com/MLNLP-World/Paper_Writing_Tips</a></td>
          <td>latex写作建议</td>
      </tr>
      <tr>
          <td><a href="https://github.com/dair-ai/ML-YouTube-Courses">https://github.com/dair-ai/ML-YouTube-Courses</a></td>
          <td>A place to discover the latest machine learning courses on YouTube.</td>
      </tr>
  </tbody>
</table>
<h1 id="miscellaneous">Miscellaneous</h1>
<table>
  <thead>
      <tr>
          <th>Name</th>
          <th>Desc</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="https://github.com/The-Run-Philosophy-Organization/run">https://github.com/The-Run-Philosophy-Organization/run</a></td>
          <td>run学指南</td>
      </tr>
      <tr>
          <td><a href="https://10beasts.net/">https://10beasts.net/</a></td>
          <td>测评</td>
      </tr>
  </tbody>
</table>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2020 《Inductive and Unsupervised Representation Learning on Graph Structured Objects》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/seed/</link>
      <pubDate>Mon, 28 Mar 2022 23:44:01 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/seed/</guid>
      <description>ICLR2020 &amp;#34;Inductive and Unsupervised Representation Learning on Graph Structured Objects&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=rkem91rtDB">Paper</a></p>
<p><a href="https://github.com/wenwen0319/SEED-Reimplementation">Code</a></p>
<h1 id="introduction">Introduction</h1>
<p>无监督图学习算法基于重构损失，不可避免的需要图相似度计算（重构embedding和输入embedding的loss), 计算复杂度较高。本文提出一种通用的归纳式无监督图学习算法<strong>SEED</strong>（Sampling, Encoding, and Embedding Distributions）。通过计算采样子图的重构损失来代替整个图的重构损失。 即 先采样子图，在用GNN编码子图，最后计算子图分布的embedding来作为整个图的representation. 过程如下图所示：</p>
<p><img loading="lazy" src="/posts/2022-03-29-seed/pic1.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="seed-sampling-encoding-and-embedding-distributions">SEED: Sampling, Encoding, and Embedding Distributions</h1>
<h2 id="anonymous-random-walk">Anonymous Random Walk</h2>
<p><strong>Definition 1 (Random Anonymous Walks[1]):</strong>  Given a random walk $\mathbf{w}=(w_1, \cdots, w_l)$ where $\langle w_i, w_{i+1} \rangle \in E$, the anonymous walk for $\mathbf{w}$ is defined as：
$$
\mathrm{aw}(\mathbf{w}) = (\mathrm{DIS}(\mathbf{w}, w_1),\mathrm{DIS}(\mathbf{w}, w_2),\cdots, \mathrm{DIS}(\mathbf{w}, w_l) )
$$
where $\mathrm{DIS}(\mathbf{w}, w_i)$ denotes the number of distinct nodes in $\mathbf{w}$ when $w_i$ first appears in $\mathbf{w}$, i.e.
$$
\mathrm{DIS}(\mathbf{w}, w_i) = |{w_1, \cdots w_p}|, \quad p = \min_j {w_j=w_i}
$$
匿名随机游走和随机游走的不同在于，匿名随机游走描述了随机游走的潜在“patterns”, 不管具体被访问的节点是什么。 距离来说，给定两条随机游走序列 $\mathbf{w_1}=(v_1, v_2, v_3, v_4, v_2)$ 和$w_2=(v_2, v_1, v_3, v_4, v_1)$, 这两个RW相关联的匿名随机游走是一样的，即$\mathrm{aw}(\mathbf{w_1}) = \mathrm{aw}(\mathbf{w_2}) = (1,2,3,4,2)$, 即使$\mathbf{w_1}$和$\mathbf{w_2}$访问不同的节点。即每个节点在RW中首次被访问时的位置就是这个点在ARW中的id,如在$\mathbf{w_2}$中，$v_1$首次访问是在第二个时刻，那么他的id就是2，在ARW中用2表示。</p>
<h2 id="sampling">Sampling</h2>
<p>本文提出WEAVE随机游走来表示子图</p>
<p><img loading="lazy" src="/posts/2022-03-29-seed/pic2.png#center" alt="你想输入的替代文字"  />
</p>
<p>上图中所有的$a$代表属性一样的节点， 所有的$b$也代表属性一样的节点，那么构造如图中两条vanilla random walks将得到两条完全相同的随机游走序列，因为序列中的节点属性排列完全一样（这里不会去构造induced subgraph）。为了可以区分两个图，提出了WEAVE, i.e.,  random Walk with EArliest Visit timE。实际上就是为每个随机游走序列上的节点拼接他在匿名随机游走序列中的index。这样就可以区分两个属性完全一样的随机游走序列。</p>
<p>简单来说这种方法会记录节点首次被访问的时间，这个时间作为节点的index，从而随机游走序列可以反映子图结构。</p>
<p>一个长度为$k$的WEAVE序列可以表示为：$X=\left[\mathbf{x}^{(0)}, \mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(k)}\right]$, 其中$\mathbf{x}^{(p)}$是序列上的第$p$个节点， $\mathbf{x}^{(p)}=\left[\mathbf{x}_{a}^{(p)}, \mathbf{x}_{t}^{(p)}\right]\in \mathbb{R}^{k \times (d+\ell)}$, 是两个向量的拼接，$\mathbf{x}_{a}^{(p)} \in \mathbb{R}^d$代表这个节点的node feature, $ \mathbf{x}_{t}^{(p)} \in \mathbb{R}^\ell$是是节点在匿名随机游走中的idx， 用onehot向量表示（即该节点首次被访问的时间）。</p>
<p>最终，如果要从输入图中sample $s$条随机游走路径，将会生成$s$个子图，用矩阵表示为$\left\{X_{1}, X_{2}, \ldots, X_{s}\right\}$。</p>
<h2 id="encoding">Encoding</h2>
<p>用$s$个随机游走序列表示$\mathcal{G}$的$s$个子图。对每个子图使用auto encoder 计算embedding:
$$
\mathbf{z}=f\left(X ; \theta_{e}\right), \quad \hat{X}=g\left(\mathbf{z} ; \theta_{d}\right)
$$
其中$X$表示一个子图（WEAVE）, 先用$f_{\theta_e}$得到这个子图的pooling embedding, 在用$g_{\theta_d}$将子图的embedding重构为矩阵$\hat{X}$。每个子图的重构损失为：
$$
\mathcal{L}=||X-\hat{X}||_{2}^{2}
$$
通过对每个子图的$\mathcal{L}$做SGD来优化$\theta_e$和$\theta_d$来使得重构误差最小。 最终对于图$\mathcal{G}$我们可以得到它的$s$个子图表示：$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$.</p>
<h2 id="embedding-distribution">Embedding Distribution</h2>
<p>假设我们已经有了输入图$\mathcal{G}$的子图表示向量集$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$, 要将他们融合成一个embedding来表示整个图。可以把这个图的子图集合看做一个distribution，每个子图是这个distribution中的一个样本。 如果两个Graph的子图分布相似，那么这两个Graph的相似度应该更高。 所以目标就变为，给定两个图$\mathcal{G}$和$\mathcal{H}$, 他们的子图表示分别为$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{s}\right\}$和$\left\{\mathbf{h}_{1}, \mathbf{h}_{2}, \cdots, \mathbf{h}_{s}\right\}$。这是两个分布的样本，我们要计算两个分布的距离，本文使用MMD, 目的是求两个分布的distribution embeddings, 然后求两个distribution embeddings间的距离。MMD可以参考<a href="https://jhuow.github.io/posts/mmd/">这里</a>。</p>
<p>用$P_{\mathcal{}G}$和$P_{\mathcal{H}}$分别表示这两个图的子图分布， 两个分布之间的MMD距离可以用下式计算得到。
$$
\begin{aligned}
\widehat{MMD}\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)=&amp; \frac{1}{s(s-1)} \sum_{i=1}^{s} \sum_{j \neq i}^{s} k\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)+\frac{1}{s(s-1)} \sum_{i=1}^{s} \sum_{j \neq i}^{s} k\left(\mathbf{h}_{i}, \mathbf{h}_{j}\right) \\
&amp;-\frac{2}{s^{2}} \sum_{i=1}^{s} \sum_{j=1}^{s} k\left(\mathbf{z}_{i}, \mathbf{h}_{j}\right) \\
=&amp;\left|\left|\hat{\mu}_{\mathcal{G}}-\hat{\mu}_{\mathcal{H}}\right|\right|_{2}^{2} .
\end{aligned}
$$
该式表示的含义为，两个图中的样本$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$和$\left\{\mathbf{h}_{1}, \mathbf{h}_{2}, \cdots, \mathbf{h}_{s}\right\}$分别映射到一个RKHS空间中，<strong>两组样本在这个RKHS空间中的均值来表示这两个分布</strong>。即：
$$
\hat{\mu}_{\mathcal{G}}=\frac{1}{s} \sum_{i=1}^{s} \phi\left(\mathbf{z}_{i}\right), \quad \hat{\mu}_{\mathcal{H}}=\frac{1}{s} \sum_{i=1}^{s} \phi\left(\mathbf{h}_{i}\right)
$$
其中$\phi(\mathbf{z}_{i})$,$\phi(\mathbf{h}_{i})$分别表示 将向量$\mathbf{z}_{i}$和$\mathbf{h}_{i}$ 映射到一个RKHS中，所以$\phi(\cdot)$是一个kernel $k(\cdot, \cdot)$的feature map函数, i.e., $k(u,v) = \langle \phi(u), \phi(v) \rangle$。$\phi(u) = k(\cdot, u)$是kernel $k$对应RKHS中的一个函数（向量）。 所以只要确定一个kernel $k(\cdot, \cdot)$，上面的$\widehat{MMD}(P_{\mathcal{G}}, P_{\mathcal{H}})$就可以求出确定值，表示两个distribution间的距离。 但是知道两个分布在RKHS中的距离还不够，需要知道这两个分布的在RKHS间的均值距离还不够， 我们需要知道这两个分布在RKHS中被映射成了什么向量，即我们要求$\phi(\cdot)$。</p>
<p>假设我们已经有了一个kernel， 这个kernel对应的映射函数是一个恒等映射，那么$\phi(u)=u$, 分布样本在RKHS中的表示就是他们本身，即 $\phi(\mathbf{z}_{i})=\mathbf{z}_{i}$, $\phi(\mathbf{h}_{i})=\mathbf{h}_{i}$。那么这分布的表示向量就是他们的样本在RKHS上的平均（均值平均误差）：
$$
\hat{\mu}_{\mathcal{G}}=\frac{1}{s} \sum_{i=1}^{s} \mathbf{z}_{i}, \quad \hat{\mu}_{\mathcal{H}}=\frac{1}{s} \sum_{i=1}^{s} \mathbf{h}_{i}
$$
如果$k$是一个其他通用kernel, 比如RBF kernel, 那么$k(u,v) = \langle \phi(u), \phi(v) \rangle$这里的$\phi(\cdot)$是不知道的，也就是仅能知道映射后的内积值，不能知道具体的映射是什么，为了求这个映射，本文用神经网络来近似这个映射。</p>
<p>具体来说，定义$\hat{\phi}\left(\cdot ; \theta_{m}\right)$是一个参数为$\theta_{m}$的MLP， 输入为分布的样本，那么用这个函数来对两个分布的样本$\{\mathbf{z_i}\}$和$\{\mathbf{h_i}\}$做映射, 然后用$\hat{\phi}\left(\cdot ; \theta_{m}\right)$来近似kernel真实的映射函数$\phi(\cdot)$。即：
$$
\hat{\mu}_{\mathcal{G}}^{\prime}=\frac{1}{s} \sum_{i=1}^{s} \hat{\phi}\left(\mathbf{z}_{i} ; \theta_{m}\right), \quad \hat{\mu}_{\mathcal{H}}^{\prime}=\frac{1}{s} \sum_{i=1}^{s} \hat{\phi}\left(\mathbf{h}_{i} ; \theta_{m}\right), \quad D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)=\left|\left|\hat{\mu}_{\mathcal{G}}^{\prime}-\hat{\mu}_{\mathcal{H}}^{\prime}\right|\right|_{2}^{2}
$$
上式中的$D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)$表示两个分布中的样本在被$\hat{\phi}\left(\cdot; \theta_{m}\right)$映射后的均值误差。用这个均值误差来近似$\widehat{MMD}(P_{\mathcal{G}}, P_{\mathcal{H}})$中由kernel $k$的映射$\phi(\cdot)$算出的Ground truth均值误差：</p>
<p>$$J\left(\theta_{m}\right)=\left|\left|D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)-\widehat{M M D}\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)\right|\right|_{2}^{2}$$</p>
<p>通过最小化$J\left(\theta_{m}\right)$,来优化$\hat{\phi}\left(\cdot; \theta_{m}\right)$,使其近似称为一个kernel的feature map函数， 即可以将样本映射到一个RKHS空间中的函数。</p>
<p>训练结束后，用$\hat{\mu}_{\mathcal{G}}^{\prime}$来表示输入图$\mathcal{G}$的最终embedding （子图分布embedding）。</p>
<h1 id="reference">Reference</h1>
<p>[1] Micali, S., and Zhu, Z. A. 2016. Reconstructing markov processes from independent and anonymous experiments. Discrete Applied Mathematics 200:108–122.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Awesome Barbell Graph with Networkx</title>
      <link>https://JhuoW.github.io/posts/barbell_graph/</link>
      <pubDate>Sun, 27 Mar 2022 15:38:50 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/barbell_graph/</guid>
      <description>&lt;h3 id=&#34;晕了&#34;&gt;晕了&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import networkx as nx
import matplotlib.pyplot as plt

n_clique, n_path = 10, 10
clique1 = nx.complete_graph(n_clique)
clique1_pos = nx.circular_layout(clique1)
clique2 = nx.complete_graph(n_clique)
clique2_mapping = {node: node + n_clique for node in clique2}
nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes
x_diff, y_diff = 8, -1
clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2}
path = nx.path_graph(n_path)
path_mapping = {node: node + 2 * n_clique for node in path}
nx.relabel_nodes(path, path_mapping, copy=False) # avoids repeated nodes
path_nodes = list(path.nodes)
path_half1_nodes = path_nodes[:n_path//2]
path_half2_nodes = path_nodes[n_path//2:]
path_dist = 0.9
clique2_entry = n_clique + n_clique // 2
path_half1_pos = {node: clique1_pos[0] + (path_dist + i * path_dist, 0) for i, node in enumerate(path_half1_nodes)}
path_half2_pos = {node: clique2_pos[clique2_entry] - (path_dist + i * path_dist, 0) for i, node in enumerate(path_half2_nodes[::-1])}
path_pos = {**path_half1_pos, **path_half2_pos}
barbell = nx.Graph()
barbell.add_edges_from(clique1.edges)
barbell.add_edges_from(clique2.edges)
barbell.add_edges_from(path.edges)
barbell.add_edges_from([(path_half1_nodes[0], 0), (path_half2_nodes[-1], clique2_entry)])
clique_pos = {**clique1_pos, **clique2_pos}
barbell_pos = {**clique_pos, **path_pos}
plt.figure(figsize=(20, 6))
nx.draw(barbell, pos=barbell_pos, with_labels=True)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://JhuoW.github.io/posts/barbell-graph/barbell.png#center&#34; alt=&#34;你想输入的替代文字&#34;  /&gt;
&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h3 id="晕了">晕了</h3>
<pre tabindex="0"><code>import networkx as nx
import matplotlib.pyplot as plt

n_clique, n_path = 10, 10
clique1 = nx.complete_graph(n_clique)
clique1_pos = nx.circular_layout(clique1)
clique2 = nx.complete_graph(n_clique)
clique2_mapping = {node: node + n_clique for node in clique2}
nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes
x_diff, y_diff = 8, -1
clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2}
path = nx.path_graph(n_path)
path_mapping = {node: node + 2 * n_clique for node in path}
nx.relabel_nodes(path, path_mapping, copy=False) # avoids repeated nodes
path_nodes = list(path.nodes)
path_half1_nodes = path_nodes[:n_path//2]
path_half2_nodes = path_nodes[n_path//2:]
path_dist = 0.9
clique2_entry = n_clique + n_clique // 2
path_half1_pos = {node: clique1_pos[0] + (path_dist + i * path_dist, 0) for i, node in enumerate(path_half1_nodes)}
path_half2_pos = {node: clique2_pos[clique2_entry] - (path_dist + i * path_dist, 0) for i, node in enumerate(path_half2_nodes[::-1])}
path_pos = {**path_half1_pos, **path_half2_pos}
barbell = nx.Graph()
barbell.add_edges_from(clique1.edges)
barbell.add_edges_from(clique2.edges)
barbell.add_edges_from(path.edges)
barbell.add_edges_from([(path_half1_nodes[0], 0), (path_half2_nodes[-1], clique2_entry)])
clique_pos = {**clique1_pos, **clique2_pos}
barbell_pos = {**clique_pos, **path_pos}
plt.figure(figsize=(20, 6))
nx.draw(barbell, pos=barbell_pos, with_labels=True)
</code></pre><p><img loading="lazy" src="/posts/barbell-graph/barbell.png#center" alt="你想输入的替代文字"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Maximum Mean Discrepancy</title>
      <link>https://JhuoW.github.io/posts/mmd/</link>
      <pubDate>Sun, 27 Mar 2022 10:45:08 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/mmd/</guid>
      <description>&lt;h1 id=&#34;mean-discrepancy-md均值差异&#34;&gt;Mean Discrepancy (MD)均值差异&lt;/h1&gt;
&lt;p&gt;判断2个分布$p$ 和$q$是否相同。&lt;/p&gt;
&lt;p&gt;$p$分布生成一个样本空间$\mathbb{P}$ (从$p$中采样$m$个样本)&lt;/p&gt;
&lt;p&gt;$q$分布生成一个样本空间$\mathbb{Q}$（从$q$中采样$n$个样本）&lt;/p&gt;
&lt;p&gt;函数$f$的输入为 分布生成的样本空间&lt;/p&gt;
&lt;p&gt;如果
$$
\begin{equation}
\begin{aligned}
\mathrm{mean}(f(\mathbb{P})) == \mathrm{mean}(f(\mathbb{Q})) \\
i.e., \frac{1}{m}\sum^m_{i=1}f(p_i) = \frac{1}{n}\sum^n_{i=1}f(q_i)
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;则$p$和$q$是同一分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MD&lt;/strong&gt; can be defined as
$$
\begin{equation}
\begin{aligned}
\mathrm{MD}&amp;amp;=|\mathrm{mean}(f(\mathbb{P})) -\mathrm{mean}(f(\mathbb{Q})) | \\
&amp;amp;= |\frac{1}{m}\sum^m_{i=1}f(p_i) - \frac{1}{n}\sum^n_{i=1}f(q_i)|
\end{aligned}
\end{equation}
$$&lt;/p&gt;
&lt;h1 id=&#34;maximum-mean-discrepancy-mmd-最大均值差异&#34;&gt;Maximum Mean Discrepancy (MMD) 最大均值差异&lt;/h1&gt;
&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;MMD:&lt;/strong&gt;  在函数集$\mathcal{F}=\{f_1, f_2, \cdots \}$中， 找到一个函数$f^*$， 使得$|\mathrm{mean}(f^*(\mathbb{P})) -\mathrm{mean}(f^*(\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的&lt;strong&gt;最大均值差异&lt;/strong&gt;（MMD）。MMD =0 表示两个分布相同。
$$
\operatorname{MMD}[\mathcal{F}, p, q]:=\sup _{f \in \mathcal{F}}\left(\mathbf{E}_{x \sim p}[f(x)]-\mathbf{E}_{y \sim q}[f(y)]\right)
$$
其中$\mathbf{E}_{x \sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\sup$为上确界直接理解为max就好。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="mean-discrepancy-md均值差异">Mean Discrepancy (MD)均值差异</h1>
<p>判断2个分布$p$ 和$q$是否相同。</p>
<p>$p$分布生成一个样本空间$\mathbb{P}$ (从$p$中采样$m$个样本)</p>
<p>$q$分布生成一个样本空间$\mathbb{Q}$（从$q$中采样$n$个样本）</p>
<p>函数$f$的输入为 分布生成的样本空间</p>
<p>如果
$$
\begin{equation}
\begin{aligned}
\mathrm{mean}(f(\mathbb{P})) == \mathrm{mean}(f(\mathbb{Q})) \\
i.e., \frac{1}{m}\sum^m_{i=1}f(p_i) = \frac{1}{n}\sum^n_{i=1}f(q_i)
\end{aligned}
\end{equation}
$$</p>
<p>则$p$和$q$是同一分布。</p>
<p><strong>MD</strong> can be defined as
$$
\begin{equation}
\begin{aligned}
\mathrm{MD}&amp;=|\mathrm{mean}(f(\mathbb{P})) -\mathrm{mean}(f(\mathbb{Q})) | \\
&amp;= |\frac{1}{m}\sum^m_{i=1}f(p_i) - \frac{1}{n}\sum^n_{i=1}f(q_i)|
\end{aligned}
\end{equation}
$$</p>
<h1 id="maximum-mean-discrepancy-mmd-最大均值差异">Maximum Mean Discrepancy (MMD) 最大均值差异</h1>
<h2 id="定义">定义</h2>
<p><strong>MMD:</strong>  在函数集$\mathcal{F}=\{f_1, f_2, \cdots \}$中， 找到一个函数$f^*$， 使得$|\mathrm{mean}(f^*(\mathbb{P})) -\mathrm{mean}(f^*(\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的<strong>最大均值差异</strong>（MMD）。MMD =0 表示两个分布相同。
$$
\operatorname{MMD}[\mathcal{F}, p, q]:=\sup _{f \in \mathcal{F}}\left(\mathbf{E}_{x \sim p}[f(x)]-\mathbf{E}_{y \sim q}[f(y)]\right)
$$
其中$\mathbf{E}_{x \sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\sup$为上确界直接理解为max就好。</p>
<h2 id="条件">条件</h2>
<p>为了准确判断分布$p$和$q$之间的距离，需要找到一个合适的函数，使得两个分布在这个函数上的距离尽可能大，但搜索空间不能过于大，所以函数空间$\mathcal{F}$要满足两个条件：</p>
<p><strong>C1:</strong> 函数集$\mathcal{F}$要足够丰富， 使得MMD尽可能准确</p>
<p><strong>C2:</strong> 考虑数据集样本数量，随着数据集的增大，MMD要能迅速收敛，要求$\mathcal{F}$足够restrictive (函数集不能无限大)</p>
<p>所以利用<a href="https://jhuow.github.io/posts/rkhs_kernel/">kernel</a> 方法，即， 将两个分布的样本空间映射到一个高维或者无限维的空间$\mathcal{H}$中，如果两个分布的样本在$\mathcal{H}$中的均值依然相等，那么这两个分布相等，MMD=0。两个分布在$\mathcal{H}$中的最大均值为MMD。</p>
<p><strong>因此，当$\mathcal{F}$是再生核Hilbert Space 上的单位球（unit ball）时，可以满足以上两个条件</strong>。 即，将$\mathcal{F}$定义为某个kernel对应的RKHS中的函数， 例如，</p>
<p>给定一个Gaussian Kernel: $k(u,v) = \{\exp({-\frac{||u-v||^2}{2\sigma}})\}_\sigma$, 这个kernel函数是一个Hilbert Space的再生核，那么这个空间可以表示为</p>
<p>$$
\begin{equation}
\mathcal{H}_k = \operatorname{span}({\Phi(x): x \in \mathcal{X}})=\left\{f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right): m \in \mathbf{N}, x_{i} \in \mathcal{X}, \alpha_{i} \in \mathbf{R}\right\} \tag{1}
\end{equation}
$$
空间$\mathcal{X}$中的每个元素$x_i$都对应于一个函数$k(\cdot,x_i)=k_{x_i}(\cdot)$, 那么$\mathcal{X}$中的所有元素所产生的函数$\{k_{x_i}(\cdot)\}_{x_i \in \mathcal{X}}$ 可以span成一个<strong>Function Space</strong>, 如公式1所示， 这个function space中的每个function可以由&quot;basis functions&quot;$\{k_{x_i}(\cdot)\}_{x_i \in \mathcal{X}}$ 通过线性组合得到。那么</p>
<p>$$f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)$$</p>
<p>可以表示kernel $k(\cdot, \cdot)$的RKHS中的每个function。 每个valid kernel都有一个RKHS $\mathcal{H}_k$与它对应。</p>
<p>我们将MMD的候选函数集$\mathcal{F}$定义为某一个kernel $k(\cdot,\cdot)$所对应的RKHS $\mathcal{H}_k$中的函数，这样就可以满足所有候选函数都在$\mathcal{H}_k$中(足够多)，同时如果kernel是Gaussian Kernel, 相当于把样本空间映射到无限高维来做MD,更加准确。</p>
<p>另外，我们限制范式norm$||f||_{\mathcal{H}_k} \leq 1$来避免上界取到无限大</p>
<h2 id="回到mmd">回到MMD</h2>
<p>已知$\mathcal{F}=\{f_1(\cdot), f_2(\cdot), \cdots \}$中的每个函数都是一个高斯核函数$k(\cdot,\cdot)$的RKHS中的函数，要从$\mathcal{H}_k$中选一个函数$f^*(\cdot)$，使得两个分布的样本间距离在$k(\cdot,\cdot)$的RKHS上最大。</p>
<p>因为$f(\cdot)$是$\mathcal{H}_k$中的一个函数，那么$f(\cdot)$可以表示为$\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)$, 此时，下式一定成立（参考<a href="https://jhuow.github.io/posts/rkhs_kernel/">这里</a>）：</p>
<p>$$
f(x) = \langle f(\cdot), k(\cdot, x) \rangle_{\mathcal{H}_k}
$$
$k(\cdot, x) = \Phi(x)$表示将$x$映射到空间$\mathcal{H}_{k}$上的值，即$x$在$\mathcal{H}_{k}$上的表示。 若$k$是Gaussian Kernel, 那么$k(\cdot, x)$就是$x$在无限维空间上的表示。</p>
<p>连续空间中$\mathbf{E}_{x \sim p}[f(x)]$可以写为：
$$
\begin{equation}
\begin{aligned}
\mathbf{E}_{x \sim p}[f(x)] &amp;= \int_x p(x)f(x) dx\\
&amp; = \int_x p(x) \langle f(\cdot), k(\cdot, x) \rangle_{\mathcal{H}_k} dx \\
&amp;= \langle \int_x p(x)f(\cdot) dx, \int_x p(x)k(\cdot, x) dx \rangle_{\mathcal{H}_k}\\
&amp;= \langle f(\cdot), \mu_p\rangle_{\mathcal{H}_k}
\end{aligned}
\end{equation}
$$
其中$\mu_p = \int_x p(x)k(\cdot, x) dx$.</p>
<p>因此，MMD可以改写为：
$$
\begin{equation}
\begin{aligned}
\operatorname{MMD}(\mathrm{p}, \mathrm{q}, \mathcal{H})&amp;:=\sup_{f \in \mathcal{H},|f|_{\mathcal{H}} \leq 1}(\underset{\mathrm{p}(\boldsymbol{x})}{\mathbb{E}}[f(\boldsymbol{x})]-\underset{\mathrm{q}(\boldsymbol{y})}{\mathbb{E}}[f(\boldsymbol{y})])\\
&amp;=\sup_{f \in \mathcal{H},|f|_{\mathcal{H}_k} \leq 1}\left(\left\langle\mu_{\mathrm{p}}-\mu_{\mathrm{q}}, f\right\rangle_{\mathcal{H}_k}\right)
\end{aligned}
\end{equation}
$$
利用内积性质：$\langle a, b \rangle \leq ||a|| ||b||$， 因为
$$
||f(\cdot)||_{\mathcal{H}_k}\leq 1
$$
,
$$
\left\langle\mu_{\mathrm{p}}-\mu_{\mathrm{q}}, f\right\rangle_{\mathcal{H}_k} \leq ||\mu_{\mathrm{p}}-\mu_{\mathrm{q}}||_{\mathcal{H}_k}||f||_{\mathcal{H}_k}
$$
Then,
$$
\sup_{f \in \mathcal{H},|f|_{\mathcal{H}_k} \leq 1}\left(\left\langle\mu_{\mathrm{p}}-\mu_{\mathrm{q}}, f\right\rangle_{\mathcal{H}_k}\right) =||\mu_{\mathrm{p}}-\mu_{\mathrm{q}}||_{\mathcal{H}_k}
$$
其中$\mu_p = \int_x p(x)k(\cdot, x) dx$, $\mu_q = \int_y q(y)k(\cdot, y) dy$ 分别表示分布的期望(均值)。 然而期望无法直接计算，因此用样本空间的均值代替分布的期望：
$$
\begin{equation}
\begin{aligned}
\mathrm{M M D}(p,q,\mathcal{H}_k) &amp; \approx \mathrm{M M D}(X,Y,\mathcal{F}_{\mathcal{H}_k})\\ &amp;=\left|\left|\frac{1}{n} \sum_{i=1}^{n} f(x_i)-\frac{1}{m} \sum_{j=1}^{m} f(x_j)\right|\right|_{\mathcal{H}_k}
\end{aligned}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\begin{aligned}
\mathrm{M M D}^2(p,q,\mathcal{H}_k) &amp; \approx \mathrm{M M D}^2(X,Y,\mathcal{F}_{\mathcal{H}_k})\\
&amp;=\left|\left|\frac{1}{n} \sum_{i=1}^{n} f(x_i)-\frac{1}{m} \sum_{j=1}^{m} f(x_j)\right|\right|_{\mathcal{H}_k}^{2}\\
&amp;= \left|\left|\frac{1}{n^{2}} \sum_{i}^{n} \sum_{i^{\prime}}^{n} \left\langle f(x_i),f(x_i^{\prime})\right\rangle-\frac{2}{n m} \sum_{i}^{n} \sum_{j}^{m} \left\langle f(x_i), f(y_j)\right\rangle+\frac{1}{m^{2}} \sum_{j}^{m} \sum_{j^{\prime}}^{m} \left\langle f(y_j), f(y_j^{\prime})\right\rangle\right|\right|_{\mathcal{H}_k} \\
&amp; = \frac{1}{n^2} K_{x, x^\prime}-\frac{2}{nm} K_{x, y}+\frac{1}{m^{2}} K_{y, y^{\prime}}
\end{aligned}
\end{equation}
$$</p>
<p>令
$$
K=\begin{bmatrix} K_{x, x^{\prime}} &amp; K_{x, y} \\ K_{x, y}&amp; K_{y, y^{\prime}} \end{bmatrix}
$$</p>
<p>$$
M=\begin{bmatrix}\frac{1}{n^{2}} &amp;-\frac{1}{n m} \\ -\frac{1}{n m}&amp; \frac{1}{m^{3}} \end{bmatrix}
$$</p>
<p>最后：
$$
\mathrm{M M D}^2(X,Y,\mathcal{F}_{\mathcal{H}_k}) = tr(KM)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>https://JhuoW.github.io/posts/rkhs_kernel/</link>
      <pubDate>Sat, 26 Mar 2022 22:39:20 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/rkhs_kernel/</guid>
      <description>&lt;h1 id=&#34;hilbert-space&#34;&gt;Hilbert Space&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;&lt;/em&gt; (&lt;a href=&#34;http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf&#34;&gt;Norm&lt;/a&gt;) Let $\mathcal{F}$ be a vector space over $\mathbb{R}$ (For example $\mathcal{F}=\mathbb{R}^n$ is a vector space). A function $||\cdot||_{\mathcal{F}}: \mathcal{F} \to [0, \inf)$ is said to be a norm on $\mathcal{F}$ if ($||\cdot||_{\mathcal{F}}$ 是一个有效norm算子要满足以下条件):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $f \in \mathcal{F}$, $||f||_{\mathcal{F}}=0$ if and only if $f=0$. (norm separates points)&lt;/li&gt;
&lt;li&gt;$|\lambda f|_{\mathcal{F}}=|\lambda||f|_{\mathcal{F}}$, $\forall \lambda \in \mathbb{R}, \forall f \in \mathcal{F}$ (positive homogeneity).&lt;/li&gt;
&lt;li&gt;$|f+g|_{\mathcal{F}} \leq|f|_{\mathcal{F}}+|g|_{\mathcal{F}}, \forall f, g \in \mathcal{F}$ (triangle inequality).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;向$||\cdot||_{\mathcal{F}}$中输入任意一个向量，只要满足以上条件，那么$||\cdot||_{\mathcal{F}}$是一个valid norm operator.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="hilbert-space">Hilbert Space</h1>
<p><em><strong>Definition 1</strong></em> (<a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf">Norm</a>) Let $\mathcal{F}$ be a vector space over $\mathbb{R}$ (For example $\mathcal{F}=\mathbb{R}^n$ is a vector space). A function $||\cdot||_{\mathcal{F}}: \mathcal{F} \to [0, \inf)$ is said to be a norm on $\mathcal{F}$ if ($||\cdot||_{\mathcal{F}}$ 是一个有效norm算子要满足以下条件):</p>
<ol>
<li>For $f \in \mathcal{F}$, $||f||_{\mathcal{F}}=0$ if and only if $f=0$. (norm separates points)</li>
<li>$|\lambda f|_{\mathcal{F}}=|\lambda||f|_{\mathcal{F}}$, $\forall \lambda \in \mathbb{R}, \forall f \in \mathcal{F}$ (positive homogeneity).</li>
<li>$|f+g|_{\mathcal{F}} \leq|f|_{\mathcal{F}}+|g|_{\mathcal{F}}, \forall f, g \in \mathcal{F}$ (triangle inequality).</li>
</ol>
<p>向$||\cdot||_{\mathcal{F}}$中输入任意一个向量，只要满足以上条件，那么$||\cdot||_{\mathcal{F}}$是一个valid norm operator.</p>
<h2 id="inner-product">Inner Product</h2>
<p>An <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">inner product</a> takes two elements of a vector space $\mathcal{X}$ and outputs a number. An inner product could be a usual dot product: $\langle\mathbf{u}, \mathbf{v}\rangle=\mathbf{u}^{\prime} \mathbf{v}=\sum_{i} u^{(i)} v^{(i)}$ (Inner Product can be Dot Product). Or the inner product could be something fancier（即内积不一定表示为点积的形式）. If an Inner Product $\langle \cdot,\cdot \rangle$ is valid, it <em><strong>MUST</strong></em>  satisfy the following conditions:</p>
<ol>
<li>
<p>Symmetry
$$\langle u, v\rangle=\langle v, u\rangle \quad \forall u, v \in \mathcal{X}$$</p>
</li>
<li>
<p>Bilinearity
$$\langle\alpha u+\beta v, w\rangle=\alpha\langle u, w\rangle+\beta\langle v, w\rangle \quad \forall u, v, w \in \mathcal{X}, \forall \alpha, \beta \in \mathbf{R}$$</p>
</li>
<li>
<p>Strict Positive Definiteness
$$
\begin{gathered}
\langle u, u\rangle \geq 0 \forall x \in \mathcal{X} \\
\langle u, u\rangle=0 \Longleftrightarrow u=0
\end{gathered}$$</p>
</li>
</ol>
<p>An  <em><strong>inner product space</strong></em> (or pre-Hilbert space) is a vector space together with an inner product. （包含内积运算的向量空间称为 内积空间，即可以定义内积运算的向量空间）。</p>
<p>Kernel is a kind of Inner Product. For example, the Gaussian kernel is defined as:
$$
\begin{equation}
\langle u, v \rangle = k(u,v) = \exp({-\frac{||u-v||^2}{2\sigma}}) \tag{1}
\end{equation}
$$</p>
<h2 id="hilbert-space-1">Hilbert Space</h2>
<p><em><strong>Definition 2</strong></em> (<a href="https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf">Hilbert Space</a>)  A Hilbert Space is an Inner Product space that is complete and separable with respect to the norm defined by the inner product.</p>
<p>&lsquo;Complete&rsquo; means sequences converge to elements of the space - there aren&rsquo;t any &ldquo;holes&rdquo; in the space.</p>
<h1 id="finite-states">Finite States</h1>
<p>Given finite input space ${x_1, x_2, \cdots x_m }$. I want to be able to take inner products between any two of them using my function $k$  as the inner product ($k$ is customized and satisfy three conditions. For example, $k$ is a Gaussian inner product as Eq.(1)). Inner products by definition are symmetric, so $k(x_i, x_j)=k(x_j, x_i)$ , which yields a symmetric matrix $\mathbf{K}$.</p>
<p>Since $\mathbf{K}$ is real symmetric, and this means we can diagonalize it （实对称阵可以对角化，即特征分解）, and the eigendecomposition takes this form:
$$
\begin{equation}
\begin{aligned}
\mathbf{K} &amp;=\mathbf{V} \Lambda \mathbf{V}^T \\
&amp;= \mathbf{V} 	\begin{bmatrix}
\lambda_1 &amp;   &amp; &amp;  \\
&amp; \lambda_2 &amp;  &amp; \\
&amp;   &amp; \cdots &amp;\
&amp;   &amp;  &amp;\lambda_m
\end{bmatrix} \mathbf{V}^T \\
&amp;= \begin{bmatrix}
v_1 &amp; v_2 &amp; \cdots v_m
\end{bmatrix} \begin{bmatrix}
\lambda_1 &amp;   &amp; &amp;  \\
&amp; \lambda_2 &amp;  &amp; \\
&amp;   &amp; \cdots &amp;\\
&amp;   &amp;  &amp;\lambda_m
\end{bmatrix}
\begin{bmatrix}
v_1^T\\
v_2^T\\
\cdots \\
v_m^T
\end{bmatrix}\\
&amp;=v_1\lambda_1 v_1^T + \cdots + v_m\lambda_m v_m^T = \sum_{t=1}^m v_t\lambda_tv_t^T
\end{aligned} \tag{2}
\end{equation}
$$
Let the $i$-th element of vector $v$ as $v^{(i)}$, then
$$
\begin{equation}
\begin{aligned}
\mathbf{K}_{ij} = k(x_i, x_j) &amp;= [\sum_{t=1}^m v_t\lambda_tv_t^T]_{ij}\\
&amp;=\sum^m_{t=1} v_t^{(i)} \lambda_t v_t^{(j)}
\end{aligned} \tag{3}
\end{equation}
$$
If $\mathbf{K}$ is a <strong>positive semi-definite</strong> (<strong>PSD</strong>) matrix, then $\lambda_1, \cdots \lambda_m \geq 0$.</p>
<blockquote>
<p><em><strong>Assumption 1</strong></em>. All $\lambda_t$ are nonnegative.</p></blockquote>
<p>We consider this feature map:
$$
\begin{equation}
\Phi\left(x_{i}\right)=\left[\sqrt{\lambda_{1}} v_{1}^{(i)}, \ldots, \sqrt{\lambda_{t}} v_{t}^{(i)}, \ldots, \sqrt{\lambda_{m}} v_{m}^{(i)}\right] \in \mathbb{R}^m \tag{4}
\end{equation}
$$
(writing it for $x_j$ too):
$$
\begin{equation}
\boldsymbol{\Phi}\left(x_{j}\right)=\left[\sqrt{\lambda_{1}} v_{1}^{(j)}, \ldots, \sqrt{\lambda_{t}} v_{t}^{(j)}, \ldots, \sqrt{\lambda_{m}} v_{m}^{(j)}\right]  \in \mathbb{R}^m \tag{5}
\end{equation}
$$
即 $\Phi: \mathcal{X} \to \mathbb{R}^m$ 将$x\in \mathcal{X}$映射到$m$维向量空间$\mathbb{R}^m$中的一个点。</p>
<p>With this choice, the inner product $k$ is just defined as a dot product in $\mathbb{R}^m$:
$$
\begin{equation}
\left\langle\Phi\left(x_{i}\right), \Phi\left(x_{j}\right)\right\rangle_{\mathbf{R}^{m}}=\sum_{t=1}^{m} \lambda_{t} v_{t}^{(i)} v_{t}^{(j)}=\left(\mathbf{V} \Lambda \mathbf{V}^{\prime}\right)_{i j}=K_{i j}=k\left(x_{i}, x_{j}\right)  \tag{6}
\end{equation}
$$
If there exists an eigenvalue $\lambda_s &lt;0$ (即$\sqrt{\lambda_s} = \sqrt{|\lambda_s|} i$). $\lambda_s$对应的特征向量$v_s$。用$v_s \in \mathbb{R}^m$的$m$个元素$v_s = [v_s^{(1)},\cdots, v_s^{(m)}]$, 来对$\Phi(x_1),\cdots, \Phi(x_m)$做线性组合：
$$
\begin{equation}
\mathbf{z}=\sum_{i=1}^{m} v_{s}^{(i)} \boldsymbol{\Phi}\left(x_{i}\right) \tag{7}
\end{equation}
$$</p>
<p>It is obvious that $\mathbf{z} \in \mathbb{R}^m$. Then calculate
$$
\begin{equation}
\begin{aligned}
|\mathbf{z}|_{2}^{2} &amp;=\langle\mathbf{z}, \mathbf{z}\rangle_{\mathbf{R}^{m}}=\sum_{i} \sum_{j} v_{s}^{(i)} \boldsymbol{\Phi}\left(x_{i}\right)^{T} \boldsymbol{\Phi}\left(x_{j}\right) v_{s}^{(j)}=\sum_{i} \sum_{j} v_{s}^{(i)} K_{i j} v_{s}^{(j)} \\
&amp;=\mathbf{v}_{s}^{T} \mathbf{K} \mathbf{v}_{s}=\lambda_{s}&lt;0
\end{aligned}  \tag{8}
\end{equation}
$$
which conflicts with the geometry of the feature space.</p>
<p>如果$\mathbf{K}$不是半正定，那么feature space $\mathbb{R}^m$存在小于0的值。所以假设Assumption不成立。即，若$k$表示有限集的内积，那么它的Gram Matrix一定半正定(PSD)，否则无法保证该空间中的norm大于0。</p>
<p>有效的内积对应的Gram Matrix 必定PSD.</p>
<h1 id="kernel">Kernel</h1>
<p><em><strong>Definition 3.</strong></em> (<a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">Kernel</a>) A function $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a kernel if</p>
<ol>
<li>$k$ is symmetric: $k(x,y) = k(y,x)$.</li>
<li>$k$ gives rise to a positive semi-definite &ldquo;Gram matrix,&rdquo; i.e., for any $m\in \mathbb{N}$ and any $x_1,\cdots,x_m$ chosen from $X$, the Gram matrix $\mathbf{K}$ defined by $\mathbf{K}_{ij} = k(x_i,x_j)$ is positive semi-definite.</li>
</ol>
<p>Another way to show that a matrix $\mathbf{K}$ is positive semi-definite is to show that
$$
\begin{equation}
\forall \mathbf{c} \in \mathbf{R}^{m}, \mathbf{c}^{T} \mathbf{K} \mathbf{c} \geq 0 \tag{9}
\end{equation}
$$
Here are some nice properties of $k$:</p>
<ul>
<li>$k(u,u) \geq 0$ (Think about the Gram matrix of $m = 1$.)</li>
<li>$k(u, v) \leq \sqrt{k(u, u) k(v, v)}$ (This is the Cauchy-Schwarz inequality.)</li>
</ul>
<h2 id="reproducing-kernel-hilbert-space-rkhs">Reproducing Kernel Hilbert Space (RKHS)</h2>
<p>给定一个kernel $k(\cdot, \cdot): \mathcal{X} \times \mathcal{X} \to \mathbb{R}$. 定义一个函数空间（space of functions）$\mathbf{R}^{\mathcal{X}}:={f: \mathcal{X} \rightarrow \mathbb{R}}$. $\mathbf{R}^{\mathcal{X}}$ 是一个 Hilbert Space， 该空间中的每个元素是一个$\mathcal{X}$映射到$\mathbb{R}$的函数。</p>
<p>令$k_x(\cdot) = k(x, \cdot)$, 假设$x$是一个定值（Constant），自变量（输入）用$\cdot$表示。那么$k(x, \cdot)$ 也是$\mathbf{R}^{\mathcal{X}}$空间中的一个函数。</p>
<p>每个函数$k_x(\cdot)$ 都与一个特定的$x \in \mathcal{X}$有关，即每个$x$对应于一个函数$k_x(\cdot) = k(\cdot, x)$. 这种对应关系表示为$\Phi(x) = k_x(\cdot) = k(x,\cdot)$, 即：
$$
\begin{equation}
\Phi: x \longmapsto k(\cdot, x) \tag{10}
\end{equation}
$$
即 $\Phi$的输入为$x\in \mathcal{X}$, 输出一个函数, 输出的函数属于$\mathbf{R}^{\mathcal{X}}$空间。</p>
<p>在连续空间$\mathcal{X}$中，$x \in \mathcal{X}$ 有无穷多种情况，那么$\Phi(x)=k_x(\cdot)=k(x, \cdot)$也有无穷多种情况，即无穷多种函数。 这些函数可以span 一个Hilbert Space:
$$
\begin{equation}
\mathcal{H}_k = \operatorname{span}({\Phi(x): x \in \mathcal{X}})=\left\{f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right): m \in \mathbf{N}, x_{i} \in \mathcal{X}, \alpha_{i} \in \mathbf{R}\right\}  \tag{11}
\end{equation}
$$
其中$k(x,\cdot)=\Phi(x)$可以理解为将$x$映射为一个函数（or vector）。上述Hilbert Space是由任意$k(x, \cdot)$线性组合而成的函数空间，该空间中的每个元素可以表示为
$$
\begin{equation}
f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)  \tag{12}
\end{equation}
$$
所以$\mathcal{H}$可以看作是kernel $k$对应的一个Hilbert Space。</p>
<p>给定$\mathcal{H}$中的任意两个函数$f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)$, $g(\cdot)=\sum_{j=1}^{m^{\prime}} \beta_{j} k\left(\cdot, x_{j}^{\prime}\right)$。注意$f(\cdot)$和$g(\cdot)$可以表示$\mathcal{H}$中任意两个元素。我们将$\mathcal{H}$上的内积定义为：
$$
\begin{equation}
\langle f, g\rangle_{\mathcal{H}_{k}}=\sum_{i=1}^{m} \sum_{j=1}^{m^{\prime}} \alpha_{i} \beta_{j} k\left(x_{i}, x_{j}^{\prime}\right) \tag{13}
\end{equation}
$$
由<a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">Proof</a>证明了该内积符合三个条件，顾上式是$\mathcal{H}$空间中一个有效的内积算子。注：$\mathcal{H}_k$表示该Hilbert Space是由函数 $k(x,\cdot)$ span而成的，与Kernel $k$有关.</p>
<p>$k(x,\cdot)$也是$\mathcal{H}_k$中的一个函数，那么它与 $f$的内积为：
$$
\begin{equation}
\langle k(\cdot, x), f\rangle_{\mathcal{H}_{k}}=
\sum_{i=1}^m \alpha_i k(x,x_i)
=f(x) \tag{14}
\end{equation}
$$
<strong>Theorem 1.</strong>  $k(\cdot, \cdot)$ is a reproducing kernel of a Hilbert space $\mathcal{H}_k$ if $f(x)=\langle k(x, \cdot), f(\cdot)\rangle$.</p>
<p>$\mathcal{H}_k$ 为$k(\cdot, \cdot)$的再生核希尔伯特空间。</p>
<p>同理，$k(\cdot, x_i)$, $k(\cdot, x_j)$都为$\mathcal{H}_k$中的函数， 计算他们的内积:
$$
\begin{equation}
\left\langle k(\cdot, x_i), k\left(\cdot, x_j\right)\right\rangle_{\mathcal{H}_{k}}=k\left(x_i, x_j\right)  \tag{15}
\end{equation}
$$
因为$ k(\cdot, x_i) = \Phi(x_i)$, $ k(\cdot, x_j) = \Phi(x_j)$, 所以
$$
\begin{equation}
k\left(x_i, x_j\right) = \left\langle  \Phi(x_i), \Phi(x_j)\right\rangle_{\mathcal{H}_{k}} \tag{16}
\end{equation}
$$
表示将$x_i$和$x_j$ 映射成$\mathcal{H}_k$中的函数（向量）后再做内积。</p>
<h4 id="参考文献">参考文献</h4>
<p>[1] <a href="https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf">https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf</a></p>
<p>[2] <a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf">http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf</a></p>
<p>[3] <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf</a></p>
<p>我把本文整理成了<a href="/posts/rkhs/RKHS.pdf">PDF</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Monte Carlo Tree Search</title>
      <link>https://JhuoW.github.io/posts/monte-carlo-tree-search/</link>
      <pubDate>Fri, 25 Mar 2022 18:09:58 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/monte-carlo-tree-search/</guid>
      <description>&lt;h1 id=&#34;单一状态monte-carlo规划多臂赌博机multi-armed-bandits&#34;&gt;单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits）&lt;/h1&gt;
&lt;p&gt;单一状态$s$, $k$种action（$k$个摇臂）。&lt;/p&gt;
&lt;p&gt;在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?&lt;/p&gt;
&lt;p&gt;多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;利用（Exploitation）： 保证在过去决策中得到最佳回报&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;探索（Exploration）：寄希望在未来能够得到更大的汇报&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。&lt;/p&gt;
&lt;p&gt;但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。&lt;/p&gt;
&lt;h2 id=&#34;悔值函数&#34;&gt;悔值函数&lt;/h2&gt;
&lt;p&gt;如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数：
$$
R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t}
$$
$i$: 第$i$个赌博机&lt;/p&gt;
&lt;p&gt;$I_t$: $t$时刻选择的赌博机&lt;/p&gt;
&lt;p&gt;$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励&lt;/p&gt;
&lt;p&gt;$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward&lt;/p&gt;
&lt;p&gt;$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。&lt;/p&gt;
&lt;p&gt;$R_n$越大，就代表$n$次决策的结果越差。&lt;/p&gt;
&lt;h2 id=&#34;上置信区间upper-confidence-bound-ucb&#34;&gt;上置信区间（Upper Confidence Bound, UCB）&lt;/h2&gt;
&lt;p&gt;UCB旨在探索和利用间去的平衡&lt;/p&gt;
&lt;p&gt;在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机：
$$
I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}.
$$
其中$I_{t}$为$t$时刻要摇的赌博机，&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="单一状态monte-carlo规划多臂赌博机multi-armed-bandits">单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits）</h1>
<p>单一状态$s$, $k$种action（$k$个摇臂）。</p>
<p>在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?</p>
<p>多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。</p>
<ul>
<li>
<p>利用（Exploitation）： 保证在过去决策中得到最佳回报</p>
</li>
<li>
<p>探索（Exploration）：寄希望在未来能够得到更大的汇报</p>
</li>
</ul>
<p>例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。</p>
<p>但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。</p>
<h2 id="悔值函数">悔值函数</h2>
<p>如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数：
$$
R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t}
$$
$i$: 第$i$个赌博机</p>
<p>$I_t$: $t$时刻选择的赌博机</p>
<p>$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励</p>
<p>$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward</p>
<p>$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。</p>
<p>$R_n$越大，就代表$n$次决策的结果越差。</p>
<h2 id="上置信区间upper-confidence-bound-ucb">上置信区间（Upper Confidence Bound, UCB）</h2>
<p>UCB旨在探索和利用间去的平衡</p>
<p>在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机：
$$
I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}.
$$
其中$I_{t}$为$t$时刻要摇的赌博机，</p>
<p>$\overline{X_{i, T_{i}(t-1)}}$要尽可能大：为$t$时刻之前赌博机$i$的平均reward 要尽可能大，</p>
<p>$C_{t-1, T_{i}(t-1)}$要尽可能小：$t$时刻之前$i$出现的次数要尽可能少</p>
<p>其中$C_{t,T_i(t)}$的取值定义如下：
$$
C_{t,T_i(t)}=\sqrt{\frac{2 \operatorname{In} t}{T_i(t)}}
$$
其中$T_i(t)$表示 $t$时刻以前（包括$t$），选到赌博机$i$的次数总和。</p>
<p>若赌博机在$t$时刻之前（包括$t$）时刻摇动的次数越少，$C_{t,T_i(t)}$越大</p>
<p>选出的$I_t$要满足在$t$之前的平均reward尽可能大（利用）， 且在$t$之前出现次数尽可能少（探索）。</p>
<p>也就是说，在第时刻，UCB算法一般会选择具有如下最大值的第$j$个赌博机：
$$
\begin{aligned}
U C B&amp;=\bar{X}_{j}+\sqrt{\frac{2 \operatorname{Inn}}{n_{j}}} \text { 或者 } U C B=\bar{X}_{j}+C \times \sqrt{\frac{2 \operatorname{In} n}{n_{j}}} \\
I_t &amp;= \mathrm{argmax}_j UCB(j)
\end{aligned}
$$
其中$\bar{X}_{j}$是第$j$个赌博机在过去时间内所获得的平均奖赏值，$n_j$是在过去时间内拉动第$j$个赌博机臂膀的总次数，$n$是过去时间内拉动所有赌博机臂膀的总次数。$C$是一个平衡因子，其决定着在选择时偏重探索还是利用。</p>
<p>从这里可看出UCB算法如何在探索-利用(exploration-exploitation)之间寻找平衡：既需要拉动在过去时间内获得最大平均奖赏的赌博机，又希望去选择拉动臂膀次数最少的赌博机。</p>
<h1 id="monte-carlo-tree-search">Monte Carlo Tree Search</h1>
<p>MCTS has four step:</p>
<ul>
<li>Selection 选择</li>
<li>Expansion 拓展</li>
<li>Simulation（rollout) 模拟</li>
<li>Backpropagation 回溯</li>
</ul>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/1.png#center" alt=""  />
</p>
<h2 id="选择">选择</h2>
<p>从根节点R开始，向下递归选择子节点，直至选择一个叶子节点L。
具体来说，通常用UCBl(Upper Confidence Bound,上限置信区间)选择最具“潜力”的后续节点：
$$
U C B=\bar{X}_{j}+\sqrt{\frac{2 \operatorname{In} n}{n_{j}}}
$$</p>
<h2 id="拓展">拓展</h2>
<p>如果L不是一个终止节点（即博弈游戏不)，则随机创建其后的一个未被访问节点，选择该节点作为后续子节点C。</p>
<h2 id="模拟">模拟</h2>
<p>从节点C出发，对游戏进行模拟，直到博弈游戏结束。</p>
<h2 id="反向传播">反向传播</h2>
<p>用模拟所得结果来回溯更新导致这个结果的每个节点中获胜次数和访问次数。</p>
<p><strong>包含两种策略学习机制：</strong></p>
<p><strong>搜索树策略</strong>：从已有的搜索树中选择或创建一个叶子结点（即蒙特卡洛中选择和拓展两个步骤).搜索树策略需要在利用和探索之间保持平衡。</p>
<p><strong>模拟策略</strong>：从非叶子结点出发模拟游戏，得到游戏仿真结果。</p>
<h1 id="例子-围棋">例子： 围棋</h1>
<ul>
<li>
<p>以围棋为例，假设根节点是执黑棋方。</p>
</li>
<li>
<p>图中每一个节点都代表一个局面，每一个局面记录两个值A/B</p>
</li>
</ul>
<p>A: 该局面被访问中黑棋胜利次数。对于黑棋表示己方胜利次数，对于白棋表示己方失败次数（对方胜利次数)；</p>
<p>B: 该局面被访问的总次数</p>
<p>初始状态：<img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>根节点（12/21）是当前局面，该局面被访问过21次，其中黑棋访问该局面后最终获胜12次。 黑执棋方遇到该局面（根节点局面）开始选择下一步action。</p>
<h3 id="选择-1">选择</h3>
<p>黑执棋方遇到（12/21）这个局面时有3个有效action, 黑执行这三个action会得到3种局面（7/10）,（5/8）, (0/3). 黑执棋方要选择其中一个action，是的棋局变为这三种中的一种，那么要如何选择action呢？我们先分别计算UCB1：</p>
<p>左一： 7/10对应的局面Reward为：
$$
\frac{7}{10} + \sqrt{\frac{\log (21)}{10}} = 1.252
$$
3中局面共21次被模拟到，其中，该局面被访问过10次（探索， 第二项）， 黑棋遇到该局面后最终获胜7次（利用， 第一项）。</p>
<p>左二：（5/8）对应局面Reward:
$$
\frac{5}{8} + \sqrt{\frac{\log(21)}{8}} = 1.243
$$
左三： （0/3）对应局面Reward:
$$
\frac{0}{3} + \sqrt{\frac{\log(21)}{3}} = 1.007
$$
由此可见，黑棋选择会导致局面（7/10）的action进行走琪。</p>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白棋遇到局面（7/10）时，有两个有效的action, 分别会导致局面A/B = 2/4和5/6, A为白棋访问该局面后失败的次数，所以白棋访问该局面最终胜利的次数应该是$B-A$：</p>
<p>左一： (2/4)对应的局面Reward (白棋尽可能获胜)为：
$$
(1-\frac{2}{4}) + \sqrt{\frac{\log(21)}{4}}=1.372
$$
左二：  (5/6)对应的局面Reward为：
$$
(1-\frac{5}{6}) + \sqrt{\frac{\log(21)}{4}}=0.879
$$
因此白棋会选择（2/4）局面</p>
<p>即<strong>每一步都寻找最佳应对方式，来最终评估更节点局面的好坏</strong></p>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白色执棋方选择action后到达局面（2/4）, 黑棋执棋方面对该局面，有两种action可选，分别会到达（1/3）和（1/1）， 分别计算UCB1 得：</p>
<p>左一： (1/3)对应reward 为：
$$
\frac{1}{3} + \sqrt{\frac{\log (21)}{3}} = 1.341
$$
左二：(1/1)对应reward为：
$$
\frac{1}{1} + \sqrt{\frac{\log (21)}{1}} = 2.745
$$
则黑棋选择action 使得局面变为(1/1)。右图中可见(1/1)为叶子节点， 白棋面对该局面暂时无候选局面（action）,则进入下一个步骤，拓展。</p>
<h3 id="拓展-1">拓展</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白棋执棋方面对局面(1/1),因为已经是叶子节点， 所以(1/1)会随机产生一个未被访问过的新节点，初始化为(0/0),接着在该节点下进行模拟。 进入第三部， 模拟。</p>
<h3 id="模拟-1">模拟</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq3.png#center" alt=""  />
</p>
<p>黑棋执棋方面局面（0/0），开始进行游戏仿真， 假设经过一些列仿真后，最终白棋获胜，即从更节点到最终模拟结束的叶子节点黑棋失败了。 进入第四部，回溯</p>
<h3 id="回溯">回溯</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq4.png#center" alt=""  />
</p>
<p>根据仿真结果来更新该仿真路径上的每个节点的A/B值。 因为该次模拟最终白棋获胜，所以向上回溯路径上的所有父节点，父节点的A不变，B+1。 若最终黑棋获胜，则父节点的A+1, B+1。</p>
<p><strong>在有限时间里， MCTS会不断重复4个步骤，所有节点的A,B 值会不断变化</strong></p>
<p><strong>到某个局面（节点）后如何下棋（action）, 会选择所有有效Action中UCB1值最大的节点，作为下棋的下一步。</strong></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NIPS2018 《DiffPool:Hierarchical Graph Representation Learning with Differentiable Pooling》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/diffpool/</link>
      <pubDate>Thu, 19 Dec 2019 19:32:36 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/diffpool/</guid>
      <description>NIPS2018 &amp;#34;Hierarchical Graph Representation Learning with Differentiable Pooling&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址： <a href="https://dl.acm.org/doi/pdf/10.5555/3327345.3327389">DiffPool</a></p>
<h1 id="introduction">Introduction</h1>
<p>传统的GNN算法在Node-level的任务如节点分类、链路预测上有着较好的效果。但是，现有的GNN方法由于其存在平面化的局限性，因此无法学习图的层级表示（意味着无法预测整个图的标签），顾无法实现图分类任务。举个栗子，一个Graph，可以分成600个subgraph，每个节点都存在于其中的某个subgraph（一个节点只存在于一个subgraph中），每个subgraph拥有一个标签，如何预测subgraph的标签是这篇文章主要想解决的问题。传统的GNN的图分类方法都是为Graph中的所有节点生成Embedding，然后将对这些Embedding做全局聚合（池化），如简单的把属于同一个subgraph的节点求和或者输入到MLP中生成一个标签向量来表示整个subgraph，但是这样可能忽略的图的层级结构信息。</p>
<p>本文提出了一种端到端的可微可微图池化模块<strong>DiffPool</strong>，原理如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-12-19-diffpool/1.png" alt=""  />
</p>
<p>在深度GNN中的每层中为节点学习可微的软簇分配，将节点映射到簇中，这些簇作为新的节点作为下一层GNN的输入。上图的Original Network部分是一个Subgraph，传统的方法是直接求出这个Subgraph中每个节点的Embedding，然后相加或输入到一个神经网络中，得到一个预测向量，这种方法可以称为“全局池化”。<strong>DiffPool</strong>中，假设第$l$层的输入是$1000$个簇（如果是第一层输入就是1000个节点），我们先设置第$l+1$层需要输入的簇的个数（假设为$100$），也就是第$l$层输出的簇个数，然后在$l$层中通过一个分配矩阵将$1000$个簇做合并，合并成100个“节点”，然后将这100个节点输入到$l+1$层中，最后图中的节点数逐渐减少，最后，图中的节点只有一个，这个节点的embedding就是整个图的表示，然后将图输入到一个多层感知机MLP中，得到预测向量，在于真值的one-hot向量做cross-entropy，得到Loss。</p>
<h1 id="modeldiffpool">Model：DiffPool</h1>
<p>一个Graph表示为$\mathcal{G} = (A,F)$，其中$A \in {0,1}^{n \times n}$是Graph的邻接矩阵，$F \in \mathbb{R}^{n \times d}$表示节点特征矩阵，每个节点有$d$维的特征。给定一个带标签的子图集$\mathcal{D}=\left\{\left(G_{1}, y_{1}\right),\left(G_{2}, y_{2}\right), \ldots\right\}$， 其中 $y_{i} \in \mathcal{Y}$表示每个子图$G_i \in \mathcal{G}$的标签，任务目标是寻找映射$f: \mathcal{G} \rightarrow \mathcal{Y}$，将图映射到标签集。我们需要一个过程来将每个子图转化为一个有限维度的向量$\mathbb{R}^D$。</p>
<h2 id="graph-neural-networks">Graph Neural Networks</h2>
<p>一般，GNN可以表示成&quot;Message Passing&quot;框架：
$$
H^{(k)}=M\left(A, H^{(k-1)} ; \theta^{(k)}\right)
$$
其中$H^{(k)} \in \mathbb{R}^{n \times d}$表示GNN迭代$k$次后的node embedding，$M$是一个Message扩散函数，由邻接矩阵$A$和一个可训练的参数$\theta^{(k)}$决定。$H^{(k-1)}$是由前一个message passing过程生成的node embedding。当$k = 1$时，第一个GNN的输入为$H^{(0)}$是原始的节点特征$H^{(0)} = F$。</p>
<p>GNN的一个主要目标是设计一个Message Passage函数$M$，GCN（kipf.2016）是一种流行的GNN，$M$的实现方式是将线性变换和ReLU非线性激活结合起来:
$$
H^{(k)}=M\left(A, H^{(k-1)} ; W^{(k)}\right)=\operatorname{ReLU}\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(k-1)} W^{(k-1)}\right)
$$
其中，$\tilde{A} = A+I$是一个加上自环的邻接矩阵，$\tilde{D}=\sum_{j} \tilde{A}_{i j}$是$\tilde{A}$的度矩阵，$W^{(k)} \in \mathbb{R}^{d \times d}$是一个可训练的权重矩阵，$W$与节点个数以及每个节点的度无关，可以看做一个特征增强矩阵，用来规定GCN的输出维度。</p>
<p>一个完整的GNN模型会迭代$K$次来输出最终的node embedding$Z = H^{(K)} \in \mathbb{R}^{n \times d}$。对于GCN，GAT，GraphSage，$K$一般取2-6。文中为了简单表示，忽略了GNN的内部结构，用$Z=GNN(A,X)$来表示一个任意的执行$K$次的GNN模块。</p>
<h2 id="gnn和池化层的堆叠">GNN和池化层的堆叠</h2>
<p>这篇工作的目标是定义一个一般的，端到端的可微策略，允许以层级的方式堆叠多个GNN模块。给定原始的邻接矩阵$A \in \mathbb{R}^{n \times n}$，$Z=GNN(A,X)$十一GNN模块的输出（假设这个GNN模块做了3次迭代）。我们需要定义一个策略来输出一个新的粗化图，这个粗化图包含$m$个节点，$m &lt; n$，它的邻接矩阵一个带权重的邻接矩阵$A&rsquo; \in \mathbb{R}^{m \times m}$，同时，输出node embedding $Z&rsquo; \in \mathbb{R}^{m \times d}$。这个粗化图（$m$个节点的图）作为下一层GNN的输入 （将$A&rsquo;$和$Z&rsquo;$输入下一个GNN层）。最后所有节点粗化为只有一个节点的图，这个节点的embedding就是这个subgraph的表示。因此，目标为：如何使用上一层GNN的输出结果，对节点做合并或池化，是的图中的节点减少，再将粗化的图输入到下一个GNN中。</p>
<h2 id="基于可学习分配的可微分池化">基于可学习分配的可微分池化</h2>
<p><strong>DiffPool</strong>通过对一个GNN模块的输出学习一个聚类分配矩阵来解决这个问题。可微池化层根据$l-1$层的GNN模块（假设是一个3次迭代的GNN模块）产生的node embedding来对节点做合并，从而产生一个粗化图，这个粗化图作为$l$层GNN模块的输入，最终，整个subgraph被粗化为一个cluster，可以看做一个节点。</p>
<h3 id="用分配矩阵进行池化">用分配矩阵进行池化</h3>
<p>$S^{(l)} \in \mathbb{R}^{n_{l} \times n_{l+1}}$表示第$l$层的聚类分配矩阵，$S^{(l)}$的每一行表示第l层的每个节点（cluster）,每一列表示$l+1$层的每个cluster（节点）。$S^{(l)}_{ij}$表示第$l$层的节点$i$属于第$l+1$层cluster $j$的概率，所以$S^{(l)}$是个概率矩阵。</p>
<p>假如已经有了第$l$层的节点分配矩阵$S^{(l)}$，将第$l$层的邻接矩阵表示为$A^{(l)}$，将第$l$层GNN模块的输出节点特征（node embedding）表示为$Z^{(l)}$，通过DiffPool层可以将第$l$层的图粗化为$\left(A^{(l+1)}, X^{(l+1)}\right)=\operatorname{DIFFPOOL}\left(A^{(l)}, Z^{(l)}\right)$，其中，$A^{(l+1)}$是$l+1$层图的邻接矩阵，是一个粗化后的图，$X^{(l+1)}$是下一层的输入特征（node/cluster embedding）：
$$
\begin{aligned}
&amp;X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1} \times d}\
&amp;A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times n_{l+1}}
\end{aligned}
$$
上面第一个公式将第$l$层节点嵌入$Z^{(l)}$转化为下一层的输入特征$X^{(l+1)}$。第二个公式将第$l$层的邻接矩阵转化为$l+1$层的粗化图邻接矩阵$A^{(l+1)}$。$n_{l+1}$是$l+1$层节点（cluster）的数量。最后，将$A^{(l+1)}$和$X^{(l+1)}$作为下一层GNN的输入。这样图中的节点就由$n_l$个下降到$n_{l+1}$个。</p>
<h3 id="学习分配矩阵s">学习分配矩阵S</h3>
<p>第$l$层的输入特征$X^{(l)}$，用一个GNN模块（代码中是一个3层的GCN）得到node embedding：
$$
Z^{(l)}=\mathrm{GNN}_{l, \text { embed }}\left(A^{(l)}, X^{(l)}\right)
$$
用另外一个GNN模块（代码中是一个3层的GCN）在用一个softmax转化为概率矩阵来的到节点分配矩阵：
$$
S^{(l)}=\operatorname{softmax}\left(\mathrm{GNN}_{l, \mathrm{pool}}\left(A^{(l)}, X^{(l)}\right)\right)
$$
$S^{(l)}$是一个$n_l \times n_{l+1}$的全链接矩阵，$S^{(l)}_{ij}$表示第$l$层的节点$i$属于第$l+1$层cluster $j$的概率。</p>
<p>$l=0$时，第一层GNN的输入是subgraph的原始邻接矩阵$A$和特征矩阵$F$，倒数第二层$l=L-1$时的分配矩阵$S^{(L-1)}$是一个全1向量，那么最后将所以节点归为一类，产生一个代表整个图的嵌入向量。</p>
<p>所以，把图节点的合并过程称为分层的图表示学习（Hierarchical Graph Representation Learning）。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Notes for Spectral Clustering</title>
      <link>https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/</link>
      <pubDate>Sat, 07 Sep 2019 09:11:09 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/</guid>
      <description>&lt;p&gt;最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。&lt;/p&gt;
&lt;p&gt;本文主要参考了：[1] &lt;a href=&#34;https://www.cnblogs.com/pinard/p/6221564.html#!comments&#34;&gt;https://www.cnblogs.com/pinard/p/6221564.html#!comments&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。&lt;/p&gt;
&lt;h1 id=&#34;基础1-无向权重图&#34;&gt;基础1： 无向权重图&lt;/h1&gt;
&lt;p&gt;对于边$(v_i,v_j)$, 它的权重$w_{ij} &amp;gt; 0$。对于没有边的节点$v_i$和$v_j$,  他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即：
$$
d_i = \sum_{j=1}^n w_{ij}
$$
根据所有节点的度值，我们可以得到一个度矩阵$D$:
$$
D=\displaystyle \left(\begin{array}{ccc}{d_{1}} &amp;amp; {\ldots} &amp;amp; {\ldots} \\\ {\ldots} &amp;amp; {d_{2}} &amp;amp; {\ldots} \\\ {\vdots} &amp;amp; {\vdots} &amp;amp; {\ddots} \\\ {\ldots} &amp;amp; {\ldots} &amp;amp; {d_{n}}\end{array}\right) ^{n\times n}
$$
是一个$n \times n$的对角阵，对角元素是每个节点的度和。&lt;/p&gt;
&lt;p&gt;定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义：
$$|A|=A 中的节点个数 $$&lt;/p&gt;
&lt;p&gt;$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。</p>
<p>本文主要参考了：[1] <a href="https://www.cnblogs.com/pinard/p/6221564.html#!comments">https://www.cnblogs.com/pinard/p/6221564.html#!comments</a></p>
<h1 id="introduction">Introduction</h1>
<p>谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。</p>
<h1 id="基础1-无向权重图">基础1： 无向权重图</h1>
<p>对于边$(v_i,v_j)$, 它的权重$w_{ij} &gt; 0$。对于没有边的节点$v_i$和$v_j$,  他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即：
$$
d_i = \sum_{j=1}^n w_{ij}
$$
根据所有节点的度值，我们可以得到一个度矩阵$D$:
$$
D=\displaystyle \left(\begin{array}{ccc}{d_{1}} &amp; {\ldots} &amp; {\ldots} \\\ {\ldots} &amp; {d_{2}} &amp; {\ldots} \\\ {\vdots} &amp; {\vdots} &amp; {\ddots} \\\ {\ldots} &amp; {\ldots} &amp; {d_{n}}\end{array}\right) ^{n\times n}
$$
是一个$n \times n$的对角阵，对角元素是每个节点的度和。</p>
<p>定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义：
$$|A|=A 中的节点个数 $$</p>
<p>$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$</p>
<h1 id="基础2相似矩阵">基础2：相似矩阵</h1>
<p>再提下谱聚类的基本思想： 距离较远的两个点之间权重值较低，距离较近的两个点之间权重值较高</p>
<p>但是在谱聚类中，我们只能获得数据点的定义，无法给出邻接矩阵，因为是离散的分布在空间中，无法得知其中的连接关系。</p>
<p>一般来说，通过样本点距离度量的相似矩阵$S$来获得邻接矩阵$W$.</p>
<p>构建邻接矩阵$W$有两种方法: $\epsilon$-邻近法， K邻近法和全连接法。</p>
<h2 id="epsilon-邻近法">$\epsilon$-邻近法</h2>
<p>$\epsilon$为距离的阈值，欧式距离$S_{ij}$表示两点$v_i$,$v_j$的坐标$x_i$,$x_j$之间的距离。 即 $S_{ij} =||x_i-x_j||^2_2$为相似矩阵$S \in \mathbb{R}^{n \times n}$的第$i$行第$j$个元素，则邻接矩阵$W$可以表示为：</p>
<p>$$
w_{ij}=\left\{\begin{array}{ll}{0} &amp; {s_{i j}&gt;\epsilon} \\  {\epsilon} &amp; {s_{i j} \leq \epsilon}\end{array}\right.
$$</p>
<p>意思是如果两点之间的距离大于$\epsilon$，那么他们之间的权重为0， 如果他们之间的距离小于$\epsilon$，他们之间的权重为$\epsilon$。 这种方法的弊端在于点之间的距离只有两种情况，不够精确，故很少采用。</p>
<h2 id="k邻近法">K邻近法</h2>
<p>利用<strong>KNN</strong>算法遍历所有样本点，取每个样本点最近的$k$个点作为近邻，只有和样本点距离最近的$k$个点的$w_{ij} &gt;0$，但会出现一种情况， $v_i$的k个近邻中有$v_j$，但$v_j$的k个近邻中没有$v_i$，这样会造成邻接矩阵$W$的不对称， 因此提供两种解决办法</p>
<p>第一种： 只要$v_j$在$v_i$的K邻域中，那么不管$v_i$在不在$v_j$的K邻域中，都把$v_i$加入$v_j$的邻域中，即：</p>
<p>$$
w_{i j}=w_{j i}=\left\{\begin{array}{ll}{0} &amp; {x_{i} \notin K N N\left(x_{j}\right) \text { and } x_{j} \notin K N N\left(x_{i}\right)} \\ {\exp \left(-\frac{\left||x_{i}-x_{j}\right||^2_2}{2 \sigma^{2}}\right)} &amp; {x_{i} \in K N N\left(x_{j}\right) \text { or } x_{j} \in K N N\left(x_{i}\right)}\end{array}\right.
$$
第二种，必须$v_i$在$v_j$的K邻域中，且$v_j$在$v_i$的K邻域中，那么才保留两者间的权重，否则都为0，即：</p>
<p>$$
w_{i j}=w_{j i}=\left\{\begin{array}{ll}{0} &amp; {x_{i} \notin K N N\left(x_{j}\right) \text { or } x_{j} \notin K N N\left(x_{i}\right)} \\ {\exp \left(-\frac{||x_{i}-x_{j}||^2_2}{2 \sigma^{2}}\right)} &amp; {x_{i} \in K N N\left(x_{j}\right) \text { and } x_{j} \in K N N\left(x_{i}\right)}\end{array}\right.
$$</p>
<h2 id="全连接法">全连接法</h2>
<p>设所有点之间的权重都大于0，可以选择不同的核函数来定义边的权重，常用的如多项式核函数，高斯核函数， Sigmod核函数。 最常用的为高斯核函数RBF，将两点之间的距离带入高斯核函数RBF中，即：
$$
w_{i j}=w_{ji}=s_{i j}=s_{ji}=\exp \left(-\frac{\left|x_{i}-x_{j}\right|_{2}^{2}}{2 \sigma^{2}}\right)
$$
其中，$sigma$为为函数的宽度参数 , 控制了函数的径向作用范围。</p>
<h1 id="基础3拉普拉斯矩阵">基础3：拉普拉斯矩阵</h1>
<p>拉普拉斯矩阵定义为$L = D-W$ 如上文所示，$D$为度矩阵，$W$为邻接矩阵。</p>
<p>拉普拉斯矩阵具有如下性质：</p>
<ol>
<li>
<p>$L$是对称阵 （因为$D$和$W$都是对称阵）</p>
</li>
<li>
<p>$L$的所有特征值都是实数 （因为$L$是对称阵）</p>
</li>
<li>
<p>对于任意向量$f$， 有$f^TLf = \displaystyle \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n w_{ij} (f_i-f_j)^2$</p>
<p>推导：
$$
\begin{aligned}
f^TLf &amp;= f^TDf-f^TWf\\
&amp;= \sum^n_{i = 1}d_if_i^2 - \sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij}\\
&amp;= \frac{1}{2}\left(\sum^n_{i=1}d_if_i^2 -2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum^n_{i=1}d_if_i^2\right)\\
&amp;由于d_i = \sum_{j = 1}^nw_{ij}, 将d_i带入上式得\\
f^TLf &amp;= \frac{1}{2} \left(\sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2-2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2\right)\\
&amp; = \frac{1}{2} \left(\sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2-2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum_{j = 1}^n\sum_{i =1}^n w_{ji}f_j^2\right)\\
&amp;= \frac{1}{2}\left(\sum_{i=1}^n\sum_{j = 1}^n w_{ij} (f_i-f_j)^2\right)
\end{aligned}
$$</p>
</li>
<li>
<p>拉普帕斯矩阵是半正定的，且$n$个实数特征值都$\geq$，即 $0=\lambda_1 \leq \lambda_2 \cdots \leq \lambda_n$，且最小的特征值为0。</p>
<p>证明，因为$f^TLf \geq 0$ 所以$L$半正定。</p>
</li>
</ol>
<h1 id="基础4无向图切图">基础4：无向图切图</h1>
<p>对于无向图$G$，目的是将图$G = (v,E)$切成相互没有连接的k个子图，每个子图是一个节点集合：$A_1,A_2,\cdots, A_k$，满足$A_i \cap A_j = \phi$ 且$A_1 \cup A_2 \cup \cdots \cup A_k = V$，对于两个节点集合$A ,B \subset V$, $A \cap B = \phi$，定义$A$,$B$之间的切图权重为：
$$
W(A,B) = \sum_{v_i\in A, v_j \in B} w_{ij}  \quad 表示A中节点到B中节点的权重和
$$
对于$k$个子图节点集合$A_1,A_2,\cdots, A_k$，定义切图$Cut$为：
$$
Cut(A_1,A_2, \cdots, A_k) = \frac{1}{2}\sum^k_{i = 1} W(A_i,\overline{A_i})
$$
其中$\overline{A_i}$是$A_i$的补集，也就是除$A_i$外其他所有节点的集合，$Cut$计算的是$A_i$中每个节点到$\overline{A_i}$中每个节点的权重总和，如果最小化$Cut$，就相当于最小化每个子集中节点到自己外节点的权重，但是会存在一个问题，如下图所示：</p>
<p><img loading="lazy" src="1.jpg" alt=""  />
</p>
<p>如果按左边那条线分割，可以保证有边子图到左边子图的权重最小，但不是最佳分割。</p>
<h1 id="谱聚类切图聚类">谱聚类：切图聚类</h1>
<p>为了避免上述效果不佳的情况，提供两种切图方式，1.RatioCut, 2.Ncut.</p>
<h2 id="ratiocut-切图">RatioCut 切图</h2>
<p>最小化$Cut(A_1,A_2, \cdots, A_k)$的同时，最大化每个子图中接待你的个数，即：A_{i}, \overline{A}_{i}</p>
<p>$$RatioCut\left(A_{1}, A_{2}, \ldots A_{k}\right)=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \overline{A_i}\right)}{\left|A_{i}\right|}$$</p>
<p>目标是最小化$RatioCut\left(A_{1}, A_{2}, \ldots A_{k}\right)$。</p>
<p>为此，我们引入一个<strong>指示向量（indicator vector）</strong>$h_j \in {h_1,h_2,\cdots, h_k}$，其中$j = 1,2,\cdots,k$，对于其中任意一个向量$h_j$，它是一个$n$维向量，即：
$$
h_j = (h_{1j},h_{2j}, \cdots, h_{nj})^T \\
h_{i j}=\left\{\begin{array}{ll}{0} &amp; {v_{i} \notin A_{j}} \\ {\frac{1}{\sqrt{\left|A_{j}\right|}}} &amp; {v_{i} \in A_{j}}\end{array}\right.
$$
$h_ij$表示节点$v_i$ 是否属于子图$A_j$, 如果属于，那么$h_{ij} = \frac{1}{\sqrt{\left|A_{j}\right|}}$，如果不属于，那么$h_{ij} = 0$。</p>
<p>那么对于$h_i^TLh_i$有：
$$
\begin{aligned}
h_i^T L h_i &amp;= \frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\
&amp;= \frac{1}{2}\left(\sum_{m\in A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2+\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2 + \sum_{m\notin A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2\right)\\
&amp; 上式中的mn是图中任意选取的两个不同的节点， 针对子图A_i及其对应的指示向量h_i,\\
&amp;任意选取的节点对有四种情况。其中 根据h_{ij}的定义，第一项和第四项为0，所以\\
&amp;=\frac{1}{2} \left(\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2 +
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2\right) \\
&amp;=\frac{1}{2} \left(\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(\frac{1}{\sqrt{\left|A_{i}\right|}})^2 +
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(-\frac{1}{\sqrt{\left|A_{i}\right|}})^2\right) \\
&amp;=\frac{1}{2}\left(\frac{1}{|A_i|}Cut(A_i,\overline{A_i}) + \frac{1}{|A_i|}Cut(A_i,\overline{A_i})\right)\\
&amp;=\frac{Cut(A_i,\overline{A_i})}{|A_i|} = RatioCut(A_i)
\end{aligned}
$$
上式$h_i^TLh_i$可以看做是子图$A_i$的RatioCut，那么：
$$
\begin{aligned}
RatioCut(A_1,A_2,\cdots,A_k) &amp;=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \overline{A_i}\right)}{\left|A_{i}\right|}  = \sum_{i = 1}^k \frac{Cut(A_i,\overline{A_i})}{|A_i|} \\
&amp;= \sum_{i=1}^k h_i^TLh^i = \sum_{i=1}^k (H^TLH)_{ii} = tr(H^TLH)
\end{aligned}
$$
每个$h_i^TLh^j$的值对应于矩阵$H^TLH$在位置$ij$处的值：
$$
H=(h_1,h_2,\cdots,h_k) \in \mathbb{R}^{n\times k}
$$
$$
h_i^TLh_j = (H^TLH)_{ij} \to h^T_iLh_i = (H^TLH)_{ii}
$$</p>
<p>由于$h_i\cdot h_j = 0, h_i \cdot h_i = 1$, 所以$H^TH = I$是一个单位矩阵</p>
<p>所以切图优化函数为：
$$
\underbrace{\arg \min }_{H} RatioCut\left(A_1,A_2,\cdots A_k\right) = \underbrace{\arg \min }_{H} \operatorname{tr}\left(H^{T} L H\right) \quad \text { s.t. } \quad H^{T} H=I
$$
$H$中的每个指示向量$h$是$n$维的，每个向量中的元素有两种取值，分别是0和$\frac{1}{\sqrt{\left|A_{j}\right|}}$， 所以每个$h$有$2^n$种可能性，所以整个$H$有$k2^n$中，因此上述目标函数是个NP-hard问题。</p>
<p>注意到$tr(H^TLH)$中每个优化子目标$h_i^TLh_i$，其中，$h$是单位正交基，基每个元素的平方和等于1，$L$为对称矩阵，<strong>此时$h^T_iLh_i$的最大值为$L$的最大特征值， $h^T_iLh_i$的最小值是$L$的最小特征值</strong>。在谱聚类中，我们的目标是找到目标函数的最小特征值，从而使目标函数最小，得到对应的特征向量。</p>
<p>对于$h^T_iLh_i$，目标是找到$L$最小的特征值，这个值就是$h^T_iLh_i$的最小值，对于$tr(H^TLH) = \sum^k_{i=1} h^T_iLh_i$，目标是找到$k$个最小的特征值，从而使$tr(H^TLH)$最小。</p>
<p>通过找到$L$最小的$k$个特征值，可以对应得到$k$个特征向量，这$k$个特征向量可以组成一个$n \times k$的矩阵，这个矩阵就是我们需要的指示向量矩阵$H$，里面包含了每个节点所属子图的信息。一般来说 我们需要对$H$按行做标准化：
$$
h_{ij}^* = \frac{h_{ij}}{\sqrt{\sum_{t=1}^kh^2_{it}}}
$$
注意到，$H$的每行是一个$k$维行向量，即$H_i = (H_{i1},H_{i2},\cdots, H_{ik})$， 表示节点$i$属于每个子图的指标值， 我们可以把$H_i$当做节点$v_i$的表示向量， 由于归一化后的$H$还不能明确指示各个样本的归属， 我们还需要对$H$代表的所有节点做一次传统聚类，如K-Means。</p>
<h2 id="ncut切图">NCut切图</h2>
<p>把$RatioCut$的分母从$|A_i|$换成$vol(A_i) = \sum_{j \in A_i}d_j$， 为$A_i$中所有节点的权重之和，一般来说$NCut$效果好于$RatioCut$:
$$
NCut(A_1,A_2,\cdots,A_k) = \frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\overline{A_i})}{vol(A_i)} = \sum^k_{i = 1}\frac{Cut(A_i)}{vol(A_i)}
$$
$NCut$指示向量$h$做了改进，$RatioCut$切图在指示向量中使用$\frac{1}{\sqrt{|A_i|}}$表示某节点归属于子图$A_i$，而$NCut$切图使用子图权重$\frac{1}{\sqrt{vol{A_i}}}$来表示某节点归属子图$A_i$如下：
$$
h_{i j}=\left\{\begin{array}{ll}{0} &amp; {v_{i} \notin A_{j}} \\ {\frac{1}{\sqrt{v o l\left(A_{j}\right)}}} &amp; {v_{i} \in A_{j}}\end{array}\right.
$$
上式表示如果节点$v_i$在子图$A_j$中，那么指示向量$h_j$的第$i$个元素为$\frac{1}{\sqrt{vol{A_j}}}$。</p>
<p>那么对于$h_i^TLh_i$有：
$$
h^T_iLh_i = \frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2 = \frac{Cut(A_i)}{vol(A_i)} =NCut(A_i)
$$
目标函数：
$$
NCut(A_i,A_2,\cdots,A_k) = \sum^k_{i = 1} NCut(A_i) = \sum^k_{i=1}h^T_iLh_i =\sum^k_{i=1}(H^TLH)_{ii} = tr(H^TLH)
$$
此时，$h_i \cdot h_j = 0$，$h_i\cdot h_i = \frac{|A_i|}{vol(A_i)} \neq 1$， 所以$H^TH \neq I$。</p>
<p>但是， 由于：$h^T_iDh_i = \sum^n_{j = 1} h_{ij}^2d_j$, $d_j$为节点$v_j$的权重和，$h_{ij}$的值表示节点j是否在子图$A_i$中，如果在子图$A_i$中，那么$h_{ij}^2 = \frac{1}{vol(A_i)}$，否则为0。</p>
<p>$$h^T_iDh_i = \frac{1}{vol(A_i)} \sum_{v_j \in A_i} d_j = \frac{1}{vol(A_i)} \cdot vol(A_i) = 1$$</p>
<p>最终目标函数为：
$$
\underbrace{\arg \min } _{H}\operatorname{tr}\left(H^{T} L H\right) \quad \text { s.t. }\quad H^{T} D H=I
$$
由于$H$中的指示向量$h$不是标准正交基，所以RatioCut中加粗的定理不能直接使用，所以，令$H = D^{-\frac{1}{2}}F$, $D^{-\frac{1}{2}}$表示对$D$对角线上元素开方后求逆，那么：
$$
H^TLH = F^TD^{-\frac{1}{2}}LD^{-\frac{1}{2}}F
$$</p>
<p>$$
H^TDH = F^TD^{-\frac{1}{2}}DD^{-\frac{1}{2}}F = F^TF=I
$$
所以目标函数转化为：
$$
\underbrace{\arg \min }_{F} \operatorname{tr}\left(F^{T} D^{-1 / 2} L D^{-1 / 2} F\right) \quad \text { s.t. } \quad F^{T} F=I
$$
同$RatioCut$，$D^{-1 / 2} L D^{-1 / 2}$是对称矩阵，$F$中的每个向量为标准正交基，只需要求出$D^{-1 / 2} L D^{-1 / 2}$的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，最后得到特征矩阵$F$，然后对$F$的行向量做一次传统聚类，如K-means.</p>
<p>一般来说， $D^{-1 / 2} L D^{-1 / 2}$相当于对拉普拉斯矩阵$L$做了一次标准化，即$\frac{L_{i j}}{\sqrt{d_{i} * d_{j}}}$.</p>
<p>我把本文整理成了<a href="/posts/2019-09-07-spectral-clustering/Spectral-Cluster.pdf">PDF</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>AAAI2017 M-NMF:《Community Preserving Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/m-nmf/</link>
      <pubDate>Wed, 29 May 2019 10:46:44 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/m-nmf/</guid>
      <description>AAAI2017 &amp;#34;Community Preserving Network Embedding&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14589">M-NMF</a></p>
<h1 id="introduction">Introduction</h1>
<p>Network Embedding的基本需求是保存网络机构和固有属性。而先前的方法主要保存了网络的围观结构（microscopic structure）如一阶相似度和二阶相似度（LINE，SDNE等）， 忽略了网络结构的介观社区结构（mesoscopic community structure）。针对这个问题，本文提出了一种模块化非负矩阵分解模型M-NMF，该模型将网络的社区结构融入network embedding中。利用节点表示和社区结构之间的一致关系（consensus relationship）， 联合优化基于非负矩阵分解的表示学习模型和基于模块化的社区发现模型， 这样就可以在节点表示中同时保存了<strong>微观网络结构</strong>和<strong>介观社区结构</strong>。</p>
<p>具体来说，对于微观结构（节点对相似性），本文使用矩阵分解来融合节点的一节相似性和二阶相似性；对于介观社区结构，通过模块度约束项来检测社区。因此，上面的两项可以通过节点的表示和基于辅助社区表示矩阵社区结构之间的一致关系来联合优化。同时，本文给出了乘法更新策略，保证了再推导参数的时候的正确性和收敛性。</p>
<h1 id="m-nmf-model">M-NMF Model</h1>
<p>对于一个无向图$G=(V,E)$, 图中有$n$个节点和$e$条边，用一个0,1二值邻接矩阵表示为$\mathbf{A}=[A_{i,j}] \in \mathbb{R}^{n \times n}$。 $\mathbf{U} \in \mathbb{R}^{n \times m}$ 为节点的表示矩阵，其中$m \leq n$，$m$是节点的嵌入维度。</p>
<h2 id="建模社区结构">建模社区结构</h2>
<p>本文采用基于模块最大化的社区发现方法 Modularity， 给定一个邻接矩阵表示的网络$\mathbf{A}$，$\mathbf{A}$包含两个社区，根据<a href="http://engr.case.edu/ray_soumya/mlrg/2006%20Modularity%20and%20community%20structure%20in%20networks.pdf">Newman 2006b</a>，模块度可以定义如下：
$$
Q=\frac{1}{4 e} \sum_{i j}\left(A_{i j}-\frac{k_{i} k_{j}}{2 e}\right) h_{i} h_{j}
$$
其中，$k_i$表示节点$i$的度，如果$i$在第一个社区中，那么$h_i=1$,否则$h_i = -1$。</p>
<p>$k_ik_j$表示将所有边一分为二 参考<a href="https://blog.csdn.net/wangyibo0201/article/details/52048248">模块度Q</a>，那么节点$i$,$j$之间可能产生的边数。$\frac{k_{i} k_{j}}{2 e}$如果所有边随机放置，节点$i$,$j$之间的期望边数。定义一个模块度矩阵$\mathbf{B} \in \mathbb{R}^{n \times n}$，其中$B_{i,j}=A_{i,j}-\frac{k_{i} k_{j}}{2 e}$，那么$Q=\frac{1}{4 e} \mathbf{h}^{T} \mathbf{B h}$，其中$\mathbf{h}=[h_i] \in \mathbb{R}^n$，表示社区成员指标器。</p>
<p>如果将$Q$拓展到$k &gt; 2$个社区，那么：
$$
Q=\operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B H}\right), \quad \text { s.t. } \quad \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{H}\right)=n
$$
其中$tr()$表示矩阵的迹（主对角线元素和），$\mathbf{H}$是社区成员指标器，$\mathbf{H} \in \mathbb{R}^{n \times k}$，每行表示一个节点所属社区的one-hot编码。</p>
<h2 id="建模微观结构">建模微观结构</h2>
<h3 id="一阶相似度">一阶相似度</h3>
<p>$$
S^{(1)} = \mathbf{A}
$$</p>
<h3 id="二阶相似度">二阶相似度</h3>
<p>表示为$S^{(2)}$，表示节点的邻域相似度， 用邻接向量的余弦相似度表示：
$$
S_{i j}^{(2)}=\frac{\mathcal{N}_{i} \mathcal{N}_{j}}{\left|\left|\mathcal{N}_{i}\right|\right|\left|\left|\mathcal{N}_{j}\right|\right|}
$$</p>
<p>结合网络的一阶结构的一阶二阶相似度，最终的网络相似度矩阵可以表示为：
$$
\mathbf{S}^{(1)}+\eta \mathbf{S}^{(2)}
$$
然后，文中引入了一个偏置矩阵$\mathbf{M} \in \mathbb{R}^{n \times m}$ 和一个非负表示矩阵$\mathbf{U} \in \mathbb{R}^{n \times m}$。所以微观结构的目标函数就是节点相似度和节点表示之间的误差：
$$
\min \left|\mathbf{S}-\mathbf{M} \mathbf{U}^{T}\right|_{F}^{2}
$$
因为$\mathbf{S} \in \mathbb{R}^{n \times n}$， 矩阵$\mathbf{U} \in \mathbb{R}^{n \times m}$，矩阵$\mathbf{M}$的作用是把$\mathbf{U}$转成$n \times n$，这样就可以计算损失函数了。</p>
<h2 id="统一的ne模型">统一的NE模型</h2>
<p>引入一个非负辅助矩阵$\mathbf{C} \in \mathbb{R}^{k \times m}$, 即为社区表示矩阵，每一行$C_r$表示第$r$个社区的$m$维表示向量。 如果一个节点的表示向量和一个社区的表示向量接近，那么这个节点就很可能在这个社区中。我们把节点$i$和社区$r$之间的从属关系定义为：
$$
\mathbf{U}_{i} \mathbf{C}_{r}
$$
如果两个向量正交，则$\mathbf{U}_{i} \mathbf{C}_{r} = 0$ 那么节点$i$不可能存在于社区$r$中。所以需要使$\mathbf{U}\mathbf{C}^T$更加近似社区指示器$\mathbf{H}$,所以定义如下目标函数：
$$
\begin{aligned}
\min_{\mathbf{M}, \mathbf{U}, \mathbf{H}, \mathbf{C}}\left|\left|\mathbf{S}-\mathbf{M} \mathbf{U}^{T}\right|\right|_{F}^{2}+\alpha\left|\left|\mathbf{H}-\mathbf{U} \mathbf{C}^{T}\right|\right|_{F}^{2}-\beta \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B} \mathbf{H}\right) \\
s.t., \quad \mathrm{M} \geqslant 0, \mathrm{U} \geqslant 0, \mathrm{H} \geqslant 0, \mathrm{C} \geqslant 0, \operatorname{tr}\left(\mathrm{H}^{T} \mathrm{H}\right)=n
\end{aligned}
$$
其中$\alpha$和$\beta$是正参数，最后一项是要最大化模块度。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>WSDM2016 《Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2019-05-12-embedding-ic/</link>
      <pubDate>Sun, 12 May 2019 22:09:38 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-05-12-embedding-ic/</guid>
      <description>WSDM2016 &amp;#34;Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>文章链接：<a href="https://dl.acm.org/citation.cfm?id=2835817">Embedding_IC</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。</p>
<p>对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：</p>
<ul>
<li>用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。</li>
<li>用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。</li>
<li>不同应用中的级联长度变化很大，难以学习和预测。</li>
</ul>
<p>本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：</p>
<ul>
<li>影响传播是二元的（被感染或不被感染），</li>
<li>扩散网络未知，</li>
<li>影响关系不依赖于传播的内容，</li>
<li>用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。</li>
</ul>
<p>本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：</p>
<p><img loading="lazy" src="1.png" alt=""  />
</p>
<h1 id="model">Model</h1>
<h2 id="notations">Notations</h2>
<p>传播事件集$\mathcal{D}={D_1,D_2,&hellip;,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。</p>
<p>给定一个社交网络，有$N$个用户：$\mathcal{U}={u_1,u_2,&hellip;,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \in \mathcal{U} \wedge t^D(u)&lt; \infty}$， 其中，$t^D: \mathcal{U} \to \mathbb{R}^+$ 表示用户被传染的时间戳， $\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \in \mathcal{U} | t^D(u)&lt;t}$。对称地，用$\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\infty)$表示最终所有被感染的用户，$\bar{D}(infty)$表示最终所有没被感染的用户。</p>
<h2 id="diffusion-model">Diffusion Model</h2>
<p>本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。</p>
<p>在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \in \mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \in \mathcal{U} \backslash I$:
$$
P(v|I) = 1-\prod_{u \in I}(1-P_{u,v})
$$
上式中，$\prod_{u \in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。</p>
<p>接下来就需要给出$P_{u,v}$的定义了，即$v$被$u$传染的概率。$z_u \in \mathbb{R}^d$是传染源用户$u$的表示向量，$\omega_{v} \in \mathbb{R}^d$是传染目标用户$v$的表示向量。那么$P_{u,v}$可以定义如下：
$$
P_{u,v} = f(z_u,\omega_{v})
$$
其中，$f: \mathbb{R}^d \times \mathbb{R}^d \to [0,1]$，是一个映射函数，把两个表示向量映射到概率空间：
$$
f\left(z_{u}, \omega_{v}\right)=\frac{1}{1+\exp \left(z_{u}^{(0)}+\omega_{v}^{(0)}+\sum_{i=1}^{d-1}\left(z_{u}^{(i)}-\omega_{v}^{(i)}\right)^{2}\right)}
$$
其中，$z_{u}^{(i)}$和$\omega_{v}^{(i)}$分别表示$z_u$和$\omega_v$的第$i$个分量。表示随距离增加而递减的传输概率，即$\left(z_{u}^{(i)}-\omega_{v}^{(i)}\right)$越大$f$越小。上式使用了sigmoid函数:$\frac{1}{1+e^{-x}}$返回一个$[0,1]$的概率。</p>
<p>值得注意的是，偏置项$z_{u}^{(0)}$和$\omega_{v}^{(0)}$的作用是反映$u$传入$v$的一般趋势，这样做的目的是避免不同的$u$和$v$产生相同的概率。</p>
<h2 id="learning-algorithm">Learning Algorithm</h2>
<p>考虑所有节点对的传播概率$\mathcal{P}={P_{u,v} | (u,v) \in \mathcal{U}^2}$ (涉及所有节点对)。那么对于特定级联$D$的概率为：
$$
P(D)=\prod_{v \in D(\infty)} P_{v}^{D} \prod_{v \in \overline{D}(\infty)}\left(1-P_{v}^{D}\right)
$$
上式中，$\prod_{v \in D(\infty)} P_{v}^{D}$表示$D$中所有被影响的用户存在的概率，$\prod_{v \in \overline{D}(\infty)}\left(1-P_{v}^{D}\right)$表示$D$中所有未被影响的用户存在的概率。所以$P(D)$就是级联$D$存在的概率。同时，可以用对数似然来表示训练级联集$\mathcal{D}$:
$$
\mathcal{L}(\mathcal{P} ; \mathcal{D})=\sum_{D \in \mathcal{D}}\left(\sum_{v \in D(\infty)} \log \left(P_{v}^{D}\right)+\sum_{v \in \overline{D}(\infty)} \log \left(1-P_{v}^{D}\right)\right)
$$
上式就是模型的目标函数。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Influence Maximization Conclusion</title>
      <link>https://JhuoW.github.io/posts/2019-05-06-im-conclusion/</link>
      <pubDate>Mon, 06 May 2019 20:16:17 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-05-06-im-conclusion/</guid>
      <description>&lt;h4 id=&#34;影响力传播模型&#34;&gt;影响力传播模型&lt;/h4&gt;
&lt;p&gt;社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;LC IT模型（独立级联模型和线性阈值模型）

WC（权重级联模型）

HD（热传播模型）

SIR（传染病模型）

MIA模型（路径相关）

投票模型

巴斯模型
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;影响力最大化算法&#34;&gt;影响力最大化算法&lt;/h4&gt;
&lt;p&gt;目前有的几个影响力最大化的算法&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A note on maximizing a submodular set function subject to a knapsack constraint
这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cost-effective outbreak detection in networks （CELF算法）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法）
这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于中心性的启发式算法&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Efficient influence maximization in social networks W.Chen （DegreeDiscount算法）
这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A potential-based node selection strategy for influence max- imization in a social network （TM算法）
这个算法也是一种启发式算法，它是以节点地潜在地影响力作为启发的因素去选择初始的节点，同时设置了一个参数c，分为两个阶段，第一个阶段启发式的选择潜在影响力最大的若干个节点，第二阶段用贪心算法对这些第一阶段选择的种子节点进行筛选，当参数c为0，也就是没有第一阶段，那么算法就变成了传统的贪心算法。算法的缺点也很明显，首先参数c的选择依赖于经验判断，其次是启发式，准确率没有保证，再次也需要使用贪心算法，因此时间复杂度很高&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h4 id="影响力传播模型">影响力传播模型</h4>
<p>社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择</p>
<pre tabindex="0"><code>LC IT模型（独立级联模型和线性阈值模型）

WC（权重级联模型）

HD（热传播模型）

SIR（传染病模型）

MIA模型（路径相关）

投票模型

巴斯模型
</code></pre><h4 id="影响力最大化算法">影响力最大化算法</h4>
<p>目前有的几个影响力最大化的算法</p>
<blockquote>
<ul>
<li>
<p>基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）</p>
<ul>
<li>
<p>A note on maximizing a submodular set function subject to a knapsack constraint
这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样</p>
</li>
<li>
<p>Cost-effective outbreak detection in networks （CELF算法）</p>
</li>
<li>
<p>Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法）
这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路</p>
</li>
</ul>
</li>
<li>
<p>基于中心性的启发式算法</p>
</li>
<li>
<p>Efficient influence maximization in social networks W.Chen （DegreeDiscount算法）
这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1</p>
<ul>
<li>
<p>A potential-based node selection strategy for influence max- imization in a social network （TM算法）
这个算法也是一种启发式算法，它是以节点地潜在地影响力作为启发的因素去选择初始的节点，同时设置了一个参数c，分为两个阶段，第一个阶段启发式的选择潜在影响力最大的若干个节点，第二阶段用贪心算法对这些第一阶段选择的种子节点进行筛选，当参数c为0，也就是没有第一阶段，那么算法就变成了传统的贪心算法。算法的缺点也很明显，首先参数c的选择依赖于经验判断，其次是启发式，准确率没有保证，再次也需要使用贪心算法，因此时间复杂度很高</p>
</li>
<li>
<p>A new centrality measure for influence maxi- mization in social networks
考虑到了传播模型中的传播度，利用这个进行启发式选择传播度最高的节点，从而得到更加精确的结果。这个传播度包括了两个方面，第一是节点自己影响他的邻居，第二个是他的邻居影响其他节点，结合影响概率一起构建的模型。优点在于启发式的选择的同时考虑了传播模型。</p>
</li>
</ul>
</li>
<li>
<p>基于影响路径的算法</p>
</li>
<li>
<p>Tractable models for information diffusion in social networks（SP1M算法）
基于影响路径的算法考虑某个节点只会尽可能地影响从这个节点开始地最短或者次最短路径上地节点，因此，可以递归地计算influenc spread而不用像贪心算法那样使用蒙特卡洛模拟，从而导致大量地计算时间，因此提高了算法地效率</p>
<ul>
<li>
<p>Scalable influence maximization for prevalent viral marketing in large-scale social networks （MIA算法）
借鉴了基于路径地影响力最大化算法的思路，提出一种利用局部图结构的树状近似算法来近似influence spread从而也是避免了蒙特卡罗模拟。此算法中，每条路径具有一个传播概率，定义为在这条路径上的每条边的传播概率的乘积。只有具有最大传播概率的路径才能够作为影响力路径来扩散影响力。同时给每一个节点计算树状度，定义为从节点出发的各条路径中，路径传播概率大于阈值 $\theta$的路径上的所有点的集合</p>
</li>
<li>
<p>Scalable and parallelizable processing of influence maximization for large-scale social networks （IPA算法）</p>
</li>
</ul>
</li>
</ul>
<p>该算法不同于chen等人提出的算法，认为每条路径是相互独立的，chen等人只选择了具有最大的propagation 概率的那条路径，但是本论文则选择所有大于阈值 $\theta$的路径，并行的计算他们的influence spread。基于路径的算法也具有缺点，比如没有理论上的准确度保证，同时，针对于特别复杂的图，空间复杂度非常大</p>
<ul>
<li>
<p>基于社区的算法</p>
<ul>
<li>Oasnet: an optimal allocation approach to in- fluence maximization in modular social networks（OASNET算法）</li>
</ul>
<p>这个算法假设社交网络划分社区后的每个社区是相互独立的，社区之间不会存在相互的影响力传播，利用CNM算法进行社区发现。种子节点的选择则分为两个阶段，第一个节点在每个社区内部利用贪心算法选择k个节点，第二个阶段则使用动态规划的方法在$C \times k$个节点中选择最终的k个节点</p>
<ul>
<li>Identifying influential nodes in complex networks with community structure</li>
</ul>
<p>这个算法基于利用社区结构发现社交网络中的最具有影响力的几个节点的研究。首先根据加权图构造概率转移矩阵，然后使用$K-Mediods$聚类方法找到最具有影响力的若干个节点。</p>
<ul>
<li>Cim: community-based influence maximization in social networks（CIM算法）</li>
</ul>
<p>chen等人基于HD（热传播）模型提出的基于社区结构的影响力最大化算法。算法分为好三个阶段，首先是社区发现，作者给出了一种$H_{Clustering}$算法用于社区发现，然后是候选节点迭代，作者根据节点的拓扑结构和它的社区特征进行选择，最后是种子节点的选择，同时考虑了诸多因素，个人认为是一个比较合理的影响力最大化算法。</p>
<ul>
<li>Conformity-aware influence maximization in online social networks （CINEMA算法）</li>
</ul>
<p>基于节点的一致性来设计的算法。传播模型中的概率定义为让第一个节点的影响力指标乘以第二个节点的一致性指标作为传播概率。</p>
<p>当然，基于社区发现的算法也有自己的缺点，首先是在社区内部进行初步的节点的选择，也需要进行蒙特卡洛模拟，因此时间复杂度也会比较大，其次，社区发现的思路，是用节点在社区内的influence spread去模拟它在whole network上的influence spread，近似效果依赖于网络结构，如果社区之间的连接边都比较少，那么近似结果是非常接近的，但是如果社区之间的连接边比较多，及即是hub节点比较多，那么近似效果可想而知</p>
</li>
</ul></blockquote>
<pre tabindex="0"><code>1.基于的贪心算法KK（kempe等学者提出的算法）
2.基于贪心算法的改进算法，利用启发式规则改进的NewGreedyIC，MixedGreedyIC，NewGreedyWC算法
	相关论文：Efficient influence maximization in social networks 2009
3.基于贪心算法的改进算法，利用子模性质改进的CELF算法，改进的CELF++算法
	相关论文：Cost-effective outbreak detection in networks 2007
			CELF++：optimizing the greedy algorithm for influence maximization in social networks 2011
4.启发式算法：随机算法，度中心算法，MaxDegree算法，Degree Discount算法
	相关论文：Efficient influence maximization in social networks
5.基于社区划分的算法：OASNET算法，CGA算法等（后面加上现阶段阅读的论文）
	相关论文：Community-based greedy algorithm for mining top-k influential nodes in mobile social networks 2010
6.MIA算法
混合式算法
</code></pre><h4 id="社区划分的算法">社区划分的算法</h4>
<blockquote>
<ul>
<li>基于模块优化的算法</li>
<li>光谱聚类的算法</li>
<li>层次分级的算法</li>
<li>基于标签传播的算法
<ul>
<li>LPA算法（目前的最快的社区划分算法，几乎是线性时间复杂度）</li>
</ul>
</li>
</ul></blockquote>
<h4 id="论文思考的几个点">论文思考的几个点</h4>
<p>基于社区发现的影响力最大化算法的分析，论文研究的目的：</p>
<ol>
<li>算法的效率保证，时间复杂度尽可能低。</li>
<li>算法的性能保证，尽可能接近最优解。（利用到子模拟性质？）</li>
</ol>
<p>毕业论文的大致的框架，总体是基于社区划分的思路，具体需要做的工作如下：</p>
<blockquote>
<ul>
<li>
<p><strong>传播模型的选择</strong>，如何改进传播模型使得切合实际的传播过程？IC，LT改进？结合PageRank算法或者思想？或者考虑改进HDM传播模型？引入时间空间的因素使得模型更加充分？</p>
<p>（1）传播模型的改进，传播模型中，针对于某个节点的从邻居获得的影响力，不应该简单的直接叠加，而是考虑每个邻居并不是等同对待的，应当区分不同的权重，针对节点之间的互动频率，互动频率高的节点，应该具有更加高的信任度，同时，也可能存在负面的影响力，即反而让节点更不可能选择新产品，这点应该在改进的模型中有所反馈。</p>
<p>（2）至于这个信任程度如何计算出来反应在传播模型中，则可以考虑，根据邻居之间的互动信息，每个节点的活跃程度，邻居节点本身是否是具有很高的度的节点，邻居节点和本节点的观点是正相关还是反相关，从而决定邻居对本节点的信任度。</p>
<p>（3）应该考虑影响力的时效性，是否可以考虑结合HDM和LTM模型一起，加上信任度参数这个观点，一起构建一个新的传播模型。</p>
</li>
<li>
<p><strong>社区发现算法的选择</strong>，社区发现的选择是非常重点的，社区发现本质上是社交网络节点的聚类，应该涉及比较有效率的聚类算法或者选择其他的距离算法？</p>
<p>社区发现聚类算法，一般都是先设置每个节点作为单独的一个社区，然后进行合并，在进行社区聚类发现的时候，不应当单独仅仅考虑边，仅仅利用边的关系，比如CGA算法就利用到了传播模型，结合传播模型进行标签传播，然后获得相应的划分的社区。同时，可以加以改进的地方，比如，社交网络的社区发现不应当仅仅考虑到拓扑结构，还有考虑节点之间的互动交流的信息，互动程度越频繁，那么两个节点在一个社区内部的概率就越大，因此要考虑这个改进点。</p>
<p>同时，借鉴了CGA算法的思想，一个节点的社区内部的影响力和整个社交网络的影响力如何区别？如何用社区内部的影响力去近似？或者考虑hub节点，社区之间的这些连接节点也有着非常重要的作用。</p>
</li>
<li>
<p><strong>社区发现是否可以处理重叠社区的情况</strong>，重叠社区会导致影响力的重复传播，如何减少这种情况的出现，如何设计算法实现重叠社区的处理？</p>
</li>
<li>
<p><strong>各个社区的重要性也是不同的，应该有选择的摒弃一些社区，先给出一个社区选择的模型，比如说利用PageRank先计算出哪些社区比较重要</strong>，有的社区人数多，有的社区人数少，但是处在中心位置，并且一些非重要的社区，往往会关注这些重要社区的传递出来的信息。考虑种子节点选择的时候，应当把社区这些因素考虑进去。我们可以忽略那些不重要的小社区，重要的社区给与比较大的加权值，同时注意影响力避免重叠传播。</p>
</li>
<li>
<p><strong>社区发现之后，如何分配每个社区的种子节点数目？</strong>，直接按照比例分配？亦或是选择一种度量社区重要程度的模型？</p>
<p>CGA算法，使用了动态规划进行贪心选择，在各个社区内部选择相应的种子节点。但是时间复杂度仍然是非常大的，是否可以考虑先在社区内部基于启发式规则，或者PageRank，计算重要的节点，然后全局进行贪心的选择？</p>
</li>
<li>
<p><strong>基于社区发现的算法，实际上是利用节点在社区内的传播来近似它在整个网络上传播的效果</strong>，因此这种近似肯定存在误差，如何减少这种误差的产生？而且这种误差还是和网络中的社区结构有关系的。</p>
</li>
<li>
<p><strong>注意充分利用社区结构的特点，划分社区之后，把社区也视为一个点，作为整体去考虑</strong></p>
</li>
<li>
<p><strong>社区内的候选节点选择</strong>，候选节点应该按照什么标准进行选择，是按照启发式的度选择？还是设计另外的模型？结合PageRank模型？</p>
</li>
<li>
<p><strong>种子节点的获取策略</strong>，如何在这些候选节点上选择出最终的种子节点？直接按照贪心策略暴力选择还是参考CELF算法进行选择？或者是涉及其他的方法？</p>
</li>
</ul></blockquote>
<h4 id="思路总结">思路总结</h4>
<blockquote>
<p>基于社区发现的影响力最大化算法框架：</p>
<ul>
<li>社区划分。</li>
<li>充分考虑社区作为一个整体性，来体现社区的一个作用，可以利用PageRank模型，来代表社区的重要程度，这个是社区的一个属性，利用这个模型，选择一些重要的社区，同时摒弃一些小的，没那么重要的社区。而且利用PageRank进行迭代，应该比传统的算法会快一些。</li>
<li>社区内部种子节点的选择，考虑的是社区内部的种子节点在社区内部的影响力传播。如何选择社区内部的种子节点。？？？？？这一点目前还需要多家考虑。</li>
<li>社区出现重叠，如何考虑？</li>
<li>种子节点在社区内的影响力只是对种子节点在全局的影响力的近似，那么需要一种方式来弥补这种误差。显然，种子节点的影响力如果想要传播到另外的社区，那么是通过社区之间的边界节点进行传播的，同时和社区本身的重要性有关，那么这部分的误差通过边界节点来弥补。</li>
<li>最终会得到若干的候选节点，这些节点使用贪心算法进行选择出最终的种子节点，考虑贪心算法的时间复杂度，那么可以考虑使用CELF思路或者是其他的CGA这类的动态规划思路去求解，不过这还是基于蒙特卡罗模拟，时间复杂度仍然相对比较大，对算法进行加速。</li>
<li>影响力传播模型，基于线性阈值模型进行改变，加上信任度参数，因为每个影响力的叠加不是平权的，和用户之间的互动，观点信息，兴趣爱好是否一直存在着相关的关系，因此在影响力传播模型中加入这个考虑因素。</li>
</ul></blockquote>
<h4 id="算法的实验部分考虑一些经典的baseline-algorithm">算法的实验部分，考虑一些经典的BaseLine Algorithm</h4>
<blockquote>
<ul>
<li>传统贪心爬山算法-KK算法</li>
<li>CELF算法</li>
<li>CGA算法</li>
<li>CIM算法</li>
<li>启发式算法（度启发式，中心性启发式）DegreeDiscount算法</li>
<li>本论文的算法</li>
</ul></blockquote>
<blockquote>
<ul>
<li>算法中参数的选择的影响</li>
<li>控制变量对比实验</li>
</ul></blockquote>
<h4 id="相关工作总结">相关工作总结</h4>
<blockquote>
<ol>
<li>
<p>Richardson和Domingos在2002年的论文，首次把这个问题作为一个研究方向提出。</p>
</li>
<li>
<p>首先应该数说到的式Kempe的2003年的论文，主要提出了</p>
</li>
</ol>
<p>a）LT IC 模型，并说明了这个问题的NP完全性</p>
<p>b）给出了贪心算法</p>
<p>c）说明了影响力递增的边界递减性质，利用子模性质说明了算法的性能保证</p></blockquote>
<h4 id="近期论文阅读总结">近期论文阅读总结</h4>
<ol>
<li>
<p>传播模型的改进，基于PageRank的改进，传统PageRank在考虑某个节点的PR值是均匀分配给链出的节点的（链出的概率为出度的倒数）（即权重级联模型），但是实际上，PR高的节点具有更高的影响力，因此考虑链出的概率不用度，而用PR值的占比，从而更加切合实际的情况。</p>
</li>
<li>
<p>还有的改进算法，改进了PageRank计算模型，把节点自身的属性，节点之间互动的属性，加入到了PageRank模型计算中，使得PageRank能够适用于社交网络中节点重要性的计算。</p>
</li>
<li>
<p>我们可以考虑把以上的两者结合起来给出一种新的信息传播模型（给出概率计算的方法）。</p>
</li>
<li>
<p>数据集选择</p>
<blockquote>
<p>参考宫秀云那篇文章</p></blockquote>
</li>
<li>
<p>总结：基于PageRank思想的影响力计算，都是在PageRank的基础上进行模型的改进，加入其他的影响因子，给出不同的权重，从而更加符合实际的应用场景。</p>
</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>社交网络影响最大化（Influence Maximization）中的IC，LT模型</title>
      <link>https://JhuoW.github.io/posts/2019-03-20-ic-lt/</link>
      <pubDate>Wed, 20 Mar 2019 21:34:08 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-03-20-ic-lt/</guid>
      <description>&lt;h1 id=&#34;the-independent-cascade-model-ic-model&#34;&gt;The Independent Cascade Model (IC Model)&lt;/h1&gt;
&lt;p&gt;IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。&lt;br&gt;
在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。&lt;br&gt;
值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。&lt;/p&gt;
&lt;h1 id=&#34;the-linear-threshold-model-lt-model&#34;&gt;The Linear Threshold Model (LT Model)&lt;/h1&gt;
&lt;p&gt;LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。&lt;br&gt;
形式上， 在图$G$中每条边$e=(u,v) \in E$有一个权重$b_{u,v}$。 我们定义$\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\sum_{u \in \mathcal{N}_I (v)} b_{u,v} \leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\theta_v$。 LT模型首先为每个节点$v$的阈值$\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="the-independent-cascade-model-ic-model">The Independent Cascade Model (IC Model)</h1>
<p>IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。<br>
在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。<br>
值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。</p>
<h1 id="the-linear-threshold-model-lt-model">The Linear Threshold Model (LT Model)</h1>
<p>LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。<br>
形式上， 在图$G$中每条边$e=(u,v) \in E$有一个权重$b_{u,v}$。 我们定义$\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\sum_{u \in \mathcal{N}_I (v)} b_{u,v} \leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\theta_v$。 LT模型首先为每个节点$v$的阈值$\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>SIGIR18 《BiNE:Bipartite Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/bine/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/bine/</guid>
      <description>SIGIR2018 &amp;#34;BiNE:Bipartite Network Embedding&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.comp.nus.edu.sg/~xiangnan/papers/sigir18-bipartiteNE.pdf">BiNE</a></p>
<h1 id="introduction">Introduction</h1>
<p><strong>Bipartite Network(二分网络)</strong>:如下图所示：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/stru.png#center" alt="你想输入的替代文字"  />
<br>
二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。</p>
<p>另一个问题，<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/r1.png#center" alt="你想输入的替代文字"  />
 <img loading="lazy" src="/posts/2019-03-13-BiNE/r2.png#center" alt="你想输入的替代文字"  />
<br>
如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。<br>
另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。</p>
<p>针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过<a href="http://www.cs.cornell.edu/home/kleinber/auth.pdf">HITS</a>来衡量。</p>
<h1 id="model">Model</h1>
<p>如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\overrightarrow{u_i}]$, $V=[\overrightarrow{v_i}]$，结构如下图所示：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/3.png" alt="你想输入的替代文字"  />
（取自作者的讲解ppt)</p>
<h2 id="explicit-relations">Explicit Relations</h2>
<p>同LINE一样， 基于直接连接的目标函数表示为：<br>
$$\mathrm{minimize} \quad O_1=-\sum_{e_{ij} \in E}w_{ij}\log \hat{P}(i,j)$$</p>
<h2 id="implicit-relations">Implicit Relations</h2>
<h3 id="构造随机游走序列">构造随机游走序列</h3>
<p>这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：<br>
$$w^U_{ij}=\sum_{k \in V}w_{ik}w_{jk}$$<br>
$$w^V_{ij}=\sum_{k \in U}w_{ki}w_{kj}$$<br>
其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。
<img loading="lazy" src="/posts/2019-03-13-BiNE/Al1.png" alt="你想输入的替代文字"  />
<br>
其中$l=\max(H(v_i)\times \max T,\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。<br>
$$D_{v_i}=\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$<br>
表示其中一次随机游走的节点集合$p$表示停止概率。</p>
<p>通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。</p>
<h3 id="对间接关系建模">对间接关系建模</h3>
<p>如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。
<img loading="lazy" src="/posts/2019-03-13-BiNE/dd.png#center" alt="你想输入的替代文字"  />
<br>
对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \in C_s(u_i)$的条件概率。所以目标函数如下：
$$\mathrm{maximize} \quad O_2 = \prod_{u_i \in S \land S \in D^U} \prod_{u_c \in C_s(u_i)}P(u_c|u_i)$$<br>
对于$D^V$同理。其中,$p(u_c|u_i) = \frac{\exp(\overrightarrow{u}_i^T \overrightarrow{\theta}_c)}{\sum^{|U|}_{k=1} \exp(\overrightarrow{u}_i^T \overrightarrow{\theta}_k))}$。</p>
<h3 id="negative-sampling">Negative Sampling</h3>
<p>本文的负采样方法是基于局部敏感哈希（LSH）来对与中心节点不相似的节点进行采样。
该方法的strategy是，给定一个中心节点，随机选取一个bucket（序列）并且这个序列不包含给定的中心节点，以此来获得和给定节点尽量不相似的负采样节点。<br>
$N^{ns}_S (u_i)$ 表示$ns$个负采样节点，对于中心节点$u_i$, 那么上文提到的条件概率$p(u_c|u_i)$可以被定义为下式：<br>
$$p(u_c,N^{ns}_S (u_i)|u_i) = \prod_{z \in {u_c} \cup N^{ns}_S (u_i)} P(z|u_i)$$<br>
其中条件概率$P(z|u_i)$定义为：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/4.png#center" alt="你想输入的替代文字"  />
<br>
其中$\sigma$表示sigmoid函数，这样就减少了上文softmax函数造成的计算量过大的问题。</p>
<h2 id="联合优化">联合优化</h2>
<p>通过随机梯度上升对3部分损失函数进行加权优化：<br>
$$\mathrm{maximize} \quad L = \alpha \log O_2+\beta \log O_3 - \gamma O_1$$
最终BiNE的整体算法流程如下：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/Al2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="conclusion">Conclusion</h1>
<p>这篇文章提出的分布式训练以及负采样策略还是很值得学习的。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>SIGIR2018 《HTNE Embedding Temporal Network via Neighborhood Formation》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/htne/</link>
      <pubDate>Sun, 17 Feb 2019 22:01:15 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/htne/</guid>
      <description>&lt;p&gt;论文地址：&lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3220054&#34;&gt;HTNE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://JhuoW.github.io/posts/2019-01-17-HTNE/Fig1.png&#34; alt=&#34;你想输入的替代文字&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。&lt;/p&gt;
&lt;p&gt;因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。&lt;/p&gt;
&lt;p&gt;通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。&lt;/p&gt;
&lt;p&gt;另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。&lt;/p&gt;
&lt;p&gt;值得注意的是，本文目标是优化邻域生成序列的极大似然估计即&lt;strong&gt;条件强度函数&lt;/strong&gt;（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;本文通过跟踪节点邻域的形成来捕获网络的形成过程。&lt;br&gt;
&lt;strong&gt;Definition 1&lt;/strong&gt; : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \in E$ 被表示为按时间顺序的时间序列，例如， $\mathbf{a}_{x,y}={a_1\to{a_2}\to{…}}\subset\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。&lt;/p&gt;
&lt;p&gt;因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 2&lt;/strong&gt; : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2&amp;hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\to(y_2,t_2)\to&amp;hellip;\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。&lt;/p&gt;
&lt;h3 id=&#34;hawkes-process&#34;&gt;Hawkes Process&lt;/h3&gt;
&lt;p&gt;点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。&lt;br&gt;
对于一个给定的节点$x \in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：&lt;br&gt;
$$ \tilde{\lambda}_{y|x}(t)=\mu_{x,y}+\sum_{t_h&amp;lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$$&lt;br&gt;
其中，$\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\sum_{t_h&amp;lt;t}$表示遍历t时刻前$x$的所有邻居。$\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：&lt;br&gt;
$$\kappa(t-t_h)=\exp(-\delta_s(t-t_h))$$&lt;br&gt;
其中，减少率 $\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\delta_s(t-t_h)$越大, $\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。&lt;br&gt;
综上所述，$\kappa$的具体意义是随时间衰减的影响，其中$\delta_s$参数表示对于不同的源节点，影响是不同的。&lt;/p&gt;
&lt;p&gt;如果$\tilde{\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。&lt;/p&gt;
&lt;p&gt;直观的来看，基本率（base rate）$\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\mu_{x,y}=f(\mathbf{e}_x,\mathbf{e}_y)=-||\mathbf{e}_x-\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\alpha_{h,y}=f(\mathbf{e}_h,\mathbf{e}_y)=-||\mathbf{e}_h-\mathbf{e}_y||^2$。&lt;br&gt;
因为条件强度函数必须为正，所以使用如下公式: $\lambda_{y|x}(t)=\exp(\tilde\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$. 这就与传统的NE方法差不多了。。。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://dl.acm.org/citation.cfm?id=3220054">HTNE</a></p>
<h2 id="introduction">Introduction</h2>
<p>本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。</p>
<p><img loading="lazy" src="/posts/2019-01-17-HTNE/Fig1.png" alt="你想输入的替代文字"  />
</p>
<p>另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。</p>
<p>因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。</p>
<p>通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。</p>
<p>另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。</p>
<p>值得注意的是，本文目标是优化邻域生成序列的极大似然估计即<strong>条件强度函数</strong>（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数</p>
<h2 id="model">Model</h2>
<h3 id="definition">Definition</h3>
<p>本文通过跟踪节点邻域的形成来捕获网络的形成过程。<br>
<strong>Definition 1</strong> : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \in E$ 被表示为按时间顺序的时间序列，例如， $\mathbf{a}_{x,y}={a_1\to{a_2}\to{…}}\subset\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。</p>
<p>因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。</p>
<p><strong>Definition 2</strong> : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2&hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\to(y_2,t_2)\to&hellip;\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。</p>
<h3 id="hawkes-process">Hawkes Process</h3>
<p>点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。<br>
对于一个给定的节点$x \in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：<br>
$$ \tilde{\lambda}_{y|x}(t)=\mu_{x,y}+\sum_{t_h&lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$$<br>
其中，$\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\sum_{t_h&lt;t}$表示遍历t时刻前$x$的所有邻居。$\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：<br>
$$\kappa(t-t_h)=\exp(-\delta_s(t-t_h))$$<br>
其中，减少率 $\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\delta_s(t-t_h)$越大, $\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。<br>
综上所述，$\kappa$的具体意义是随时间衰减的影响，其中$\delta_s$参数表示对于不同的源节点，影响是不同的。</p>
<p>如果$\tilde{\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。</p>
<p>直观的来看，基本率（base rate）$\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\mu_{x,y}=f(\mathbf{e}_x,\mathbf{e}_y)=-||\mathbf{e}_x-\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\alpha_{h,y}=f(\mathbf{e}_h,\mathbf{e}_y)=-||\mathbf{e}_h-\mathbf{e}_y||^2$。<br>
因为条件强度函数必须为正，所以使用如下公式: $\lambda_{y|x}(t)=\exp(\tilde\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$. 这就与传统的NE方法差不多了。。。</p>
<h3 id="attention">Attention</h3>
<p>根据论文中（3）式，可以看出，$\sum_{t_h&lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$这一部分主要描述了历史邻居对当前邻居的影响，但是完全忽略了源节点$x$，因为源节点$x$的变化也会影响到历史邻居对当前邻居的亲近程度(affinity)。因此，本文引入了<strong>attention model</strong>。as follows：<br>
$$w_{h,x} = \frac{\exp(-||\mathbf{e}_x-\mathbf{e}_h||^2)}{\sum_{h&rsquo;}{\exp(-||\mathbf{e}_x-\mathbf{e}_{h&rsquo;}||^2)}}$$<br>
这是一个softmax函数 来根据源节点$x$的不同为它的邻居赋予不同权重。</p>
<p>最后， 历史邻居与当前邻居的连接紧密程度可以表示为:
$$\alpha_{h,y}=w_{h,x}f(\mathbf{e}_h,\mathbf{e}_y)$$</p>
<h3 id="optimization">Optimization</h3>
<p>目标函数即为给定节点$x$以及基于邻域形成序列的霍克斯过程, 生成节点$y$的条件概率。 公式如下：
$$p(y|x, \mathcal{H}_x(t)) = \frac{\lambda_{y|x}(t)}{\sum_{y&rsquo;}{\lambda_{y&rsquo;|x}(t)}}$$
目标函数即为所有节点对的极大似然：
$$\log \mathcal{L}=\sum_{x\in{\mathcal{V}}}{\sum_{y\in{\mathcal{H}_x}}}{\log{p(y|x,\mathcal{H}(t))}}$$</p>
<p>最后，由于softmax过程是calculating expensive，所以采用负采样优化损失函数。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>AAAI2018 《GraphGAN:Graph Representation Learning with Generative Adversarial Nets》Reading Notes</title>
      <link>https://JhuoW.github.io/posts/graphgan/</link>
      <pubDate>Tue, 22 Jan 2019 16:57:05 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/graphgan/</guid>
      <description>AAAI2018 &amp;#34;GraphGAN:Graph Representation Learning with Generative Adversarial Nets&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址:<a href="https://arxiv.org/abs/1711.08267">GraphGAN</a></p>
<h2 id="introduction">Introduction</h2>
<p>GAN是CV上非常火的方法，作者将其引入Network Embedding领域。为了达到这个目的，首先，作者将当前的NE model分成了两类，分别是生成模型(generative model)和判别模型(discriminative model)。简单来说，生成模型。<br>
所谓生成模型，就是对于每个节点$v_c$，都存在一个潜在的真实连接分布$p_{true}(v|v_c)$, 这个分布反映了由条件生成样本的概率，因此，生成式模型的目的就是求网络中边的极大似然。比较经典的方法有DeepWalk，是通过随机游走来获得center node的上下文，然后最大化上下文节点的似然。Node2vec拓展了DeepWalk,提出了biased随机游走，使得模型在为给定节点生成上下文节点时具有更多的灵活性。<br>
所谓判别模型，目的是学习一个分类器来直接预测边是否存在。将两个节点视为输入特征，然后输出预测的两个节点之间有边的概率，即$p(edge|(v_i,v_j))$,常见的方法有SDNE，PPNE。</p>
<p>于是，本文结合生成对抗网络(GAN),提出了GraphGAN来同一生成和判别器。其中生成器$G(v|v_c)$试图拟合真实的连接概率分布$p_{true}(v|v_c)$,生成最可能和$v_c$有链接的点。判别器$D(v,v_c)$尝试区分强连接节点对和若连接节点对，然后计算$v$和$v_c$存在边的可能性。简单来说，就是把生成器生成的可能与$v_c$有边的节点放入判别器计算两者有边的概率。</p>
<p>除此之外GAN框架下，然而生成器的连接分布是通过softmax实现的，但是传统的softmax无法适配生成器，原因如下:<br>
(1).传统的softmax对网络中的所有节点一视同仁，缺乏对网络结构相似度信息的考虑。<br>
(2).计算成本太高。<br>
因此，论文中提出了新的生成器实现方法 Graph Softmax。并且证明GS具有归一化（normalization）、网络结构感知（graph structure awareness）和高计算效率（computational efficiency）的性质。相应的，文中也提出了基于随机游走(random walk)的生成器策略。</p>
<h2 id="model">Model</h2>
<p>这里挑特别的来说。$\mathcal{N}(v_c)$表示和$v_c$直接相连的节点集。我们把给定节点$v_c$与任意节点$v \in \mathcal{V}$的真实连接分布表示为$p_{true}(v|v_c)$。这么看$\mathcal{N}(v_c)$可以看做从$p_{true}(v|v_c)$中采样的集合。文章的目的是学习两个模型:<br>
<strong>Generator</strong> $G(v|v_c;\theta_G)$ 生成器，尝试拟合真实连接分布$p_{true}(v|v_c)$，并且从节点集$\mathcal{V}$中生成最有可能和$v_c$相连的节点。<br>
<strong>Discriminator</strong> $D(v,v_c;\theta_G)$ 判别器，目的是判断已对接点$(v,v_c)$的连接性。$D(v,v_c;\theta_G)$输出$v$和$v_c$有边的概率。</p>
<p>生成器$G$尝试完美拟合$p_{true}(v|v_c)$，并且生成和$v_c$真实邻居高度相似的节点来欺骗判别器。相反，判别器尝试发现输入的顶点是真实的节点对或者有生成器生成出的样本。这是一个极大极小游戏，目标函数$V(G,D)$:<br>
$$\min_{\theta_G} \max_{\theta_D} V(G,D)=\sum_{c=1}^V (\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]+\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))])$$<br>
上面这个公式是本文最关键的公式，以我的分析就是：在给定$\theta_D$的情况下，对其最小化。先来分析$\max_{\theta_D}V(G,D)$,即给定$\theta_G$,使原式最大。当给定$\theta_G$时，通过改变$\theta_D$,使$\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]$达到最大（是判别器D拟合真实分布），同时使$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$最大，即尽可能减小选定节点$v_c$与判别器中生成的$v_c$邻居节点与$v_c$连接的可能性。然后在给定$\theta_D$的情况下，通过改变生成器$\theta_G$继续生成节点，使得$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$尽可能小，即上一步的判别器尽可能把当前生成器生成的节点认为是邻居。 之后再更新判别器,以此类推。这样$G$和$D$各自提高性能，最终$G$生成的分布和真实连接分布无法区分，这样达到最好的效果。整体过程如下图所示:</p>
<p><img loading="lazy" src="/posts/2019-01-22-GraphGAN/1.png" alt="你想输入的替代文字"  />
</p>
<h3 id="discriminator-optimization">Discriminator Optimization</h3>
<p>对于判别器来说是个简单的sigmoid函数，来计算两个输入节点的表示向量:<br>
$$D(v,v_c;\theta_D)=\frac{1}{1+\exp(-d^\top_v d_{v_c})}$$<br>
其中，$d_v$,$d_{v_c}$是两个输入节点关于判别器$D$的表示向量，$\theta_D$是所有节点表示向量的结合。注意到上面的公式只涉及$v$和$v_c$, 我们只需要更新$d_v$,$d_{v_c}$，通过梯度下降的方法可以实现：<br>
<img loading="lazy" src="/posts/2019-01-22-GraphGAN/2.png" alt="你想输入的替代文字"  />
</p>
<h3 id="generator-optimization">Generator Optimization</h3>
<p>对于生成器来说，目标是最小化判别器将生成的样本判断为负样本的对数似然（概率）。换句话说，生成器会调整自己的连接分布（通过调整生成的所有节点的向量表示$\theta_G$）来提升对生成样本的判别分数。由于$v$的采样时离散的，所以使用policy gradient来计算$V(G,D)$关于$\theta_G$的梯度：<br>
<img loading="lazy" src="/posts/2019-01-22-GraphGAN/3.png" alt="你想输入的替代文字"  />
<br>
为了理解上述公式，注意到$\nabla_{\theta_G}V(G,D)$是一个由$\log(1-D(v,v_c;\theta_D))$加权的梯度$\nabla_{\theta_G}\log G(v|v_c;\theta_G)$的求和，直观上说，这意味着有高概率是负样本的节点会将生成器G“拉着”远离自己（因为我们在$\theta_G$上执行梯度下降)。</p>
<h3 id="graph-softmax">Graph Softmax</h3>
<p>graph softmax的核心思想是定义一种新的计算连接性概率的方式，满足以下性质:</p>
<ul>
<li>归一化：$\sum_{v \neq v_c;\theta_G}=1$。</li>
<li>图结构感知：生成器充分利用网络中的结构信息，来估计真实连接分布，如果两个节点在图上越远，那么他们间有边的概率越小。</li>
<li>高效的计算：和传统的softmax不同，$G(v|v_c;\theta_G)$的计算应只涉及图中的一小部分点。</li>
</ul>
<p>因此，本文以图中节点$v_c$为例，以$v_c$为根构建一棵BFS树$T_c$。$\mathcal{N}_c(v)$为节点$v$在$T_c$上的邻居集合（包括他的父节点和所有子节点）。对于一个given vertex $v$和它的一个邻居$v_i \in \mathcal{N}_c(v)$,定义概率为:<br>
$$p_c(v_i|v)=\frac{\exp (g_{v_i}^\top g_v)}{\sum_{v_i \in \mathcal{N}_c(v)} \exp(g_{v_j}^\top g_v)}$$<br>
这是一个在$\mathcal{N}_c(v)$上的softmax函数。</p>
<p>为了计算$G(v|v_c;\theta_G)$,注意到在$T_c$上，根节点$v_c$到每个节点$v$都有一条唯一的路径， 把这条路径记为$P_{v_c \to v}=(v_{r_0},v_{r_1},&hellip;,v_{r_m})$,其中$v_{r_0}=v_c$, $v_{r_m}=v$,那么在graph softmax中，将$G(v|v_c;\theta_G)$定义为:<br>
$$G(v|v_c;\theta_G)\triangleq (\prod^m_{j=1} p_c(v_{r_j}|v_{r_{j-1}})) \cdot p_c(v_{r_{m-1}}|v_{r_m})$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>CIKM2018 《Enhanced Network Embeddings via Exploiting Edge Labels》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2019-01-22-ne-edge-labels/</link>
      <pubDate>Tue, 22 Jan 2019 11:02:29 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-01-22-ne-edge-labels/</guid>
      <description>CIKM2018 &amp;#34;Enhanced Network Embeddings via Exploiting Edge Labels&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址: <a href="https://arxiv.org/abs/1809.05124?context=physics.soc-ph">Enhanced Network Embeddings via Exploiting Edge Labels</a></p>
<h2 id="introduction">Introduction</h2>
<p>这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。</p>
<p>具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个<strong>半监督NE模型</strong>。</p>
<h2 id="problem-definition">Problem Definition</h2>
<p>假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,&hellip;,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \subseteq L$。目的还是一样，学习一个映射函数$\Phi \to \mathbb{R}^{|V| \times d}$, 其中$d \ll |V|$。</p>
<h2 id="method">Method</h2>
<p>首先定义损失函数:<br>
$$\mathcal{L}=(1-\lambda)\mathcal{L}_s+\lambda\mathcal{L}_r$$<br>
其中$\mathcal{L}_s$表示预测节点邻域的损失。$\mathcal{L}_r$表示预测边label的损失。$\lambda$是两种损失的权重。</p>
<h3 id="structural-loss">Structural Loss</h3>
<p>第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：<br>
$$\mathcal{L}_s=-\sum_{u \in C(v)} \log Pr(u|v)$$<br>
这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：<br>
$$Pr(u|v)=\frac{\exp(\Phi(u) \cdot \Phi&rsquo;(v))}{\sum_{u&rsquo; \in V} \exp(\Phi(u&rsquo;) \cdot \Phi&rsquo;(v))}$$<br>
这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\Phi(v)$和$\Phi&rsquo;(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。<br>
剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：<br>
$$C(v)={v_{i-w},&hellip;,v_{i-1}} \cup {v_{i+1},&hellip;,v_{i+w}}$$</p>
<h3 id="relational-loss">Relational Loss</h3>
<p>由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \in E$,可以用一下方法来表示这条边:<br>
$$\Phi(e)=g(\Phi(u),\Phi(v))$$<br>
其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：<br>
$$\Phi(e)=\Phi(u) \oplus \Phi(v)$$<br>
这样我们就获得了edge embedding。直接将它输入前馈神经网络，前馈神经网络第$k$层的定义为:<br>
$$h^{(k)}=f(W^{(k)}h^{(k-1)}+b^{(k)})$$<br>
其中 $h^{(0)}=\Phi(e)$，$f$是除最后一层外采用relu激活函数，最后一层采用sigmoid函数激活,最后一层输出为$\hat{y_i}$。最后最小化二元交叉熵损失函数：</p>
<p>$$\mathcal{L}_r=\sum^{|L|}_{i=1} H(y_i,\hat{y_i}) + (1-y_i) \cdot \log (1-\hat{y_i})$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2016 SDNE:《Structral Deep Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/sdne/</link>
      <pubDate>Mon, 21 Jan 2019 15:34:36 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/sdne/</guid>
      <description>KDD2016 &amp;#34;Structral Deep Network Embedding&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">SDNE</a></p>
<h3 id="introduction">Introduction</h3>
<p>这是一篇比较早的Network Embedding论文， 较早将深度模型应用到NE任务。 首先，本文提出了当前网络表示学习中遇到的三个问题：<br>
<strong>（1）. 高度非线性</strong><br>
<strong>（2）. 尽可能保持网络结构</strong><br>
<strong>（3）. 现实网络的高度稀疏性</strong><br>
SDNE的主要目标就是保持网络结构的一阶相似性和二阶相似性。<br>
一阶相似性就是网络中边相连的节点对之间具有的相似性。<br>
二阶相似性就是在一个Graph中，拥有共同邻居但是不直接向相连的两个节点具有的相似性。</p>
<p>其中，一阶相似性主要反映了网络的局部特征。 二阶相似性反映了网络的全局特征。</p>
<h3 id="model">Model</h3>
<p>本文的模型主要如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-01-21-SDNE/SDNE.png#center" alt="你想输入的替代文字"  />
</p>
<p>这张图看上去有点复杂，实则原理非常简单。</p>
<p>模型分为无监督部分和有监督部分，无监督部分是一个<strong>深度自编码器</strong> 用来捕获二阶相似度（保留全局结构），监督部分是一个拉普拉斯特征映射捕获一阶相似度（局部结构）。呃呃呃，Emmmmm ,不知道是我理解有问题还是其他原因，文章里说的和我理解的不太一样 /(ㄒoㄒ)/~~。 然后介绍一下具体的模型结构：</p>
<p>深度自编码器的编码部分：</p>
<p>$$y_i^{(k)}=\sigma{(W^{(k)}y_i^{(k-1)}+b^{(k)})}, k=2,&hellip;,K$$</p>
<p>假设第$k$层是 节点$v$的表示向量（仅考虑全局信息），那么从第$k$层开始解码，最终得到$\hat{x_i}$, 所以自编码器的误差就是输入节点$v$的邻接向量的重构误差。所以，二阶相似度损失函数定义为:<br>
$$\mathcal{L}=\sum_{i=1}^n{||\hat{x_i}-x_i||^2_2}$$</p>
<p>值得注意的是，由于网络的稀疏性，邻接矩阵中的0元素远多于非0元素，使用邻接矩阵作为输入的话要处理很多0，这样就做了太多无用功了。为了解决这个问题，对损失函数做了改进如下：<br>
$$\mathcal{L_{2nd}}=\sum_{i=1}^n||(\hat{x_i}-x_i)\odot{b_i}||^2_2=||\hat{X}-X\odot{B}||^2_F$$</p>
<p>其中$\odot$是哈马达乘积，表示对应元素相乘。$b_i={b_{i,j}}^n_{j=1}$， 邻接矩阵中的0对应$b=1$, 非0元素的$b&gt;1$,这样的目的是对于有边连接的节点增加惩罚。可以理解为对有边连接的节点赋予更高权重。</p>
<p>以上我们获得了二阶相似度的损失函数。在介绍一阶相似度之前，我们先来看看<strong>拉普拉斯映射（Laplacian Eigenmap）</strong>  其实LE也是一种经典的NRL方法，主要目的也是降维。其目标函数如下所示:<br>
$$\sum_{i,j} W_{ij}||y_i-y_j||^2$$<br>
LE是通过构建相似关系图来重构局部特征结构,如果放在网络结构中来说,如果节点$v_i$和$v_j$很接近（有边），那么他们在embedding space中的距离也应该相应接近。$y_i$和$y_j$就表示他们在特征空间中的表示。因此，本文定义了保持一阶相似度的目标函数：<br>
$$\mathcal{L_{1st}}=\sum_{i,j=1}^n{s_{i,j}||y_i^{(K)}-y_j^{(K)}||^2_2}=\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}$$<br>
具体来说，$K$就是自编码器第$K$层的输出，即编码结果，需要保持一条边的两个节点在嵌入空间中的表示相对接近。</p>
<p>最终 结合一阶相似度和二阶相似度，本文给出了SDNE的目标函数：<br>
$$\mathcal{L_{mix}}=\mathcal{L_{2nd}+\alpha{\mathcal{L_{1st}}}}+\nu{\mathcal{L_{reg}}}
=||(\hat{X}-X)\odot{B}||^2_F+\alpha{\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}}+\nu{\mathcal{L_{reg}}}$$<br>
其中，为了防止过拟合，添加了$\mathcal L2$-norm单元$\mathcal{L_{reg}}$来防止过拟合:<br>
$$\mathcal{L_{reg}}=\frac{1}{2}\sum_{k=1}^k({||W^{(k)}||^2_F+||\hat{W}^{k}||_F^2})$$</p>
<h3 id="optimization">Optimization</h3>
<p>使用SGD来优化$\mathcal{L_{mix}}$。具体算法如下：<br>
<img loading="lazy" src="/posts/2019-01-21-SDNE/al.png#center" alt="你想输入的替代文字"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2018 《Graph Attention Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gat/</link>
      <pubDate>Fri, 14 Sep 2018 23:01:31 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gat/</guid>
      <description>ICLR2018 &amp;#34;Graph Attention Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1710.10903">GAT</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文介绍了一种新型的神经网络架构用来处理图结构。即__Graph Attention Networks__(<strong>GATs</strong>)。该方法利用masked self-attentional layer，即通过网络层的堆叠，可以获取网络中每个节点的领域特征，同时为领域中的不同节点指定不同的权重。这样做的好处是可以不需要各种高成本的矩阵运算也不依赖于的图结构信息。通过这种方式，GAT可以解决基于谱的图神经网络存在的问题，同时，GAT可以使用用归纳（inductive）和直推（transductive）问题。</p>
<p><strong>归纳学习</strong>:先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。</p>
<p><strong>直推学习</strong>:先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。</p>
<h1 id="architecture">Architecture</h1>
<p>图$G$中有$N$个节点，他们的特征向量为：$\textbf{h}={\vec{h_1},\vec{h_2},&hellip;,\vec{h_N}}$，其中，$\vec{h_i} \in \mathbb{R}^F$，$F$是每个节点特征数。我们的目的是输出一个新的节点特征向量集$\textbf{h&rsquo;}={\vec{h_1&rsquo;},\vec{h_2&rsquo;},&hellip;,\vec{h_N&rsquo;}}$，其中$\vec{h_i&rsquo;} \in \mathbb{R}^{F&rsquo;}$。 本质就是修改特征向量的维度（Network embedding）</p>
<p>为了获得足够的表达能力以将输入特征变换为更高级别的特征，需要至少一个可学习的线性变换。因此，以任意节点$i$和$j$为例，分别对节点$i$和节点$j$的特征向量做线性变换$W \in \mathbb{R}^{F \times F&rsquo;}$，这样 就将$\vec{h_i}$和$\vec{h_j}$从$F$维的向量转化为$F&rsquo;$维的向量：
$$
e_{ij} = a(W\vec{h_i},W\vec{h_j})
$$
上式中，分别对$\vec{h_i}$和$\vec{h_j}$做线性变换，然后使用self-attention为图中的每一个节点分配注意力（权重）。上式中，注意力机制$a$是一个$\mathbb{R}^{F&rsquo;} \times \mathbb{R}^{F&rsquo;} \to \mathbb{R}$的映射。最终得到的$e_{ij}$是节点$j$对节点$i$的影响力系数（一个实数）。</p>
<p>但是，上面的方法考虑其他节点对$i$的影响时，将图中的所有节点都纳入了考虑范围，这样就丢失了图的结构信息。因此，本文引入<strong>masked attention</strong>机制，即计算影响力系数$e_{ij}$时， 仅考虑节点$i$的<strong>一部分邻居节点</strong> $j \in \mathcal{N}_i$（$i$也属于$\mathcal{N}_i$）。使用softmax将节点$i$部分邻居的注意力系数分配到(0,1)上：
$$
\alpha_{ij} = \mathrm{softmax}_j(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i}\exp(e_{ik})}
$$
在本文中，$a$是一个单层前馈神经网络，参数是一个权重向量$\vec{\text{a}} \in \mathbb{R}^{2F&rsquo;}$，然后使用负半轴斜率为0.2的<a href="https://blog.csdn.net/sinat_33027857/article/details/80192789">LeakyReLU</a>作为非线性激活函数：
$$
\alpha_{ij} = \frac{\exp(\mathrm{LeakyReLU}(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_j}]))}{\sum_{k\in \mathcal{N}_i} \exp(\mathrm{LeakyReLU}(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_k}]))}
$$
其中$||$表示向量的连接操作。上述过程可以用下图表示：</p>
<p><img loading="lazy" src="/posts/2019-04-14-GAT/1.png#center" alt=""  />
</p>
<p>这样，我们就可以获得节点$j$对节点$i$的注意力系数$\alpha_{ij}$，那么，节点$i$最终的输出特征$\vec{h_i&rsquo;}$就是对$\mathcal{N}_i$中所有节点的加权（加注意力）求和：
$$
\vec{h_i&rsquo;} = \sigma (\sum_{j \in \mathcal{N}_i}\alpha_{ij} W\vec{h_j})
$$</p>
<p>另外，本文使用<strong>multi-head attention</strong>来稳定self-attention的学习过程，如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-04-14-GAT/2.png#center" alt=""  />
</p>
<p>图中是$K=3$ heads的multi-head attention，不同颜色的箭头表示一个独立的attention计算，每个邻居节点做三次attention计算。每次attention计算就是一个普通的self-attention，输出的结果是一个$\vec{h_i&rsquo;}$。multi-head attention为每个节点$i$输出3个不同的$\vec{h_i&rsquo;}$,，然后将这三个向量做连接或者取平均，得到最终的$\vec{h_i&rsquo;}$：
$$
\vec{h_i&rsquo;} = ||^K_{k=1} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)
$$
上式为把不同$k$的向量做连接操作，其中$\alpha_{ij}^k$和$\mathbf{W}^{k}$表示第$k$个head的结果，我们可以注意到，最终输出的结果是$KF&rsquo;$维的。除了concat之外，我们还可以通过求平均的方式来获得$\vec{h_i&rsquo;}$:
$$
\vec{h^\prime_i}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)
$$</p>
<h1 id="comparisions">Comparisions</h1>
<ul>
<li>
<p>GAT是计算高效的。self-attention是在所有边上并行计算，并且输出的特征在所有边上并行计算，从而不需要昂贵的矩阵计算和特征分解。单个head的GAT的时间复杂度为$O\left(|V| F F^{\prime}+|E| F^{\prime}\right)$，其中$F$是输入的特征数，$|V|$和$|E|$分别是节点数和边数。复杂度与GCN相同。</p>
</li>
<li>
<p>与GCN不同的是，GAT为同一邻域中的节点分配不同的重要性（different importance），提升了模型容量。</p>
</li>
<li>
<p>注意机制以共享的方式应用于图中的所有边（共享$\mathbf{W}$），因此它不依赖于对全局图结构的预先访问或其所有节点的（特征）。这样有以下提升：</p>
<ul>
<li>不必是无向图。如果$i \to j$不存在,可以直接不用计算$\alpha_{ij}$。</li>
<li>可直接应用于归纳学习。</li>
</ul>
</li>
<li>
<p>GAT可以被描述为一种特殊的<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf">MoNet(Geometric deep learning on graphs and manifolds using mixture model cnns)</a>。</p>
</li>
</ul>
<h1 id="reference">Reference</h1>
<p>参考：</p>
<p>GCN：https://arxiv.org/abs/1609.02907</p>
<p><a href="https://zhuanlan.zhihu.com/p/34232818">https://zhuanlan.zhihu.com/p/34232818</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/59176692">https://zhuanlan.zhihu.com/p/59176692</a></p>
<p><a href="https://arxiv.org/abs/1710.10903">https://arxiv.org/abs/1710.10903</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2017 《metapath2vec:Scalable Representation Learning for Heterogeneous Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/metapath2vec/</link>
      <pubDate>Fri, 29 Jun 2018 16:29:18 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/metapath2vec/</guid>
      <description>KDD2017 &amp;#34;metapath2vec:Scalable Representation Learning for Heterogeneous Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址： <a href="https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf">metapath2vec</a></p>
<h1 id="introduction">Introduction</h1>
<p>真实网络中通常包含多种类型的节点和关系，这些节点和关系共同组成了异质信息网络（Heterogeneous Information Network），传统的NE方法如DeepWalk, Line , node2vec更多关注与同质网络从而忽略了网络的异质性。针对这个问题，这篇文章提出了metapath2vec基于meta-path的随机游走来构建节点的异质邻域，采用异质Skip-gram模型来学习节点的representation。 另外，在负采样时也考虑异质性，从而进一步提出了metapath2vec++。如下：</p>
<p><img loading="lazy" src="/posts/2019-06-29-metapath/1.png" alt=""  />
</p>
<h1 id="definition">Definition</h1>
<p><strong>Definition 1:</strong> 异质网络： $G = (V,E,T)$，其中每个节点和边分别对应一个映射 $\phi(v) : V \rightarrow T_{V}$ 和$\varphi(e) : E \rightarrow T_{E}$。 其中$T_V$和$T_E$分别表示节点的类型集合以及关系的类型集合。且$\left|T_{V}\right|+\left|T_{E}\right|&gt;2$。</p>
<p><strong>Definition 2:</strong> 异质网络表示学习 为网络中的节点学习一个$d$维的表示 $\mathrm{X} \in \mathbb{R}^{|V| \times d}$，$d \ll|V|$。节点的向量表示可以捕获节点间结构和语义关系。</p>
<h1 id="model">Model</h1>
<h2 id="metapath2vec">metapath2vec</h2>
<p>Skip-Gram模型是给定一个节点，预测向下文节点的概率。metapath2vec的目的是考虑多种类型节点和多种类型关系的情况下，最大化网络概率。为了将异质信息融入skip-gram模型中，首先提出异质skip-gram。</p>
<h3 id="heterogeneous-skip-gram">Heterogeneous Skip-Gram</h3>
<p>对于节点类型$|T_V| &gt; 1$的网络$G$， 给定一个节点$v$，需要最大化属于类型$t \in T_V$的异质上下文节点的概率：
$$
\arg \max_{\theta} \sum_{v \in V} \sum_{t \in T_{V}} \sum_{c_{t} \in N_{t}(v)} \log p\left(c_{t} | v ; \theta\right)
$$
其中，$N_t(v)$表示$v$属于第$t$个类型的邻居节点。 $p\left(c_{t} | v ; \theta\right)$ 表示给定节点$v$，它的$t$类型邻域概率： $p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u \in V} e^{X_{u} \cdot X_{v}}}$，是一个softmax函数，$X_v$是节点$v$的表示。由于softmax函数的分母每一次都要遍历所有节点，所以采用负采样改进$\log p(\cdot)$ 如下：
$$
\log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u^{m} \sim P(u)}\left[\log \sigma\left(-X_{u^{m}} \cdot X_{v}\right)\right]
$$
其中 $\sigma(x)=\frac{1}{1+e^{-x}}$。</p>
<h3 id="meta-path-based-random-walks">Meta-Path-Based Random Walks</h3>
<p>在第$i$步时，转移概率$p(v^{i+1}|v^i)$表示为忽略节点类型情况下$v^i$的邻居分布。但是，PathSim提出，异质信息网络中的随机游走偏向于高度可见的节点，即具有主导数量路径的节点，所以 本文设计了基于元路径的随机游走来生成path，从而能够捕获不同类型节点间的结构联系和语义关系，提出了促进异构网络结构转换为metapath2vec的skip-gram。</p>
<p>一个meta-path模式$\mathcal{P}: V_{1} \stackrel{R_{1}}{\longrightarrow} V_{2} \stackrel{R_{2}}{\longrightarrow} \dots V_{t} \stackrel{R_{t}}{\longrightarrow} V_{t+1} \cdots \stackrel{R_{l-1}}{\longrightarrow} V_{l}$， 其中 $R=R_{1} \circ R_{2} \circ \cdots \circ R_{l-1}$ 节点类型$V_{1}$到$V_{l}$之间的组合关系。那么节点间的跳转概率定义为：
$$
p\left(v^{i+1} | v_{t}^{i}, \mathcal{P}\right)=\left\{\begin{array}{cc}{\frac{1}{\left|N_{t+1}\left(v_{t}^{i}\right)\right|}} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \in E, \phi\left(v^{i+1}\right)=t+1} \\
{0} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \in E, \phi\left(v^{i+1}\right) \neq t+1} \\
{0} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \notin E}\end{array}\right.
$$
其中$v^i_t \in V_t$，$N_{t+1}\left(v_{t}^{i}\right)$表示属于$t$类型的节点$v$的属于$t+1$类型的邻居。如果下一个节点$v^{i+1}$和$v^i_t$之间有边，并且$v^{i+1}$是$t+1$类型的节点 那么转移概率服从平均分布。其中，$v^{i+1}$服从meta-path所定义的下移节点类型。如图（a）中，原路径为$OAPVPAO$，那么节点$a_4$的下一个节点必然要是$P$类。 由于meta-path的对称性，所以：
$$
p\left(v^{i+1} | v_{t}^{i}\right)=p\left(v^{i+1} | v_{1}^{i}\right), \text { if } t=l
$$</p>
<h2 id="metapath2vec-1">metapath2vec++</h2>
<p>由于softmax做归一化时没有考虑节点类型，分母是对所有节点求和，所以为了融合节点类型，给出<strong>Heterogeneous negative sampling</strong>:
$$
p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u_{t} \in V_{t}} e^{X_{u_{t}} \cdot X_{v}}}
$$
<strong>如图（c）所示，metapath2vec++对每种类型节点指定不同的一组多项式分布</strong>，相当于在输出层根据节点类型，把异质网络分解成不同的同质网络，同样采用负采用的方法简化计算：
$$
O(\mathrm{X})=\log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u_{t}^{m} \sim P_{t}\left(u_{t}\right)}\left[\log \sigma\left(-X_{u_{t}^{m}} \cdot X_{v}\right)\right]
$$
算法如下：</p>
<p><img loading="lazy" src="/posts/2019-06-29-metapath/2.png#center" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>OpenCV轮廓提取并计算图片中某一封闭区域的面积</title>
      <link>https://JhuoW.github.io/posts/pic-closed-edge/</link>
      <pubDate>Mon, 02 Apr 2018 15:43:44 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/pic-closed-edge/</guid>
      <description>&lt;p&gt;最近遇到一个问题，如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积&lt;br&gt;
&lt;img loading=&#34;lazy&#34; src=&#34;https://JhuoW.github.io/posts/2019-04-02-Pic-closed-edge/1.png&#34; alt=&#34;&#34;  /&gt;
&lt;br&gt;
之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：&lt;br&gt;
&lt;img loading=&#34;lazy&#34; src=&#34;https://JhuoW.github.io/posts/2019-04-02-Pic-closed-edge/edge.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;br&gt;
我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：&lt;br&gt;
{% codeblock %}&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import cv2
import numpy as np

# Input image
img = cv2.imread(&#39;cut.jpeg&#39;, cv2.IMREAD_GRAYSCALE)

# Needed due to JPG artifacts
_, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)

# Dilate to better detect contours
temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))

# Find largest contour
_, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
largestCnt = []
for cnt in cnts:
    if len(cnt) &amp;gt; len(largestCnt):
        largestCnt = cnt

# Determine center of area of largest contour
M = cv2.moments(largestCnt)
x = int(M[&amp;quot;m10&amp;quot;] / M[&amp;quot;m00&amp;quot;])
y = int(M[&amp;quot;m01&amp;quot;] / M[&amp;quot;m00&amp;quot;])

# Initiale mask for flood filling
width, height = temp.shape
mask = img2 = np.ones((width + 2, height + 2), np.uint8) * 255
mask[1:width, 1:height] = 0

# Generate intermediate image, draw largest contour, flood filled
temp = np.zeros(temp.shape, np.uint8)
temp = cv2.drawContours(temp, largestCnt, -1, 255, cv2.FILLED)
_, temp, mask, _ = cv2.floodFill(temp, mask, (x, y), 255)
temp = cv2.morphologyEx(temp, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))

# Count pixels in desired region
area = cv2.countNonZero(temp)

# Put result on original image
img = cv2.putText(img, str(area), (x, y), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, 255)

cv2.imshow(&#39;Input&#39;, img)
cv2.imshow(&#39;Temp image&#39;, temp)

cv2.waitKey(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{% endcodeblock %}&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>最近遇到一个问题，如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积<br>
<img loading="lazy" src="/posts/2019-04-02-Pic-closed-edge/1.png" alt=""  />
<br>
之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：<br>
<img loading="lazy" src="/posts/2019-04-02-Pic-closed-edge/edge.jpeg" alt=""  />
<br>
我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：<br>
{% codeblock %}</p>
<pre><code>import cv2
import numpy as np

# Input image
img = cv2.imread('cut.jpeg', cv2.IMREAD_GRAYSCALE)

# Needed due to JPG artifacts
_, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)

# Dilate to better detect contours
temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))

# Find largest contour
_, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
largestCnt = []
for cnt in cnts:
    if len(cnt) &gt; len(largestCnt):
        largestCnt = cnt

# Determine center of area of largest contour
M = cv2.moments(largestCnt)
x = int(M[&quot;m10&quot;] / M[&quot;m00&quot;])
y = int(M[&quot;m01&quot;] / M[&quot;m00&quot;])

# Initiale mask for flood filling
width, height = temp.shape
mask = img2 = np.ones((width + 2, height + 2), np.uint8) * 255
mask[1:width, 1:height] = 0

# Generate intermediate image, draw largest contour, flood filled
temp = np.zeros(temp.shape, np.uint8)
temp = cv2.drawContours(temp, largestCnt, -1, 255, cv2.FILLED)
_, temp, mask, _ = cv2.floodFill(temp, mask, (x, y), 255)
temp = cv2.morphologyEx(temp, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))

# Count pixels in desired region
area = cv2.countNonZero(temp)

# Put result on original image
img = cv2.putText(img, str(area), (x, y), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, 255)

cv2.imshow('Input', img)
cv2.imshow('Temp image', temp)

cv2.waitKey(0)
</code></pre>
<p>{% endcodeblock %}</p>
<p>最后我们可以得到一个比较准确的轮廓：<br>
<img loading="lazy" src="/posts/2019-04-02-Pic-closed-edge/img_trk.jpg" alt=""  />
<br>
面积如图中所示：<br>
<img loading="lazy" src="/posts/2019-04-02-Pic-closed-edge/img_tr.jpg" alt=""  />
</p>
<p>参考：<br>
<a href="https://stackoverflow.com/questions/55467031/how-to-get-the-area-of-the-contours">https://stackoverflow.com/questions/55467031/how-to-get-the-area-of-the-contours</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ACL2017 《CANE:Context-Aware Network Embedding for Relation Modeling》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/context-aware-ne/</link>
      <pubDate>Fri, 09 Mar 2018 20:41:15 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/context-aware-ne/</guid>
      <description>ACL2017 &amp;#34;CANE:Context-Aware Network Embedding for Relation Modeling&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://aclanthology.org/P17-1158.pdf">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>在现实世界的社交网络中，一个顶点在与不同的邻居顶点交互时可能表现出不同的方面 (aspect)，这是很直观的。例如，研究人员通常与各种合作伙伴就不同的研究主题进行合作（如下图所示），社交媒体用户与分享不同兴趣的各种朋友联系，一个网页出于不同目的链接到多个其它网页。然而，现有的大多数 NE 方法只为每个顶点安排一个 single embedding 向量，并产生以下两个问题：</p>
<ul>
<li>这些方法在与不同邻居交互时，无法灵活转换不同的aspect</li>
<li>在这些模型中，一个顶点倾向于迫使它的所有邻居之间的 embedding彼此靠近，但事实上并非一直如此。例如下图中，左侧用户和右侧用户共享较少的共同兴趣，但是由于他们都链接到中间用户，因此被认为彼此接近。因此，这使得顶点 embedding 没有区分性。</li>
</ul>
<p><img loading="lazy" src="/posts/CANE/1.png#center" alt=""  />
</p>
<p>为了解决上述问题，本文提出了一个 CANE框架，用于精确建模顶点之间的关系。更具体而言，论文在信息网络上应用 CANE。信息网络的每个顶点还包含丰富的外部信息，例如文本、标签 、或者其它元数据。在这种场景下，上下文的重要性对 network embedding 更为关键。在不失一般性的情况下，论文在基于文本的信息网络中实现了 CANE，但是 CANE可以很容易地扩展到其它类型的信息网络。</p>
<p>在传统的 network embedding模型中，每个顶点都表达为一个静态的 embedding 向量，即 context-free embedding 。相反，CANE 根据与当前顶点交互的不同邻居，从而将动态的 embedding分配给当前顶点，称为 context-aware embedding。以一个顶点$u$为例：当与不同的邻居交互时， 的 context-free embedding保持不变；而当面对不同的邻居时， $u$的 context-aware embedding是动态的。</p>
<p>当顶点$u$与它的邻居顶点$v$交互时，它们彼此相关的 context embedding 分别来自它们的文本信息。对于每个顶点，可以轻松地使用神经模型neural model ，例如卷积神经网络和循环神经网络来构建 context-free embedding 和 text-based embedding 。为了实现 context-aware text-based embedding，论文引入了 selective attention 方案，并在这些神经模型中建立了  和  之间的互注意力 mutual attention 。mutual attention 预期引导神经模型强调那些被相邻顶点 focus 的单词，并最终获得 context-aware embedding。每个顶点的 context-free embedding 和 context-aware embedding 都可以通过使用现有的 network embedding 方法学到（如 DeepWalk，LINE，node2vec）并拼接起来。</p>
<p>论文对不同领域的三个真实数据集进行了实验。与其它 state-of-the-art 方法相比，链接预测的实验结果展示了论文框架的有效性。结果表明，context-aware embedding 对于网络分析至关重要，特别是对于那些涉及顶点之间复杂交互的任务，如链接预测。论文还通过顶点分类和案例研究来探索论文框架的性能，这再次证实了 CANE 模型的灵活性和优越性。</p>
<h1 id="model">Model</h1>
<p>给定信息网络$G = (V,E,T)$, ，其中$V$为顶点集合， $E \subseteq V \times V$为边集合， $T$为顶点的文本信息。每条边表示顶点之间的关系，并且关联一个权重$w_{u,v}$ 。这里顶点$v \in V$的文本信息表达为单词序列$\mathcal{S}_{v}=\left(t_{1}, t_{2}, \cdots, t_{n_{v}}\right)$，其中$n_{v}=\left|\mathcal{S}_{v}\right|$为序列$\mathcal{S}_{v}$的长度。</p>
<p>NE旨在根据网络结构和关联信息（如文本和 label）为每个顶点$v \in V$ 学习低维embedding$\overrightarrow{\mathbf{v}} \in \mathbb{R}^{d}$, 其中 $d \ll |V|$为embedding的维度。</p>
<p>Context-free Embedding 的定义：传统的 network representation learning 模型为每个顶点学习 context-free embedding 。这意味着顶点的 embedding 是固定的，并且不会根据顶点的上下文信息（即与之交互的另一个顶点）而改变。</p>
<p>Context-aware Embedding 的定义：与现有的、学习 context-free embedding 的NE 模型不同，CANE 根据顶点不同的上下文来学习该顶点的各种 embedding 。具体而言，对于边$e_{u,v}$， CANE 学习 context-aware embedding $\overrightarrow{\mathbf{v}}_{(u)}$和$\overrightarrow{\mathbf{u}}_{(v)}$</p>
<h2 id="framework">Framework</h2>
<p>为了充分利用网络结构和关联的文本信息，为顶点$v$提供了两种类型的 embedding：structure-based embedding $\overrightarrow{\mathbf{v}}^{s}$, text-based embedding  $\overrightarrow{\mathbf{v}}^{t}$。structure-based embeding 可以捕获网络结构中的信息，而 text-based embedding 可以捕获关联文本信息中的文本含义（textual meaning）。使用这些 embedding，可以简单地拼接它们并获得顶点 embedding 为：
$$
\overrightarrow{\mathbf{V}}=\overrightarrow{\mathbf{V}}^{s} \oplus \overrightarrow{\mathbf{V}}^{t}
$$
其中$\oplus$为拼接算子。注意， text-based embedding $\overrightarrow{\mathbf{v}}^{t}$可以是 context-free 的、也可以是context-aware的，这将在后面详细介绍。当$\overrightarrow{\mathbf{v}}^{t}$是 context-aware时，整个顶点 embedding $\overrightarrow{\mathbf{v}}$也将是 context-aware。</p>
<p>通过上述定义，CANE旨在最大化所有边上的目标函数，如下所示：
$$
\mathcal{L}=\sum_{e \in E} L(e)
$$
这里每条边的目标函数$L(e)$由两部分组成：
$$
L(e) = L_s(e) + L_t(e)
$$
$L_s(e)$表示基于结构的目标函数, $L_t(e)$表示基于文本的目标函数。 接下来分别对这两个目标函数进行详细介绍。</p>
<h2 id="基于结构的目标函数">基于结构的目标函数</h2>
<p>为了不失一般性，假设网络是有向的，因为无向边可以被认为是两个方向相反、且权重相等的有向边。因此，基于结构的目标函数旨在使用 structure-based embedding来衡量观察到一条有向边的对数似然（log-likelihood），即：
$$
L_{s}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)
$$
遵从 LINE，将上式中的条件概率定义为：
$$
p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)=\frac{\exp \left(\overrightarrow{\mathbf{u}}^{s} \cdot \overrightarrow{\mathbf{v}}^{s}\right)}{\sum_{z \in V} \overrightarrow{\mathbf{u}}^{s} \cdot \overrightarrow{\mathbf{z}}^{s}}
$$</p>
<h2 id="基于文本的目标函数">基于文本的目标函数</h2>
<p>现实世界社交网络中的顶点通常伴随着关联的文本信息。因此，本文提出了基于文本的目标函数来利用这些文本信息，并学习 text-based embedding 。</p>
<p>基于文本的目标函数$L_{t}(e)$可以通过各种度量指标来定义。为了与$L_{s}(e)$ 兼容，将$L_{t}(e)$定义如下：
$$
L_{t}(e)=\alpha L_{t, t}(e)+\beta L_{t, s}(e)+\gamma L_{s, t}(e)
$$
其中：$\alpha$，$\beta$，$\gamma$为对应部分的重要性，为超参数。</p>
<p>$L_{t,t}(e)$，$L_{t, s}(e)$，$L_{s, t}(e)$定义为：
$$
\begin{aligned}
&amp;L_{t, t}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{t} \mid \overrightarrow{\mathbf{u}}^{t}\right) \\
&amp;L_{t, s}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{t} \mid \overrightarrow{\mathbf{u}}^{s}\right) \\
&amp;L_{s, t}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{t}\right)
\end{aligned}
$$
这里构建了 structure-based embedding 、 text-based embedding 之间的桥梁，使得信息在结构和文本之间流动。上式中的条件概率将两种类型的顶点 embedding 映射到相同的 representation 空间中，但是考虑到它们自身的特点，这里并未强制它们相同。类似地，使用 softmax 函数来计算概率，如公式$p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)$所示。</p>
<p>structure-based embedding 被视为 parameter，与传统的 network embedding 模型相同。但是对于 text-based embedding，本文从顶点的关联文本信息中获取它们。此外， text-based embedding 可以通过 context-free 的方式、或者 context-aware 的方式获取。</p>
<h2 id="context-free-text-embedding">Context-Free Text Embedding</h2>
<p>有多种神经网络模型可以从单词序列 word sequence 中获取 text embedding，如卷积神经网络 CNN、循环神经网络RNN。在这项工作中，研究了用于文本建模的不同神经网络，包括 CNN、Bidirectional RNN、GRU，并采用了性能最好的 CNN。CNN 可以在单词之间捕获局部语义依赖性。</p>
<p>CNN 以一个顶点的单词序列作为输入，通过 lookup、卷积、池化这三层得到基于文本的 embedding 。</p>
<ul>
<li>
<p>lookup：给定一个单词序列 $\mathcal{S}=\left(t_{1}, t_{2}, \cdots, t_{n}\right)$, lookup layer 将每个单词$t_i$转换为对应的 word embedding $\overrightarrow{\mathbf{t}}_{i} \in \mathbb{R}^{d^{\prime}}$, 并且获得 embedding序列$\mathbf{S}=\left(\overrightarrow{\mathbf{t}}_{1}, \cdots, \overrightarrow{\mathbf{t}}_{n}\right)$, 其中$d^{\prime}$为word embedding的维度。</p>
</li>
<li>
<p>卷积：lookup之后，卷积层提取输入的embedding序列$\mathbf{S}$局部特征 local feature。具体而言，它使用卷积矩阵$\mathbf{C} \in \mathbb{R}^{d \times\left(l \times d^{\prime}\right)}$在长度为$l$的滑动窗口上进行卷积操作，如下所示：</p>
</li>
</ul>
<p>$$
\overrightarrow{\mathbf{x}}_{i}=\mathbf{C} * \mathbf{S}_{i: i+l-1}+\overrightarrow{\mathbf{b}}
$$</p>
<p>$\mathbf{C}$表示$d$个卷积核，每个卷积核的尺寸为$l \times d^\prime$。 $d$为卷积核数量，也称作输出通道数。</p>
<p>其中：$\mathbf{S}_{i: i+l-1} \in \mathbb{R}^{l \times d}$为第$i$个滑动窗口内所有word embedding拼接得到的矩阵。$\overrightarrow{\mathbf{b}} \in \mathbb{R}^{d}$为bias。注意，在单词序列的边缘添加了零填充向量。</p>
<ul>
<li>最大池化：为了获得 text embedding $\overrightarrow{\mathbf{v}}^{t}$, 对 $\left\{\overrightarrow{\mathbf{x}}_{1}, \cdots, \overrightarrow{\mathbf{x}}_{n}\right\}$进行最大池化和非线性变换，如下所示：</li>
</ul>
<p>$$
\begin{gathered}
r_{i}=\tanh \left(\max \left(x_{1, i}, x_{2, i}, \cdots, x_{n, i}\right)\right), \quad i=1, \cdots, d \\
\overrightarrow{\mathbf{v}}^{t}=\left(r_{1}, \cdots, r_{d}\right)^{\top} \in \mathbb{R}^{d}
\end{gathered}
$$</p>
<p>其中max是在 embedding维度上进行的，tanh为逐元素的函数。</p>
<h2 id="context-aware-text-embedding">Context-Aware Text Embedding</h2>
<p>假设顶点在与其它顶点交互时扮演不同的角色。换句话讲，对于给定的顶点，其它不同的顶点与它有不同的焦点，这将导致 context-aware text embedding。</p>
<p>为了实现这一点，采用 mutual attention来获得 context-aware text embedding。mutual attention使 CNN 中的池化层能够感知 edge中的顶点 pair，从而使得来自一个顶点的文本信息可以直接影响另一个顶点的 text embedding，反之亦然。</p>
<p><img loading="lazy" src="/posts/CANE/2.png#center" alt=""  />
</p>
<p>如上图所示，对于context-aware text embedding的生成过程， 给定一条边$e_{u,v}$和两个对应的文本序列$\mathcal{S}_u$和$\mathcal{S}_v$, 可以通过卷积层得到矩阵$\mathbf{P} \in \mathbb{R}^{d \times m}$和$\mathbf{Q} \in \mathbb{R}^{d \times n}$ 。这里$m$表示$\mathcal{S}_u$的长度，$n$表示$\mathcal{S}_v$的长度。通过引入一个注意力矩阵$\mathbf{A} \in \mathbb{R}^{d \times d}$，计算相关性矩阵$\mathbf{F} \in \mathbb{R}^{m \times n}$:
$$
\mathbf{F}=\tanh \left(\mathbf{P}^{\top} \mathbf{A} \mathbf{Q}\right)
$$
其中$\mathbf{F}$中的每个元素$\mathbf{F}_{i,j}$表示两个隐向量（即$\overrightarrow{\mathbf{p}}_{i}$， $\overrightarrow{\mathbf{q}}_{j}$）之间的 pair-wise相关性得分。 然后，沿 $\mathbf{F}$的行和列进行池化操作从而生成重要性向量，分别称作行池化row-pooling和列池化column-pooling。根据实验，均值池化的性能优于最大池化。因此，采用如下的均值池化操作：
$$
\begin{aligned}
g_{i}^{p} &amp;=\operatorname{mean}\left(F_{i, 1}, F_{i, 2}, \cdots, F_{i, n}\right) \\
g_{i}^{q} &amp;=\operatorname{mean}\left(F_{1, i}, F_{2, i}, \cdots, F_{m, i}\right)
\end{aligned}
$$
其中：$\overrightarrow{\mathbf{g}}^{p}=\left(g_{1}^{p}, \cdots, g_{m}^{p}\right)^{\top} \in \mathbb{R}^{m}$表示行池化向量，为$\mathbf{P}$的重要性向量。$\overrightarrow{\mathbf{g}}^{q}=\left(g_{1}^{q}, \cdots, g_{m}^{q}\right)^{\top} \in \mathbb{R}^{m}$表示列池化向量，为$\mathbf{Q}$的重要性向量。</p>
<p>接下来，使用softmax函数将重要性向量$\overrightarrow{\mathbf{g}}^{p}$和$\overrightarrow{\mathbf{g}}^{q}$转换为注意力向量 $\overrightarrow{\mathbf{a}}^{p}$和$\overrightarrow{\mathbf{a}}^{q}$：
$$
\begin{aligned}
a_{i}^{p}=\frac{\exp \left(g_{i}^{p}\right)}{\sum_{j=1}^{m} \exp \left(g_{j}^{p}\right)}, \quad a_{i}^{q}=\frac{\exp \left(g_{i}^{q}\right)}{\sum_{j=1}^{n} \exp \left(g_{j}^{q}\right)} \
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\overrightarrow{\mathbf{a}}^{p}=\left(a_{1}^{p}, \cdots, a_{m}^{p}\right)^{\top}  \in \mathbb{R}^{m} \\
\overrightarrow{\mathbf{a}}^{q}=\left(a_{1}^{q}, \cdots, a_{n}^{q}\right)^{\top} \in \mathbb{R}^{n}
\end{aligned}
$$
最后，顶点$u$和$v$ 的 context-aware text embedding计算为：
$$
\overrightarrow{\mathbf{u}}_{(v)}^{t}=\mathbf{P} \overrightarrow{\mathbf{a}}^{p}, \quad \overrightarrow{\mathbf{v}}_{(u)}^{t}=\mathbf{Q} \overrightarrow{\mathbf{a}}^{q}
$$
现在，给定一条边$e_{u,v}$，可以获得 context-aware embedding，它是 structure embedding 和 context-aware text embedding的拼接：
$$
\overrightarrow{\mathbf{u}}_{(v)}=\overrightarrow{\mathbf{u}}^{s} \oplus \overrightarrow{\mathbf{u}}_{(v)}^{t}, \quad \overrightarrow{\mathbf{v}}_{(u)}=\overrightarrow{\mathbf{v}}^{s} \oplus \overrightarrow{\mathbf{v}}_{(u)}^{t}
$$</p>
<h2 id="cane优化过程">CANE优化过程</h2>
<p>CANE 旨在最大化若干个条件概率。直观而言，使用 softmax函数优化条件概率在计算上代价太大。因此，使用负采样，并将目标函数转换为以下形式：
$$
\log \sigma(\overrightarrow{\mathbf{u}} \cdot \overrightarrow{\mathbf{v}})+k \times \mathbb{E}_{z \sim P(v)}[\log \sigma(-\overrightarrow{\mathbf{u}} \cdot \overrightarrow{\mathbf{z}})]
$$
其中$k$为负采样比例， $\sigma$为sigmoid, $P(v) \propto d_{v}^{3 / 4}$为负顶点的采样分布，$d_{v}$为顶点$v$的out-degree。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>深度学习中的优化算法总结</title>
      <link>https://JhuoW.github.io/posts/optimizer/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/optimizer/</guid>
      <description>&lt;p&gt;最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。&lt;/p&gt;
&lt;h1 id=&#34;gradient-desent梯度下降&#34;&gt;Gradient Desent(梯度下降)&lt;/h1&gt;
&lt;p&gt;目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。&lt;br&gt;
(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。&lt;br&gt;
(2).目标函数关于参数$x$在epoch $t$时的梯度：&lt;br&gt;
$$g_t = \nabla_x f(x_t)$$&lt;br&gt;
(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：&lt;br&gt;
$$x_{t+1} = x_t-\eta_t g_t$$&lt;br&gt;
其中$x_{t+1}$为$t+1$时刻的参数值。&lt;/p&gt;
&lt;h2 id=&#34;stochastic-gradient-desent随机梯度下降&#34;&gt;Stochastic Gradient Desent(随机梯度下降)&lt;/h2&gt;
&lt;p&gt;梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。&lt;br&gt;
首先给出无偏估计的定义，稍后会用到：&lt;br&gt;
&lt;a href=&#34;https://www.cnblogs.com/notwice/p/8538539.html&#34;&gt;无偏估计&lt;/a&gt;：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。&lt;/p&gt;
&lt;p&gt;深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \frac{\displaystyle\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：&lt;br&gt;
$$\nabla f_{batch}(x) = \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x)$$&lt;br&gt;
如果使用GD来优化：&lt;br&gt;
$$x_{t+1} = x_{t}- \eta_t \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x) \ = x_t-\eta_t \nabla f_{batch}(x)$$
上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。&lt;/p&gt;
&lt;p&gt;随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \in {1, \cdots,n}$, 每次从中随机选取一个样本来更新参数$x$。&lt;br&gt;
$$x_{t+1} = x_{t}-\eta_t \nabla f_i(x)$$&lt;br&gt;
这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\nabla f_i(x)$是对梯度$\nabla f_{batch}(x)$的无偏估计，因为：&lt;br&gt;
$$E_i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f_{batch}(\boldsymbol{x})$$&lt;br&gt;
符合无偏估计的定义。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。</p>
<h1 id="gradient-desent梯度下降">Gradient Desent(梯度下降)</h1>
<p>目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。<br>
(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。<br>
(2).目标函数关于参数$x$在epoch $t$时的梯度：<br>
$$g_t = \nabla_x f(x_t)$$<br>
(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：<br>
$$x_{t+1} = x_t-\eta_t g_t$$<br>
其中$x_{t+1}$为$t+1$时刻的参数值。</p>
<h2 id="stochastic-gradient-desent随机梯度下降">Stochastic Gradient Desent(随机梯度下降)</h2>
<p>梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。<br>
首先给出无偏估计的定义，稍后会用到：<br>
<a href="https://www.cnblogs.com/notwice/p/8538539.html">无偏估计</a>：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。</p>
<p>深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \frac{\displaystyle\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：<br>
$$\nabla f_{batch}(x) = \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x)$$<br>
如果使用GD来优化：<br>
$$x_{t+1} = x_{t}- \eta_t \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x) \ = x_t-\eta_t \nabla f_{batch}(x)$$
上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。</p>
<p>随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \in {1, \cdots,n}$, 每次从中随机选取一个样本来更新参数$x$。<br>
$$x_{t+1} = x_{t}-\eta_t \nabla f_i(x)$$<br>
这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\nabla f_i(x)$是对梯度$\nabla f_{batch}(x)$的无偏估计，因为：<br>
$$E_i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f_{batch}(\boldsymbol{x})$$<br>
符合无偏估计的定义。</p>
<h2 id="momentum动量法">Momentum(动量法)</h2>
<h3 id="exponentially-weighted-moving-averagesema">Exponentially weighted moving averages(EMA)</h3>
<p>EMA,指数加权移动平均数。</p>
<p>在GD中,如果学习率过大，会导致目标函数发散，而无法逼近最小值，如下图所示：<br>
<img loading="lazy" src="/posts/2019-02-28-Optimizer/images/EMA_1.png" alt="1"  />
<br>
如果学习率很低，那么会缓慢接近最优点，如下图红色轨迹：<br>
<img loading="lazy" src="/posts/2019-02-28-Optimizer/images/EMA_2.png" alt="2"  />
<br>
我们希望在学习率较小的时候可以更快逼近最优点，在学习率大的时候自变量可以不发散，即在正确的方向上加速下降并且抑制震荡，也就是达到如下的效果：<br>
<img loading="lazy" src="/posts/2019-02-28-Optimizer/images/EMA_3.png" alt="3"  />
</p>
<p>因此引入EMA。给定参数$0 \leq \gamma &lt; 1$,当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合。<br>
$$y_t = \gamma y_{t-1} + (1-\gamma) x_t$$<br>
展开上式:<br>
$$\begin{split}\begin{aligned}
y_t  &amp;= (1-\gamma) x_t + \gamma y_{t-1}\\
&amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + \gamma^2y_{t-2}\\
&amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + (1-\gamma) \cdot \gamma^2x_{t-2} + \gamma^3y_{t-3}\\
&amp;\ldots
\end{aligned}\end{split}$$<br>
上式可以看出当前时刻变量是对过去时刻变量做指数加权，离当前时刻越近，加权越大（越接近1）。<br>
在现实中，我们将$y_t$看作是最近$1/(1-\gamma)$个时间步的$x_t$的加权平均，当$\gamma = 0.95$时，是最近20个时间步的$x_t$值的加权平均。当$\gamma=0.9$时,可以看做是最近10个时间步加权平均。</p>
<h3 id="动量法">动量法</h3>
<p>$$\begin{split}\begin{aligned}
\boldsymbol{v}_t &amp;= \gamma \boldsymbol{v}_{t-1} + \eta_t \boldsymbol{g}_t, \\
\boldsymbol{x}_t &amp;= \boldsymbol{x}_{t-1} - \boldsymbol{v}_t,
\end{aligned}\end{split}$$<br>
其中$g_t = \nabla f_i(x)$上式可以看出，如果$\gamma=0$，则上式就是一个普通的随机梯度下降法。$0 \leq \gamma &lt; 1$. $\gamma$一般取0.9。<br>
一般，初始化$v_0=0$, 则<br>
$$v_1=\eta_t g_t \\ v_2=\gamma v_1+\eta_t g_t = \eta_t g_t(\gamma+1) \\ v_3 = \eta_t g_t (\gamma^2+\gamma+1) \\ v_{inf} = \frac{(\eta_t g_t)\cdot(1-\gamma^{inf+1})}{1-\gamma}\approx \frac{(\eta_t g_t)}{1-\gamma}$$</p>
<p>相比原始梯度下降算法，动量梯度下降算法有助于加速收敛。当梯度与动量方向一致时，动量项会增加，而相反时，动量项减少，因此动量梯度下降算法可以减少训练的震荡过程。</p>
<p>换种方式理解动量法：<br>
<img loading="lazy" src="/posts/2019-02-28-Optimizer/images/m.jpg" alt="4"  />
<br>
如上图所示，A点为起始点，首先计算A点的梯度$\nabla a$，下降到B点，<br>
$$\theta_{new} = \theta-\eta\nabla a$$<br>
其中$\theta$为参数， $\eta$为学习率<br>
到达B点后要加上A点的梯度，但是A点的梯度有个衰减值$\gamma$,推荐取0.9，相当于加上一个来自A点递减的加速度。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。所以B点的参数更新公式是这样的：<br>
$$v_t = \gamma v_{t-1}+\eta \nabla b$$<br>
$$\theta_{new} = \theta-v_t$$<br>
其中$v_{t-1}$表示之前所有步骤累计的动量和，$\nabla b$为B点的梯度方向。这样一步一步下去，带着初速度的小球就会极速的奔向谷底。</p>
<h1 id="adagrad">AdaGrad</h1>
<p>假设目标函数有两个参数分别为$x_1$,$x_2$,若梯度下降迭代过程中，始终使用相同的学习率$\eta$:<br>
$$x_{1_{new}} = x_1-\eta \frac{\partial f}{\partial x_1}$$
$$x_{2_{new}} = x_2-\eta \frac{\partial f}{\partial x_2}$$<br>
AdaGard算法根据自变量在每个维度的梯度值来调整各个维度上的学习率，避免学习率难以适应维度的问题。adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。<br>
$\nabla_{\theta_i} J(\theta)$表示第$i$个参数的梯度，其中$\theta=(\theta_1,\theta_2,&hellip;)$有$n$个参数。如果使用SGD来优化第$i$个参数，我们可以表示为:<br>
$$\theta_{i_new} = \theta_i-\eta \nabla_{\theta_i}J(\theta)$$<br>
如果使用Adagrad，则可以表示为这样:<br>
$$\theta_{i,t+1}=\theta_{i,t}-\frac{\eta}{\sqrt{G_{i,t}+\epsilon}} \nabla_{\theta_{i,t}}J(\theta)$$<br>
$i,t$ 表示优化参数$\theta_i$时的第$t$次迭代，$\epsilon$防止分母为0，可以取$10^{-6}$,$G_{i,t}$表示对参数$\theta_i$优化的前$t$步的梯度的累加：<br>
$$G_{i,t} = G_{i,t-1}+\nabla_{\theta_{i,t}}J(\theta) $$<br>
新公式可以简化成:<br>
$$\theta_{t+1}= \theta_t-\frac{\eta}{\sqrt{G_t+\epsilon}}\nabla_{\theta_t}J(\theta)$$<br>
可以从上式看出，随着迭代的推移，新的学习率$\frac{\eta}{\sqrt{G_t+\epsilon}}$在缩小，说明Adagrad一开始激励收敛，到了训练的后期惩罚收敛，收敛速度变慢</p>
<h1 id="rmsprop">RMSprop</h1>
<p>主要解决Adagrad学习率过快衰减问题，类似动量的思想，引入一个超参数，在积累梯度平方项进行衰减.<br>
$$s = \gamma \cdot s +(1-\gamma) \cdot \nabla J(\theta) \odot \nabla J(\theta) $$<br>
参数$\theta$的迭代目标函数可以改写为:<br>
$$\theta_{new} = \theta - \frac{\eta}{\sqrt{s+\varepsilon}} \odot \nabla J(\theta)$$<br>
可以看出$s$是梯度的平方的指数加权移动平均值，$\gamma$一般取0.9，有助于解决 Adagrad中学习率下降过快的情况。</p>
<h2 id="adaptive-moment-estimationadam">Adaptive moment estimation(Adam)</h2>
<p>Adam可以说是用的最多的优化算法，Adam通过计算一阶矩估计和二阶矩估计为不同的参数设计独立的自适应学习率。</p>
<h2 id="adabound">Adabound</h2>
<p>正在学习中</p>
<p>参考文献：<br>
<a href="https://zhuanlan.zhihu.com/p/32626442">https://zhuanlan.zhihu.com/p/32626442</a><br>
<a href="https://zhuanlan.zhihu.com/p/31630368">https://zhuanlan.zhihu.com/p/31630368</a><br>
<a href="https://zh.gluon.ai/">https://zh.gluon.ai/</a><br>
<a href="https://blog.csdn.net/tsyccnh/article/details/76270707">https://blog.csdn.net/tsyccnh/article/details/76270707</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
