<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>LLMs and Graphs | JhuoW‘s Notes</title>
<meta name=keywords content="GNN,LLM"><meta name=description content="Large Language Model, GNNs, and Foundation Models"><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/posts/graphllm/><meta name=google-site-verification content="G-6F49SGED6V"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.2eadbb982468c11a433a3e291f01326f2ba43f065e256bf792dbd79640a92316.js integrity="sha256-Lq27mCRowRpDOj4pHwEybyukPwZeJWv3ktvXlkCpIxY=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://JhuoW.github.io/posts/graphllm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6F49SGED6V"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6F49SGED6V")}</script><meta property="og:title" content="LLMs and Graphs"><meta property="og:description" content="Large Language Model, GNNs, and Foundation Models"><meta property="og:type" content="article"><meta property="og:url" content="https://JhuoW.github.io/posts/graphllm/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-25T01:04:50+08:00"><meta property="article:modified_time" content="2024-05-25T01:04:50+08:00"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="LLMs and Graphs"><meta name=twitter:description content="Large Language Model, GNNs, and Foundation Models"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JhuoW.github.io/posts/"},{"@type":"ListItem","position":2,"name":"LLMs and Graphs","item":"https://JhuoW.github.io/posts/graphllm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLMs and Graphs","name":"LLMs and Graphs","description":"Large Language Model, GNNs, and Foundation Models","keywords":["GNN","LLM"],"articleBody":"1. Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning (TAPE) Shallow model: Encoding the textual attributes using shallow or hand-crafted features such as skip-gram or bag-of-words (BoW) which used in PgG and DGL are limited in the complexity of the semantic features they can capture.\nLM: 指相对较小并且可以被fine-tune的模型。GIANT fine-tune an LM using neighborhood prediction task （在neighborhood prediction task上来微调LM模型）. GLEM fine-tune an LM to predict the label distribution from GNN’s output (GLEM 用GNN预测的伪标签作为监督信号，让LM来fine tune 节点的文本表示)。这些工作需要大量的计算资源，并且由于要微调模型的参数，所以选取的LM相对较小，比如BERT和DeBERTa，因此缺乏LLM的推理能力。\nLLMs refer to very large language models such as GPT-3/4.\nThe present work: LLM augmentation using explanations: 使用解释作为node feature。通过LLM来解释它的预测，这些解释传达了text与prediction的相关知识和LLM的推理步骤，这些解释信息更易于小LM模型吸收消化。如上图所示，首先绿框中的节点text属性通过自定义的prompt来询问LLM，比如GPT-3.5，让GPT来生成关于文本类别的预测排名list（黄色框所示），并且提供这些得到这些预测的解释理由。接着，原始text，LLM的预测，以及解释共同用来fine-tune LM，比如BERT或DeBERTa。然后，LM将他们转化为节点的features用于下游预测。\nFormalization LM for text classification $$ h_n=\\mathbf{L M}\\left(s_n\\right) \\in \\mathbb{R}^d $$ 其中$s_n \\in \\mathcal{D}^{L_n}$是节点$n$的文本属性。LM是已经被预训练的模型如BERT和DeBERTa。LM可以将文本属性编码为文本的表示向量$h_n$。\nLLM and prompting\n输入token序列 $x=\\left(x_1, x_2, \\ldots, x_q\\right)$，目标是输出token序列 $y=\\left(y_1, y_2, \\ldots, y_m\\right)$。并且在输入token序列中加入prompt $p$来对输出施加约束。LLM旨在优化一下条件概率： $$ p(y \\mid \\hat{x})=\\prod_{i=1}^m p\\left(y_i \\mid y_{","wordCount":"1019","inLanguage":"en","datePublished":"2024-05-25T01:04:50+08:00","dateModified":"2024-05-25T01:04:50+08:00","author":{"@type":"Person","name":"JhuoW"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://JhuoW.github.io/posts/graphllm/"},"publisher":{"@type":"Organization","name":"JhuoW‘s Notes","logo":{"@type":"ImageObject","url":"https://JhuoW.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="JhuoW's Notes (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>JhuoW's Notes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://JhuoW.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://JhuoW.github.io/posts/>Posts</a></div><h1 class=post-title>LLMs and Graphs</h1><div class=post-description>Large Language Model, GNNs, and Foundation Models</div><div class=post-meta><span title='2024-05-25 01:04:50 +0800 +08'>May 25, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;JhuoW</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-harnessing-explanations-llm-to-lm-interpreter-for-enhanced-text-attributed-graph-representation-learning-tape aria-label="1. Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning (TAPE)">1. Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning (TAPE)</a><ul><li><a href=#formalization aria-label=Formalization>Formalization</a></li><li><a href=#tape aria-label=TAPE>TAPE</a><ul><li><a href=#%e4%bd%bf%e7%94%a8llms%e7%94%9f%e6%88%90%e9%a2%84%e6%b5%8b%e5%92%8c%e8%a7%a3%e9%87%8a aria-label=使用LLMs生成预测和解释>使用LLMs生成预测和解释</a></li><li><a href=#fine-tuning-lm-interpreter-and-node-feature-extraction aria-label="Fine-Tuning LM Interpreter and Node Feature Extraction">Fine-Tuning LM Interpreter and Node Feature Extraction</a></li><li><a href=#gnn-training-with-tape-features aria-label="GNN Training with TAPE features">GNN Training with TAPE features</a></li></ul></li></ul></li><li><a href=#2-exploring-the-potential-of-large-language-models-llms-in-learning-on-graphs aria-label="2. Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs">2. Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</a><ul><li><a href=#llms aria-label=LLMs>LLMs</a><ul><li><a href=#embedding-visible-llms aria-label="Embedding-visible LLMs">Embedding-visible LLMs</a></li><li><a href=#embedding-invisible-llms aria-label="Embedding-invisible LLMs">Embedding-invisible LLMs</a></li><li><a href=#detailed-four-types-of-llms aria-label="Detailed four types of LLMs">Detailed four types of LLMs</a></li></ul></li><li><a href=#llms-as-enhancers aria-label=LLMs-as-Enhancers>LLMs-as-Enhancers</a><ul><li><a href=#feature-level-enhancement aria-label="Feature-level Enhancement">Feature-level Enhancement</a><ul><li><a href=#1-cascading-structure-%e7%ba%a7%e8%81%94%e7%bb%93%e6%9e%84 aria-label="1. Cascading Structure 级联结构">1. Cascading Structure 级联结构</a></li><li><a href=#2-iterative-structure-%e8%bf%ad%e4%bb%a3%e7%bb%93%e6%9e%84 aria-label="2. Iterative Structure 迭代结构">2. Iterative Structure 迭代结构</a></li><li><a href=#node-classification-comparison aria-label="Node Classification Comparison">Node Classification Comparison</a></li></ul></li><li><a href=#text-level-enhancement aria-label="Text-level Enhancement">Text-level Enhancement</a><ul><li><a href=#1-tape aria-label="1. TAPE">1. TAPE</a></li><li><a href=#2-knowledge-enhanced-augmentation-kea aria-label="2. Knowledge-Enhanced Augmentation (KEA)">2. Knowledge-Enhanced Augmentation (KEA)</a></li><li><a href=#node-classification-comparison-1 aria-label="Node Classification Comparison">Node Classification Comparison</a></li></ul></li></ul></li><li><a href=#llms-as-predictors aria-label=LLMs-as-Predictors>LLMs-as-Predictors</a></li></ul></li><li><a href=#3-one-for-all-towards-training-one-graph-model-for-all-classification-tasks aria-label="3. One for All: Towards Training One Graph Model for All Classification Tasks">3. One for All: Towards Training One Graph Model for All Classification Tasks</a><ul><li><a href=#challenge aria-label=Challenge>Challenge</a></li><li><a href=#ofa-one-for-all aria-label="OFA (One-for-All)">OFA (One-for-All)</a><ul><li><a href=#%e5%b0%86%e4%b8%8d%e5%90%8cdomian%e7%9a%84%e5%9b%be%e8%bd%ac%e5%8c%96%e4%b8%ba%e7%bb%9f%e4%b8%80%e7%9a%84tag%e5%bd%a2%e5%bc%8f aria-label=将不同Domian的图转化为统一的TAG形式>将不同Domian的图转化为统一的TAG形式</a></li><li><a href=#%e7%94%a8nodes-of-interest-noi%e6%9d%a5%e7%bb%9f%e4%b8%80%e4%b8%8d%e5%90%8c%e7%9a%84%e5%9b%be%e4%bb%bb%e5%8a%a1 aria-label="用Nodes-of-Interest (NOI)来统一不同的图任务">用Nodes-of-Interest (NOI)来统一不同的图任务</a></li><li><a href=#graph-in-context-learning%e7%9a%84%e5%9b%be%e6%8f%90%e7%a4%ba%e8%8c%83%e5%bc%8fgraph-prompting-paradigm aria-label="Graph In-Context Learning的图提示范式（Graph Prompting Paradigm）">Graph In-Context Learning的图提示范式（Graph Prompting Paradigm）</a></li></ul></li></ul></li><li><a href=#4-talk-like-a-graph-encoding-graphs-for-large-language-models aria-label="4. Talk like a Graph: Encoding Graphs for Large Language Models">4. Talk like a Graph: Encoding Graphs for Large Language Models</a><ul><ul><li><a href=#graph-encoding-function-gg aria-label="Graph encoding function $g(G)$">Graph encoding function $g(G)$</a></li></ul><li><a href=#experiments aria-label=Experiments>Experiments</a></li></ul></li><li><a href=#5-can-language-models-solve-graph-problems-in-natural-language aria-label="5. Can Language Models Solve Graph Problems in Natural Language?">5. Can Language Models Solve Graph Problems in Natural Language?</a></li></ul></div></details></div><div class=post-content><h1 id=1-harnessing-explanations-llm-to-lm-interpreter-for-enhanced-text-attributed-graph-representation-learning-tape>1. Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning (TAPE)<a hidden class=anchor aria-hidden=true href=#1-harnessing-explanations-llm-to-lm-interpreter-for-enhanced-text-attributed-graph-representation-learning-tape>#</a></h1><p><strong>Shallow model:</strong> Encoding the textual attributes using shallow or hand-crafted features such as skip-gram or bag-of-words (BoW) which used in PgG and DGL <strong>are limited in the complexity of the semantic features they can capture.</strong></p><p><strong>LM:</strong> 指相对较小并且可以被fine-tune的模型。GIANT <strong>fine-tune</strong> an LM using neighborhood prediction task （在neighborhood prediction task上来微调LM模型）. GLEM <strong>fine-tune</strong> an LM to predict the label distribution from GNN&rsquo;s output (GLEM 用GNN预测的伪标签作为监督信号，让LM来fine tune 节点的文本表示)。这些工作需要大量的计算资源，并且由于要微调模型的参数，所以选取的LM相对较小，比如BERT和DeBERTa，因此缺乏LLM的推理能力。</p><p>LLMs refer to very large language models such as GPT-3/4.</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/TAPE.png#center alt=TAPE></p><p><strong>The present work: LLM augmentation using explanations:</strong> 使用解释作为node feature。通过LLM来解释它的预测，这些解释传达了text与prediction的相关知识和LLM的推理步骤，这些解释信息更易于小LM模型吸收消化。如上图所示，首先绿框中的节点text属性通过自定义的prompt来询问LLM，比如GPT-3.5，让GPT来生成关于文本类别的<strong>预测排名list</strong>（黄色框所示），并且<strong>提供这些得到这些预测的解释理由</strong>。接着，原始text，LLM的预测，以及解释共同用来fine-tune LM，比如BERT或DeBERTa。然后，LM将他们转化为节点的features用于下游预测。</p><h2 id=formalization>Formalization<a hidden class=anchor aria-hidden=true href=#formalization>#</a></h2><p><strong>LM for text classification</strong>
$$
h_n=\mathbf{L M}\left(s_n\right) \in \mathbb{R}^d
$$
其中$s_n \in \mathcal{D}^{L_n}$是节点$n$的文本属性。LM是已经被预训练的模型如BERT和DeBERTa。LM可以将文本属性编码为文本的表示向量$h_n$。</p><p><strong>LLM and prompting</strong></p><p>输入token序列 $x=\left(x_1, x_2, \ldots, x_q\right)$，目标是输出token序列 $y=\left(y_1, y_2, \ldots, y_m\right)$。并且在输入token序列中加入prompt $p$来对输出施加约束。LLM旨在优化一下条件概率：
$$
p(y \mid \hat{x})=\prod_{i=1}^m p\left(y_i \mid y_{&lt;i}, \hat{x}\right)
$$
其中$\hat{x}=\left(p, x_1, x_2, \ldots, x_q\right)$是使用prompt约束的input token sequence。</p><h2 id=tape>TAPE<a hidden class=anchor aria-hidden=true href=#tape>#</a></h2><h3 id=使用llms生成预测和解释>使用LLMs生成预测和解释<a hidden class=anchor aria-hidden=true href=#使用llms生成预测和解释>#</a></h3><p>LLMs的prompt包括文章的title和abstract，并要求LLMs预测paper的一个或多个类别标签，并且将这些类别标签按从高到低的概率排序，并且要求LLMs提供预测的解释理由，完整prompt如下图所示：</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/tape_prompt.png#center alt=TAPE></p><p>Abstract输入节点对应paper的摘要，Title输入paper的题目，question则要求模型输出一个或多个paper的预测类别标签并按照可能性排序，同时给出预测的解释。</p><h3 id=fine-tuning-lm-interpreter-and-node-feature-extraction>Fine-Tuning LM Interpreter and Node Feature Extraction<a hidden class=anchor aria-hidden=true href=#fine-tuning-lm-interpreter-and-node-feature-extraction>#</a></h3><p>将LM作为LLM得到文本解释的“理解器”。给定两个预训练好的LMs $\mathrm{LM}_{\text {orig }}$ 和$\mathrm{LM}_{\text {expl }}$，他们的输入分别是原始文本特征$s^{\text {orig }}$和该文本特征的解释$s^{\text {expl }}$，这样我们可以分别得到原始文本和解释的text embeddings:
$$
h_{\text {orig }}=\mathrm{LM}_{\text {orig }}\left(s^{\text {orig }}\right) \in \mathbb{R}^{N \times d}, \quad h_{\text {expl }}=\mathrm{LM}_{\text {expl }}\left(s^{\text {expl }}\right) \in \mathbb{R}^{N \times d}
$$
然后要对LM进行Fine-tuning使其与下游任务。首先使用MLP将原始文本和解释的text embedding分别映射到标签空间：
$$
y_{\text {orig }}=\operatorname{MLP}_{\text {orig }}\left(h_{\text {orig }}\right) \in \mathbb{R}^{N \times C}, \quad y_{\text {expl }}=\operatorname{MLP}_{\text {expl }}\left(h_{\text {expl }}\right) \in \mathbb{R}^{N \times C}
$$
通过最小化分类的cross-entropy loss来对LM进行fine-tuning，从而使$\mathrm{LM}_{\text {orig }}$ 和$\mathrm{LM}_{\text {expl }}$可以分别学习到文本和解释与标签之间的关联（什么样的原始文本对应于什么样的标签、什么样的解释对应于什么样的标签）。</p><p><strong>Ranked prediction features</strong> LLM同时也给出了对于每个节点文本的类别可能性排名，同样也是有价值的信息。假设每个节点有$C=5$个可能的类别数，对这些类别分别做one-hot编码，节点$i$排名第1的类别为类别4，那么$p_{i,1} = [0,0,0,1,0]$，那么LLM为节点$i$预测的top-$k$个label可以拼接成一个$kC$维的向量。然后通过一个线性变换:
$$
h_{\text{pred}} = \operatorname{MLP}_{\text {pred }}(\operatorname{Concat}(p_{i,1}, \cdots, p_{i,k})) \in \mathbb{R}^{N\times d_P}
$$
可以得到每个节点的排序预测特征，如Figure 1中的$h_{\text{pred}}$所示。</p><p><strong>TAPE node feature: $\{[h_{\text {orig }}, h_{\text {expl }}, h_{\text{pred}}]\}$</strong>。其中$h_{\text {orig }}$和$h_{\text {expl }}$是通过LM 对下游任务标签做fine-tuning后的文本特征和解释特征，用来简历原始文本和解释与label之间的联系，$h_{\text{pred}}$是LLMs预测类别标签编码。</p><h3 id=gnn-training-with-tape-features>GNN Training with TAPE features<a hidden class=anchor aria-hidden=true href=#gnn-training-with-tape-features>#</a></h3><p>对于TAPE的三种特征：LM fine-tuning的原始text embedding $h_{\text {orig }}$, LM fine-tuning的解释text embedding $h_{\text {expl }}$，以及LLM的prediction embedding $h_{\text{pred}}$，使用3个GNN模型分别预测labels:
$$
\begin{aligned}
\hat{y}_{\text{orig}} &= \operatorname{GNN}_{\text{orig}}(h_{\text {orig }}, A) \in \mathbb{R}^{N \times C}, \\
\hat{y}_{\text{expl}} &= \operatorname{GNN}_{\text{expl}}(h_{\text {expl }}, A) \in \mathbb{R}^{N \times C}, \\
\hat{y}_{\text{pred}} &= \operatorname{GNN}_{\text{pred}}(h_{\text {pred }}, A) \in \mathbb{R}^{N \times C}
\end{aligned}
$$</p><p>然后基于不同特征得到的预测取平均可以得到模型最终的预测label：
$$
\hat{y}=\operatorname{mean}\left(\hat{y}_{\text {orig }}, \hat{y}_{\text {expl }}, \hat{y}_{\text {pred }}\right) \in \mathbb{R}^{N \times C}
$$</p><h1 id=2-exploring-the-potential-of-large-language-models-llms-in-learning-on-graphs>2. Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs<a hidden class=anchor aria-hidden=true href=#2-exploring-the-potential-of-large-language-models-llms-in-learning-on-graphs>#</a></h1><p><a href=https://arxiv.org/abs/2307.03393>paper</a></p><p>Q1: 能否利用LLMs来弥补图神经网络对contextualized knowledge和semantic comprehension理解不足的缺陷？</p><p>Q2: LLM能否独立运行于图结构任务？</p><h2 id=llms>LLMs<a hidden class=anchor aria-hidden=true href=#llms>#</a></h2><h3 id=embedding-visible-llms>Embedding-visible LLMs<a hidden class=anchor aria-hidden=true href=#embedding-visible-llms>#</a></h3><p>可以获得words, sentences, documents的具体representations（embeddings），如BERT, Sentence-BERT和Deberta。</p><h3 id=embedding-invisible-llms>Embedding-invisible LLMs<a hidden class=anchor aria-hidden=true href=#embedding-invisible-llms>#</a></h3><p>用户无法获取和操作embeddings，通常部署在web服务上，如ChatGPT，只能通过text来进行交互</p><h3 id=detailed-four-types-of-llms>Detailed four types of LLMs<a hidden class=anchor aria-hidden=true href=#detailed-four-types-of-llms>#</a></h3><p><strong>Pre-trained Languagge Models (PLMs):</strong> 指相对较小的LLMs，比如Bert和Deberta，并且可以根据下游数据集进行fine-tuning，比如在下游数据集上fine-tune Deberta然后取最后一个hidden state的embeddings作为text embedding。</p><p><strong>Deep Sentence Embedding Models:</strong> 使用PLMs作为base encoders，并且将训练好的PLMs进一步进行监督或对比学习<strong>预训练</strong>。这样的模型通常不需要Fine-tuning。</p><p><strong>Large Language Models:</strong> 与PLM相比，LLMs具有更强的能力和更多数量级的参数。</p><h2 id=llms-as-enhancers>LLMs-as-Enhancers<a hidden class=anchor aria-hidden=true href=#llms-as-enhancers>#</a></h2><p><strong>利用LLMs来增强节点的文本属性特征，然后用GNN生成预测。</strong></p><p>How LLMs can enhance GNNs by leveraging their <strong>extensive knowledge</strong> and <strong>semantic comprehension</strong> capability?</p><p>Challenge: 不同的LLMs能力不同，越强大的模型有更多使用限制，因此需要对不同的LLMs针对性设计使用策略来充分利用他们的能力。</p><h3 id=feature-level-enhancement>Feature-level Enhancement<a hidden class=anchor aria-hidden=true href=#feature-level-enhancement>#</a></h3><h4 id=1-cascading-structure-级联结构>1. Cascading Structure 级联结构<a hidden class=anchor aria-hidden=true href=#1-cascading-structure-级联结构>#</a></h4><p><img loading=lazy src=/posts/2024-05-26-graphllm/cascading.png#center alt="Cascading Structure"></p><p>先试用embedding-visible的LLMs来对dataset中的text attribute做fine-tuning，然后生成每个节点的文本特征的embeddings，然后将这些embeddings作为node features，与图结构一起数据GNN中来训练GNN模型。</p><h4 id=2-iterative-structure-迭代结构>2. Iterative Structure 迭代结构<a hidden class=anchor aria-hidden=true href=#2-iterative-structure-迭代结构>#</a></h4><p><img loading=lazy src=/posts/2024-05-26-graphllm/iterative.png#center alt="iterative Structure"></p><p>如GLEM[1]，在E步根据真实标签和GNN预测的伪标签来训练PLM，在M步根据真实标签和PLM预测的伪标签和PLM学到的text embedding作为node feature来训练GNN模型，然后训练好的GNN模型和PLM模型都可以用来作为节点标签预测器。</p><h4 id=node-classification-comparison>Node Classification Comparison<a hidden class=anchor aria-hidden=true href=#node-classification-comparison>#</a></h4><p>实验结果来看，</p><ol><li>在下游数据集上fine-tuned PLM模型取最后一层作为text embedding，然后GNN作为predictor的效果来看，fine-tuned PLM并不比简单点TF-IDF强。对于不同的text embedding方式（Fine-tuned PLM，PLM without fine-tuning，online sentence embedding）GNN的表现各不相同。</li><li>在监督训练数据较少的情况下，fine-tuned PLM和迭代结构的GLEM学习到的text embedding用到GNN后，得到的结果比普通的TF-IDF差。</li><li>使用Deep Sentence Embedding Models 如sentence-bert学习到的text embedding + GNN predictor的效果较好。</li><li>LLama的效果弱于deep sentence embedding models，说明简单的增加参数并不能生成对GNN有用的text embedding。</li></ol><h3 id=text-level-enhancement>Text-level Enhancement<a hidden class=anchor aria-hidden=true href=#text-level-enhancement>#</a></h3><h4 id=1-tape>1. TAPE<a hidden class=anchor aria-hidden=true href=#1-tape>#</a></h4><p>使用文本和LLM解释来fine-tuning PLM，LLM的分类解释和分类排序作为增强text embedding。</p><h4 id=2-knowledge-enhanced-augmentation-kea>2. Knowledge-Enhanced Augmentation (KEA)<a hidden class=anchor aria-hidden=true href=#2-knowledge-enhanced-augmentation-kea>#</a></h4><p>使用额外的knowledge来增强PLM。</p><h4 id=node-classification-comparison-1>Node Classification Comparison<a hidden class=anchor aria-hidden=true href=#node-classification-comparison-1>#</a></h4><p>实验结果来看，</p><ol><li>TAPE的效果主要受益于LLMs生成的文本解释。</li><li>TAPE原文中采用PLM （Deberta）作为LM，将text attribute和explanation以及任务的label用来fine-tune PLM。而local sentence embedding model e5-large不fine-tune 直接用text attribute以及LLM的explanation来输出embeddings，相比于TAPE中使用的fine-tuned PLM，e5得到了更好的效果。</li></ol><h2 id=llms-as-predictors>LLMs-as-Predictors<a hidden class=anchor aria-hidden=true href=#llms-as-predictors>#</a></h2><p><strong>LLMs作为独立的predictor</strong></p><p>How LLMs can be adapted to explicit graph structures as a <strong>predictor</strong>?</p><p>Challenge: 如何设计prompt使得LLM可以处理图中的structure和attribute信息。</p><p>直接使用LLM来预测节点类别标签可以使用以下几种prompt策略：</p><ol><li><strong>Zero-shot prompts:</strong> 给定单一节点的文本信息，让LLM预测它的类别标签，如下面的prompt所示。</li></ol><p><img loading=lazy src=/posts/2024-05-26-graphllm/zero_shot_prompt.png#center alt="zero-shot prompt"></p><p>​ 其中，Paper是一个节点的text attribute，Task提供可能的类别标签，然后prompt要求LLM输出一个最可能的类别。</p><ol start=2><li><strong>Few-shot prompts:</strong> 如下图所示，先按照zero-shot的形式给出一些node samples的 prompts，<strong>以及这些samples的ground-truth标签</strong>。最后，给出目标节点的prompt，要求LLMs输出节点的类别。</li></ol><p><img loading=lazy src=/posts/2024-05-26-graphllm/few_shot_prompt.png#center alt="few-shot prompt"></p><ol start=3><li><strong>Zero-shot prompts with CoT (Chain-of-Thoughts):</strong> 基于zero-shot prompt，在prompt中进一步要求LLM生成思考过程，也就是<strong>Think it step by step and output the reason in one sentence</strong> (用一句话来概括推理步骤)。</li></ol><p><img loading=lazy src=/posts/2024-05-26-graphllm/zero_shot_cot.png#center alt="zero-shot cot"></p><ol start=4><li><strong>Few-shot prompts with CoT:</strong> 使用第三个prompt的zero-shot prompts with CoT来分别为多个sample生成推理步骤的sentences，然后将这些这些samples的文本内容、Ground-truth标签和CoT process共同作为prompt，并且要求LLM为当前的目标node输出预测标签以及CoT。</li></ol><p><img loading=lazy src=/posts/2024-05-26-graphllm/few_shot_cot.png#center alt="few-shot cot"></p><h1 id=3-one-for-all-towards-training-one-graph-model-for-all-classification-tasks>3. One for All: Towards Training One Graph Model for All Classification Tasks<a hidden class=anchor aria-hidden=true href=#3-one-for-all-towards-training-one-graph-model-for-all-classification-tasks>#</a></h1><p><strong>What is in-context learning?[2]</strong></p><p>&ldquo;In-Context Learning is a way to use LLMs to learn tasks given only a few examples. During in-context learning, we give the LM a prompt that consists of a list of input-output pairs that demonstrate how to perform a task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens. For example, to answer the two prompts below, the model needs to examine the training examples to figure out the input distribution (financial or general news), output distribution (Positive/Negative or topic), input-output mapping (sentiment or topic classification), and the formatting. "</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/icl.gif#center alt=ICL></p><p>如上图（from [2]）所示，在prompt中包含一些上下文input-output pairs来演示（demonstrate）如何执行任务。在这些prompt的最后添加text input token来让LLM学习上下文中的演示来执行text input token的任务。</p><h2 id=challenge>Challenge<a hidden class=anchor aria-hidden=true href=#challenge>#</a></h2><p>Foundation Model指用单一模型来解决多个任务。但是Graph Foundation Model面临以下挑战：（1）不同来源的图数据在feature表征上通常完全不同。比如molecular graph中的节点特征是其中原子nominal feature的索引，e-commerce networks中的节点特征通常用Bag-of-Word来编码。这些不同来源的图数据特征维度、语义信息和尺度的差别都很大，几乎不可能用同一个模型来学习他们的表示。（2）不同的下游任务涉及图的不同部分（节点级、边级、图级）需要不同的策略和方法来学习表示。（3）如何设计统一的模型来实现跨领域和in-context learning是不确定的。</p><h2 id=ofa-one-for-all>OFA (One-for-All)<a hidden class=anchor aria-hidden=true href=#ofa-one-for-all>#</a></h2><h3 id=将不同domian的图转化为统一的tag形式>将不同Domian的图转化为统一的TAG形式<a hidden class=anchor aria-hidden=true href=#将不同domian的图转化为统一的tag形式>#</a></h3><p>用human-interpretable language来描述节点和边的属性，从而使LLM可以将这些text attribute编码到同一个空间。具体来说，通过以下方式来生成每个节点的text feature:</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/node_text_feature.png#center alt="text feature"></p><p>将不同domain的graph的节点属性用统一形式的文本来描述，如上图中，对于一个节点的特征，它的文本描述以<strong>Feature node</strong>开头，后面的每个$&lt;\text{feature describe}>:&lt;\text{feature content}>$是该节点的一个type-content文本对。如对于一个molecule graph，其中的一个节点是原子，那么它的type是<strong>Atom</strong>，它的content是该原子的属性文本描述<strong>Carbon, Atomic number 6, helix chirality</strong>。同理，边的text feature以<strong>Feature edge</strong>开头进行文本描述。</p><p>在用以上方式得到节点$v_i$的text feature $s_{v_i}$和边$e_{ij}$的text feature $s_{e_{ij}}$后，用LLM（这里用sentence transformer）来将每个节点和边的text feature编码为vector embeddings：
$$
x_i = \operatorname{LLM}(s_{v_i}), \quad x_{ij} = \operatorname{LLM}(s_{e_{ij}})
$$</p><h3 id=用nodes-of-interest-noi来统一不同的图任务>用Nodes-of-Interest (NOI)来统一不同的图任务<a hidden class=anchor aria-hidden=true href=#用nodes-of-interest-noi来统一不同的图任务>#</a></h3><p>图上的任务主要分为3中：node-level tasks，link-level tasks和graph-level tasks。如果要用一个模型来处理这些任务的话，<strong>需要将这些任务统一为一个任务以便于在图数据上训练</strong>。</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/NOI.png#center alt=NOI></p><p>Nodes-of-Interest (<strong>NOI</strong>)指的是一个任务的<strong>目标节点</strong>，如上图的蓝色节点所示，表示为$\mathcal{T}$。对于节点级任务，NOI是待预测节点集合，边级任务NOI是待预测是否有边的节点对，图级任务NOI是待预测图中的所有节点。若一个NOI节点$v$的$h$-hop局部子图表示为$\mathcal{S}_h(v)$，那么NOI中所有目标节点的局部子图共同构成了一个<strong>NOI subgraph</strong>，表示为$\mathcal{G}_h (\mathcal{T})$。
$$
\mathcal{G}_h(\mathcal{T})=\bigcup_{v \in \mathcal{T}} \mathcal{S}_h(v)=\left\{\bigcup_{v \in \mathcal{T}} \mathcal{V}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{E}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{R}_v^h\right\} \quad \text{NOI中所有节点的局部子图共同构成}
$$
<strong>NOI prompt node</strong>：定义NOI prompt node，该节点的文本信息用来描述任务，比如某个数据集上的节点分类、图分类等。NOI prompt node的节点text feature以如下形式构建：</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/NOI_prompt_node.png#center alt=NOI></p><p>即每个数据集的一个task对应于一个prompt node，该node用来描述task。这样，把不同的图任务都用节点的text feature这种统一的形式来表达。</p><h3 id=graph-in-context-learning的图提示范式graph-prompting-paradigm>Graph In-Context Learning的图提示范式（Graph Prompting Paradigm）<a hidden class=anchor aria-hidden=true href=#graph-in-context-learning的图提示范式graph-prompting-paradigm>#</a></h3><p>LLM的一大特性就是它可以通过prompt来实现in-context learning，使得模型可以在不用fine-tuning的情况下适应于不同的任务。比如在few-shot场景下，目标是基于一篇paper的摘要和内容预测它的类别，我们可以为LLM提供每个类别的$k$篇papers作为context加入prompt中，来指导模型基于这些提供的context来生成目标摘要和内容的类别预测。（通过一些<strong>任务相关的其他信息</strong>来指导模型对目标样本的预测）</p><p>本文发现实现图上in-context learning的核心在于操作输入图使其和下游任务对齐。<strong>Graph Prompting Paradigm (GPP)</strong> 即图提示范式旨在操作输入图使其可以从输入数据本身获得任务相关的信息。如图2中的虚线所示，用来描述任务的NOI prompt node与所有NOI node建立连接（任务的目标节点），表示在这些NOI nodes上执行对应prompt的任务（如节点分类，链路预测，图分类等）。图中的$p2t$和$t2p$ edge表示NOI节点和prompt node之间的边。下一步建立NOI prompt node和具体类别之间的联系，如图2中的<strong>Class Node</strong>用来描述该分类任务的每个类别的类信息，任务有多少个类，就有多少个Class Node。Class Node的Text feature如下图所示：</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/class_node.png#center alt=NOI></p><p><strong>Zero-shot Learning:</strong> 对于NOI中的一个节点$q$，用于描述它的任务的NOI prompt node $p_q$，以及Class Node $\{c_i | i \in [N]\}$ ，其中$N$为任务的类别数。prompt node，class node，以及和prompt node 连接的所有边（包括与NOI连接的边）共同构成了<strong>prompt graph</strong> $\mathcal{P}=\left(\mathcal{V}_p, \mathcal{E}_p, \mathcal{R}_p\right)$。</p><p>然后，prompt graph $\mathcal{P}=\left(\mathcal{V}_p, \mathcal{E}_p, \mathcal{R}_p\right)$和NOI subgraph (即NOI节点的局部子图)相结合为<strong>通用graph model</strong>的输入图$\mathcal{G}_m=\left(\mathcal{V}_q^h \cup \mathcal{V}_p, \mathcal{E}_q^h \cup \mathcal{E}_p, \mathcal{R}_q^h \cup \mathcal{R}_p\right)$。如图2中的（a）(b)（c）都是graph model的输入图。通过模型的学习，可以得到每个Class node 的embeddings，如类别$c_i$的Class node embedding为$h_{c_i}$。因为$c_i$与任务相关的prompt node 连接，而prompt node与目标NOI node 连接，因此可以用$h_{c_i}$来推断NOI node属于类别$c_i$的概率：
$$
P[\text { NOI belongs to class } i]=\sigma\left(\operatorname{MLP}\left(h_{c_i}\right)\right)
$$</p><h1 id=4-talk-like-a-graph-encoding-graphs-for-large-language-models>4. Talk like a Graph: Encoding Graphs for Large Language Models<a hidden class=anchor aria-hidden=true href=#4-talk-like-a-graph-encoding-graphs-for-large-language-models>#</a></h1><p>prompt engineering的目的是找到一个合适的方式使LLM $f$可以解析问题$Q$，并且得到他的Answer $\mathcal{A}$，即$\mathcal{A} = f(Q)$。本工作的目标是为LLM$f$提供图$G$，使得LLM可以对图做推理QA，即$\mathcal{A} = f(G,Q)$。在该工作中，固定LLM $f$的参数不变，并引入一个图编码函数（graph encoding function）$g(G): G \to W$用于将图结构数据编码成text，以及一个问题重解析函数$q(Q): W \to W$。训练数据$D$有图$G$，问题$Q$和回答$S$组成，即每个训练数据$(G,Q,S) \in D$。训练目标是固定大语言模型$f$不变，找到最佳的图编码函数$g$和问题重解析函数$q$，使得给定训练$G$和$Q$，得到回答$S$的分数最高：
$$
\max _{g, q} \mathbb{E}_{G, Q, S \in D} \operatorname{score}_f(g(G), q(Q), S)
$$</p><h3 id=graph-encoding-function-gg>Graph encoding function $g(G)$<a hidden class=anchor aria-hidden=true href=#graph-encoding-function-gg>#</a></h3><p>$g(G)$用于将$G$映射为LLM可以处理的token：首先，encode图中的节点，然后encode图中的边。具体编码技术如下图所示：</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/encoding_graph.png#center alt=NOI></p><p><strong>Encoding Nodes</strong>:</p><ol><li><p>整型节点编码：$G$ describes a graph among nodes 0,1,2,3,4,5,6,7</p></li><li><p>使用well-known English first names：$G$ describes a friendship among James, Robert, Michael, Mary.</p></li><li><p>使用电视剧《权力的游戏》和《南方公园》中流行的角色名字。</p></li><li><p>包括美国政治家的名字。</p></li><li><p>用字母表示的。</p></li></ol><p><strong>Representing Edges</strong>:</p><ol><li>用括号表示边：The edge in $G$ given as (0,1), (0,2), &mldr;, (6,7), (7,8)</li><li>Friendship: source node and target node are friends. 如 We have the following edges in $G$: James and Robert are friends, &mldr; Jennifer and Linda are friends。对应于上面的well-known English first name表示节点</li><li>Coauthorship: 比如 James and Robert wrote a paper together 来表示一条边</li><li>Social network: James and Robert are connected来表示一条边</li><li>Arrows: A->B 来表示边</li><li>Incident: Node 8 is connected to nodes 3,7 来表示每个节点的<strong>邻域</strong>。</li></ol><h2 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h2><p>使用PaLM 62B作为LLM，在不同图任务以及不同图编码器下的准确率比较，最有效的prompt (zero-shot、zero-cot、few-show、cot、cot-bag)用下划线标出，最佳图编码器用加粗标出。实验使用ER Graph作为数据集，ER Graph的统计数据如下表所示，可以看出平均节点数为12.37，平均边数为39.79，平均度为5.70。对于边存在任务，有53.96%的情况不存在边，对于cycle check任务，有81.96%的情况存在cycle（因为ER graph很可能存在cycle）。</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/graphqa_dataset.png#center alt=graphqa_datase></p><p>从下面的实验结果可以看出，LLM在所有prompt heuristic的所有graph encoding function上预测的最高边存在概率为44.5%，76%的情况存在cycle。而在Node degree 任务上，均与真实平均度5.7差距较大，平均边数也与真实情况差距较大。</p><p>简单的Prompt比如zero-shot 在简单的任务上比复杂的prompt比如zero-cot效果更好，因为简单的任务无需多跳推理。</p><p>graph encoding function对LLM影响巨大。</p><p>整型节点编码可以提升算数性能，比如node degree、node count和edge count的预测。</p><p><img loading=lazy src=/posts/2024-05-26-graphllm/graphqa_exp.png#center alt=graphqa_exp></p><h1 id=5-can-language-models-solve-graph-problems-in-natural-language>5. Can Language Models Solve Graph Problems in Natural Language?<a hidden class=anchor aria-hidden=true href=#5-can-language-models-solve-graph-problems-in-natural-language>#</a></h1><p>[1] Learning on Large-scale Text-attributed Graphs via Variational Inference.</p><p>[2] <a href=https://medium.com/@francescofranco_39234/in-context-learning-icl-a775cd8b7261>https://medium.com/@francescofranco_39234/in-context-learning-icl-a775cd8b7261</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://JhuoW.github.io/tags/gnn/>GNN</a></li><li><a href=https://JhuoW.github.io/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=next href=https://JhuoW.github.io/posts/orderedgnn/><span class=title>Next Page »</span><br><span>ICLR2023《Ordered GNN：Ordering Message Passing to Deal with Heterophily and Over-smoothing》 Reading Nodes</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share LLMs and Graphs on twitter" href="https://twitter.com/intent/tweet/?text=LLMs%20and%20Graphs&amp;url=https%3a%2f%2fJhuoW.github.io%2fposts%2fgraphllm%2f&amp;hashtags=GNN%2cLLM"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share LLMs and Graphs on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fJhuoW.github.io%2fposts%2fgraphllm%2f&amp;title=LLMs%20and%20Graphs&amp;summary=LLMs%20and%20Graphs&amp;source=https%3a%2f%2fJhuoW.github.io%2fposts%2fgraphllm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share LLMs and Graphs on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fJhuoW.github.io%2fposts%2fgraphllm%2f&title=LLMs%20and%20Graphs"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share LLMs and Graphs on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fJhuoW.github.io%2fposts%2fgraphllm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share LLMs and Graphs on whatsapp" href="https://api.whatsapp.com/send?text=LLMs%20and%20Graphs%20-%20https%3a%2f%2fJhuoW.github.io%2fposts%2fgraphllm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share LLMs and Graphs on telegram" href="https://telegram.me/share/url?text=LLMs%20and%20Graphs&amp;url=https%3a%2f%2fJhuoW.github.io%2fposts%2fgraphllm%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://giscus.app/client.js data-repo=JhuoW/WebComments data-repo-id=R_kgDOHHz8Ug data-category=Announcements data-category-id=DIC_kwDOHHz8Us4COa5e data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>Copyright &copy; 2025 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span></span><br><script>function siteTime(){var h=1e3,r=h*60,i=r*60,n=i*24,x=n*365,e=new Date,d=2019,w=1,_=16,y=19,b=15,C=11,l=e.getFullYear(),O=e.getMonth()+1,f=e.getDate(),p=e.getHours(),g=e.getMinutes(),v=e.getSeconds(),m=Date.UTC(d,w,_,y,b,C),j=Date.UTC(l,O,f,p,g,v),t=j-m,o=Math.floor(t/x),s=Math.floor(t/n-o*365),a=Math.floor((t-(o*365+s)*n)/i),c=Math.floor((t-(o*365+s)*n-a*i)/r),u=Math.floor((t-(o*365+s)*n-a*i-c*r)/h);d==l?document.getElementById("sitetime").innerHTML="本站已运行 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒":document.getElementById("sitetime").innerHTML="本站已运行 "+o+" 年 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒"}setInterval(siteTime,1e3)</script><span id=sitetime>载入运行时间...</span>
<script type=text/javascript id=clustrmaps src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=268&t=tt&d=YsONH-MzO6L7yrkA73Z_QW7LuMTfdUhk0uhb_KaBv-g&co=f5f5f5&cmo=3acc3a&cmn=ff5353&ct=808080"></script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>