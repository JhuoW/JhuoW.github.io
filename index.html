<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.95.0"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var t=MathJax.Hub.getAllJax(),e;for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>JhuoW‘s Notes</title><meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="Jhuo’s Notes"><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/><meta name=google-site-verification content="G-6F49SGED6V"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://JhuoW.github.io/index.xml><link rel=alternate type=application/json href=https://JhuoW.github.io/index.json><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-6F49SGED6V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6F49SGED6V",{anonymize_ip:!1})}</script><meta property="og:title" content="JhuoW‘s Notes"><meta property="og:description" content="Jhuo’s Notes"><meta property="og:type" content="website"><meta property="og:url" content="https://JhuoW.github.io/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="JhuoW‘s Notes"><meta name=twitter:description content="Jhuo’s Notes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"JhuoW‘s Notes","url":"https://JhuoW.github.io/","description":"Jhuo’s Notes","thumbnailUrl":"https://JhuoW.github.io/favicon.ico","sameAs":["https://github.com/JhuoW","mailto:jhuow@proton.me","https://t.me/funjhuow","https://open.spotify.com/playlist/0HMI5oRLTSYuZPYie7B1bG?si=f2ec0a118a8a4297","https://gitlab.com/JhuoW","https://arxiv.gtflashlab.com/","https://JhuoW.github.io/index.xml","https://github.com/The-Run-Philosophy-Organization/run","https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="JhuoW's Notes (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>JhuoW's Notes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://JhuoW.github.io/gallery/ title=Gallery><span>Gallery</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>💟 Welcome to JhuoW&rsquo;s Notebook 🌻</h1></header><section class=entry-content><p><p>Hey, this is JhuoW. I use this blog to write notes about machine learning and AI conference papers.</p><ul><li>I&rsquo;m a chatterbox~ Please feel free to chat with me via email or telegram 👇.</li></ul></p></section><footer class=entry-footer><div class=social-icons><a href=https://github.com/JhuoW target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=mailto:jhuow@proton.me target=_blank rel="noopener noreferrer me" title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a><a href=https://t.me/funjhuow target=_blank rel="noopener noreferrer me" title=Telegram><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21.198 2.433a2.242 2.242.0 00-1.022.215l-8.609 3.33c-2.068.8-4.133 1.598-5.724 2.21a405.15 405.15.0 01-2.849 1.09c-.42.147-.99.332-1.473.901-.728.968.193 1.798.919 2.286 1.61.516 3.275 1.009 4.654 1.472.509 1.793.997 3.592 1.48 5.388.16.36.506.494.864.498l-.002.018s.281.028.555-.038a2.1 2.1.0 00.933-.517c.345-.324 1.28-1.244 1.811-1.764l3.999 2.952.032.018s.442.311 1.09.355c.324.022.75-.04 1.116-.308.37-.27.613-.702.728-1.196.342-1.492 2.61-12.285 2.997-14.072l-.01.042c.27-1.006.17-1.928-.455-2.474a1.654 1.654.0 00-1.034-.407z"/></svg></a><a href="https://open.spotify.com/playlist/0HMI5oRLTSYuZPYie7B1bG?si=f2ec0a118a8a4297" target=_blank rel="noopener noreferrer me" title=Spotify><svg fill="currentcolor" stoke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.4.0.0 5.4.0 12s5.4 12 12 12 12-5.4 12-12S18.66.0 12 0zm5.521 17.34c-.24.359-.66.48-1.021.24-2.82-1.74-6.36-2.101-10.561-1.141-.418.122-.779-.179-.899-.539-.12-.421.18-.78.54-.9 4.56-1.021 8.52-.6 11.64 1.32.42.18.479.659.301 1.02zm1.44-3.3c-.301.42-.841.6-1.262.3-3.239-1.98-8.159-2.58-11.939-1.38-.479.12-1.02-.12-1.14-.6s.12-1.021.6-1.141C9.6 9.9 15 10.561 18.72 12.84c.361.181.54.78.241 1.2zm.12-3.36C15.24 8.4 8.82 8.16 5.16 9.301c-.6.179-1.2-.181-1.38-.721-.18-.601.18-1.2.72-1.381 4.26-1.26 11.28-1.02 15.721 1.621.539.3.719 1.02.419 1.56-.299.421-1.02.599-1.559.3z"/></svg></a><a href=https://gitlab.com/JhuoW target=_blank rel="noopener noreferrer me" title=Gitlab><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22.65 14.39 12 22.13 1.35 14.39a.84.84.0 01-.3-.94l1.22-3.78 2.44-7.51A.42.42.0 014.82 2a.43.43.0 01.58.0.42.42.0 01.11.18l2.44 7.49h8.1l2.44-7.51A.42.42.0 0118.6 2a.43.43.0 01.58.0.42.42.0 01.11.18l2.44 7.51L23 13.45a.84.84.0 01-.35.94z"/></svg></a><a href=https://arxiv.gtflashlab.com/ target=_blank rel="noopener noreferrer me" title=Arxiv><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4v16a2 2 0 002 2h12a2 2 0 002-2V8.342a2 2 0 00-.602-1.43l-4.44-4.342A2 2 0 0013.56 2H6A2 2 0 004 4z"/><path d="M9 13h6"/><path d="M9 17h3"/><path d="M14 2v4a2 2 0 002 2h4"/></svg></a><a href=https://JhuoW.github.io/index.xml target=_blank rel="noopener noreferrer me" title=Rss><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a><a href=https://github.com/The-Run-Philosophy-Organization/run target=_blank rel="noopener noreferrer me" title=External-Link><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-external-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M11 7H6A2 2 0 004 9v9a2 2 0 002 2h9a2 2 0 002-2v-5"/><line x1="10" y1="14" x2="20" y2="4"/><polyline points="15 4 20 4 20 9"/></svg></a><a href="https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM" target=_blank rel="noopener noreferrer me" title=Other><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2>MLP and GNNs</h2></header><section class=entry-content><p>最近一些工作通过解耦Message-Passing 和 Feature Learning的方式来提升GNN的可拓展性，这里对一小部分相关工作做一个小总结。
1. Combining Label Propagation and Simple Models Out-performs Graph Neural Networks （ICLR2021） 模型首先忽略图结构，用简单模型（MLP），只使用节点特征预测label：
$$ \min \sum_{i \in L_{t}} \ell\left(f\left(x_{i}\right), y_{i}\right) $$ 考虑一个inductive bias：预测误差与邻近度关系强相关，对图中所有节点的误差做校正。
具体来说，首先计算一个初始的误差矩阵$E$，其中训练集误差如下 $$ E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:} $$ 其他节点的误差未知：$E_{L_{v},:}=0, \quad E_{U,:}=0$。然后通过Label Propagation将误差矩阵在图上做平滑，使得相邻节点的误差相似：
$$ \hat{E}=\underset{W \in \mathbb{R}^{n \times c}}{\arg \min } \operatorname{trace}\left(W^{T}(I-S) W\right)+\mu||W-E||_{F}^{2} $$ 由此得到所有节点的误差矩阵$\hat{E}$。然后用$\hat{E}$对基础MLP预测做校正，这个post-processing过程不涉及训练参数，校正后的预测为： $$ Z^{(r)} = Z + \hat{E} $$ 考虑homophily：校正的预测label要满足相邻节点label相似。 注意，这里不直接对$Z^{(r)}$做Label Propagation，而是构造了一个label矩阵$H \in \mathbb{R}^{n \times c}$，其中将训练集真实label和验证+测试集校正label加入$H$中，然后对$H$做label propagation： $$ \begin{aligned} H_{L_{t},:}&=Y_{L_{t},:} \\ H_{L_{v} \cup U,:}&=Z_{L_{v} \cup U,:}^{(r)} \end{aligned} $$ Label Prop: $$ H^{(t+1)}=(1-\alpha) H+\alpha S H^{(t)} $$ 最后直接用收敛的$H$做预测，即$\hat{Y} = H^{\infty}$，node $i$ 的预测class为： $$ y_i = \arg \max _{j \in\{1, \ldots, c\}} \hat{Y}_{i j} $$...</p></section><footer class=entry-footer><span title="2022-08-27 00:00:00 +0000 UTC">August 27, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to MLP and GNNs" href=https://JhuoW.github.io/posts/glnn/></a></article><article class=post-entry><header class=entry-header><h2>ICLR2021 《Combining Label Propagation and Simple Models Out-performs Graph Neural Networks》 Reading Notes</h2></header><section class=entry-content><p>paper
Introduction 本文研究了结合更简单的模型来处理transductive node classification任务。 主要包括1个预测模块和两个后处理（post-processing）模块：
Base predictor：忽略图结构，用简单模型（如MLP或线性模型）使用节点特征预测label Error correction：校正步骤，将训练数据中的不确定性（误差）传播到图上，来校正Base predictor的预测 Smoothing：在图上平滑预测 其中只有第一步base predictor的参数是可学习的，即涉及图结构的操作（Correction和Smoothing）无需参数学习，这种简单的模型使得参数数量减少了几个数量级，训练时间也减少了几个数量级，并且可以轻松扩展到大规模图。
相比于过去的GNN+LP的方法，C&S更加高效：1）C&S首先只使用节点特征进行低成本的base prediction；2）然后再使用标签传播对基础预测进行校正 ；3）最后对最终预测进行平滑。 第一步是预测操作，后两部是后处理操作，也就是第一步为一个独立的端到端模型，后两部基于一个inductive bias来调整节点的表示。即homophily假设：相连节点的误差和label是相似的（正相关）。训练节点的误差和它相连节点的误差应相似，那么就用训练节点的误差来校正邻居节点。
因此，将标签更加直接的整合到GNN的学习算法中是本文性能的关键，并且发现LP与node features是相互互补的信号。实验表明，在OGB-Products上，参数量比GNN少了2个数量级，训练时间也减少2个数量级。
Correct and Smooth (C&S) Model 给定无向图$G=(V,E)$，$A$为邻接矩阵，$S=D^{-1 / 2} A D^{-1 / 2}$为归一化邻接矩阵。节点集划分为labeled nodes $V_L$和unlabeled nodes $V_U$，其中$V = V_L \cup V_U$。进一步，labeled nodes可以分为训练节点集$V_{L_t}$和验证节点集$V_{L_v}$。训练集和验证集的label分别为$Y_{L_t:}$和$Y_{L_v:}$， 每行为label的one-hot向量。
Simple Base Predictor $$ \min \sum_{i \in L_{t}} \ell\left(f\left(x_{i}\right), y_{i}\right) $$
$f(\cdot)$为简单的训练模型+softmax，如浅层MLP， $\ell$为cross-entropy loss。 基于训练节点$V_{L_t}$特征的模型$f$可以得到输出预测$Z \in \mathbb{R}^{n\times c}$， 其中$Z$的每行是softmax得到的分类概率分布。Simple Base Predictor是一个独立训练的端到端模型。
Correcting Base Prediction with Error Correlation (使用邻居误差关联来纠正基础预测） 通过融合标签信息来提高base prediction $Z$的准确率。 本文期望base prediction中的误差沿着图中的边正相关，即节点$i$出的预测误差在它的邻居处也会出现相似的误差。为了实现这个目的，首先定义一个误差矩阵$E \in \mathbb{R}^{n \times c}$用来保存每个节点的预测误差，其中误差为训练数据集上的残差（只有训练节点由误差）其他没有训练过程中不知道label的节点误差设为0： $$ E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:} \quad 为训练集节点 V_{L_t}的误差 $$...</p></section><footer class=entry-footer><span title="2022-07-11 09:42:15 +0800 CST">July 11, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICLR2021 《Combining Label Propagation and Simple Models Out-performs Graph Neural Networks》 Reading Notes" href=https://JhuoW.github.io/posts/c_and_s/></a></article><article class=post-entry><header class=entry-header><h2>ICLR2022 《GLASS：GNN with Labeling Tricks for Subgraph Representation Learning》 Reading Notes</h2></header><section class=entry-content><p>Paper
Introduction SubGNN在学习子图representation时保留子图的三种属性：Position，Neighborhood，Structure，每种属性包含Internal 和Border两方面，并且要精心设计不同的anchor patch，所以过于复杂。通过分析SubGNN和普通GNN，作者发现子图表示的核心可能是区分子图内部和外部节点。基于此发现，本文提出了一种labeling trick, 即max-zero-one，来提升子图GNN的表达能力和可拓展性。
Subgraph representation task 如上图所示，目标子图$\mathbb{S}$被嵌入在整个图中，并且可能拥有多个连通分量，目标是学习子图的表示向量，使其可以预测子图的属性。NeurIPS2020文章SubGNN提出子图级message-passing来代替节点级的message passing，并且设计了三个message passing通道，每个通道分为内部和边界模块，分别捕获子图分量间的交互，以及子图与图的其他部分之间的交互。尽管取得了比普通GNN更好的效果，但是SubGNN需要繁琐的预计算，因为SubGNN通过不同采样规则的anchor patch来传递子图分量之间，以及子图分量与图其他部分之间的相关性，而三个通道共6个aspects需要不同的采样规则，以及各自的message passing，计算十分冗长（这里有解读）。另外 SubGNN对每个aspects需要使用不同的anchor patch随机采样策略，无法保证采样的anchor patch是最优的，因此效果的方差较大，使得鲁棒性堪忧。
通过对比SubGNN相较于普通GNN的优势，作者发现对于子图任务来说，区分子图内部节点和外部节点非常重要。基于这个发现，本文提出了一种labeling trick，即max-zero-one labeling trick，来标注每个节点是否在子图外或者子图内。
Labeling Trick [1]: 使用GNN生成multi-node representations （即，为一组节点，例如子图生成表示向量），该方法说明了为高阶结构生成有表达能力的representation，需要捕获结构内不同节点间的交互。在实现上，labeling trick通过一个专门设计的label来表示节点的结构特征，这个label与node feature 结合作为新的feature输入GNN中。
注： 本文只考虑诱导子图，即每个子图的每个连通分量保留原图中的所有边。
Plain GNN and SubGNN 如上图所示，$G$是一个regular graph, 所以在没有节点feature的情况下，每个节点的embedding相同，所以GNN无法区分子图$\mathcal{S}$和$\mathcal{S}^\prime$。如下图所示，Plain GNN 在message passing中子图$\mathcal{S}$内部节点1同时接收来自子图内和子图外的邻居信息，并不会加以区分。同样$\mathcal{S}^\prime$中节点3也同时接收子图内外节点，因此对于Plain GNN ，它无法区分节点1和3，因此无法区分两个子图。
而SubGNN引入了3个通道：position (P)，neighborhood (N), 和structure (S) 每个通道分别学习Internal 和Border两方面，共6个属性融入子图表示学习中。对于子图$\mathcal{S}$，为了捕获某个通道$i$的属性，SubGNN首先随机采样$n_A$个anchor patches： $\mathbb{A}_{i}=\left\{\mathcal{A}_{i}^{(1)}, \ldots, \mathcal{A}_{i}^{\left(n_{A}\right)}\right\}$，然后学习$\mathcal{S}$中的每个连通分量在这个属性$i$下的表示向量，通过子图内部连通分量和anchor patches之间的消息传递，来捕获子图内部连通分量的相对位置/邻域/结构信息，以及子图连通分量相对于子图外部分的位置/邻域/结构信息。如图2右边所示。对于通道$i$，它的Internal和border两方面采样的anchor patches表示为$\mathbb{A}_{i}=\left\{\mathcal{A}_{i}^{(1)}, \ldots, \mathcal{A}_{i}^{\left(n_{A}\right)}\right\}$，对于子图$\mathcal{S}$的一个连通分量$\mathcal{S}^{(c)}$，要学习该连通分量的表示，可使用一下subgraph-level message passing layer: $$ \begin{aligned} &\boldsymbol{a}_{i, \mathcal{S}^{(c)}}=\sum_{\mathcal{A}_{i} \in \mathbb{A}_{i}} \gamma_{i}\left(\mathcal{S}^{(c)}, \mathcal{A}_{i}\right) \boldsymbol{g}_{\mathcal{A}_{i}}, \\ &\boldsymbol{h}_{i, \mathcal{S}^{(c)}}^{(k)}=\sigma\left(W_{i} \cdot\left[\boldsymbol{a}_{i, \mathcal{S}^{(c)}}, \boldsymbol{h}_{i, \mathcal{S}^{(c)}}^{(k-1)}\right]\right) \end{aligned} $$ 其中$\gamma_{i}\left(\mathcal{S}^{(c)}, \mathcal{A}_{i}\right)$是子图分量$\mathcal{S}^{(c)}$和一个anchor patch $\mathcal{A}_{i}$的相似度。即每个子图分量依照与anchor patch 的相似度聚合来自anchor的信息。由于相似度函数的存在，SubGNN实际上是使用与子图分量$\mathcal{S}^{(c)}$接近或结构相似的anchor patch对$\mathcal{S}^{(c)}$的representation做平滑，即$\mathcal{S}^{(c)}$聚合更多与它结构相似的anchor patches的信息。通过精心设计的anchor和subgraph-level message passing，6个属性可以被各自保留，然后在融合。...</p></section><footer class=entry-footer><span title="2022-06-09 23:01:07 +0800 CST">June 9, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICLR2022 《GLASS：GNN with Labeling Tricks for Subgraph Representation Learning》 Reading Notes" href=https://JhuoW.github.io/posts/glass/></a></article><article class=post-entry><header class=entry-header><h2>NeurIPS2020 《Subgraph Neural Networks》 Reading Notes</h2></header><section class=entry-content><p>paper
Introduction GNN通常关注节点级任务和图级任务，缺少针对子图级预测任务的方法。针对这个问题，本文提出SubGNNs用于解耦子图在不同结构aspect的表示。为了学习准确的子图表示，SubGNN在子图的连通分量和随机采样的anchor patches之间进行消息传递，从而学习高准确度的子图表示。SubGNN指定了三个通道，每个通道捕获子图不同的拓扑结构属性。
从拓扑的角度来看，子图是非常具有挑战性的结构，对子图的预测存在以下挑战：
如要对更大且size不同的子图做联合预测，挑战在于如何表征含有多个分量，甚至分量间间隔较远的子图。 子图包含了高阶连通模式（connectivity patterns），这些连通模式不仅存在于子图内节点之间，也存在与子图内节点与子图外部节点之间， 挑战在于如何将子图边界信息和子图外部信息注入GNN中。 子图可能存在于图中的一个特定区域，也可能它的连通分量分布于多个局部邻域，挑战在于如何学习子图在图中的位置。 子图间共享边（sharing edges）和非边（non-edges）存在相关性，挑战在于如何将这种子图间的依赖融合进模型中，同时任然能够将特征信息考虑在内进行辅助归纳推理。 本文提出SubGNN以解决上述挑战， SubGNN的核心原则是子图级的消息传递，可以捕获子图位置、邻域、结构三种特征
Formulating Subgraph Prediction 给定无向图$G=(V,E)$，它的一个子图表示为$S=\left(V^{\prime}, E^{\prime}\right)$，每个子图$S$有一个label $y_{S}$，并且子图$S$可能包含多个连通分量，连通分量表示为$S^{(C)}$。
Problem (Subgraph Representations and Property Prediction) 给定子图集合 $\mathcal{S} = \left\{S_{1}, S_{2}, \ldots, S_{n}\right\}$，SubGNN $E_S$为每个子图$S\in \mathcal{S}$生成一个$d_s$维的表示向量$\mathbf{Z}_S \in \mathbb{R}^{d_{s}}$， 然后用这些子图的表示向量学习一个子图分类器 $f: \mathcal{S} \rightarrow\{1,2, \ldots, C\}$，使得输入子图得到预测label: $f(S)=\hat{y}_{S}$。
本文针对子图分类任务，所提出的模型为一个可学习的embedding函数$E_{S}: S \rightarrow \mathbb{R}^{d_{s}}$， 将每个子图映射为低维表示向量，这些表示向量可以捕获子图拓扑对预测重要的aspects。具体来说，对于一个子图，message再它的连通分量之间传递，这使得我们可以对多个连通分量的子图学习有意义的表示。
SUBGNN: Properties of subgraph topology 子图拥有独特的内部结构，边界连通性，邻域概念，以及和图其他部分的相对位置。直觉上，我们的目标是以最大的似然保存保存特定的图属性。本文设计模型以考虑6种特定的图结构属性：
具体来说：
(1) Position.
Border Position: 该属性保留子图和图的其他部分之间的距离，通过这种距离关系，可以区分两个同构但处于不同位置的子图。
Internal Position：子图自己连通分量之间的距离。
(2) Neighborhood.
Border Neighborhood：为子图的边界邻域，表示子图$S$中任意节点的$k$跳邻域中（不属于子图$S$）的节点集合。
Internal Neighborhood：子图内每个连通分量的边界邻域，每个连通分量$S^{(c)}$中任意节点的$k$跳邻域中（不属于子图$S^{(c)}$）的节点集合。...</p></section><footer class=entry-footer><span title="2022-05-27 17:13:33 +0800 CST">May 27, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NeurIPS2020 《Subgraph Neural Networks》 Reading Notes" href=https://JhuoW.github.io/posts/subgnn/></a></article><article class=post-entry><header class=entry-header><h2>NeurIPS2021 《Decoupling the Depth and Scope of Graph Neural Networks》 Reading Notes</h2></header><section class=entry-content><p>paper
Introduction 现有的GNN在图和模型的size方面可拓展性有限。 对于大图，增加模型的深度对导致scope(感受野)大小成指数放大。深层model主要面临两个基本挑战： 1. oversmoothing导致表达能力下降，2. 邻域爆炸导致计算成本高昂。
本文旨在结构GNN的depth 和 scope，首先提取子图作为有限大小（bounded-size）的scope, 然后将任意深度的GNN用于子图上。 由于提取出的局部子图是由少量关键邻居组成，且排除了不相关的邻居，所以深层GNN也可以学到informative representations。
增加GNN层数会造成以下基本障碍：
Expresivity (oversmoothing): 邻居的迭代混合导致不同节点的切入向量收敛到一个固定的低维子空间 Scalability (neighbor explosion): 多跳邻居递归导致感受野大小呈指数级增长 为了研究导致表达能力和可拓展性缺陷的根本原因，本文提出了以下insight:
Two views on the graph: 如果从全局视角来看两个节点，如果两个节点在同一个图的同一个连通分量中，那么这两个节点在随机游走中存在到达概率，无论他们间隔多远。而本文给出了图的局部视角，具体来说， 给定节点$v$的局部子图$\mathcal{G}_{[v]}$，将$\mathcal{G}_{[v]}$仅包含节点$v$的特性，整个图看一看做所有子图$\mathcal{G}_{[v]}$的集合。那么$v$的邻域不在是所有节点$\mathcal{V}$，而它的邻域只存在于$\mathcal{V}_{[v]}$中。 如果节点$u$不在$\mathcal{V}_{[v]}$中，$u$将永远不会被考虑为$v$的邻居，无论GNN有多深。
Scope of GNNs: 加深GNN层次所造成的的表达能力和可拓展性问题都和GNN不断扩大的感受野（scope）有关。随着层次变深，感受野不断变大，使得每个节点包含的信息重叠越多，最终收敛到同一个子空间，导致oversmoothing; 另外 感受野变大，导致每个节点的邻居数呈指数级上升，导致邻居爆炸。所以GNN的层数加深会导致感受野变大（耦合），即$L$层GNN的感受野为全部$L$-hop以内的邻居，层数深度（depth）和感受野大小(scope)的强耦合限制了GNN的设计。
Decoupling the GNN depth and scope: 为了解耦GNN的深度（depth）与感受野(scope)，使得加层数与感受野无关。对于节点$v$，首先为它提取一个小的子图$\mathcal{G}_{[v]}$，然后在小的子图上应用任意层数的GNN。若GNN的层数$L^\prime$大于感受野的跳数，那么子图中的每对节点会交换多次信息，额外的消息传递有助于GNN更好的融合scope内的信息，从而增强表达能力。
Decoupling the Depth and Scope of GNNs Definition (Depth of subgraph) ：假设子图$\mathcal{G}_{[v]}$是连通的，$\mathcal{G}_{[v]}$的depth定义为$\max _{u \in \mathcal{V}_{[v]}} d(u, v)$, 其中$d(u, v)$表示$u$到$v$的最短路径。
本文提出shaDow-GNN，它包含了一个子图提取器$\text { EXTRACT}$。 shaDow-GNN的过程如下：
用子图提取器$\operatorname{EXTRACT}(v, \mathcal{G})$为节点$v$提取一个连通子图$\mathcal{G}_{[v]}$，子图的深度（距离$v$最远的节点和$v$之间的跳数）为$L$。 构建一个$L^\prime$层的GNN并应用在$\mathcal{G}_{[v]}$上。 如果 $L^\prime > L$那么可以反映decoupling，因为GNN层数此时与scope无关，层数加深不会影响感受野。 本文从三个不同的角度理论证明了shaDow-GNN可以提升GNN的表达能力。...</p></section><footer class=entry-footer><span title="2022-05-21 00:00:00 +0000 UTC">May 21, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NeurIPS2021 《Decoupling the Depth and Scope of Graph Neural Networks》 Reading Notes" href=https://JhuoW.github.io/posts/decouplinggcn/></a></article><article class=post-entry><header class=entry-header><h2>NeurIPS2021 《From Canonical Correlation Analysis to Self-supervised Graph Neural Networks》 Reading Notes</h2></header><section class=entry-content><p>paper
Introduction 本文提出了一种新型的Graph Contrastive Learning构造Contrastive pairs的方式，即将跨图的同维度特征作为positive pairs， 不同维度特征作为negative pairs。 和过去的GCL方法相比，本文无需互信息估计器（MI Estimator），映射头（Projector），不对称结构（asymmetric structures）。 并且理论证明了该方法可以看做Information Bottleneck 原则在自监督设置下的实例。
具体来说，受典型相关分析（From Canonical Correlation Analysis）的启发，本文提出了一种简单有效的GCL框架，从而是模型避免复杂难以理解多模块设计。 和过去的方法相同的是，为输入图以随机增强的方式生成两个view， 目的是为两个view学习共享的 node representations 通过共享的GNN Encoder。不同在于，本文利用了典型相关分析（CCA），具体来说，新目标旨在最大化同一输入的两个增强views之间的相关性，同时对单个视图表示的不同（特征）维度进行去相关（避免不同维度捕获相同信息，即同一个view内的不同维度channel互为negative pairs）。 这么做的目的是 1）本质上追求的是丢弃增强后变化的信息，同时保留增强后不变的信息，以及 2）防止维度崩溃（即不同维度捕获相同的信息）。
和其他方法的对比如上图所示， 本文提出的CCA-SSG无需negative pairs， 参数化的互信息估计器， projection head或者不对称结构。对比对的数量仅为$O(D^2)$, 其中$D$为输出维度。
Canonical Correlation Analysis CCA: Identify and Quantify the associations between two sets of variables， 即CCA用来衡量两组随机变量的相关性，每组可能有很多Random Variables.
从相关系数引入：
Pearson 相关系数： 给定两组数据集$X$， $Y$。 其中$X \in \mathbb{R}^{N \times 1}$ 表示只有一个随机变量（属性），样本数为$N$。 $Y \in \mathbb{R}^{M \times 1}$: 一个随机变量，样本量为$M$。那么Pearson 相关系数$\rho$定义为： $$ \rho(X,Y)= \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y} $$ 其中$\sigma_X$，$\sigma_Y$分别为$X$和$Y$的标准差。$\mathrm{Cov}(X,Y)$为$X$, $Y$的协方差。$\rho \in [-1,1]$。 $\rho$越接近1， $X$和$Y$的线性相关性越高。$\rho$越接近0，$X$和$Y$的线性相关性月底。...</p></section><footer class=entry-footer><span title="2022-04-14 22:54:10 +0800 CST">April 14, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NeurIPS2021 《From Canonical Correlation Analysis to Self-supervised Graph Neural Networks》 Reading Notes" href=https://JhuoW.github.io/posts/cca-ssg/></a></article><article class=post-entry><header class=entry-header><h2>ICML2020 《Contrastive Multi-View Representation Learning on Graphs》 Reading Notes</h2></header><section class=entry-content><p>paper
Introduction 本文旨在通过多视图Contrastive Learning 来学习节点表示和图表示。其中对比视图为结构视图（structural view）。本文发现，两个以上的对比视图不会提升性能（我觉得仅是针对本文的Diffusion-based view吧~）。 本文实验性的表明了基于一阶邻居和图扩散视图做CL可以达到最好的效果。
为了将对比学习应用到图表示学习任务，本文提出通过最大化图的不同结构视角的互信息来作为监督信号。通过对提出框架的系统研究，本文展示了一些GCL和visual CL上的不同： （1）将view数量（即增强）增加到两个以上不会提高性能，最好的性能是通过对比来自一阶邻居view的embedding和graph diffusion的embedding，(2) 与对比图编码或多尺度编码相比，跨视图对比节点和图编码在node classification 和 graph classification上都能获得更好的结果。 (3) 与分层图池化方法（例如DiffPool相比）一个简单的Readout在这node classification 和 graph classification上实现了更好的性能，以及 (4) 应用正则化（early stopping除外） 或归一化层对性能有负面影响。
Method MVGRL通过最大化一个view的node embedding和另一个view的graph embedding之间的 互信息来学习节点和图表示。如上图所示，MVGRL由以下几个部分构成
增强机制：将样本图转化为同一个图的相关view， 这个view只是structural view， 不会改变原图中的node feature，然后对两个增强图中的相同节点（identical node）进行子采样，类似于CV中的域剪裁。 两个专用的GNNs， 每个view一个GNN，再接一个共享的MLP作为projection head，来为两个view学习representation。 图池化层， 在MLP后学习两个图的graph-level representation。 判别器 来对比一个图的embedding和另一个图的节点embedding,并对他们的一致性（agreement）评分。 Augmentations 考虑两种类型的图增强：(1) 对初始节点特征进行操作的特征空间增强，例如，mask或添加高斯噪声，以及 (2) 通过添加或删除连通性、子采样或使用最短路径或diffusion matrix生成全局视图来对做图结构增强。 前一种增强可能是有问题的，因为许多数据集不带有初始节点特征。 此外，观察到在任一空间上屏蔽或添加噪声都会降低性能。 因此，本文选择生成全局视图，然后进行子采样。
实验表明，在大多数情况下，最好的结果是通过将邻接矩阵转化为扩散矩阵，并将这两个矩阵视为同一图的结构的两个一致view。因为邻接矩阵和扩散矩阵分别提供了图结构的局部和全局视图，从这两种view中学习到的表示之间最大一致性，从而鼓励模型同时编码的局部和全局信息。
Diffusion matrix从全局角度提供了任意节点对之间的相关性，其中$\mathbf{T} \in \mathbb{R}^{n \times n}$是通用转移矩阵，$\Theta$是权重系数，决定了全局和局部信息的比例，即对于每个节点，不同层次信息的比重， $\Theta_{k}$越大，表示全局信息权重越大。 令$\sum_{k=0}^{\infty} \theta_{k}=1, \theta_{k} \in[0,1]$，$\lambda_{i} \in[0,1]$,其中$\lambda$是$\mathbf{T}$的特征向量， 这样来保证$\mathbf{S}$可以收敛到一个固定矩阵。扩散用快速近似值和稀疏化方法计算： $$ \mathbf{S}=\sum_{k=0}^{\infty} \Theta_{k} \mathbf{T}^{k} \in \mathbb{R}^{n \times n} $$ 给定一个邻接矩阵$\mathbf{A} \in \mathbb{R}^{n \times n}$和一个对角度矩阵$\mathbf{D} \in \mathbb{R}^{n \times n}$, Personalized PageRank (PPR)和Heat Kernel分别为两种不同的Diffusion matrix实例。对于PPR和HK，转移概率矩阵定义为$\mathbf{T}=\mathbf{A} \mathbf{D}^{-1}$。PPR将第$k$层的权重系数设置为$\theta_{k}=\alpha(1-\alpha)^{k}$, 而HK将第$k$层的权重系数设置为$\theta_{k}=e^{-t} t^{k} / k !...</p></section><footer class=entry-footer><span title="2022-04-12 22:21:29 +0800 CST">April 12, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICML2020 《Contrastive Multi-View Representation Learning on Graphs》 Reading Notes" href=https://JhuoW.github.io/posts/mvgrl/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://JhuoW.github.io/page/2/>Next Page »</a></nav></footer></main><footer class=footer><span>Copyright &copy; 2022 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span></span><br><script>function siteTime(){var u=1e3,r=u*60,a=r*60,n=a*24,x=n*365,e=new Date,d=2019,O=1,w=16,_=19,y=15,m=11,l=e.getFullYear(),C=e.getMonth()+1,f=e.getDate(),p=e.getHours(),g=e.getMinutes(),v=e.getSeconds(),b=Date.UTC(d,O,w,_,y,m),j=Date.UTC(l,C,f,p,g,v),s=j-b,o=Math.floor(s/x),t=Math.floor(s/n-o*365),i=Math.floor((s-(o*365+t)*n)/a),c=Math.floor((s-(o*365+t)*n-i*a)/r),h=Math.floor((s-(o*365+t)*n-i*a-c*r)/u);d==l?document.getElementById("sitetime").innerHTML="本站已运行 "+t+" 天 "+i+" 小时 "+c+" 分钟 "+h+" 秒":document.getElementById("sitetime").innerHTML="本站已运行 "+o+" 年 "+t+" 天 "+i+" 小时 "+c+" 分钟 "+h+" 秒"}setInterval(siteTime,1e3)</script><span id=sitetime>载入运行时间...</span>
<script type=text/javascript id=clustrmaps src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=268&t=tt&d=YsONH-MzO6L7yrkA73Z_QW7LuMTfdUhk0uhb_KaBv-g&co=f5f5f5&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>