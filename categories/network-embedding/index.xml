<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Network Embedding on JhuoW‘s Notes</title>
    <link>https://JhuoW.github.io/categories/network-embedding/</link>
    <description>Recent content in Network Embedding on JhuoW‘s Notes</description>
    <image>
      <url>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 May 2019 10:46:44 +0000</lastBuildDate><atom:link href="https://JhuoW.github.io/categories/network-embedding/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AAAI2017 M-NMF:《Community Preserving Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/m-nmf/</link>
      <pubDate>Wed, 29 May 2019 10:46:44 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/m-nmf/</guid>
      <description>AAAI2017 &amp;#34;Community Preserving Network Embedding&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14589">M-NMF</a></p>
<h1 id="introduction">Introduction</h1>
<p>Network Embedding的基本需求是保存网络机构和固有属性。而先前的方法主要保存了网络的围观结构（microscopic structure）如一阶相似度和二阶相似度（LINE，SDNE等）， 忽略了网络结构的介观社区结构（mesoscopic community structure）。针对这个问题，本文提出了一种模块化非负矩阵分解模型M-NMF，该模型将网络的社区结构融入network embedding中。利用节点表示和社区结构之间的一致关系（consensus relationship）， 联合优化基于非负矩阵分解的表示学习模型和基于模块化的社区发现模型， 这样就可以在节点表示中同时保存了<strong>微观网络结构</strong>和<strong>介观社区结构</strong>。</p>
<p>具体来说，对于微观结构（节点对相似性），本文使用矩阵分解来融合节点的一节相似性和二阶相似性；对于介观社区结构，通过模块度约束项来检测社区。因此，上面的两项可以通过节点的表示和基于辅助社区表示矩阵社区结构之间的一致关系来联合优化。同时，本文给出了乘法更新策略，保证了再推导参数的时候的正确性和收敛性。</p>
<h1 id="m-nmf-model">M-NMF Model</h1>
<p>对于一个无向图$G=(V,E)$, 图中有$n$个节点和$e$条边，用一个0,1二值邻接矩阵表示为$\mathbf{A}=[A_{i,j}] \in \mathbb{R}^{n \times n}$。 $\mathbf{U} \in \mathbb{R}^{n \times m}$ 为节点的表示矩阵，其中$m \leq n$，$m$是节点的嵌入维度。</p>
<h2 id="建模社区结构">建模社区结构</h2>
<p>本文采用基于模块最大化的社区发现方法 Modularity， 给定一个邻接矩阵表示的网络$\mathbf{A}$，$\mathbf{A}$包含两个社区，根据<a href="http://engr.case.edu/ray_soumya/mlrg/2006%20Modularity%20and%20community%20structure%20in%20networks.pdf">Newman 2006b</a>，模块度可以定义如下：
$$
Q=\frac{1}{4 e} \sum_{i j}\left(A_{i j}-\frac{k_{i} k_{j}}{2 e}\right) h_{i} h_{j}
$$
其中，$k_i$表示节点$i$的度，如果$i$在第一个社区中，那么$h_i=1$,否则$h_i = -1$。</p>
<p>$k_ik_j$表示将所有边一分为二 参考<a href="https://blog.csdn.net/wangyibo0201/article/details/52048248">模块度Q</a>，那么节点$i$,$j$之间可能产生的边数。$\frac{k_{i} k_{j}}{2 e}$如果所有边随机放置，节点$i$,$j$之间的期望边数。定义一个模块度矩阵$\mathbf{B} \in \mathbb{R}^{n \times n}$，其中$B_{i,j}=A_{i,j}-\frac{k_{i} k_{j}}{2 e}$，那么$Q=\frac{1}{4 e} \mathbf{h}^{T} \mathbf{B h}$，其中$\mathbf{h}=[h_i] \in \mathbb{R}^n$，表示社区成员指标器。</p>
<p>如果将$Q$拓展到$k &gt; 2$个社区，那么：
$$
Q=\operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B H}\right), \quad \text { s.t. } \quad \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{H}\right)=n
$$
其中$tr()$表示矩阵的迹（主对角线元素和），$\mathbf{H}$是社区成员指标器，$\mathbf{H} \in \mathbb{R}^{n \times k}$，每行表示一个节点所属社区的one-hot编码。</p>
<h2 id="建模微观结构">建模微观结构</h2>
<h3 id="一阶相似度">一阶相似度</h3>
<p>$$
S^{(1)} = \mathbf{A}
$$</p>
<h3 id="二阶相似度">二阶相似度</h3>
<p>表示为$S^{(2)}$，表示节点的邻域相似度， 用邻接向量的余弦相似度表示：
$$
S_{i j}^{(2)}=\frac{\mathcal{N}_{i} \mathcal{N}_{j}}{\left|\left|\mathcal{N}_{i}\right|\right|\left|\left|\mathcal{N}_{j}\right|\right|}
$$</p>
<p>结合网络的一阶结构的一阶二阶相似度，最终的网络相似度矩阵可以表示为：
$$
\mathbf{S}^{(1)}+\eta \mathbf{S}^{(2)}
$$
然后，文中引入了一个偏置矩阵$\mathbf{M} \in \mathbb{R}^{n \times m}$ 和一个非负表示矩阵$\mathbf{U} \in \mathbb{R}^{n \times m}$。所以微观结构的目标函数就是节点相似度和节点表示之间的误差：
$$
\min \left|\mathbf{S}-\mathbf{M} \mathbf{U}^{T}\right|_{F}^{2}
$$
因为$\mathbf{S} \in \mathbb{R}^{n \times n}$， 矩阵$\mathbf{U} \in \mathbb{R}^{n \times m}$，矩阵$\mathbf{M}$的作用是把$\mathbf{U}$转成$n \times n$，这样就可以计算损失函数了。</p>
<h2 id="统一的ne模型">统一的NE模型</h2>
<p>引入一个非负辅助矩阵$\mathbf{C} \in \mathbb{R}^{k \times m}$, 即为社区表示矩阵，每一行$C_r$表示第$r$个社区的$m$维表示向量。 如果一个节点的表示向量和一个社区的表示向量接近，那么这个节点就很可能在这个社区中。我们把节点$i$和社区$r$之间的从属关系定义为：
$$
\mathbf{U}_{i} \mathbf{C}_{r}
$$
如果两个向量正交，则$\mathbf{U}_{i} \mathbf{C}_{r} = 0$ 那么节点$i$不可能存在于社区$r$中。所以需要使$\mathbf{U}\mathbf{C}^T$更加近似社区指示器$\mathbf{H}$,所以定义如下目标函数：
$$
\begin{aligned}
\min_{\mathbf{M}, \mathbf{U}, \mathbf{H}, \mathbf{C}}\left|\left|\mathbf{S}-\mathbf{M} \mathbf{U}^{T}\right|\right|_{F}^{2}+\alpha\left|\left|\mathbf{H}-\mathbf{U} \mathbf{C}^{T}\right|\right|_{F}^{2}-\beta \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B} \mathbf{H}\right) \\
s.t., \quad \mathrm{M} \geqslant 0, \mathrm{U} \geqslant 0, \mathrm{H} \geqslant 0, \mathrm{C} \geqslant 0, \operatorname{tr}\left(\mathrm{H}^{T} \mathrm{H}\right)=n
\end{aligned}
$$
其中$\alpha$和$\beta$是正参数，最后一项是要最大化模块度。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>SIGIR18 《BiNE:Bipartite Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/bine/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/bine/</guid>
      <description>SIGIR2018 &amp;#34;BiNE:Bipartite Network Embedding&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.comp.nus.edu.sg/~xiangnan/papers/sigir18-bipartiteNE.pdf">BiNE</a></p>
<h1 id="introduction">Introduction</h1>
<p><strong>Bipartite Network(二分网络)</strong>:如下图所示：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/stru.png#center" alt="你想输入的替代文字"  />
<br>
二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。</p>
<p>另一个问题，<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/r1.png#center" alt="你想输入的替代文字"  />
 <img loading="lazy" src="/posts/2019-03-13-BiNE/r2.png#center" alt="你想输入的替代文字"  />
<br>
如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。<br>
另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。</p>
<p>针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过<a href="http://www.cs.cornell.edu/home/kleinber/auth.pdf">HITS</a>来衡量。</p>
<h1 id="model">Model</h1>
<p>如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\overrightarrow{u_i}]$, $V=[\overrightarrow{v_i}]$，结构如下图所示：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/3.png" alt="你想输入的替代文字"  />
（取自作者的讲解ppt)</p>
<h2 id="explicit-relations">Explicit Relations</h2>
<p>同LINE一样， 基于直接连接的目标函数表示为：<br>
$$\mathrm{minimize} \quad O_1=-\sum_{e_{ij} \in E}w_{ij}\log \hat{P}(i,j)$$</p>
<h2 id="implicit-relations">Implicit Relations</h2>
<h3 id="构造随机游走序列">构造随机游走序列</h3>
<p>这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：<br>
$$w^U_{ij}=\sum_{k \in V}w_{ik}w_{jk}$$<br>
$$w^V_{ij}=\sum_{k \in U}w_{ki}w_{kj}$$<br>
其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。
<img loading="lazy" src="/posts/2019-03-13-BiNE/Al1.png" alt="你想输入的替代文字"  />
<br>
其中$l=\max(H(v_i)\times \max T,\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。<br>
$$D_{v_i}=\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$<br>
表示其中一次随机游走的节点集合$p$表示停止概率。</p>
<p>通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。</p>
<h3 id="对间接关系建模">对间接关系建模</h3>
<p>如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。
<img loading="lazy" src="/posts/2019-03-13-BiNE/dd.png#center" alt="你想输入的替代文字"  />
<br>
对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \in C_s(u_i)$的条件概率。所以目标函数如下：
$$\mathrm{maximize} \quad O_2 = \prod_{u_i \in S \land S \in D^U} \prod_{u_c \in C_s(u_i)}P(u_c|u_i)$$<br>
对于$D^V$同理。其中,$p(u_c|u_i) = \frac{\exp(\overrightarrow{u}_i^T \overrightarrow{\theta}_c)}{\sum^{|U|}_{k=1} \exp(\overrightarrow{u}_i^T \overrightarrow{\theta}_k))}$。</p>
<h3 id="negative-sampling">Negative Sampling</h3>
<p>本文的负采样方法是基于局部敏感哈希（LSH）来对与中心节点不相似的节点进行采样。
该方法的strategy是，给定一个中心节点，随机选取一个bucket（序列）并且这个序列不包含给定的中心节点，以此来获得和给定节点尽量不相似的负采样节点。<br>
$N^{ns}_S (u_i)$ 表示$ns$个负采样节点，对于中心节点$u_i$, 那么上文提到的条件概率$p(u_c|u_i)$可以被定义为下式：<br>
$$p(u_c,N^{ns}_S (u_i)|u_i) = \prod_{z \in {u_c} \cup N^{ns}_S (u_i)} P(z|u_i)$$<br>
其中条件概率$P(z|u_i)$定义为：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/4.png#center" alt="你想输入的替代文字"  />
<br>
其中$\sigma$表示sigmoid函数，这样就减少了上文softmax函数造成的计算量过大的问题。</p>
<h2 id="联合优化">联合优化</h2>
<p>通过随机梯度上升对3部分损失函数进行加权优化：<br>
$$\mathrm{maximize} \quad L = \alpha \log O_2+\beta \log O_3 - \gamma O_1$$
最终BiNE的整体算法流程如下：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/Al2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="conclusion">Conclusion</h1>
<p>这篇文章提出的分布式训练以及负采样策略还是很值得学习的。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>AAAI2018 《GraphGAN:Graph Representation Learning with Generative Adversarial Nets》Reading Notes</title>
      <link>https://JhuoW.github.io/posts/graphgan/</link>
      <pubDate>Tue, 22 Jan 2019 16:57:05 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/graphgan/</guid>
      <description>AAAI2018 &amp;#34;GraphGAN:Graph Representation Learning with Generative Adversarial Nets&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址:<a href="https://arxiv.org/abs/1711.08267">GraphGAN</a></p>
<h2 id="introduction">Introduction</h2>
<p>GAN是CV上非常火的方法，作者将其引入Network Embedding领域。为了达到这个目的，首先，作者将当前的NE model分成了两类，分别是生成模型(generative model)和判别模型(discriminative model)。简单来说，生成模型。<br>
所谓生成模型，就是对于每个节点$v_c$，都存在一个潜在的真实连接分布$p_{true}(v|v_c)$, 这个分布反映了由条件生成样本的概率，因此，生成式模型的目的就是求网络中边的极大似然。比较经典的方法有DeepWalk，是通过随机游走来获得center node的上下文，然后最大化上下文节点的似然。Node2vec拓展了DeepWalk,提出了biased随机游走，使得模型在为给定节点生成上下文节点时具有更多的灵活性。<br>
所谓判别模型，目的是学习一个分类器来直接预测边是否存在。将两个节点视为输入特征，然后输出预测的两个节点之间有边的概率，即$p(edge|(v_i,v_j))$,常见的方法有SDNE，PPNE。</p>
<p>于是，本文结合生成对抗网络(GAN),提出了GraphGAN来同一生成和判别器。其中生成器$G(v|v_c)$试图拟合真实的连接概率分布$p_{true}(v|v_c)$,生成最可能和$v_c$有链接的点。判别器$D(v,v_c)$尝试区分强连接节点对和若连接节点对，然后计算$v$和$v_c$存在边的可能性。简单来说，就是把生成器生成的可能与$v_c$有边的节点放入判别器计算两者有边的概率。</p>
<p>除此之外GAN框架下，然而生成器的连接分布是通过softmax实现的，但是传统的softmax无法适配生成器，原因如下:<br>
(1).传统的softmax对网络中的所有节点一视同仁，缺乏对网络结构相似度信息的考虑。<br>
(2).计算成本太高。<br>
因此，论文中提出了新的生成器实现方法 Graph Softmax。并且证明GS具有归一化（normalization）、网络结构感知（graph structure awareness）和高计算效率（computational efficiency）的性质。相应的，文中也提出了基于随机游走(random walk)的生成器策略。</p>
<h2 id="model">Model</h2>
<p>这里挑特别的来说。$\mathcal{N}(v_c)$表示和$v_c$直接相连的节点集。我们把给定节点$v_c$与任意节点$v \in \mathcal{V}$的真实连接分布表示为$p_{true}(v|v_c)$。这么看$\mathcal{N}(v_c)$可以看做从$p_{true}(v|v_c)$中采样的集合。文章的目的是学习两个模型:<br>
<strong>Generator</strong> $G(v|v_c;\theta_G)$ 生成器，尝试拟合真实连接分布$p_{true}(v|v_c)$，并且从节点集$\mathcal{V}$中生成最有可能和$v_c$相连的节点。<br>
<strong>Discriminator</strong> $D(v,v_c;\theta_G)$ 判别器，目的是判断已对接点$(v,v_c)$的连接性。$D(v,v_c;\theta_G)$输出$v$和$v_c$有边的概率。</p>
<p>生成器$G$尝试完美拟合$p_{true}(v|v_c)$，并且生成和$v_c$真实邻居高度相似的节点来欺骗判别器。相反，判别器尝试发现输入的顶点是真实的节点对或者有生成器生成出的样本。这是一个极大极小游戏，目标函数$V(G,D)$:<br>
$$\min_{\theta_G} \max_{\theta_D} V(G,D)=\sum_{c=1}^V (\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]+\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))])$$<br>
上面这个公式是本文最关键的公式，以我的分析就是：在给定$\theta_D$的情况下，对其最小化。先来分析$\max_{\theta_D}V(G,D)$,即给定$\theta_G$,使原式最大。当给定$\theta_G$时，通过改变$\theta_D$,使$\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]$达到最大（是判别器D拟合真实分布），同时使$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$最大，即尽可能减小选定节点$v_c$与判别器中生成的$v_c$邻居节点与$v_c$连接的可能性。然后在给定$\theta_D$的情况下，通过改变生成器$\theta_G$继续生成节点，使得$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$尽可能小，即上一步的判别器尽可能把当前生成器生成的节点认为是邻居。 之后再更新判别器,以此类推。这样$G$和$D$各自提高性能，最终$G$生成的分布和真实连接分布无法区分，这样达到最好的效果。整体过程如下图所示:</p>
<p><img loading="lazy" src="/posts/2019-01-22-GraphGAN/1.png" alt="你想输入的替代文字"  />
</p>
<h3 id="discriminator-optimization">Discriminator Optimization</h3>
<p>对于判别器来说是个简单的sigmoid函数，来计算两个输入节点的表示向量:<br>
$$D(v,v_c;\theta_D)=\frac{1}{1+\exp(-d^\top_v d_{v_c})}$$<br>
其中，$d_v$,$d_{v_c}$是两个输入节点关于判别器$D$的表示向量，$\theta_D$是所有节点表示向量的结合。注意到上面的公式只涉及$v$和$v_c$, 我们只需要更新$d_v$,$d_{v_c}$，通过梯度下降的方法可以实现：<br>
<img loading="lazy" src="/posts/2019-01-22-GraphGAN/2.png" alt="你想输入的替代文字"  />
</p>
<h3 id="generator-optimization">Generator Optimization</h3>
<p>对于生成器来说，目标是最小化判别器将生成的样本判断为负样本的对数似然（概率）。换句话说，生成器会调整自己的连接分布（通过调整生成的所有节点的向量表示$\theta_G$）来提升对生成样本的判别分数。由于$v$的采样时离散的，所以使用policy gradient来计算$V(G,D)$关于$\theta_G$的梯度：<br>
<img loading="lazy" src="/posts/2019-01-22-GraphGAN/3.png" alt="你想输入的替代文字"  />
<br>
为了理解上述公式，注意到$\nabla_{\theta_G}V(G,D)$是一个由$\log(1-D(v,v_c;\theta_D))$加权的梯度$\nabla_{\theta_G}\log G(v|v_c;\theta_G)$的求和，直观上说，这意味着有高概率是负样本的节点会将生成器G“拉着”远离自己（因为我们在$\theta_G$上执行梯度下降)。</p>
<h3 id="graph-softmax">Graph Softmax</h3>
<p>graph softmax的核心思想是定义一种新的计算连接性概率的方式，满足以下性质:</p>
<ul>
<li>归一化：$\sum_{v \neq v_c;\theta_G}=1$。</li>
<li>图结构感知：生成器充分利用网络中的结构信息，来估计真实连接分布，如果两个节点在图上越远，那么他们间有边的概率越小。</li>
<li>高效的计算：和传统的softmax不同，$G(v|v_c;\theta_G)$的计算应只涉及图中的一小部分点。</li>
</ul>
<p>因此，本文以图中节点$v_c$为例，以$v_c$为根构建一棵BFS树$T_c$。$\mathcal{N}_c(v)$为节点$v$在$T_c$上的邻居集合（包括他的父节点和所有子节点）。对于一个given vertex $v$和它的一个邻居$v_i \in \mathcal{N}_c(v)$,定义概率为:<br>
$$p_c(v_i|v)=\frac{\exp (g_{v_i}^\top g_v)}{\sum_{v_i \in \mathcal{N}_c(v)} \exp(g_{v_j}^\top g_v)}$$<br>
这是一个在$\mathcal{N}_c(v)$上的softmax函数。</p>
<p>为了计算$G(v|v_c;\theta_G)$,注意到在$T_c$上，根节点$v_c$到每个节点$v$都有一条唯一的路径， 把这条路径记为$P_{v_c \to v}=(v_{r_0},v_{r_1},&hellip;,v_{r_m})$,其中$v_{r_0}=v_c$, $v_{r_m}=v$,那么在graph softmax中，将$G(v|v_c;\theta_G)$定义为:<br>
$$G(v|v_c;\theta_G)\triangleq (\prod^m_{j=1} p_c(v_{r_j}|v_{r_{j-1}})) \cdot p_c(v_{r_{m-1}}|v_{r_m})$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>CIKM2018 《Enhanced Network Embeddings via Exploiting Edge Labels》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2019-01-22-ne-edge-labels/</link>
      <pubDate>Tue, 22 Jan 2019 11:02:29 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-01-22-ne-edge-labels/</guid>
      <description>CIKM2018 &amp;#34;Enhanced Network Embeddings via Exploiting Edge Labels&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址: <a href="https://arxiv.org/abs/1809.05124?context=physics.soc-ph">Enhanced Network Embeddings via Exploiting Edge Labels</a></p>
<h2 id="introduction">Introduction</h2>
<p>这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。</p>
<p>具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个<strong>半监督NE模型</strong>。</p>
<h2 id="problem-definition">Problem Definition</h2>
<p>假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,&hellip;,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \subseteq L$。目的还是一样，学习一个映射函数$\Phi \to \mathbb{R}^{|V| \times d}$, 其中$d \ll |V|$。</p>
<h2 id="method">Method</h2>
<p>首先定义损失函数:<br>
$$\mathcal{L}=(1-\lambda)\mathcal{L}_s+\lambda\mathcal{L}_r$$<br>
其中$\mathcal{L}_s$表示预测节点邻域的损失。$\mathcal{L}_r$表示预测边label的损失。$\lambda$是两种损失的权重。</p>
<h3 id="structural-loss">Structural Loss</h3>
<p>第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：<br>
$$\mathcal{L}_s=-\sum_{u \in C(v)} \log Pr(u|v)$$<br>
这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：<br>
$$Pr(u|v)=\frac{\exp(\Phi(u) \cdot \Phi&rsquo;(v))}{\sum_{u&rsquo; \in V} \exp(\Phi(u&rsquo;) \cdot \Phi&rsquo;(v))}$$<br>
这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\Phi(v)$和$\Phi&rsquo;(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。<br>
剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：<br>
$$C(v)={v_{i-w},&hellip;,v_{i-1}} \cup {v_{i+1},&hellip;,v_{i+w}}$$</p>
<h3 id="relational-loss">Relational Loss</h3>
<p>由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \in E$,可以用一下方法来表示这条边:<br>
$$\Phi(e)=g(\Phi(u),\Phi(v))$$<br>
其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：<br>
$$\Phi(e)=\Phi(u) \oplus \Phi(v)$$<br>
这样我们就获得了edge embedding。直接将它输入前馈神经网络，前馈神经网络第$k$层的定义为:<br>
$$h^{(k)}=f(W^{(k)}h^{(k-1)}+b^{(k)})$$<br>
其中 $h^{(0)}=\Phi(e)$，$f$是除最后一层外采用relu激活函数，最后一层采用sigmoid函数激活,最后一层输出为$\hat{y_i}$。最后最小化二元交叉熵损失函数：</p>
<p>$$\mathcal{L}_r=\sum^{|L|}_{i=1} H(y_i,\hat{y_i}) + (1-y_i) \cdot \log (1-\hat{y_i})$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2016 SDNE:《Structral Deep Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/sdne/</link>
      <pubDate>Mon, 21 Jan 2019 15:34:36 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/sdne/</guid>
      <description>KDD2016 &amp;#34;Structral Deep Network Embedding&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">SDNE</a></p>
<h3 id="introduction">Introduction</h3>
<p>这是一篇比较早的Network Embedding论文， 较早将深度模型应用到NE任务。 首先，本文提出了当前网络表示学习中遇到的三个问题：<br>
<strong>（1）. 高度非线性</strong><br>
<strong>（2）. 尽可能保持网络结构</strong><br>
<strong>（3）. 现实网络的高度稀疏性</strong><br>
SDNE的主要目标就是保持网络结构的一阶相似性和二阶相似性。<br>
一阶相似性就是网络中边相连的节点对之间具有的相似性。<br>
二阶相似性就是在一个Graph中，拥有共同邻居但是不直接向相连的两个节点具有的相似性。</p>
<p>其中，一阶相似性主要反映了网络的局部特征。 二阶相似性反映了网络的全局特征。</p>
<h3 id="model">Model</h3>
<p>本文的模型主要如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-01-21-SDNE/SDNE.png#center" alt="你想输入的替代文字"  />
</p>
<p>这张图看上去有点复杂，实则原理非常简单。</p>
<p>模型分为无监督部分和有监督部分，无监督部分是一个<strong>深度自编码器</strong> 用来捕获二阶相似度（保留全局结构），监督部分是一个拉普拉斯特征映射捕获一阶相似度（局部结构）。呃呃呃，Emmmmm ,不知道是我理解有问题还是其他原因，文章里说的和我理解的不太一样 /(ㄒoㄒ)/~~。 然后介绍一下具体的模型结构：</p>
<p>深度自编码器的编码部分：</p>
<p>$$y_i^{(k)}=\sigma{(W^{(k)}y_i^{(k-1)}+b^{(k)})}, k=2,&hellip;,K$$</p>
<p>假设第$k$层是 节点$v$的表示向量（仅考虑全局信息），那么从第$k$层开始解码，最终得到$\hat{x_i}$, 所以自编码器的误差就是输入节点$v$的邻接向量的重构误差。所以，二阶相似度损失函数定义为:<br>
$$\mathcal{L}=\sum_{i=1}^n{||\hat{x_i}-x_i||^2_2}$$</p>
<p>值得注意的是，由于网络的稀疏性，邻接矩阵中的0元素远多于非0元素，使用邻接矩阵作为输入的话要处理很多0，这样就做了太多无用功了。为了解决这个问题，对损失函数做了改进如下：<br>
$$\mathcal{L_{2nd}}=\sum_{i=1}^n||(\hat{x_i}-x_i)\odot{b_i}||^2_2=||\hat{X}-X\odot{B}||^2_F$$</p>
<p>其中$\odot$是哈马达乘积，表示对应元素相乘。$b_i={b_{i,j}}^n_{j=1}$， 邻接矩阵中的0对应$b=1$, 非0元素的$b&gt;1$,这样的目的是对于有边连接的节点增加惩罚。可以理解为对有边连接的节点赋予更高权重。</p>
<p>以上我们获得了二阶相似度的损失函数。在介绍一阶相似度之前，我们先来看看<strong>拉普拉斯映射（Laplacian Eigenmap）</strong>  其实LE也是一种经典的NRL方法，主要目的也是降维。其目标函数如下所示:<br>
$$\sum_{i,j} W_{ij}||y_i-y_j||^2$$<br>
LE是通过构建相似关系图来重构局部特征结构,如果放在网络结构中来说,如果节点$v_i$和$v_j$很接近（有边），那么他们在embedding space中的距离也应该相应接近。$y_i$和$y_j$就表示他们在特征空间中的表示。因此，本文定义了保持一阶相似度的目标函数：<br>
$$\mathcal{L_{1st}}=\sum_{i,j=1}^n{s_{i,j}||y_i^{(K)}-y_j^{(K)}||^2_2}=\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}$$<br>
具体来说，$K$就是自编码器第$K$层的输出，即编码结果，需要保持一条边的两个节点在嵌入空间中的表示相对接近。</p>
<p>最终 结合一阶相似度和二阶相似度，本文给出了SDNE的目标函数：<br>
$$\mathcal{L_{mix}}=\mathcal{L_{2nd}+\alpha{\mathcal{L_{1st}}}}+\nu{\mathcal{L_{reg}}}
=||(\hat{X}-X)\odot{B}||^2_F+\alpha{\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}}+\nu{\mathcal{L_{reg}}}$$<br>
其中，为了防止过拟合，添加了$\mathcal L2$-norm单元$\mathcal{L_{reg}}$来防止过拟合:<br>
$$\mathcal{L_{reg}}=\frac{1}{2}\sum_{k=1}^k({||W^{(k)}||^2_F+||\hat{W}^{k}||_F^2})$$</p>
<h3 id="optimization">Optimization</h3>
<p>使用SGD来优化$\mathcal{L_{mix}}$。具体算法如下：<br>
<img loading="lazy" src="/posts/2019-01-21-SDNE/al.png#center" alt="你想输入的替代文字"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2017 《metapath2vec:Scalable Representation Learning for Heterogeneous Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/metapath2vec/</link>
      <pubDate>Fri, 29 Jun 2018 16:29:18 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/metapath2vec/</guid>
      <description>KDD2017 &amp;#34;metapath2vec:Scalable Representation Learning for Heterogeneous Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址： <a href="https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf">metapath2vec</a></p>
<h1 id="introduction">Introduction</h1>
<p>真实网络中通常包含多种类型的节点和关系，这些节点和关系共同组成了异质信息网络（Heterogeneous Information Network），传统的NE方法如DeepWalk, Line , node2vec更多关注与同质网络从而忽略了网络的异质性。针对这个问题，这篇文章提出了metapath2vec基于meta-path的随机游走来构建节点的异质邻域，采用异质Skip-gram模型来学习节点的representation。 另外，在负采样时也考虑异质性，从而进一步提出了metapath2vec++。如下：</p>
<p><img loading="lazy" src="/posts/2019-06-29-metapath/1.png" alt=""  />
</p>
<h1 id="definition">Definition</h1>
<p><strong>Definition 1:</strong> 异质网络： $G = (V,E,T)$，其中每个节点和边分别对应一个映射 $\phi(v) : V \rightarrow T_{V}$ 和$\varphi(e) : E \rightarrow T_{E}$。 其中$T_V$和$T_E$分别表示节点的类型集合以及关系的类型集合。且$\left|T_{V}\right|+\left|T_{E}\right|&gt;2$。</p>
<p><strong>Definition 2:</strong> 异质网络表示学习 为网络中的节点学习一个$d$维的表示 $\mathrm{X} \in \mathbb{R}^{|V| \times d}$，$d \ll|V|$。节点的向量表示可以捕获节点间结构和语义关系。</p>
<h1 id="model">Model</h1>
<h2 id="metapath2vec">metapath2vec</h2>
<p>Skip-Gram模型是给定一个节点，预测向下文节点的概率。metapath2vec的目的是考虑多种类型节点和多种类型关系的情况下，最大化网络概率。为了将异质信息融入skip-gram模型中，首先提出异质skip-gram。</p>
<h3 id="heterogeneous-skip-gram">Heterogeneous Skip-Gram</h3>
<p>对于节点类型$|T_V| &gt; 1$的网络$G$， 给定一个节点$v$，需要最大化属于类型$t \in T_V$的异质上下文节点的概率：
$$
\arg \max_{\theta} \sum_{v \in V} \sum_{t \in T_{V}} \sum_{c_{t} \in N_{t}(v)} \log p\left(c_{t} | v ; \theta\right)
$$
其中，$N_t(v)$表示$v$属于第$t$个类型的邻居节点。 $p\left(c_{t} | v ; \theta\right)$ 表示给定节点$v$，它的$t$类型邻域概率： $p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u \in V} e^{X_{u} \cdot X_{v}}}$，是一个softmax函数，$X_v$是节点$v$的表示。由于softmax函数的分母每一次都要遍历所有节点，所以采用负采样改进$\log p(\cdot)$ 如下：
$$
\log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u^{m} \sim P(u)}\left[\log \sigma\left(-X_{u^{m}} \cdot X_{v}\right)\right]
$$
其中 $\sigma(x)=\frac{1}{1+e^{-x}}$。</p>
<h3 id="meta-path-based-random-walks">Meta-Path-Based Random Walks</h3>
<p>在第$i$步时，转移概率$p(v^{i+1}|v^i)$表示为忽略节点类型情况下$v^i$的邻居分布。但是，PathSim提出，异质信息网络中的随机游走偏向于高度可见的节点，即具有主导数量路径的节点，所以 本文设计了基于元路径的随机游走来生成path，从而能够捕获不同类型节点间的结构联系和语义关系，提出了促进异构网络结构转换为metapath2vec的skip-gram。</p>
<p>一个meta-path模式$\mathcal{P}: V_{1} \stackrel{R_{1}}{\longrightarrow} V_{2} \stackrel{R_{2}}{\longrightarrow} \dots V_{t} \stackrel{R_{t}}{\longrightarrow} V_{t+1} \cdots \stackrel{R_{l-1}}{\longrightarrow} V_{l}$， 其中 $R=R_{1} \circ R_{2} \circ \cdots \circ R_{l-1}$ 节点类型$V_{1}$到$V_{l}$之间的组合关系。那么节点间的跳转概率定义为：
$$
p\left(v^{i+1} | v_{t}^{i}, \mathcal{P}\right)=\left\{\begin{array}{cc}{\frac{1}{\left|N_{t+1}\left(v_{t}^{i}\right)\right|}} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \in E, \phi\left(v^{i+1}\right)=t+1} \\
{0} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \in E, \phi\left(v^{i+1}\right) \neq t+1} \\
{0} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \notin E}\end{array}\right.
$$
其中$v^i_t \in V_t$，$N_{t+1}\left(v_{t}^{i}\right)$表示属于$t$类型的节点$v$的属于$t+1$类型的邻居。如果下一个节点$v^{i+1}$和$v^i_t$之间有边，并且$v^{i+1}$是$t+1$类型的节点 那么转移概率服从平均分布。其中，$v^{i+1}$服从meta-path所定义的下移节点类型。如图（a）中，原路径为$OAPVPAO$，那么节点$a_4$的下一个节点必然要是$P$类。 由于meta-path的对称性，所以：
$$
p\left(v^{i+1} | v_{t}^{i}\right)=p\left(v^{i+1} | v_{1}^{i}\right), \text { if } t=l
$$</p>
<h2 id="metapath2vec-1">metapath2vec++</h2>
<p>由于softmax做归一化时没有考虑节点类型，分母是对所有节点求和，所以为了融合节点类型，给出<strong>Heterogeneous negative sampling</strong>:
$$
p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u_{t} \in V_{t}} e^{X_{u_{t}} \cdot X_{v}}}
$$
<strong>如图（c）所示，metapath2vec++对每种类型节点指定不同的一组多项式分布</strong>，相当于在输出层根据节点类型，把异质网络分解成不同的同质网络，同样采用负采用的方法简化计算：
$$
O(\mathrm{X})=\log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u_{t}^{m} \sim P_{t}\left(u_{t}\right)}\left[\log \sigma\left(-X_{u_{t}^{m}} \cdot X_{v}\right)\right]
$$
算法如下：</p>
<p><img loading="lazy" src="/posts/2019-06-29-metapath/2.png#center" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ACL2017 《CANE:Context-Aware Network Embedding for Relation Modeling》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/cane/</link>
      <pubDate>Fri, 09 Mar 2018 20:41:15 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/cane/</guid>
      <description>ACL2017 &amp;#34;CANE:Context-Aware Network Embedding for Relation Modeling&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://aclanthology.org/P17-1158.pdf">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>在现实世界的社交网络中，一个顶点在与不同的邻居顶点交互时可能表现出不同的方面 (aspect)，这是很直观的。例如，研究人员通常与各种合作伙伴就不同的研究主题进行合作（如下图所示），社交媒体用户与分享不同兴趣的各种朋友联系，一个网页出于不同目的链接到多个其它网页。然而，现有的大多数 NE 方法只为每个顶点安排一个 single embedding 向量，并产生以下两个问题：</p>
<ul>
<li>这些方法在与不同邻居交互时，无法灵活转换不同的aspect</li>
<li>在这些模型中，一个顶点倾向于迫使它的所有邻居之间的 embedding彼此靠近，但事实上并非一直如此。例如下图中，左侧用户和右侧用户共享较少的共同兴趣，但是由于他们都链接到中间用户，因此被认为彼此接近。因此，这使得顶点 embedding 没有区分性。</li>
</ul>
<p><img loading="lazy" src="/posts/CANE/1.png#center" alt=""  />
</p>
<p>为了解决上述问题，本文提出了一个 CANE框架，用于精确建模顶点之间的关系。更具体而言，论文在信息网络上应用 CANE。信息网络的每个顶点还包含丰富的外部信息，例如文本、标签 、或者其它元数据。在这种场景下，上下文的重要性对 network embedding 更为关键。在不失一般性的情况下，论文在基于文本的信息网络中实现了 CANE，但是 CANE可以很容易地扩展到其它类型的信息网络。</p>
<p>在传统的 network embedding模型中，每个顶点都表达为一个静态的 embedding 向量，即 context-free embedding 。相反，CANE 根据与当前顶点交互的不同邻居，从而将动态的 embedding分配给当前顶点，称为 context-aware embedding。以一个顶点$u$为例：当与不同的邻居交互时， 的 context-free embedding保持不变；而当面对不同的邻居时， $u$的 context-aware embedding是动态的。</p>
<p>当顶点$u$与它的邻居顶点$v$交互时，它们彼此相关的 context embedding 分别来自它们的文本信息。对于每个顶点，可以轻松地使用神经模型neural model ，例如卷积神经网络和循环神经网络来构建 context-free embedding 和 text-based embedding 。为了实现 context-aware text-based embedding，论文引入了 selective attention 方案，并在这些神经模型中建立了  和  之间的互注意力 mutual attention 。mutual attention 预期引导神经模型强调那些被相邻顶点 focus 的单词，并最终获得 context-aware embedding。每个顶点的 context-free embedding 和 context-aware embedding 都可以通过使用现有的 network embedding 方法学到（如 DeepWalk，LINE，node2vec）并拼接起来。</p>
<p>论文对不同领域的三个真实数据集进行了实验。与其它 state-of-the-art 方法相比，链接预测的实验结果展示了论文框架的有效性。结果表明，context-aware embedding 对于网络分析至关重要，特别是对于那些涉及顶点之间复杂交互的任务，如链接预测。论文还通过顶点分类和案例研究来探索论文框架的性能，这再次证实了 CANE 模型的灵活性和优越性。</p>
<h1 id="model">Model</h1>
<p>给定信息网络$G = (V,E,T)$, ，其中$V$为顶点集合， $E \subseteq V \times V$为边集合， $T$为顶点的文本信息。每条边表示顶点之间的关系，并且关联一个权重$w_{u,v}$ 。这里顶点$v \in V$的文本信息表达为单词序列$\mathcal{S}_{v}=\left(t_{1}, t_{2}, \cdots, t_{n_{v}}\right)$，其中$n_{v}=\left|\mathcal{S}_{v}\right|$为序列$\mathcal{S}_{v}$的长度。</p>
<p>NE旨在根据网络结构和关联信息（如文本和 label）为每个顶点$v \in V$ 学习低维embedding$\overrightarrow{\mathbf{v}} \in \mathbb{R}^{d}$, 其中 $d \ll |V|$为embedding的维度。</p>
<p>Context-free Embedding 的定义：传统的 network representation learning 模型为每个顶点学习 context-free embedding 。这意味着顶点的 embedding 是固定的，并且不会根据顶点的上下文信息（即与之交互的另一个顶点）而改变。</p>
<p>Context-aware Embedding 的定义：与现有的、学习 context-free embedding 的NE 模型不同，CANE 根据顶点不同的上下文来学习该顶点的各种 embedding 。具体而言，对于边$e_{u,v}$， CANE 学习 context-aware embedding $\overrightarrow{\mathbf{v}}_{(u)}$和$\overrightarrow{\mathbf{u}}_{(v)}$</p>
<h2 id="framework">Framework</h2>
<p>为了充分利用网络结构和关联的文本信息，为顶点$v$提供了两种类型的 embedding：structure-based embedding $\overrightarrow{\mathbf{v}}^{s}$, text-based embedding  $\overrightarrow{\mathbf{v}}^{t}$。structure-based embeding 可以捕获网络结构中的信息，而 text-based embedding 可以捕获关联文本信息中的文本含义（textual meaning）。使用这些 embedding，可以简单地拼接它们并获得顶点 embedding 为：
$$
\overrightarrow{\mathbf{V}}=\overrightarrow{\mathbf{V}}^{s} \oplus \overrightarrow{\mathbf{V}}^{t}
$$
其中$\oplus$为拼接算子。注意， text-based embedding $\overrightarrow{\mathbf{v}}^{t}$可以是 context-free 的、也可以是context-aware的，这将在后面详细介绍。当$\overrightarrow{\mathbf{v}}^{t}$是 context-aware时，整个顶点 embedding $\overrightarrow{\mathbf{v}}$也将是 context-aware。</p>
<p>通过上述定义，CANE旨在最大化所有边上的目标函数，如下所示：
$$
\mathcal{L}=\sum_{e \in E} L(e)
$$
这里每条边的目标函数$L(e)$由两部分组成：
$$
L(e) = L_s(e) + L_t(e)
$$
$L_s(e)$表示基于结构的目标函数, $L_t(e)$表示基于文本的目标函数。 接下来分别对这两个目标函数进行详细介绍。</p>
<h2 id="基于结构的目标函数">基于结构的目标函数</h2>
<p>为了不失一般性，假设网络是有向的，因为无向边可以被认为是两个方向相反、且权重相等的有向边。因此，基于结构的目标函数旨在使用 structure-based embedding来衡量观察到一条有向边的对数似然（log-likelihood），即：
$$
L_{s}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)
$$
遵从 LINE，将上式中的条件概率定义为：
$$
p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)=\frac{\exp \left(\overrightarrow{\mathbf{u}}^{s} \cdot \overrightarrow{\mathbf{v}}^{s}\right)}{\sum_{z \in V} \overrightarrow{\mathbf{u}}^{s} \cdot \overrightarrow{\mathbf{z}}^{s}}
$$</p>
<h2 id="基于文本的目标函数">基于文本的目标函数</h2>
<p>现实世界社交网络中的顶点通常伴随着关联的文本信息。因此，本文提出了基于文本的目标函数来利用这些文本信息，并学习 text-based embedding 。</p>
<p>基于文本的目标函数$L_{t}(e)$可以通过各种度量指标来定义。为了与$L_{s}(e)$ 兼容，将$L_{t}(e)$定义如下：
$$
L_{t}(e)=\alpha L_{t, t}(e)+\beta L_{t, s}(e)+\gamma L_{s, t}(e)
$$
其中：$\alpha$，$\beta$，$\gamma$为对应部分的重要性，为超参数。</p>
<p>$L_{t,t}(e)$，$L_{t, s}(e)$，$L_{s, t}(e)$定义为：
$$
\begin{aligned}
&amp;L_{t, t}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{t} \mid \overrightarrow{\mathbf{u}}^{t}\right) \\
&amp;L_{t, s}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{t} \mid \overrightarrow{\mathbf{u}}^{s}\right) \\
&amp;L_{s, t}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{t}\right)
\end{aligned}
$$
这里构建了 structure-based embedding 、 text-based embedding 之间的桥梁，使得信息在结构和文本之间流动。上式中的条件概率将两种类型的顶点 embedding 映射到相同的 representation 空间中，但是考虑到它们自身的特点，这里并未强制它们相同。类似地，使用 softmax 函数来计算概率，如公式$p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)$所示。</p>
<p>structure-based embedding 被视为 parameter，与传统的 network embedding 模型相同。但是对于 text-based embedding，本文从顶点的关联文本信息中获取它们。此外， text-based embedding 可以通过 context-free 的方式、或者 context-aware 的方式获取。</p>
<h2 id="context-free-text-embedding">Context-Free Text Embedding</h2>
<p>有多种神经网络模型可以从单词序列 word sequence 中获取 text embedding，如卷积神经网络 CNN、循环神经网络RNN。在这项工作中，研究了用于文本建模的不同神经网络，包括 CNN、Bidirectional RNN、GRU，并采用了性能最好的 CNN。CNN 可以在单词之间捕获局部语义依赖性。</p>
<p>CNN 以一个顶点的单词序列作为输入，通过 lookup、卷积、池化这三层得到基于文本的 embedding 。</p>
<ul>
<li>
<p>lookup：给定一个单词序列 $\mathcal{S}=\left(t_{1}, t_{2}, \cdots, t_{n}\right)$, lookup layer 将每个单词$t_i$转换为对应的 word embedding $\overrightarrow{\mathbf{t}}_{i} \in \mathbb{R}^{d^{\prime}}$, 并且获得 embedding序列$\mathbf{S}=\left(\overrightarrow{\mathbf{t}}_{1}, \cdots, \overrightarrow{\mathbf{t}}_{n}\right)$, 其中$d^{\prime}$为word embedding的维度。</p>
</li>
<li>
<p>卷积：lookup之后，卷积层提取输入的embedding序列$\mathbf{S}$局部特征 local feature。具体而言，它使用卷积矩阵$\mathbf{C} \in \mathbb{R}^{d \times\left(l \times d^{\prime}\right)}$在长度为$l$的滑动窗口上进行卷积操作，如下所示：</p>
</li>
</ul>
<p>$$
\overrightarrow{\mathbf{x}}_{i}=\mathbf{C} * \mathbf{S}_{i: i+l-1}+\overrightarrow{\mathbf{b}}
$$</p>
<p>$\mathbf{C}$表示$d$个卷积核，每个卷积核的尺寸为$l \times d^\prime$。 $d$为卷积核数量，也称作输出通道数。</p>
<p>其中：$\mathbf{S}_{i: i+l-1} \in \mathbb{R}^{l \times d}$为第$i$个滑动窗口内所有word embedding拼接得到的矩阵。$\overrightarrow{\mathbf{b}} \in \mathbb{R}^{d}$为bias。注意，在单词序列的边缘添加了零填充向量。</p>
<ul>
<li>最大池化：为了获得 text embedding $\overrightarrow{\mathbf{v}}^{t}$, 对 $\left\{\overrightarrow{\mathbf{x}}_{1}, \cdots, \overrightarrow{\mathbf{x}}_{n}\right\}$进行最大池化和非线性变换，如下所示：</li>
</ul>
<p>$$
\begin{gathered}
r_{i}=\tanh \left(\max \left(x_{1, i}, x_{2, i}, \cdots, x_{n, i}\right)\right), \quad i=1, \cdots, d \\
\overrightarrow{\mathbf{v}}^{t}=\left(r_{1}, \cdots, r_{d}\right)^{\top} \in \mathbb{R}^{d}
\end{gathered}
$$</p>
<p>其中max是在 embedding维度上进行的，tanh为逐元素的函数。</p>
<h2 id="context-aware-text-embedding">Context-Aware Text Embedding</h2>
<p>假设顶点在与其它顶点交互时扮演不同的角色。换句话讲，对于给定的顶点，其它不同的顶点与它有不同的焦点，这将导致 context-aware text embedding。</p>
<p>为了实现这一点，采用 mutual attention来获得 context-aware text embedding。mutual attention使 CNN 中的池化层能够感知 edge中的顶点 pair，从而使得来自一个顶点的文本信息可以直接影响另一个顶点的 text embedding，反之亦然。</p>
<p><img loading="lazy" src="/posts/CANE/2.png#center" alt=""  />
</p>
<p>如上图所示，对于context-aware text embedding的生成过程， 给定一条边$e_{u,v}$和两个对应的文本序列$\mathcal{S}_u$和$\mathcal{S}_v$, 可以通过卷积层得到矩阵$\mathbf{P} \in \mathbb{R}^{d \times m}$和$\mathbf{Q} \in \mathbb{R}^{d \times n}$ 。这里$m$表示$\mathcal{S}_u$的长度，$n$表示$\mathcal{S}_v$的长度。通过引入一个注意力矩阵$\mathbf{A} \in \mathbb{R}^{d \times d}$，计算相关性矩阵$\mathbf{F} \in \mathbb{R}^{m \times n}$:
$$
\mathbf{F}=\tanh \left(\mathbf{P}^{\top} \mathbf{A} \mathbf{Q}\right)
$$
其中$\mathbf{F}$中的每个元素$\mathbf{F}_{i,j}$表示两个隐向量（即$\overrightarrow{\mathbf{p}}_{i}$， $\overrightarrow{\mathbf{q}}_{j}$）之间的 pair-wise相关性得分。 然后，沿 $\mathbf{F}$的行和列进行池化操作从而生成重要性向量，分别称作行池化row-pooling和列池化column-pooling。根据实验，均值池化的性能优于最大池化。因此，采用如下的均值池化操作：
$$
\begin{aligned}
g_{i}^{p} &amp;=\operatorname{mean}\left(F_{i, 1}, F_{i, 2}, \cdots, F_{i, n}\right) \\
g_{i}^{q} &amp;=\operatorname{mean}\left(F_{1, i}, F_{2, i}, \cdots, F_{m, i}\right)
\end{aligned}
$$
其中：$\overrightarrow{\mathbf{g}}^{p}=\left(g_{1}^{p}, \cdots, g_{m}^{p}\right)^{\top} \in \mathbb{R}^{m}$表示行池化向量，为$\mathbf{P}$的重要性向量。$\overrightarrow{\mathbf{g}}^{q}=\left(g_{1}^{q}, \cdots, g_{m}^{q}\right)^{\top} \in \mathbb{R}^{m}$表示列池化向量，为$\mathbf{Q}$的重要性向量。</p>
<p>接下来，使用softmax函数将重要性向量$\overrightarrow{\mathbf{g}}^{p}$和$\overrightarrow{\mathbf{g}}^{q}$转换为注意力向量 $\overrightarrow{\mathbf{a}}^{p}$和$\overrightarrow{\mathbf{a}}^{q}$：
$$
\begin{aligned}
a_{i}^{p}=\frac{\exp \left(g_{i}^{p}\right)}{\sum_{j=1}^{m} \exp \left(g_{j}^{p}\right)}, \quad a_{i}^{q}=\frac{\exp \left(g_{i}^{q}\right)}{\sum_{j=1}^{n} \exp \left(g_{j}^{q}\right)} \
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\overrightarrow{\mathbf{a}}^{p}=\left(a_{1}^{p}, \cdots, a_{m}^{p}\right)^{\top}  \in \mathbb{R}^{m} \\
\overrightarrow{\mathbf{a}}^{q}=\left(a_{1}^{q}, \cdots, a_{n}^{q}\right)^{\top} \in \mathbb{R}^{n}
\end{aligned}
$$
最后，顶点$u$和$v$ 的 context-aware text embedding计算为：
$$
\overrightarrow{\mathbf{u}}_{(v)}^{t}=\mathbf{P} \overrightarrow{\mathbf{a}}^{p}, \quad \overrightarrow{\mathbf{v}}_{(u)}^{t}=\mathbf{Q} \overrightarrow{\mathbf{a}}^{q}
$$
现在，给定一条边$e_{u,v}$，可以获得 context-aware embedding，它是 structure embedding 和 context-aware text embedding的拼接：
$$
\overrightarrow{\mathbf{u}}_{(v)}=\overrightarrow{\mathbf{u}}^{s} \oplus \overrightarrow{\mathbf{u}}_{(v)}^{t}, \quad \overrightarrow{\mathbf{v}}_{(u)}=\overrightarrow{\mathbf{v}}^{s} \oplus \overrightarrow{\mathbf{v}}_{(u)}^{t}
$$</p>
<h2 id="cane优化过程">CANE优化过程</h2>
<p>CANE 旨在最大化若干个条件概率。直观而言，使用 softmax函数优化条件概率在计算上代价太大。因此，使用负采样，并将目标函数转换为以下形式：
$$
\log \sigma(\overrightarrow{\mathbf{u}} \cdot \overrightarrow{\mathbf{v}})+k \times \mathbb{E}_{z \sim P(v)}[\log \sigma(-\overrightarrow{\mathbf{u}} \cdot \overrightarrow{\mathbf{z}})]
$$
其中$k$为负采样比例， $\sigma$为sigmoid, $P(v) \propto d_{v}^{3 / 4}$为负顶点的采样分布，$d_{v}$为顶点$v$的out-degree。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>SIGIR2018 《HTNE Embedding Temporal Network via Neighborhood Formation》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/htne/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/htne/</guid>
      <description>论文地址：HTNE
Introduction 本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。
另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。
因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。
通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。
另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。
值得注意的是，本文目标是优化邻域生成序列的极大似然估计即条件强度函数（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数
Model Definition 本文通过跟踪节点邻域的形成来捕获网络的形成过程。
Definition 1 : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \in E$ 被表示为按时间顺序的时间序列，例如， $\mathbf{a}_{x,y}={a_1\to{a_2}\to{…}}\subset\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。
因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。
Definition 2 : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2&amp;hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\to(y_2,t_2)\to&amp;hellip;\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。
Hawkes Process 点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。
对于一个给定的节点$x \in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：
$$ \tilde{\lambda}_{y|x}(t)=\mu_{x,y}+\sum_{t_h&amp;lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$$
其中，$\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\sum_{t_h&amp;lt;t}$表示遍历t时刻前$x$的所有邻居。$\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：
$$\kappa(t-t_h)=\exp(-\delta_s(t-t_h))$$
其中，减少率 $\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\delta_s(t-t_h)$越大, $\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。
综上所述，$\kappa$的具体意义是随时间衰减的影响，其中$\delta_s$参数表示对于不同的源节点，影响是不同的。
如果$\tilde{\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。
直观的来看，基本率（base rate）$\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\mu_{x,y}=f(\mathbf{e}_x,\mathbf{e}_y)=-||\mathbf{e}_x-\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\alpha_{h,y}=f(\mathbf{e}_h,\mathbf{e}_y)=-||\mathbf{e}_h-\mathbf{e}_y||^2$。
因为条件强度函数必须为正，所以使用如下公式: $\lambda_{y|x}(t)=\exp(\tilde\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$.</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://dl.acm.org/citation.cfm?id=3220054">HTNE</a></p>
<h2 id="introduction">Introduction</h2>
<p>本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。</p>
<p><img loading="lazy" src="/posts/2019-01-17-HTNE/Fig1.png" alt="你想输入的替代文字"  />
</p>
<p>另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。</p>
<p>因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。</p>
<p>通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。</p>
<p>另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。</p>
<p>值得注意的是，本文目标是优化邻域生成序列的极大似然估计即<strong>条件强度函数</strong>（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数</p>
<h2 id="model">Model</h2>
<h3 id="definition">Definition</h3>
<p>本文通过跟踪节点邻域的形成来捕获网络的形成过程。<br>
<strong>Definition 1</strong> : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \in E$ 被表示为按时间顺序的时间序列，例如， $\mathbf{a}_{x,y}={a_1\to{a_2}\to{…}}\subset\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。</p>
<p>因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。</p>
<p><strong>Definition 2</strong> : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2&hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\to(y_2,t_2)\to&hellip;\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。</p>
<h3 id="hawkes-process">Hawkes Process</h3>
<p>点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。<br>
对于一个给定的节点$x \in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：<br>
$$ \tilde{\lambda}_{y|x}(t)=\mu_{x,y}+\sum_{t_h&lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$$<br>
其中，$\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\sum_{t_h&lt;t}$表示遍历t时刻前$x$的所有邻居。$\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：<br>
$$\kappa(t-t_h)=\exp(-\delta_s(t-t_h))$$<br>
其中，减少率 $\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\delta_s(t-t_h)$越大, $\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。<br>
综上所述，$\kappa$的具体意义是随时间衰减的影响，其中$\delta_s$参数表示对于不同的源节点，影响是不同的。</p>
<p>如果$\tilde{\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。</p>
<p>直观的来看，基本率（base rate）$\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\mu_{x,y}=f(\mathbf{e}_x,\mathbf{e}_y)=-||\mathbf{e}_x-\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\alpha_{h,y}=f(\mathbf{e}_h,\mathbf{e}_y)=-||\mathbf{e}_h-\mathbf{e}_y||^2$。<br>
因为条件强度函数必须为正，所以使用如下公式: $\lambda_{y|x}(t)=\exp(\tilde\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$. 这就与传统的NE方法差不多了。。。</p>
<h3 id="attention">Attention</h3>
<p>根据论文中（3）式，可以看出，$\sum_{t_h&lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$这一部分主要描述了历史邻居对当前邻居的影响，但是完全忽略了源节点$x$，因为源节点$x$的变化也会影响到历史邻居对当前邻居的亲近程度(affinity)。因此，本文引入了<strong>attention model</strong>。as follows：<br>
$$w_{h,x} = \frac{\exp(-||\mathbf{e}_x-\mathbf{e}_h||^2)}{\sum_{h&rsquo;}{\exp(-||\mathbf{e}_x-\mathbf{e}_{h&rsquo;}||^2)}}$$<br>
这是一个softmax函数 来根据源节点$x$的不同为它的邻居赋予不同权重。</p>
<p>最后， 历史邻居与当前邻居的连接紧密程度可以表示为:
$$\alpha_{h,y}=w_{h,x}f(\mathbf{e}_h,\mathbf{e}_y)$$</p>
<h3 id="optimization">Optimization</h3>
<p>目标函数即为给定节点$x$以及基于邻域形成序列的霍克斯过程, 生成节点$y$的条件概率。 公式如下：
$$p(y|x, \mathcal{H}_x(t)) = \frac{\lambda_{y|x}(t)}{\sum_{y&rsquo;}{\lambda_{y&rsquo;|x}(t)}}$$
目标函数即为所有节点对的极大似然：
$$\log \mathcal{L}=\sum_{x\in{\mathcal{V}}}{\sum_{y\in{\mathcal{H}_x}}}{\log{p(y|x,\mathcal{H}(t))}}$$</p>
<p>最后，由于softmax过程是calculating expensive，所以采用负采样优化损失函数。</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
