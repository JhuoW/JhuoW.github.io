<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>Posts | JhuoW‘s Notes</title>
<meta name=keywords content><meta name=description content="Posts - JhuoW‘s Notes"><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/posts/><meta name=google-site-verification content="G-6F49SGED6V"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://JhuoW.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://JhuoW.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-6F49SGED6V"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6F49SGED6V")}</script><meta property="og:title" content="Posts"><meta property="og:description" content="Jhuo’s Notes"><meta property="og:type" content="website"><meta property="og:url" content="https://JhuoW.github.io/posts/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Jhuo’s Notes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JhuoW.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="JhuoW's Notes (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>JhuoW's Notes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://JhuoW.github.io/>Home</a></div><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2>Maximum Mean Discrepancy</h2></header><section class=entry-content><p>Mean Discrepancy (MD)均值差异 判断2个分布$p$ 和$q$是否相同。
$p$分布生成一个样本空间$\mathbb{P}$ (从$p$中采样$m$个样本)
$q$分布生成一个样本空间$\mathbb{Q}$（从$q$中采样$n$个样本）
函数$f$的输入为 分布生成的样本空间
如果 $$ \begin{equation} \begin{aligned} \mathrm{mean}(f(\mathbb{P})) == \mathrm{mean}(f(\mathbb{Q})) \\ i.e., \frac{1}{m}\sum^m_{i=1}f(p_i) = \frac{1}{n}\sum^n_{i=1}f(q_i) \end{aligned} \end{equation} $$
则$p$和$q$是同一分布。
MD can be defined as $$ \begin{equation} \begin{aligned} \mathrm{MD}&=|\mathrm{mean}(f(\mathbb{P})) -\mathrm{mean}(f(\mathbb{Q})) | \\ &= |\frac{1}{m}\sum^m_{i=1}f(p_i) - \frac{1}{n}\sum^n_{i=1}f(q_i)| \end{aligned} \end{equation} $$
Maximum Mean Discrepancy (MMD) 最大均值差异 定义 MMD: 在函数集$\mathcal{F}=\{f_1, f_2, \cdots \}$中， 找到一个函数$f^*$， 使得$|\mathrm{mean}(f^*(\mathbb{P})) -\mathrm{mean}(f^*(\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的最大均值差异（MMD）。MMD =0 表示两个分布相同。 $$ \operatorname{MMD}[\mathcal{F}, p, q]:=\sup _{f \in \mathcal{F}}\left(\mathbf{E}_{x \sim p}[f(x)]-\mathbf{E}_{y \sim q}[f(y)]\right) $$ 其中$\mathbf{E}_{x \sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\sup$为上确界直接理解为max就好。
...</p></section><footer class=entry-footer><span title='2022-03-27 10:45:08 +0800 +08'>March 27, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Maximum Mean Discrepancy" href=https://JhuoW.github.io/posts/mmd/></a></article><article class=post-entry><header class=entry-header><h2>Reproducing Kernel Hilbert Space</h2></header><section class=entry-content><p>Hilbert Space Definition 1 (Norm) Let $\mathcal{F}$ be a vector space over $\mathbb{R}$ (For example $\mathcal{F}=\mathbb{R}^n$ is a vector space). A function $||\cdot||_{\mathcal{F}}: \mathcal{F} \to [0, \inf)$ is said to be a norm on $\mathcal{F}$ if ($||\cdot||_{\mathcal{F}}$ 是一个有效norm算子要满足以下条件):
For $f \in \mathcal{F}$, $||f||_{\mathcal{F}}=0$ if and only if $f=0$. (norm separates points) $|\lambda f|_{\mathcal{F}}=|\lambda||f|_{\mathcal{F}}$, $\forall \lambda \in \mathbb{R}, \forall f \in \mathcal{F}$ (positive homogeneity). $|f+g|_{\mathcal{F}} \leq|f|_{\mathcal{F}}+|g|_{\mathcal{F}}, \forall f, g \in \mathcal{F}$ (triangle inequality). 向$||\cdot||_{\mathcal{F}}$中输入任意一个向量，只要满足以上条件，那么$||\cdot||_{\mathcal{F}}$是一个valid norm operator.
...</p></section><footer class=entry-footer><span title='2022-03-26 22:39:20 +0800 +08'>March 26, 2022</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Reproducing Kernel Hilbert Space" href=https://JhuoW.github.io/posts/rkhs_kernel/></a></article><article class=post-entry><header class=entry-header><h2>Monte Carlo Tree Search</h2></header><section class=entry-content><p>单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。
在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?
多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。
利用（Exploitation）： 保证在过去决策中得到最佳回报
探索（Exploration）：寄希望在未来能够得到更大的汇报
例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。
但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。
悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机
$I_t$: $t$时刻选择的赌博机
$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励
$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward
$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。
$R_n$越大，就代表$n$次决策的结果越差。
上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡
在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}. $$ 其中$I_{t}$为$t$时刻要摇的赌博机，
...</p></section><footer class=entry-footer><span title='2022-03-25 18:09:58 +0800 +08'>March 25, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Monte Carlo Tree Search" href=https://JhuoW.github.io/posts/monte-carlo-tree-search/></a></article><article class=post-entry><header class=entry-header><h2>NIPS2018 《DiffPool:Hierarchical Graph Representation Learning with Differentiable Pooling》 Reading Notes</h2></header><section class=entry-content><p>论文地址： DiffPool
Introduction 传统的GNN算法在Node-level的任务如节点分类、链路预测上有着较好的效果。但是，现有的GNN方法由于其存在平面化的局限性，因此无法学习图的层级表示（意味着无法预测整个图的标签），顾无法实现图分类任务。举个栗子，一个Graph，可以分成600个subgraph，每个节点都存在于其中的某个subgraph（一个节点只存在于一个subgraph中），每个subgraph拥有一个标签，如何预测subgraph的标签是这篇文章主要想解决的问题。传统的GNN的图分类方法都是为Graph中的所有节点生成Embedding，然后将对这些Embedding做全局聚合（池化），如简单的把属于同一个subgraph的节点求和或者输入到MLP中生成一个标签向量来表示整个subgraph，但是这样可能忽略的图的层级结构信息。
本文提出了一种端到端的可微可微图池化模块DiffPool，原理如下图所示：
在深度GNN中的每层中为节点学习可微的软簇分配，将节点映射到簇中，这些簇作为新的节点作为下一层GNN的输入。上图的Original Network部分是一个Subgraph，传统的方法是直接求出这个Subgraph中每个节点的Embedding，然后相加或输入到一个神经网络中，得到一个预测向量，这种方法可以称为“全局池化”。DiffPool中，假设第$l$层的输入是$1000$个簇（如果是第一层输入就是1000个节点），我们先设置第$l+1$层需要输入的簇的个数（假设为$100$），也就是第$l$层输出的簇个数，然后在$l$层中通过一个分配矩阵将$1000$个簇做合并，合并成100个“节点”，然后将这100个节点输入到$l+1$层中，最后图中的节点数逐渐减少，最后，图中的节点只有一个，这个节点的embedding就是整个图的表示，然后将图输入到一个多层感知机MLP中，得到预测向量，在于真值的one-hot向量做cross-entropy，得到Loss。
Model：DiffPool 一个Graph表示为$\mathcal{G} = (A,F)$，其中$A \in {0,1}^{n \times n}$是Graph的邻接矩阵，$F \in \mathbb{R}^{n \times d}$表示节点特征矩阵，每个节点有$d$维的特征。给定一个带标签的子图集$\mathcal{D}=\left\{\left(G_{1}, y_{1}\right),\left(G_{2}, y_{2}\right), \ldots\right\}$， 其中 $y_{i} \in \mathcal{Y}$表示每个子图$G_i \in \mathcal{G}$的标签，任务目标是寻找映射$f: \mathcal{G} \rightarrow \mathcal{Y}$，将图映射到标签集。我们需要一个过程来将每个子图转化为一个有限维度的向量$\mathbb{R}^D$。
Graph Neural Networks 一般，GNN可以表示成"Message Passing"框架： $$ H^{(k)}=M\left(A, H^{(k-1)} ; \theta^{(k)}\right) $$ 其中$H^{(k)} \in \mathbb{R}^{n \times d}$表示GNN迭代$k$次后的node embedding，$M$是一个Message扩散函数，由邻接矩阵$A$和一个可训练的参数$\theta^{(k)}$决定。$H^{(k-1)}$是由前一个message passing过程生成的node embedding。当$k = 1$时，第一个GNN的输入为$H^{(0)}$是原始的节点特征$H^{(0)} = F$。
GNN的一个主要目标是设计一个Message Passage函数$M$，GCN（kipf.2016）是一种流行的GNN，$M$的实现方式是将线性变换和ReLU非线性激活结合起来: $$ H^{(k)}=M\left(A, H^{(k-1)} ; W^{(k)}\right)=\operatorname{ReLU}\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(k-1)} W^{(k-1)}\right) $$ 其中，$\tilde{A} = A+I$是一个加上自环的邻接矩阵，$\tilde{D}=\sum_{j} \tilde{A}_{i j}$是$\tilde{A}$的度矩阵，$W^{(k)} \in \mathbb{R}^{d \times d}$是一个可训练的权重矩阵，$W$与节点个数以及每个节点的度无关，可以看做一个特征增强矩阵，用来规定GCN的输出维度。
...</p></section><footer class=entry-footer><span title='2019-12-19 19:32:36 +0000 UTC'>December 19, 2019</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NIPS2018 《DiffPool:Hierarchical Graph Representation Learning with Differentiable Pooling》 Reading Notes" href=https://JhuoW.github.io/posts/diffpool/></a></article><article class=post-entry><header class=entry-header><h2>Notes for Spectral Clustering</h2></header><section class=entry-content><p>最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。
本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments
Introduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。
基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} > 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\displaystyle \left(\begin{array}{ccc}{d_{1}} & {\ldots} & {\ldots} \\\ {\ldots} & {d_{2}} & {\ldots} \\\ {\vdots} & {\vdots} & {\ddots} \\\ {\ldots} & {\ldots} & {d_{n}}\end{array}\right) ^{n\times n} $$ 是一个$n \times n$的对角阵，对角元素是每个节点的度和。
定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义： $$|A|=A 中的节点个数 $$
$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$
...</p></section><footer class=entry-footer><span title='2019-09-07 09:11:09 +0000 UTC'>September 7, 2019</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Notes for Spectral Clustering" href=https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/></a></article><article class=post-entry><header class=entry-header><h2>AAAI2017 M-NMF:《Community Preserving Network Embedding》 Reading Notes</h2></header><section class=entry-content><p>论文地址：M-NMF
Introduction Network Embedding的基本需求是保存网络机构和固有属性。而先前的方法主要保存了网络的围观结构（microscopic structure）如一阶相似度和二阶相似度（LINE，SDNE等）， 忽略了网络结构的介观社区结构（mesoscopic community structure）。针对这个问题，本文提出了一种模块化非负矩阵分解模型M-NMF，该模型将网络的社区结构融入network embedding中。利用节点表示和社区结构之间的一致关系（consensus relationship）， 联合优化基于非负矩阵分解的表示学习模型和基于模块化的社区发现模型， 这样就可以在节点表示中同时保存了微观网络结构和介观社区结构。
具体来说，对于微观结构（节点对相似性），本文使用矩阵分解来融合节点的一节相似性和二阶相似性；对于介观社区结构，通过模块度约束项来检测社区。因此，上面的两项可以通过节点的表示和基于辅助社区表示矩阵社区结构之间的一致关系来联合优化。同时，本文给出了乘法更新策略，保证了再推导参数的时候的正确性和收敛性。
M-NMF Model 对于一个无向图$G=(V,E)$, 图中有$n$个节点和$e$条边，用一个0,1二值邻接矩阵表示为$\mathbf{A}=[A_{i,j}] \in \mathbb{R}^{n \times n}$。 $\mathbf{U} \in \mathbb{R}^{n \times m}$ 为节点的表示矩阵，其中$m \leq n$，$m$是节点的嵌入维度。
建模社区结构 本文采用基于模块最大化的社区发现方法 Modularity， 给定一个邻接矩阵表示的网络$\mathbf{A}$，$\mathbf{A}$包含两个社区，根据Newman 2006b，模块度可以定义如下： $$ Q=\frac{1}{4 e} \sum_{i j}\left(A_{i j}-\frac{k_{i} k_{j}}{2 e}\right) h_{i} h_{j} $$ 其中，$k_i$表示节点$i$的度，如果$i$在第一个社区中，那么$h_i=1$,否则$h_i = -1$。
$k_ik_j$表示将所有边一分为二 参考模块度Q，那么节点$i$,$j$之间可能产生的边数。$\frac{k_{i} k_{j}}{2 e}$如果所有边随机放置，节点$i$,$j$之间的期望边数。定义一个模块度矩阵$\mathbf{B} \in \mathbb{R}^{n \times n}$，其中$B_{i,j}=A_{i,j}-\frac{k_{i} k_{j}}{2 e}$，那么$Q=\frac{1}{4 e} \mathbf{h}^{T} \mathbf{B h}$，其中$\mathbf{h}=[h_i] \in \mathbb{R}^n$，表示社区成员指标器。
如果将$Q$拓展到$k > 2$个社区，那么： $$ Q=\operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B H}\right), \quad \text { s.t. } \quad \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{H}\right)=n $$ 其中$tr()$表示矩阵的迹（主对角线元素和），$\mathbf{H}$是社区成员指标器，$\mathbf{H} \in \mathbb{R}^{n \times k}$，每行表示一个节点所属社区的one-hot编码。
...</p></section><footer class=entry-footer><span title='2019-05-29 10:46:44 +0000 UTC'>May 29, 2019</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to AAAI2017 M-NMF:《Community Preserving Network Embedding》 Reading Notes" href=https://JhuoW.github.io/posts/m-nmf/></a></article><article class=post-entry><header class=entry-header><h2>WSDM2016 《Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model》 Reading Notes</h2></header><section class=entry-content><p>文章链接：Embedding_IC
Introduction 本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。
对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：
用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。 用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。 不同应用中的级联长度变化很大，难以学习和预测。 本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：
影响传播是二元的（被感染或不被感染）， 扩散网络未知， 影响关系不依赖于传播的内容， 用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。 本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：
Model Notations 传播事件集$\mathcal{D}={D_1,D_2,…,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。
给定一个社交网络，有$N$个用户：$\mathcal{U}={u_1,u_2,…,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \in \mathcal{U} \wedge t^D(u)&lt; \infty}$， 其中，$t^D: \mathcal{U} \to \mathbb{R}^+$ 表示用户被传染的时间戳， $\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \in \mathcal{U} | t^D(u)&lt;t}$。对称地，用$\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\infty)$表示最终所有被感染的用户，$\bar{D}(infty)$表示最终所有没被感染的用户。
Diffusion Model 本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。
在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \in \mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \in \mathcal{U} \backslash I$: $$ P(v|I) = 1-\prod_{u \in I}(1-P_{u,v}) $$ 上式中，$\prod_{u \in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。
...</p></section><footer class=entry-footer><span title='2019-05-12 22:09:38 +0000 UTC'>May 12, 2019</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to WSDM2016 《Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model》 Reading Notes" href=https://JhuoW.github.io/posts/2019-05-12-embedding-ic/></a></article><article class=post-entry><header class=entry-header><h2>Influence Maximization Conclusion</h2></header><section class=entry-content><p>影响力传播模型 社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择
LC IT模型（独立级联模型和线性阈值模型）
WC（权重级联模型）
HD（热传播模型）
SIR（传染病模型）
MIA模型（路径相关）
投票模型
巴斯模型 影响力最大化算法 目前有的几个影响力最大化的算法
基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）
A note on maximizing a submodular set function subject to a knapsack constraint 这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样
Cost-effective outbreak detection in networks （CELF算法）
Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法） 这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路
基于中心性的启发式算法
Efficient influence maximization in social networks W.Chen （DegreeDiscount算法） 这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1
A potential-based node selection strategy for influence max- imization in a social network （TM算法） 这个算法也是一种启发式算法，它是以节点地潜在地影响力作为启发的因素去选择初始的节点，同时设置了一个参数c，分为两个阶段，第一个阶段启发式的选择潜在影响力最大的若干个节点，第二阶段用贪心算法对这些第一阶段选择的种子节点进行筛选，当参数c为0，也就是没有第一阶段，那么算法就变成了传统的贪心算法。算法的缺点也很明显，首先参数c的选择依赖于经验判断，其次是启发式，准确率没有保证，再次也需要使用贪心算法，因此时间复杂度很高
...</p></section><footer class=entry-footer><span title='2019-05-06 20:16:17 +0000 UTC'>May 6, 2019</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Influence Maximization Conclusion" href=https://JhuoW.github.io/posts/2019-05-06-im-conclusion/></a></article><article class=post-entry><header class=entry-header><h2>社交网络影响最大化（Influence Maximization）中的IC，LT模型</h2></header><section class=entry-content><p>The Independent Cascade Model (IC Model) IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。
在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。
值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。
The Linear Threshold Model (LT Model) LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。
形式上， 在图$G$中每条边$e=(u,v) \in E$有一个权重$b_{u,v}$。 我们定义$\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\sum_{u \in \mathcal{N}_I (v)} b_{u,v} \leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\theta_v$。 LT模型首先为每个节点$v$的阈值$\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。</p></section><footer class=entry-footer><span title='2019-03-20 21:34:08 +0000 UTC'>March 20, 2019</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to 社交网络影响最大化（Influence Maximization）中的IC，LT模型" href=https://JhuoW.github.io/posts/2019-03-20-ic-lt/></a></article><article class=post-entry><header class=entry-header><h2>SIGIR18 《BiNE:Bipartite Network Embedding》 Reading Notes</h2></header><section class=entry-content><p>论文地址：BiNE
Introduction Bipartite Network(二分网络):如下图所示：
二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。
另一个问题，
如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。
另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。
针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过HITS来衡量。
Model 如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\overrightarrow{u_i}]$, $V=[\overrightarrow{v_i}]$，结构如下图所示：
（取自作者的讲解ppt)
Explicit Relations 同LINE一样， 基于直接连接的目标函数表示为：
$$\mathrm{minimize} \quad O_1=-\sum_{e_{ij} \in E}w_{ij}\log \hat{P}(i,j)$$
Implicit Relations 构造随机游走序列 这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：
$$w^U_{ij}=\sum_{k \in V}w_{ik}w_{jk}$$
$$w^V_{ij}=\sum_{k \in U}w_{ki}w_{kj}$$
其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。 其中$l=\max(H(v_i)\times \max T,\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。
$$D_{v_i}=\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$
表示其中一次随机游走的节点集合$p$表示停止概率。
通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。
...</p></section><footer class=entry-footer><span title='2019-03-13 00:00:00 +0000 UTC'>March 13, 2019</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to SIGIR18 《BiNE:Bipartite Network Embedding》 Reading Notes" href=https://JhuoW.github.io/posts/bine/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://JhuoW.github.io/posts/page/4/>« Prev Page</a>
<a class=next href=https://JhuoW.github.io/posts/page/6/>Next Page »</a></nav></footer></main><footer class=footer><span>Copyright &copy; 2025 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span></span><br><script>function siteTime(){var h=1e3,r=h*60,i=r*60,n=i*24,x=n*365,e=new Date,d=2019,w=1,_=16,y=19,b=15,C=11,l=e.getFullYear(),O=e.getMonth()+1,f=e.getDate(),p=e.getHours(),g=e.getMinutes(),v=e.getSeconds(),m=Date.UTC(d,w,_,y,b,C),j=Date.UTC(l,O,f,p,g,v),t=j-m,o=Math.floor(t/x),s=Math.floor(t/n-o*365),a=Math.floor((t-(o*365+s)*n)/i),c=Math.floor((t-(o*365+s)*n-a*i)/r),u=Math.floor((t-(o*365+s)*n-a*i-c*r)/h);d==l?document.getElementById("sitetime").innerHTML="本站已运行 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒":document.getElementById("sitetime").innerHTML="本站已运行 "+o+" 年 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒"}setInterval(siteTime,1e3)</script><span id=sitetime>载入运行时间...</span>
<script type=text/javascript id=clustrmaps src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=268&t=tt&d=YsONH-MzO6L7yrkA73Z_QW7LuMTfdUhk0uhb_KaBv-g&co=f5f5f5&cmo=3acc3a&cmn=ff5353&ct=808080"></script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>