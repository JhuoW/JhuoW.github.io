<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var t=MathJax.Hub.getAllJax(),e;for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>Everything about Graph Laplacian | JhuoW‘s Notes</title><meta name=keywords content="Laplacian"><meta name=description content="Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem."><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/posts/laplacian/><meta name=google-site-verification content="G-6F49SGED6V"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6F49SGED6V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6F49SGED6V",{anonymize_ip:!1})}</script><meta property="og:title" content="Everything about Graph Laplacian"><meta property="og:description" content="Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem."><meta property="og:type" content="article"><meta property="og:url" content="https://JhuoW.github.io/posts/laplacian/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-02T15:00:20+08:00"><meta property="article:modified_time" content="2022-04-02T15:00:20+08:00"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Everything about Graph Laplacian"><meta name=twitter:description content="Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JhuoW.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Everything about Graph Laplacian","item":"https://JhuoW.github.io/posts/laplacian/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Everything about Graph Laplacian","name":"Everything about Graph Laplacian","description":"Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem.","keywords":["Laplacian"],"articleBody":"Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem. Another application is spectral matching that solves for graph matching.\nBasic notations We consider simple graphs (no multiple edges or loops), $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})$ :\n  $\\mathcal{V}(\\mathcal{G})=\\left\\{v_{1}, \\ldots, v_{n}\\right\\}$ is called the vertex set with $n=|\\mathcal{V}|$;\n  $\\mathcal{E}(\\mathcal{G})=\\left\\{e_{i j}\\right\\}$ is called the edge set with $m=|\\mathcal{E}|$;\n  An edge $e_{i j}$ connects vertices $v_{i}$ and $v_{j}$ if they are adjacent or neighbors. One possible notation for adjacency is $v_{i} \\sim v_{j}$;\n  The number of neighbors of a node $v$ is called the degree of $v$ and is denoted by $d(v), d\\left(v_{i}\\right)=\\sum_{v_{i} \\sim v_{j}} e_{i j}$. If all the nodes of a graph have the same degree, the graph is regular; The nodes of an Eulerian graph have even degree.\n  A graph is complete if there is an edge between every pair of vertices.\n  Subgraph of a graph   $\\mathcal{H}$ is a subgraph of $\\mathcal{G}$ if $\\mathcal{V}(\\mathcal{H}) \\subseteq \\mathcal{V}(\\mathcal{G})$ and $\\mathcal{E}(\\mathcal{H}) \\subseteq \\mathcal{E}(\\mathcal{G})$;\n  a subgraph $\\mathcal{H}$ is an induced subgraph of $\\mathcal{G}$ if two vertices of $\\mathcal{V}(\\mathcal{H})$ are adjacent if and only if they are adjacent in $\\mathcal{G}$.\n  A clique is a complete subgraph of a graph.\n  A path of $k$ vertices is a sequence of $k$ distinct vertices such that consecutive vertices are adjacent.\n  A cycle is a connected subgraph where every vertex has exactly two neighbors.\n  A graph containing no cycles is a forest. A connected forest is a tree.\n  A k-partite graph  A graph is called k-partite if its set of vertices admits a partition into $k$ classes such that the vertices of the same class are not adjacent. An example of a bipartite graph.  The adjacency matrix of a graph  For a graph with $n$ vertices, the entries of the $n \\times n$ adjacency matrix are defined by:  $$ \\mathbf{A}:= \\begin{cases}A_{i j}=1 \u0026 \\text { if there is an edge } e_{i j} \\\\ A_{i j}=0 \u0026 \\text { if there is no edge } \\\\ A_{i i}=0 \u0026 \\end{cases} $$\n$$ \\begin{aligned} \u0026 \\mathbf{A}=\\left[\\begin{array}{llll}0 \u0026 1 \u0026 1 \u0026 0 \\\\1 \u0026 0 \u0026 1 \u0026 1 \\\\1 \u0026 1 \u0026 0 \u0026 0 \\\\0 \u0026 1 \u0026 0 \u0026 0\\end{array}\\right] \\end{aligned} $$\nEigenvalues and eigenvectors   A is a real-symmetric matrix: it has $n$ real eigenvalues and its $n$ real eigenvectors form an orthonormal basis.\n  Let $\\left\\{\\lambda_{1}, \\ldots, \\lambda_{i}, \\ldots, \\lambda_{r}\\right\\}$ be the set of distinct eigenvalues.\n  The eigenspace $S_{i}$ contains the eigenvectors associated with $\\lambda_{i}$ :\n  $$ S_{i}=\\left\\{\\boldsymbol{x} \\in \\mathbb{R}^{n} \\mid \\mathbf{A} \\boldsymbol{x}=\\lambda_{i} \\boldsymbol{x}\\right\\} $$\n  For real-symmetric matrices, the algebraic multiplicity is equal to the geometric multiplicity, for all the eigenvalues.\n  The dimension of $S_{i}$ (geometric multiplicity) is equal to the multiplicity of $\\lambda_{i}$.\n  If $\\lambda_{i} \\neq \\lambda_{j}$ then $S_{i}$ and $S_{j}$ are mutually orthogonal.\n  Real-valued functions on graphs   We consider real-valued functions on the set of the graph’s vertices, $\\boldsymbol{f}: \\mathcal{V} \\longrightarrow \\mathbb{R}$. Such a function assigns a real number to each graph node.\n  $\\boldsymbol{f}$ is a vector indexed by the graph’s vertices, hence $\\boldsymbol{f} \\in \\mathbb{R}^{n}$.\n  Notation: $\\boldsymbol{f}=\\left(f\\left(v_{1}\\right), \\ldots, f\\left(v_{n}\\right)\\right)=(f(1), \\ldots, f(n))$.\n  The eigenvectors of the adjacency matrix, $\\mathbf{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x}$, can be viewed as eigenfunctions.\n  Matrix A as an operator and quadratic form  The adjacency matrix can be viewed as an operator  $$ \\boldsymbol{g}=\\mathbf{A} \\boldsymbol{f} ; g(i)=\\sum_{i \\sim j} f(j) $$\n It can also be viewed as a quadratic form:  $$ \\boldsymbol{f}^{\\top} \\mathbf{A} \\boldsymbol{f}=\\sum_{e_{i j}} f(i) f(j) $$\nThe incidence matrix of a graph   Let each edge in the graph have an arbitrary but fixed orientation;\n  The incidence matrix of a graph is a $|\\mathcal{E}| \\times|\\mathcal{V}|(m \\times n)$ matrix defined as follows:\n  $$ \\nabla:= \\begin{cases}\\nabla_{e v}=-1 \u0026 \\text { if } v \\text { is the initial vertex of edge } e \\\\ \\nabla_{e v}=1 \u0026 \\text { if } v \\text { is the terminal vertex of edge } e \\\\ \\nabla_{e v}=0 \u0026 \\text { if } v \\text { is not in } e\\end{cases} $$\n$$ \\begin{aligned} \u0026 \\nabla=\\left[\\begin{array}{cccc}-1 \u0026 1 \u0026 0 \u0026 0 \\\\1 \u0026 0 \u0026 -1 \u0026 0 \\\\0 \u0026 -1 \u0026 1 \u0026 0 \\\\0 \u0026 -1 \u0026 0 \u0026 +1\\end{array}\\right] \\end{aligned} $$\nThe incidence matrix: A discrete differential operator   The mapping $\\boldsymbol{f} \\longrightarrow \\nabla \\boldsymbol{f}$ is known as the co-boundary mapping of the graph.\n  $(\\nabla \\boldsymbol{f})\\left(e_{i j}\\right)=f\\left(v_{j}\\right)-f\\left(v_{i}\\right)$\n  $$ \\left(\\begin{array}{c} f(2)-f(1) \\\\ f(1)-f(3) \\\\ f(3)-f(2) \\\\ f(4)-f(2) \\end{array}\\right)=\\left[\\begin{array}{cccc} -1 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 0 \u0026 -1 \u0026 0 \\\\ 0 \u0026 -1 \u0026 1 \u0026 0 \\\\ 0 \u0026 -1 \u0026 0 \u0026 +1 \\end{array}\\right]\\left(\\begin{array}{c} f(1) \\\\ f(2) \\\\ f(3) \\\\ f(4) \\end{array}\\right) $$\nThe Laplacian matrix of a graph   $\\mathbf{L}=\\nabla^{\\top} \\nabla$\n  $(\\mathbf{L} \\boldsymbol{f})\\left(v_{i}\\right)=\\sum_{v_{j} \\sim v_{i}}\\left(f\\left(v_{i}\\right)-f\\left(v_{j}\\right)\\right)$\n  Connection between the Laplacian and the adjacency matrices:\n  $$ \\mathbf{L}=\\mathbf{D}-\\mathbf{A} $$\n The degree matrix: $\\mathbf{D}:=D_{i i}=d\\left(v_{i}\\right)$.  $$ \\mathbf{L}=\\left[\\begin{array}{cccc} 2 \u0026 -1 \u0026 -1 \u0026 0 \\\\ -1 \u0026 3 \u0026 -1 \u0026 -1 \\\\ -1 \u0026 -1 \u0026 2 \u0026 0 \\\\ 0 \u0026 -1 \u0026 0 \u0026 1 \\end{array}\\right] $$\nThe Laplacian matrix of an undirected weighted graph   We consider undirected weighted graphs: Each edge $e_{i j}$ is weighted by $w_{i j}0$.\n  The Laplacian as an operator:\n  $$ (\\mathbf{L} \\boldsymbol{f})\\left(v_{i}\\right)=\\sum_{v_{j} \\sim v_{i}} w_{i j}\\left(f\\left(v_{i}\\right)-f\\left(v_{j}\\right)\\right) $$\n As a quadratic form:  $$ \\boldsymbol{f}^{\\top} \\mathbf{L} \\boldsymbol{f}=\\frac{1}{2} \\sum_{e_{i j}} w_{i j}\\left(f\\left(v_{i}\\right)-f\\left(v_{j}\\right)\\right)^{2} $$\n  L is symmetric and positive semi-definite.\n  L has $n$ non-negative, real-valued eigenvalues: $0=\\lambda_{1} \\leq \\lambda_{2} \\leq \\ldots \\leq \\lambda_{n} .$\n  The Laplacian of a 3D discrete surface (mesh)   A graph vertex $v_{i}$ is associated with a 3D point $\\boldsymbol{v}_{i}$.\n  The weight of an edge $e_{i j}$ is defined by the Gaussian kernel:\n  $$ w_{i j}=\\exp \\left(-\\left|\\boldsymbol{v}_{i}-\\boldsymbol{v}_{j}\\right|^{2} / \\sigma^{2}\\right) $$\n  $0 \\leq w_{\\min } \\leq w_{i j} \\leq w_{\\max } \\leq 1$\n  Hence, the geometric structure of the mesh is encoded in the weights.\n  Other weighting functions were proposed in the literature.\n  The Laplacian of a cloud of points   3-nearest neighbor graph\n  $\\varepsilon$-radius graph\n  KNN may guarantee that the graph is connected (depends on the implementation)\n  $\\varepsilon$-radius does not guarantee that the graph has one connected component\n  The Laplacian of a graph with one connected component   $Lu =\\lambda \\boldsymbol{u}$.\n  $\\mathbf{L} \\mathbf{1}_{n}=\\mathbf{0}, \\lambda_{1}=0$ is the smallest eigenvalue.\n  The one vector: $\\mathbf{1}_{n}=(1 \\ldots 1)^{\\top}$.\n  $0=\\boldsymbol{u}^{\\top} \\mathbf{L} \\boldsymbol{u}=\\sum_{i, j=1}^{n} w_{i j}(u(i)-u(j))^{2}$.\n  If any two vertices are connected by a path, then $\\boldsymbol{u}=(u(1), \\ldots, u(n))$ needs to be constant at all vertices such that the quadratic form vanishes. Therefore, a graph with one connected component has the constant vector $\\boldsymbol{u}_{1}=\\mathbf{1}_{n}$ as the only eigenvector with eigenvalue 0 .\n  A graph with $k1$ connected components  Each connected component has an associated Laplacian. Therefore, we can write matrix $\\mathbf{L}$ as a block diagonal matrix:  $$ \\mathbf{L}=\\left[\\begin{array}{lll} \\mathbf{L}_{1} \u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \\\\ \u0026 \u0026 \\mathbf{L}_{k} \\end{array}\\right] $$\n  The spectrum of $\\mathbf{L}$ is given by the union of the spectra of $\\mathbf{L}_{i}$.\n  Each block corresponds to a connected component, hence each matrix $\\mathbf{L}_{i}$ has an eigenvalue 0 with multiplicity 1 .\n  The spectrum of $\\mathbf{L}$ is given by the union of the spectra of $\\mathbf{L}_{i}$.\n  The eigenvalue $\\lambda_{1}=0$ has multiplicity $k$.\n  The eigenspace of $\\lambda_{1}=0$ with multiplicity $k$  The eigenspace corresponding to $\\lambda_{1}=\\ldots=\\lambda_{k}=0$ is spanned by the $k$ mutually orthogonal vectors:  $$ \\begin{aligned} \\boldsymbol{u}_{1} \u0026=\\mathbf{1}_{L_{1}} \\\\ \u0026 \\cdots \\\\ \\boldsymbol{u}_{k} \u0026=\\mathbf{1}_{L_{k}} \\end{aligned} $$\n  with $\\mathbf{1}_{L_{i}}=(0000111110000)^{\\top} \\in \\mathbb{R}^{n}$\n  These vectors are the indicator vectors of the graph’s connected components.\n  Notice that $\\mathbf{1}_{L_{1}}+\\ldots+\\mathbf{1}_{L_{k}}=\\mathbf{1}_{n}$\n  The Fiedler vector of the graph Laplacian   The first non-null eigenvalue $\\lambda_{k+1}$ is called the Fiedler value.\n  The corresponding eigenvector $\\boldsymbol{u}_{k+1}$ is called the Fiedler vector.\n  The multiplicity of the Fiedler eigenvalue is always equal to $1 .$\n  The Fiedler value is the algebraic connectivity of a graph, the further from 0 , the more connected.\n  The Fidler vector has been extensively used for spectral bi-partioning\n  Theoretical results are summarized in Spielman \u0026 Teng 2007: http://cs-www.cs.yale.edu/homes/spielman/\n  Eigenvectors of the Laplacian of connected graphs   $\\boldsymbol{u}_{1}=\\mathbf{1}_{n}, \\mathbf{L} \\mathbf{1}_{n}=\\mathbf{0}$.\n  $\\boldsymbol{u}_{2}$ is the the Fiedler vector with multiplicity 1 .\n  The eigenvectors form an orthonormal basis: $\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{u}_{j}=\\delta_{i j}$.\n  For any eigenvector $\\boldsymbol{u}_{i}=\\left(\\boldsymbol{u}_{i}\\left(v_{1}\\right) \\ldots \\boldsymbol{u}_{i}\\left(v_{n}\\right)\\right)^{\\top}, 2 \\leq i \\leq n$ :\n  $$ \\boldsymbol{u}_{i}^{\\top} \\mathbf{1}_{n}=0 $$\n Hence the components of $\\boldsymbol{u}_{i}, 2 \\leq i \\leq n$ satisfy:  $$ \\sum_{j=1}^{n} \\boldsymbol{u}_{i}\\left(v_{j}\\right)=0 $$\n Each component is bounded by:  $$ -1Laplacian embedding: Mapping a graph on a line  Map a weighted graph onto a line such that connected nodes stay as close as possible, i.e., minimize $\\sum_{i, j=1}^{n} w_{i j}\\left(f\\left(v_{i}\\right)-f\\left(v_{j}\\right)\\right)^{2}$, or:  $$ \\arg \\min _{\\boldsymbol{f}} \\boldsymbol{f}^{\\top} \\mathbf{L} \\boldsymbol{f} \\text { with: } \\boldsymbol{f}^{\\top} \\boldsymbol{f}=1 \\text { and } \\boldsymbol{f}^{\\top} \\mathbf{1}=0 $$\n  The solution is the eigenvector associated with the smallest nonzero eigenvalue of the eigenvalue problem: $\\mathbf{L} \\boldsymbol{f}=\\lambda \\boldsymbol{f}$, namely the Fiedler vector $\\boldsymbol{u}_{2}$.\n  For more details on this minimization see Golub \u0026 Van Loan Matrix Computations, chapter 8 (The symmetric eigenvalue problem).\n  Example of mapping a graph on the Fiedler vector:\nLaplacian embedding   Embed the graph in a $k$-dimensional Euclidean space. The embedding is given by the $n \\times k$ matrix $\\mathbf{F}=\\left[\\boldsymbol{f}_{1} \\boldsymbol{f}_{2} \\ldots \\boldsymbol{f}_{k}\\right]$ where the $i$-th row of this matrix $-\\boldsymbol{f}^{(i)}-$ corresponds to the Euclidean coordinates of the $i$-th graph node $v_{i}$.\n  We need to minimize:\n  $$ \\arg \\min_{\\boldsymbol{f}_{1} \\ldots} \\sum_{k}^{n} \\sum_{i, j=1}^{n} w_{i j}\\left|\\left|\\boldsymbol{f}^{(i)}-\\boldsymbol{f}^{(j)}\\right|\\right|^{2} \\text { with: } \\mathbf{F}^{\\top} \\mathbf{F}=\\mathbf{I} $$\n The solution is provided by the matrix of eigenvectors corresponding to the $k$ lowest nonzero eigenvalues of the eigenvalue problem $\\mathbf{L} \\boldsymbol{f}=\\lambda \\boldsymbol{f}$.  Spectral embedding using the unnormalized Laplacian   Compute the eigendecomposition $\\mathbf{L}=\\mathbf{D}-\\mathbf{A}$.\n  Select the $k$ smallest non-null eigenvalues $\\lambda_{2} \\leq \\ldots \\leq \\lambda_{k+1}$\n  $\\lambda_{k+2}-\\lambda_{k+1}=$ eigengap.\n  We obtain the $n \\times k$ matrix $\\mathbf{U}=\\left[\\boldsymbol{u}_{2} \\ldots \\boldsymbol{u}_{k+1}\\right]$ :\n  $$ \\mathbf{U}=\\left[\\begin{array}{ccc} \\boldsymbol{u}_{2}\\left(v_{1}\\right) \u0026 \\ldots \u0026 \\boldsymbol{u}_{k+1}\\left(v_{1}\\right) \\\\ \\vdots \u0026 \u0026 \\vdots \\\\ \\boldsymbol{u}_{2}\\left(v_{n}\\right) \u0026 \\ldots \u0026 \\boldsymbol{u}_{k+1}\\left(v_{n}\\right) \\end{array}\\right] $$\n  $\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{u}_{j}=\\delta_{i j}$ (orthonormal vectors), hence $\\mathbf{U}^{\\top} \\mathbf{U}=\\mathbf{I}_{k}$.\n  Column $i(2 \\leq i \\leq k+1)$ of this matrix is a mapping on the eigenvector $\\boldsymbol{u}_{i}$.\n  Euclidean L-embedding of the graph’s vertices  (Euclidean) L-embedding of a graph:  $$ \\mathbf{X}=\\boldsymbol{\\Lambda}_{k}^{-\\frac{1}{2}} \\mathbf{U}^{\\top}=\\left[\\begin{array}{llll} \\boldsymbol{x}_{1} \u0026 \\ldots \u0026 \\boldsymbol{x}_{j} \\ldots \u0026 \\boldsymbol{x}_{n} \\end{array}\\right] $$\nThe coordinates of a vertex $v_{j}$ are:\n$$ \\boldsymbol{x}_{j}=\\left(\\begin{array}{c} \\frac{\\boldsymbol{u}_{2}\\left(v_{j}\\right)}{\\sqrt{\\lambda_{2}}} \\\\ \\vdots \\\\ \\frac{\\boldsymbol{u}_{k+1}\\left(v_{j}\\right)}{\\sqrt{\\lambda_{k+1}}} \\end{array}\\right) $$\nJustification for choosing the L-embedding Both\n  the commute-time distance (CTD) and\n  the principal-component analysis of a graph (graph PCA)\n  are two important concepts; They allow to reason “statistically” on a graph. They are both associated with the unnormalized Laplacian matrix.\nThe commute-time distance   The CTD is a well known quantity in Markov chains;\n  It is the average number of (weighted) edges that it takes, starting at vertex $v_{i}$, to randomly reach vertex $v_{j}$ for the first time and go back;\n  The CTD decreases as the number of connections between the two nodes increases;\n  It captures the connectivity structure of a small graph volume rather than a single path between the two vertices - such as the shortest-path geodesic distance.\n  The CTD can be computed in closed form:\n  $$ \\operatorname{CTD}^{2}\\left(v_{i}, v_{j}\\right)=\\operatorname{vol}(\\mathcal{G})\\left|\\left|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}\\right|\\right|^{2} $$\nThe graph PCA  The mean (remember that $\\sum_{j=1}^{n} \\boldsymbol{u}_{i}\\left(v_{j}\\right)=0$ ):  $$ \\overline{\\boldsymbol{x}}=\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{x}_{j}=\\boldsymbol{\\Lambda}_{k}^{-\\frac{1}{2}}\\left(\\begin{array}{c} \\sum_{j=1}^{n} \\boldsymbol{u}_{2}\\left(v_{j}\\right) \\\\ \\vdots \\\\ \\sum_{j=1}^{n} \\boldsymbol{u}_{k+1}\\left(v_{j}\\right) \\end{array}\\right)=\\left(\\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array}\\right) $$\n The covariance matrix:  $$ \\mathbf{S}=\\frac{1}{n} \\sum_{j=1}^{n} \\boldsymbol{x}_{j} \\boldsymbol{x}_{j}^{\\top}=\\frac{1}{n} \\mathbf{X} \\mathbf{X}^{\\top}=\\frac{1}{n} \\boldsymbol{\\Lambda}_{k}^{-\\frac{1}{2}} \\mathbf{U}^{\\top} \\mathbf{U} \\boldsymbol{\\Lambda}_{k}^{-\\frac{1}{2}}=\\frac{1}{n} \\boldsymbol{\\Lambda}_{k}^{-1} $$\n The vectors $\\boldsymbol{u}_{2}, \\ldots, \\boldsymbol{u}_{k+1}$ are the directions of maximum variance of the graph embedding, with $\\lambda_{2}^{-1} \\geq \\ldots \\geq \\lambda_{k+1}^{-1}$.  Other Laplacian matrices  The normalized graph Laplacian (symmetric and semi-definite positive):  $$ \\mathbf{L}_{n}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{L} \\mathbf{D}^{-\\frac{1}{2}}=\\mathbf{I}-\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}} $$\n The transition matrix (allows an analogy with Markov chains):  $$ \\mathbf{L}_{t}=\\mathbf{D}^{-1} \\mathbf{A} $$\n The random-walk graph Laplacian:  $$ \\mathbf{L}_{r}=\\mathbf{D}^{-1} \\mathbf{L}=\\mathbf{I}-\\mathbf{L}_{t} $$\n These matrices are similar:  $$ \\mathbf{L}_{r}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{L} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{D}^{\\frac{1}{2}}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{L}_{n} \\mathbf{D}^{\\frac{1}{2}} $$\nEigenvalues and eigenvectors of $\\mathrm{L}_{n}$ and $\\mathrm{L}_{r}$  $\\mathbf{L}_{r} \\boldsymbol{w}=\\lambda \\boldsymbol{w} \\Longleftrightarrow \\mathbf{L} \\boldsymbol{w}=\\lambda \\mathbf{D} \\boldsymbol{w}$, hence:  $$ \\mathbf{L}_{r}: \\quad \\lambda_{1}=0 ; \\quad \\boldsymbol{w}_{1}=\\mathbf{1} $$\n $\\mathbf{L}_{n} \\boldsymbol{v}=\\lambda \\boldsymbol{v}$. By virtue of the similarity transformation between the two matrices:  $$ \\mathbf{L}_{n}: \\quad \\lambda_{1}=0 \\quad \\boldsymbol{v}_{1}=\\mathbf{D}^{\\frac{1}{2}} \\mathbf{1} $$\n More generally, the two matrices have the same eigenvalues:  $$ 0=\\lambda_{1} \\leq \\ldots \\leq \\lambda_{i} \\ldots \\leq \\lambda_{n} $$\n Their eigenvectors are related by:  $$ \\boldsymbol{v}_{i}=\\mathbf{D}^{\\frac{1}{2}} \\boldsymbol{w}_{i}, \\forall i=1 \\ldots n $$\nSpectral embedding using the random-walk Laplacian $\\mathbf{L}_{r}$  The $n \\times k$ matrix contains the first $k$ eigenvectors of $\\mathbf{L}_{r}$ :  $$ \\mathbf{W}=\\left[\\begin{array}{lll} \\boldsymbol{w}_{2} \u0026 \\ldots \u0026 \\boldsymbol{w}_{k+1} \\end{array}\\right] $$\n It is straightforward to obtain the following expressions, where $\\boldsymbol{d}$ and $\\mathbf{D}$ are the degree-vector and the degree-matrix:  $$ \\begin{gathered} \\boldsymbol{w}_{i}^{\\top} \\boldsymbol{d}=0, \\forall i, 2 \\leq i \\leq n \\\\ \\mathbf{W}^{\\top} \\mathbf{D W}=\\mathbf{I}_{k} \\end{gathered} $$\n The isometric embedding using the random-walk Laplacian:  $$ \\mathbf{Y}=\\mathbf{W}^{\\top}=\\left[\\begin{array}{lll} \\boldsymbol{y}_{1} \u0026 \\ldots \u0026 \\boldsymbol{y}_{n} \\end{array}\\right] $$\nThe normalized additive Laplacian  Some authors use the following matrix:  $$ \\mathbf{L}_{a}=\\frac{1}{d_{\\max }}\\left(\\mathbf{A}+d_{\\max } \\mathbf{I}-\\mathbf{D}\\right) $$\n This matrix is closely related to L:  $$ \\mathbf{L}_{a}=\\frac{1}{d_{\\max }}\\left(d_{\\max } \\mathbf{I}-\\mathbf{L}\\right) $$\n and we have:  $$ \\mathbf{L}_{a} \\boldsymbol{u}=\\mu \\boldsymbol{u} \\Longleftrightarrow \\mathbf{L} \\boldsymbol{u}=\\lambda \\boldsymbol{u}, \\mu=1-\\frac{\\lambda}{d_{\\max }} $$\nThe graph partitioning problem  The graph-cut problem: Partition the graph such that:  (1) Edges between groups have very low weight, and\n(2) Edges within a group have high weight.\n$\\operatorname{cut}\\left(A_{1}, \\ldots, A_{k}\\right):=\\frac{1}{2} \\sum_{i=1}^{k} W\\left(A_{i}, \\bar{A}_{i}\\right)$ with $W(A, B)=\\sum_{i \\in A, j \\in B} w_{i j}$\n Ratio cut:  $$ \\operatorname{RatioCut}\\left(A_{1}, \\ldots, A_{k}\\right):=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\bar{A}_{i}\\right)}{\\left|A_{i}\\right|} $$\n Normalized cut:  $$ \\operatorname{NCut}\\left(A_{1}, \\ldots, A_{k}\\right):=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\bar{A}_{i}\\right)}{\\operatorname{vol}\\left(A_{i}\\right)} $$\nWhat is spectral clustering? See my Blog of Spectral Clustering (in Chinese).\n  Both ratio-cut and normalized-cut minimizations are NP-hard problems\n  Spectral clustering is a way to solve relaxed versions of these problems:\n  (1) The smallest non-null eigenvectors of the unnormalized Laplacian approximate the RatioCut minimization criterion, and\n(2) The smallest non-null eigenvectors of the random-walk Laplacian approximate the NCut criterion.\nSpectral clustering using the random-walk Laplacian   For details see (von Luxburg ‘07)\n  Input: Laplacian $\\mathbf{L}_{r}$ and the number $k$ of clusters to compute.\n  Output: Cluster $C_{1}, \\ldots, C_{k}$.\n  (3) Compute W formed with the first $k$ eigenvectors of the random-walk Laplacian.\n(2) Determine the spectral embedding $\\mathbf{Y}=\\mathbf{W}^{\\top}$\n(3) Cluster the columns $\\boldsymbol{y}_{j}, j=1, \\ldots, n$ into $k$ clusters using the K-means algorithm.\nK-means clustering See Bishop'2006 (pages 424-428) for more details.\n  What is a cluster: a group of points whose inter-point distance are small compared to distances to points outside the cluster.\n  Cluster centers: $\\boldsymbol{\\mu}_{1}, \\ldots, \\boldsymbol{\\mu}_{k}$.\n  Goal: find an assignment of points to clusters as well as a set of vectors $\\mu_{i}$.\n  Notations: For each point $\\boldsymbol{y}_{j}$ there is a binary indicator variable $r_{j i} \\in{0,1}$.\n  Objective: minimize the following distorsion measure:\n  $$ J=\\sum_{j=1}^{n} \\sum_{i=1}^{k} r_{j i}\\left|\\left|\\boldsymbol{y}_{j}-\\boldsymbol{\\mu}_{i}\\right|\\right|^{2} $$\nThe K-means algorithm (1) Initialization: Choose initial values for $\\boldsymbol{\\mu}_{1}, \\ldots, \\boldsymbol{\\mu}_{k}$.\n(2) First step: Assign the $j$-th point to the closest cluster center:\n$$ r_{j i}= \\begin{cases}1 \u0026 \\text { if } i=\\arg \\min_{l}\\left|\\left|\\boldsymbol{y}_{j}-\\mu_{l}\\right|\\right|^{2} \\\\ 0 \u0026 \\text { otherwise }\\end{cases} $$\n(3) Second Step: Minimize $J$ to estimate the cluster centers:\n$$ \\boldsymbol{\\mu}_{i}=\\frac{\\sum_{j=1}^{n} r_{j i} \\boldsymbol{y}_{j}}{\\sum_{j=1}^{n} r_{j i}} $$\n(4) Convergence: Repeat until no more change in the assignments.\nThe Laplacian and the Rayleigh quotient As usual, for a graph $G=(V, E)$, let $A$ be its adjacency matrix and $D$ be the diagonal matrix with $D(v, v)=d_{v}$. Then, the random walk on $G$ will be taken according to the transition matrix $P=D^{-1} A$. We also define the stationary distribution $\\pi$ with $\\pi(x)=d_{x} / \\operatorname{vol} G$.\nOur discussion of random walks on $G$ left off with the result\n$$ \\left|\\left|f P^{t}-\\pi\\right|\\right|_{2} \\leq \\max_{i \\neq 0}\\left|\\rho_{i}\\right|^{t} \\frac{\\max_{x} \\sqrt{d_{x}}}{\\min_{y} \\sqrt{d_{y}}} $$\nwhere $f$ is a probability distribution (i.e. $f \\geq 0$ and $\\sum_{x} f(x)=1$ ) and $1=\\rho_{0} \\geq \\rho_{1} \\geq \\ldots \\geq \\rho_{n-1}$ are the eigenvalues of $P$. This inequality implies that convergence to the stationary distribution $\\pi$ will follow if $\\max \\left\\{\\left|\\rho_{1}\\right|,\\left|\\rho_{n-1}\\right|\\right\\}The transition probability matrix $P$ is similar to the matrix $M=D^{\\frac{1}{2}} P D^{-\\frac{1}{2}}$, so $P$ and $M$ have the same eigenvalues. We previously introduced the Laplacian of the graph as $\\mathcal{L}=I-M$, so it has eigenvalues $0=\\lambda_{0} \\leq \\lambda_{1} \\leq \\ldots \\leq \\lambda_{n-1}$ (where $\\lambda_{i}=1-\\rho_{i}$ ).\nThe main tool we’ll use to study the spectrum of $\\mathcal{L}$ is the Rayleigh quotient $R(f)$ of $\\mathcal{L}$, defined (for our purposes) as\n$$ R(f)=\\frac{f L f^{*}}{f D f^{*}} $$\nwhere $L=D-A$ is the combinatorial Laplacian. This is the same as the usual sense of the Rayleigh quotient $g \\mathcal{L} g^{*} / g g^{*}$ with the subtitution $f=g D^{-\\frac{1}{2}}$. Following this equivalence, if the $\\phi_{i}$ are the eigenvectors of $\\mathcal{L}$, we’ll call the $\\psi_{i}=\\phi_{i} D^{-\\frac{1}{2}}$ the harmonic eigenvectors of $\\mathcal{L}$.\nEmploying the Rayleigh quotient, we see that the eigenvalue $\\lambda_{1}$ can be written as\n$$ \\lambda_{1}=\\inf_{\\substack{f \\\\ \\sum_{x} f(x) d_{x}=0}} R(f) . $$\nSince the eigenvector associated with $\\lambda_{0}$ is $\\phi_{0}=1 D^{\\frac{1}{2}}$, the condition $\\sum_{x} f(x) d_{x}=0$ is an orthogonality condition. Such variational characterizations can also be made for the other eigenvalues:\n$$ \\lambda_{n-1}=\\sup _{f} R(f) $$\nand, in general, $$ \\lambda_{i}=\\sup_{h_{0}, h_{1}, \\ldots, h_{i-1}} \\inf_{\\substack{f: \\\\ \\sum_{x} f(x) h_{j}(x) d_{x}=0 \\\\ \\forall j \\in{0, \\ldots, i-1}}} R(f) $$ The following characterization of the Rayleigh quotient (demonstrated last time) will be useful later: $$ R(f)=\\frac{\\sum_{x \\sim y}(f(x)-f(y))^{2}}{\\sum_{x} f^{2}(x) d_{x}} . $$\nTo this point, we have done a lot of linear algebra. We are not here to teach linear algebra; we are here to take linear algebra one step further to understand what is happening in the graph.\nThe Cheeger Ratio and The Cheeger Constant In many areas of mathematics the questions of “best” comes into play. What is the best bound for a given constant? What is the best way of row reducing a certain matrix? In this section, we will describe a way to make the “best possible cut” of a graph $G=(V, E)$, where a cut may be either an edge-cut or a vertex-cut, and this cut will split $G$ into two disconnected pieces.\nWe would like a way to measure the quality of a cut that is made to $G$. That is, would it be better to cut 4 edges which cause us to lose 20 vertices, or is it better to cut 10 edges which would result in the removal of 120 vetices?\nSuppose we are given a graph $G=(V, E)$ and a subset $S \\subseteq V$. We wish to define the folling two sets:\n$$ \\partial S={{u, v} \\mid u \\in S, v \\notin S} $$\nand\n$$ \\delta S={v \\notin S \\mid v \\sim u, u \\in S} . $$\nDefinition 1 For any vertex set $W$, the volume of $W$ is given by\n$$ \\operatorname{vol}(W)=\\sum_{x W} d_{x}, $$\nwhere $d_{x}$ is the degree of $\\mathrm{x}$ in $W$.\nDefinition 2 The Cheerger Ratio for a vertex set $S$ is\n$$ h(S)=\\frac{|\\partial S|}{\\min {\\operatorname{vol}(S), \\operatorname{vol}(\\bar{S})}}, $$\nwhere $\\bar{S}=V-S$.\nIt is first worth noting that in terms of this defintion of the Cheeger ratio, we are gauging the quality of our cut by taking a measure of what’s been cut off of $G$. There are other forms of the Cheeger ratio as well. For example, we can use $|\\delta S|$ instead of $|\\partial S|,|S|($ or $\\bar{S})$ instead of $\\operatorname{vol}(S)$ (or $\\operatorname{vol}(\\bar{S}))$, or $|S||\\bar{S}|$ instead of $\\min {\\operatorname{vol}(S), \\operatorname{vol}(\\bar{S})}$.\nDefinition 3 For any graph $G=(V, E)$, the Cheeger Constant of $G$ is given by\n$$ h_{G}=\\min_S h(S) . $$\nNow, if we consider the case where $\\operatorname{vol}(S) \\leq \\frac{1}{2} \\operatorname{vol}(G)$, then we can see that\n$$ |\\partial S| \\geq h_{G}(\\operatorname{vol}(S)) . $$\nThe Cheeger Inequality Given a graph $G$, we can define $\\lambda_{1}$ to be the first nontrivial eignevalue of the Laplacian, $\\mathcal{L}$, of $G$.\nFor any graph $G$,\n$$ 2 h_{G} \\geq \\lambda_{1} \\geq \\frac{h_{G}^{2}}{2} $$\nReference https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf\nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\nhttps://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf\nhttps://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf\n","wordCount":"3275","inLanguage":"en","datePublished":"2022-04-02T15:00:20+08:00","dateModified":"2022-04-02T15:00:20+08:00","author":{"@type":"Person","name":"JhuoW"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://JhuoW.github.io/posts/laplacian/"},"publisher":{"@type":"Organization","name":"JhuoW‘s Notes","logo":{"@type":"ImageObject","url":"https://JhuoW.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="JhuoW's Notes (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>JhuoW's Notes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://JhuoW.github.io/gallery/ title=Gallery><span>Gallery</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://JhuoW.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://JhuoW.github.io/posts/>Posts</a></div><h1 class=post-title>Everything about Graph Laplacian</h1><div class=post-meta><span title="2022-04-02 15:00:20 +0800 CST">April 2, 2022</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;JhuoW</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#basic-notations aria-label="Basic notations">Basic notations</a></li><li><a href=#subgraph-of-a-graph aria-label="Subgraph of a graph">Subgraph of a graph</a></li><li><a href=#a-k-partite-graph aria-label="A k-partite graph">A k-partite graph</a></li><li><a href=#the-adjacency-matrix-of-a-graph aria-label="The adjacency matrix of a graph">The adjacency matrix of a graph</a></li><li><a href=#eigenvalues-and-eigenvectors aria-label="Eigenvalues and eigenvectors">Eigenvalues and eigenvectors</a></li><li><a href=#real-valued-functions-on-graphs aria-label="Real-valued functions on graphs">Real-valued functions on graphs</a></li><li><a href=#matrix-a-as-an-operator-and-quadratic-form aria-label="Matrix A as an operator and quadratic form">Matrix A as an operator and quadratic form</a></li><li><a href=#the-incidence-matrix-of-a-graph aria-label="The incidence matrix of a graph">The incidence matrix of a graph</a></li><li><a href=#the-incidence-matrix-a-discrete-differential-operator aria-label="The incidence matrix: A discrete differential operator">The incidence matrix: A discrete differential operator</a></li><li><a href=#the-laplacian-matrix-of-a-graph aria-label="The Laplacian matrix of a graph">The Laplacian matrix of a graph</a></li><li><a href=#the-laplacian-matrix-of-an-undirected-weighted-graph aria-label="The Laplacian matrix of an undirected weighted graph">The Laplacian matrix of an undirected weighted graph</a></li><li><a href=#the-laplacian-of-a-3d-discrete-surface-mesh aria-label="The Laplacian of a 3D discrete surface (mesh)">The Laplacian of a 3D discrete surface (mesh)</a></li><li><a href=#the-laplacian-of-a-cloud-of-points aria-label="The Laplacian of a cloud of points">The Laplacian of a cloud of points</a></li><li><a href=#the-laplacian-of-a-graph-with-one-connected-component aria-label="The Laplacian of a graph with one connected component">The Laplacian of a graph with one connected component</a></li><li><a href=#a-graph-with-k1-connected-components aria-label="A graph with $k&amp;gt;1$ connected components">A graph with $k>1$ connected components</a></li><li><a href=#the-eigenspace-of-lambda_10-with-multiplicity-k aria-label="The eigenspace of $\lambda_{1}=0$ with multiplicity $k$">The eigenspace of $\lambda_{1}=0$ with multiplicity $k$</a></li><li><a href=#the-fiedler-vector-of-the-graph-laplacian aria-label="The Fiedler vector of the graph Laplacian">The Fiedler vector of the graph Laplacian</a></li><li><a href=#eigenvectors-of-the-laplacian-of-connected-graphs aria-label="Eigenvectors of the Laplacian of connected graphs">Eigenvectors of the Laplacian of connected graphs</a></li><li><a href=#laplacian-embedding-mapping-a-graph-on-a-line aria-label="Laplacian embedding: Mapping a graph on a line">Laplacian embedding: Mapping a graph on a line</a></li><li><a href=#laplacian-embedding aria-label="Laplacian embedding">Laplacian embedding</a></li><li><a href=#spectral-embedding-using-the-unnormalized-laplacian aria-label="Spectral embedding using the unnormalized Laplacian">Spectral embedding using the unnormalized Laplacian</a></li><li><a href=#euclidean-l-embedding-of-the-graphs-vertices aria-label="Euclidean L-embedding of the graph&amp;rsquo;s vertices">Euclidean L-embedding of the graph&rsquo;s vertices</a></li><li><a href=#justification-for-choosing-the-l-embedding aria-label="Justification for choosing the L-embedding">Justification for choosing the L-embedding</a></li><li><a href=#the-commute-time-distance aria-label="The commute-time distance">The commute-time distance</a></li><li><a href=#the-graph-pca aria-label="The graph PCA">The graph PCA</a></li><li><a href=#other-laplacian-matrices aria-label="Other Laplacian matrices">Other Laplacian matrices</a></li><li><a href=#eigenvalues-and-eigenvectors-of-mathrml_n-and-mathrml_r aria-label="Eigenvalues and eigenvectors of $\mathrm{L}_{n}$ and $\mathrm{L}_{r}$">Eigenvalues and eigenvectors of $\mathrm{L}_{n}$ and $\mathrm{L}_{r}$</a></li><li><a href=#spectral-embedding-using-the-random-walk-laplacian-mathbfl_r aria-label="Spectral embedding using the random-walk Laplacian $\mathbf{L}_{r}$">Spectral embedding using the random-walk Laplacian $\mathbf{L}_{r}$</a></li><li><a href=#the-normalized-additive-laplacian aria-label="The normalized additive Laplacian">The normalized additive Laplacian</a></li><li><a href=#the-graph-partitioning-problem aria-label="The graph partitioning problem">The graph partitioning problem</a></li><li><a href=#what-is-spectral-clustering aria-label="What is spectral clustering?">What is spectral clustering?</a></li><li><a href=#spectral-clustering-using-the-random-walk-laplacian aria-label="Spectral clustering using the random-walk Laplacian">Spectral clustering using the random-walk Laplacian</a></li><li><a href=#k-means-clustering aria-label="K-means clustering">K-means clustering</a></li><li><a href=#the-k-means-algorithm aria-label="The K-means algorithm">The K-means algorithm</a></li><li><a href=#the-laplacian-and-the-rayleigh-quotient aria-label="The Laplacian and the Rayleigh quotient">The Laplacian and the Rayleigh quotient</a></li><li><a href=#the-cheeger-ratio-and-the-cheeger-constant aria-label="The Cheeger Ratio and The Cheeger Constant">The Cheeger Ratio and The Cheeger Constant</a></li><li><a href=#the-cheeger-inequality aria-label="The Cheeger Inequality">The Cheeger Inequality</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem. Another application is spectral matching that solves for graph matching.</p><h1 id=basic-notations>Basic notations<a hidden class=anchor aria-hidden=true href=#basic-notations>#</a></h1><p>We consider simple graphs (no multiple edges or loops), $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ :</p><ul><li><p>$\mathcal{V}(\mathcal{G})=\left\{v_{1}, \ldots, v_{n}\right\}$ is called the vertex set with $n=|\mathcal{V}|$;</p></li><li><p>$\mathcal{E}(\mathcal{G})=\left\{e_{i j}\right\}$ is called the edge set with $m=|\mathcal{E}|$;</p></li><li><p>An edge $e_{i j}$ connects vertices $v_{i}$ and $v_{j}$ if they are adjacent or neighbors. One possible notation for adjacency is $v_{i} \sim v_{j}$;</p></li><li><p>The number of neighbors of a node $v$ is called the degree of $v$ and is denoted by $d(v), d\left(v_{i}\right)=\sum_{v_{i} \sim v_{j}} e_{i j}$. If all the nodes of a graph have the same degree, the graph is regular; The nodes of an Eulerian graph have even degree.</p></li><li><p>A graph is complete if there is an edge between every pair of vertices.</p></li></ul><h1 id=subgraph-of-a-graph>Subgraph of a graph<a hidden class=anchor aria-hidden=true href=#subgraph-of-a-graph>#</a></h1><ul><li><p>$\mathcal{H}$ is a subgraph of $\mathcal{G}$ if $\mathcal{V}(\mathcal{H}) \subseteq \mathcal{V}(\mathcal{G})$ and $\mathcal{E}(\mathcal{H}) \subseteq \mathcal{E}(\mathcal{G})$;</p></li><li><p>a subgraph $\mathcal{H}$ is an induced subgraph of $\mathcal{G}$ if two vertices of $\mathcal{V}(\mathcal{H})$ are adjacent if and only if they are adjacent in $\mathcal{G}$.</p></li><li><p>A clique is a complete subgraph of a graph.</p></li><li><p>A path of $k$ vertices is a sequence of $k$ distinct vertices such that consecutive vertices are adjacent.</p></li><li><p>A cycle is a connected subgraph where every vertex has exactly two neighbors.</p></li><li><p>A graph containing no cycles is a forest. A connected forest is a tree.</p></li></ul><h1 id=a-k-partite-graph>A k-partite graph<a hidden class=anchor aria-hidden=true href=#a-k-partite-graph>#</a></h1><ul><li>A graph is called k-partite if its set of vertices admits a partition into $k$ classes such that the vertices of the same class are not adjacent.</li><li>An example of a bipartite graph.</li></ul><p><img loading=lazy src=/posts/2022-04-02-laplacian/1.png#center alt=你想输入的替代文字></p><h1 id=the-adjacency-matrix-of-a-graph>The adjacency matrix of a graph<a hidden class=anchor aria-hidden=true href=#the-adjacency-matrix-of-a-graph>#</a></h1><ul><li>For a graph with $n$ vertices, the entries of the $n \times n$ adjacency matrix are defined by:</li></ul><p>$$
\mathbf{A}:= \begin{cases}A_{i j}=1 & \text { if there is an edge } e_{i j} \\ A_{i j}=0 & \text { if there is no edge } \\ A_{i i}=0 & \end{cases}
$$</p><p>$$
\begin{aligned}
& \mathbf{A}=\left[\begin{array}{llll}0 & 1 & 1 & 0 \\1 & 0 & 1 & 1 \\1 & 1 & 0 & 0 \\0 & 1 & 0 & 0\end{array}\right]
\end{aligned}
$$</p><p><img loading=lazy src=/posts/2022-04-02-laplacian/2.png#center alt=你想输入的替代文字></p><h1 id=eigenvalues-and-eigenvectors>Eigenvalues and eigenvectors<a hidden class=anchor aria-hidden=true href=#eigenvalues-and-eigenvectors>#</a></h1><ul><li><p>A is a real-symmetric matrix: it has $n$ real eigenvalues and its $n$ real eigenvectors form an orthonormal basis.</p></li><li><p>Let $\left\{\lambda_{1}, \ldots, \lambda_{i}, \ldots, \lambda_{r}\right\}$ be the set of distinct eigenvalues.</p></li><li><p>The eigenspace $S_{i}$ contains the eigenvectors associated with $\lambda_{i}$ :</p></li></ul><p>$$
S_{i}=\left\{\boldsymbol{x} \in \mathbb{R}^{n} \mid \mathbf{A} \boldsymbol{x}=\lambda_{i} \boldsymbol{x}\right\}
$$</p><ul><li><p>For real-symmetric matrices, the algebraic multiplicity is equal to the geometric multiplicity, for all the eigenvalues.</p></li><li><p>The dimension of $S_{i}$ (geometric multiplicity) is equal to the multiplicity of $\lambda_{i}$.</p></li><li><p>If $\lambda_{i} \neq \lambda_{j}$ then $S_{i}$ and $S_{j}$ are mutually orthogonal.</p></li></ul><h1 id=real-valued-functions-on-graphs>Real-valued functions on graphs<a hidden class=anchor aria-hidden=true href=#real-valued-functions-on-graphs>#</a></h1><ul><li><p>We consider real-valued functions on the set of the graph&rsquo;s vertices, $\boldsymbol{f}: \mathcal{V} \longrightarrow \mathbb{R}$. Such a function assigns a real number to each graph node.</p></li><li><p>$\boldsymbol{f}$ is a vector indexed by the graph&rsquo;s vertices, hence $\boldsymbol{f} \in \mathbb{R}^{n}$.</p></li><li><p>Notation: $\boldsymbol{f}=\left(f\left(v_{1}\right), \ldots, f\left(v_{n}\right)\right)=(f(1), \ldots, f(n))$.</p></li><li><p>The eigenvectors of the adjacency matrix, $\mathbf{A} \boldsymbol{x}=\lambda \boldsymbol{x}$, can be viewed as eigenfunctions.</p></li></ul><p><img loading=lazy src=/posts/2022-04-02-laplacian/3.png#center alt=你想输入的替代文字></p><h1 id=matrix-a-as-an-operator-and-quadratic-form>Matrix A as an operator and quadratic form<a hidden class=anchor aria-hidden=true href=#matrix-a-as-an-operator-and-quadratic-form>#</a></h1><ul><li>The adjacency matrix can be viewed as an operator</li></ul><p>$$
\boldsymbol{g}=\mathbf{A} \boldsymbol{f} ; g(i)=\sum_{i \sim j} f(j)
$$</p><ul><li>It can also be viewed as a quadratic form:</li></ul><p>$$
\boldsymbol{f}^{\top} \mathbf{A} \boldsymbol{f}=\sum_{e_{i j}} f(i) f(j)
$$</p><h1 id=the-incidence-matrix-of-a-graph>The incidence matrix of a graph<a hidden class=anchor aria-hidden=true href=#the-incidence-matrix-of-a-graph>#</a></h1><ul><li><p>Let each edge in the graph have an arbitrary but fixed orientation;</p></li><li><p>The incidence matrix of a graph is a $|\mathcal{E}| \times|\mathcal{V}|(m \times n)$ matrix defined as follows:</p></li></ul><p>$$
\nabla:= \begin{cases}\nabla_{e v}=-1 & \text { if } v \text { is the initial vertex of edge } e \\ \nabla_{e v}=1 & \text { if } v \text { is the terminal vertex of edge } e \\ \nabla_{e v}=0 & \text { if } v \text { is not in } e\end{cases}
$$</p><p>$$
\begin{aligned}
& \nabla=\left[\begin{array}{cccc}-1 & 1 & 0 & 0 \\1 & 0 & -1 & 0 \\0 & -1 & 1 & 0 \\0 & -1 & 0 & +1\end{array}\right]
\end{aligned}
$$</p><p><img loading=lazy src=/posts/2022-04-02-laplacian/2.png#center alt=你想输入的替代文字></p><h1 id=the-incidence-matrix-a-discrete-differential-operator>The incidence matrix: A discrete differential operator<a hidden class=anchor aria-hidden=true href=#the-incidence-matrix-a-discrete-differential-operator>#</a></h1><ul><li><p>The mapping $\boldsymbol{f} \longrightarrow \nabla \boldsymbol{f}$ is known as the co-boundary mapping of the graph.</p></li><li><p>$(\nabla \boldsymbol{f})\left(e_{i j}\right)=f\left(v_{j}\right)-f\left(v_{i}\right)$</p></li></ul><p>$$
\left(\begin{array}{c}
f(2)-f(1) \\
f(1)-f(3) \\
f(3)-f(2) \\
f(4)-f(2)
\end{array}\right)=\left[\begin{array}{cccc}
-1 & 1 & 0 & 0 \\
1 & 0 & -1 & 0 \\
0 & -1 & 1 & 0 \\
0 & -1 & 0 & +1
\end{array}\right]\left(\begin{array}{c}
f(1) \\
f(2) \\
f(3) \\
f(4)
\end{array}\right)
$$</p><h1 id=the-laplacian-matrix-of-a-graph>The Laplacian matrix of a graph<a hidden class=anchor aria-hidden=true href=#the-laplacian-matrix-of-a-graph>#</a></h1><ul><li><p>$\mathbf{L}=\nabla^{\top} \nabla$</p></li><li><p>$(\mathbf{L} \boldsymbol{f})\left(v_{i}\right)=\sum_{v_{j} \sim v_{i}}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)$</p></li><li><p>Connection between the Laplacian and the adjacency matrices:</p></li></ul><p>$$
\mathbf{L}=\mathbf{D}-\mathbf{A}
$$</p><ul><li>The degree matrix: $\mathbf{D}:=D_{i i}=d\left(v_{i}\right)$.</li></ul><p>$$
\mathbf{L}=\left[\begin{array}{cccc}
2 & -1 & -1 & 0 \\
-1 & 3 & -1 & -1 \\
-1 & -1 & 2 & 0 \\
0 & -1 & 0 & 1
\end{array}\right]
$$</p><h1 id=the-laplacian-matrix-of-an-undirected-weighted-graph>The Laplacian matrix of an undirected weighted graph<a hidden class=anchor aria-hidden=true href=#the-laplacian-matrix-of-an-undirected-weighted-graph>#</a></h1><ul><li><p>We consider undirected weighted graphs: Each edge $e_{i j}$ is weighted by $w_{i j}>0$.</p></li><li><p>The Laplacian as an operator:</p></li></ul><p>$$
(\mathbf{L} \boldsymbol{f})\left(v_{i}\right)=\sum_{v_{j} \sim v_{i}} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)
$$</p><ul><li>As a quadratic form:</li></ul><p>$$
\boldsymbol{f}^{\top} \mathbf{L} \boldsymbol{f}=\frac{1}{2} \sum_{e_{i j}} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)^{2}
$$</p><ul><li><p>L is symmetric and positive semi-definite.</p></li><li><p>L has $n$ non-negative, real-valued eigenvalues: $0=\lambda_{1} \leq \lambda_{2} \leq \ldots \leq \lambda_{n} .$</p></li></ul><h1 id=the-laplacian-of-a-3d-discrete-surface-mesh>The Laplacian of a 3D discrete surface (mesh)<a hidden class=anchor aria-hidden=true href=#the-laplacian-of-a-3d-discrete-surface-mesh>#</a></h1><ul><li><p>A graph vertex $v_{i}$ is associated with a 3D point $\boldsymbol{v}_{i}$.</p></li><li><p>The weight of an edge $e_{i j}$ is defined by the Gaussian kernel:</p></li></ul><p>$$
w_{i j}=\exp \left(-\left|\boldsymbol{v}_{i}-\boldsymbol{v}_{j}\right|^{2} / \sigma^{2}\right)
$$</p><ul><li><p>$0 \leq w_{\min } \leq w_{i j} \leq w_{\max } \leq 1$</p></li><li><p>Hence, the geometric structure of the mesh is encoded in the weights.</p></li><li><p>Other weighting functions were proposed in the literature.</p></li></ul><h1 id=the-laplacian-of-a-cloud-of-points>The Laplacian of a cloud of points<a hidden class=anchor aria-hidden=true href=#the-laplacian-of-a-cloud-of-points>#</a></h1><ul><li><p>3-nearest neighbor graph</p></li><li><p>$\varepsilon$-radius graph</p></li><li><p>KNN may guarantee that the graph is connected (depends on the implementation)</p></li><li><p>$\varepsilon$-radius does not guarantee that the graph has one connected component</p></li></ul><p><img loading=lazy src=/posts/2022-04-02-laplacian/4.png#center alt=你想输入的替代文字></p><h1 id=the-laplacian-of-a-graph-with-one-connected-component>The Laplacian of a graph with one connected component<a hidden class=anchor aria-hidden=true href=#the-laplacian-of-a-graph-with-one-connected-component>#</a></h1><ul><li><p>$Lu =\lambda \boldsymbol{u}$.</p></li><li><p>$\mathbf{L} \mathbf{1}_{n}=\mathbf{0}, \lambda_{1}=0$ is the smallest eigenvalue.</p></li><li><p>The one vector: $\mathbf{1}_{n}=(1 \ldots 1)^{\top}$.</p></li><li><p>$0=\boldsymbol{u}^{\top} \mathbf{L} \boldsymbol{u}=\sum_{i, j=1}^{n} w_{i j}(u(i)-u(j))^{2}$.</p></li><li><p>If any two vertices are connected by a path, then $\boldsymbol{u}=(u(1), \ldots, u(n))$ needs to be constant at all vertices such that the quadratic form vanishes. Therefore, a graph with one connected component has the constant vector $\boldsymbol{u}_{1}=\mathbf{1}_{n}$ as the only eigenvector with eigenvalue 0 .</p></li></ul><h1 id=a-graph-with-k1-connected-components>A graph with $k>1$ connected components<a hidden class=anchor aria-hidden=true href=#a-graph-with-k1-connected-components>#</a></h1><ul><li>Each connected component has an associated Laplacian. Therefore, we can write matrix $\mathbf{L}$ as a block diagonal matrix:</li></ul><p>$$
\mathbf{L}=\left[\begin{array}{lll}
\mathbf{L}_{1} & & \\
& \ddots & \\
& & \mathbf{L}_{k}
\end{array}\right]
$$</p><ul><li><p>The spectrum of $\mathbf{L}$ is given by the union of the spectra of $\mathbf{L}_{i}$.</p></li><li><p>Each block corresponds to a connected component, hence each matrix $\mathbf{L}_{i}$ has an eigenvalue 0 with multiplicity 1 .</p></li><li><p>The spectrum of $\mathbf{L}$ is given by the union of the spectra of $\mathbf{L}_{i}$.</p></li><li><p>The eigenvalue $\lambda_{1}=0$ has multiplicity $k$.</p></li></ul><h1 id=the-eigenspace-of-lambda_10-with-multiplicity-k>The eigenspace of $\lambda_{1}=0$ with multiplicity $k$<a hidden class=anchor aria-hidden=true href=#the-eigenspace-of-lambda_10-with-multiplicity-k>#</a></h1><ul><li>The eigenspace corresponding to $\lambda_{1}=\ldots=\lambda_{k}=0$ is spanned by the $k$ mutually orthogonal vectors:</li></ul><p>$$
\begin{aligned}
\boldsymbol{u}_{1} &=\mathbf{1}_{L_{1}} \\
& \cdots \\
\boldsymbol{u}_{k} &=\mathbf{1}_{L_{k}}
\end{aligned}
$$</p><ul><li><p>with $\mathbf{1}_{L_{i}}=(0000111110000)^{\top} \in \mathbb{R}^{n}$</p></li><li><p>These vectors are the indicator vectors of the graph&rsquo;s connected components.</p></li><li><p>Notice that $\mathbf{1}_{L_{1}}+\ldots+\mathbf{1}_{L_{k}}=\mathbf{1}_{n}$</p></li></ul><h1 id=the-fiedler-vector-of-the-graph-laplacian>The Fiedler vector of the graph Laplacian<a hidden class=anchor aria-hidden=true href=#the-fiedler-vector-of-the-graph-laplacian>#</a></h1><ul><li><p>The first non-null eigenvalue $\lambda_{k+1}$ is called the Fiedler value.</p></li><li><p>The corresponding eigenvector $\boldsymbol{u}_{k+1}$ is called the Fiedler vector.</p></li><li><p>The multiplicity of the Fiedler eigenvalue is always equal to $1 .$</p></li><li><p>The Fiedler value is the algebraic connectivity of a graph, the further from 0 , the more connected.</p></li><li><p>The Fidler vector has been extensively used for spectral bi-partioning</p></li><li><p>Theoretical results are summarized in Spielman & Teng 2007: <a href=http://cs-www.cs.yale.edu/homes/spielman/>http://cs-www.cs.yale.edu/homes/spielman/</a></p></li></ul><h1 id=eigenvectors-of-the-laplacian-of-connected-graphs>Eigenvectors of the Laplacian of connected graphs<a hidden class=anchor aria-hidden=true href=#eigenvectors-of-the-laplacian-of-connected-graphs>#</a></h1><ul><li><p>$\boldsymbol{u}_{1}=\mathbf{1}_{n}, \mathbf{L} \mathbf{1}_{n}=\mathbf{0}$.</p></li><li><p>$\boldsymbol{u}_{2}$ is the the Fiedler vector with multiplicity 1 .</p></li><li><p>The eigenvectors form an orthonormal basis: $\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{j}=\delta_{i j}$.</p></li><li><p>For any eigenvector $\boldsymbol{u}_{i}=\left(\boldsymbol{u}_{i}\left(v_{1}\right) \ldots \boldsymbol{u}_{i}\left(v_{n}\right)\right)^{\top}, 2 \leq i \leq n$ :</p></li></ul><p>$$
\boldsymbol{u}_{i}^{\top} \mathbf{1}_{n}=0
$$</p><ul><li>Hence the components of $\boldsymbol{u}_{i}, 2 \leq i \leq n$ satisfy:</li></ul><p>$$
\sum_{j=1}^{n} \boldsymbol{u}_{i}\left(v_{j}\right)=0
$$</p><ul><li>Each component is bounded by:</li></ul><p>$$
-1&lt;\boldsymbol{u}_{i}\left(v_{j}\right)&lt;1
$$</p><h1 id=laplacian-embedding-mapping-a-graph-on-a-line>Laplacian embedding: Mapping a graph on a line<a hidden class=anchor aria-hidden=true href=#laplacian-embedding-mapping-a-graph-on-a-line>#</a></h1><ul><li>Map a weighted graph onto a line such that connected nodes stay as close as possible, i.e., minimize $\sum_{i, j=1}^{n} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)^{2}$, or:</li></ul><p>$$
\arg \min _{\boldsymbol{f}} \boldsymbol{f}^{\top} \mathbf{L} \boldsymbol{f} \text { with: } \boldsymbol{f}^{\top} \boldsymbol{f}=1 \text { and } \boldsymbol{f}^{\top} \mathbf{1}=0
$$</p><ul><li><p>The solution is the eigenvector associated with the smallest nonzero eigenvalue of the eigenvalue problem: $\mathbf{L} \boldsymbol{f}=\lambda \boldsymbol{f}$, namely the Fiedler vector $\boldsymbol{u}_{2}$.</p></li><li><p>For more details on this minimization see Golub & Van Loan Matrix Computations, chapter 8 (The symmetric eigenvalue problem).</p></li></ul><p><em><strong>Example of mapping a graph on the Fiedler vector</strong></em>:</p><p><img loading=lazy src=/posts/2022-04-02-laplacian/5.png#center alt=你想输入的替代文字></p><h1 id=laplacian-embedding>Laplacian embedding<a hidden class=anchor aria-hidden=true href=#laplacian-embedding>#</a></h1><ul><li><p>Embed the graph in a $k$-dimensional Euclidean space. The embedding is given by the $n \times k$ matrix $\mathbf{F}=\left[\boldsymbol{f}_{1} \boldsymbol{f}_{2} \ldots \boldsymbol{f}_{k}\right]$ where the $i$-th row of this matrix $-\boldsymbol{f}^{(i)}-$ corresponds to the Euclidean coordinates of the $i$-th graph node $v_{i}$.</p></li><li><p>We need to minimize:</p></li></ul><p>$$
\arg \min_{\boldsymbol{f}_{1} \ldots} \sum_{k}^{n} \sum_{i, j=1}^{n} w_{i j}\left|\left|\boldsymbol{f}^{(i)}-\boldsymbol{f}^{(j)}\right|\right|^{2} \text { with: } \mathbf{F}^{\top} \mathbf{F}=\mathbf{I}
$$</p><ul><li>The solution is provided by the matrix of eigenvectors corresponding to the $k$ lowest nonzero eigenvalues of the eigenvalue problem $\mathbf{L} \boldsymbol{f}=\lambda \boldsymbol{f}$.</li></ul><h1 id=spectral-embedding-using-the-unnormalized-laplacian>Spectral embedding using the unnormalized Laplacian<a hidden class=anchor aria-hidden=true href=#spectral-embedding-using-the-unnormalized-laplacian>#</a></h1><ul><li><p>Compute the eigendecomposition $\mathbf{L}=\mathbf{D}-\mathbf{A}$.</p></li><li><p>Select the $k$ smallest non-null eigenvalues $\lambda_{2} \leq \ldots \leq \lambda_{k+1}$</p></li><li><p>$\lambda_{k+2}-\lambda_{k+1}=$ eigengap.</p></li><li><p>We obtain the $n \times k$ matrix $\mathbf{U}=\left[\boldsymbol{u}_{2} \ldots \boldsymbol{u}_{k+1}\right]$ :</p></li></ul><p>$$
\mathbf{U}=\left[\begin{array}{ccc}
\boldsymbol{u}_{2}\left(v_{1}\right) & \ldots & \boldsymbol{u}_{k+1}\left(v_{1}\right) \\
\vdots & & \vdots \\
\boldsymbol{u}_{2}\left(v_{n}\right) & \ldots & \boldsymbol{u}_{k+1}\left(v_{n}\right)
\end{array}\right]
$$</p><ul><li><p>$\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{j}=\delta_{i j}$ (orthonormal vectors), hence $\mathbf{U}^{\top} \mathbf{U}=\mathbf{I}_{k}$.</p></li><li><p>Column $i(2 \leq i \leq k+1)$ of this matrix is a mapping on the eigenvector $\boldsymbol{u}_{i}$.</p></li></ul><h1 id=euclidean-l-embedding-of-the-graphs-vertices>Euclidean L-embedding of the graph&rsquo;s vertices<a hidden class=anchor aria-hidden=true href=#euclidean-l-embedding-of-the-graphs-vertices>#</a></h1><ul><li>(Euclidean) L-embedding of a graph:</li></ul><p>$$
\mathbf{X}=\boldsymbol{\Lambda}_{k}^{-\frac{1}{2}} \mathbf{U}^{\top}=\left[\begin{array}{llll}
\boldsymbol{x}_{1} & \ldots & \boldsymbol{x}_{j} \ldots & \boldsymbol{x}_{n}
\end{array}\right]
$$</p><p>The coordinates of a vertex $v_{j}$ are:</p><p>$$
\boldsymbol{x}_{j}=\left(\begin{array}{c}
\frac{\boldsymbol{u}_{2}\left(v_{j}\right)}{\sqrt{\lambda_{2}}} \\
\vdots \\
\frac{\boldsymbol{u}_{k+1}\left(v_{j}\right)}{\sqrt{\lambda_{k+1}}}
\end{array}\right)
$$</p><h1 id=justification-for-choosing-the-l-embedding>Justification for choosing the L-embedding<a hidden class=anchor aria-hidden=true href=#justification-for-choosing-the-l-embedding>#</a></h1><p>Both</p><ul><li><p>the commute-time distance (CTD) and</p></li><li><p>the principal-component analysis of a graph (graph PCA)</p></li></ul><p>are two important concepts; They allow to reason &ldquo;statistically&rdquo; on a graph. They are both associated with the unnormalized Laplacian matrix.</p><h1 id=the-commute-time-distance>The commute-time distance<a hidden class=anchor aria-hidden=true href=#the-commute-time-distance>#</a></h1><ul><li><p>The CTD is a well known quantity in Markov chains;</p></li><li><p>It is the average number of (weighted) edges that it takes, starting at vertex $v_{i}$, to randomly reach vertex $v_{j}$ for the first time and go back;</p></li><li><p>The CTD decreases as the number of connections between the two nodes increases;</p></li><li><p>It captures the connectivity structure of a small graph volume rather than a single path between the two vertices - such as the shortest-path geodesic distance.</p></li><li><p>The CTD can be computed in closed form:</p></li></ul><p>$$
\operatorname{CTD}^{2}\left(v_{i}, v_{j}\right)=\operatorname{vol}(\mathcal{G})\left|\left|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right|\right|^{2}
$$</p><h1 id=the-graph-pca>The graph PCA<a hidden class=anchor aria-hidden=true href=#the-graph-pca>#</a></h1><ul><li>The mean (remember that $\sum_{j=1}^{n} \boldsymbol{u}_{i}\left(v_{j}\right)=0$ ):</li></ul><p>$$
\overline{\boldsymbol{x}}=\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{x}_{j}=\boldsymbol{\Lambda}_{k}^{-\frac{1}{2}}\left(\begin{array}{c}
\sum_{j=1}^{n} \boldsymbol{u}_{2}\left(v_{j}\right) \\
\vdots \\
\sum_{j=1}^{n} \boldsymbol{u}_{k+1}\left(v_{j}\right)
\end{array}\right)=\left(\begin{array}{c}
0 \\
\vdots \\
0
\end{array}\right)
$$</p><ul><li>The covariance matrix:</li></ul><p>$$
\mathbf{S}=\frac{1}{n} \sum_{j=1}^{n} \boldsymbol{x}_{j} \boldsymbol{x}_{j}^{\top}=\frac{1}{n} \mathbf{X} \mathbf{X}^{\top}=\frac{1}{n} \boldsymbol{\Lambda}_{k}^{-\frac{1}{2}} \mathbf{U}^{\top} \mathbf{U} \boldsymbol{\Lambda}_{k}^{-\frac{1}{2}}=\frac{1}{n} \boldsymbol{\Lambda}_{k}^{-1}
$$</p><ul><li>The vectors $\boldsymbol{u}_{2}, \ldots, \boldsymbol{u}_{k+1}$ are the directions of maximum variance of the graph embedding, with $\lambda_{2}^{-1} \geq \ldots \geq \lambda_{k+1}^{-1}$.</li></ul><h1 id=other-laplacian-matrices>Other Laplacian matrices<a hidden class=anchor aria-hidden=true href=#other-laplacian-matrices>#</a></h1><ul><li>The normalized graph Laplacian (symmetric and semi-definite positive):</li></ul><p>$$
\mathbf{L}_{n}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}}=\mathbf{I}-\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}
$$</p><ul><li>The transition matrix (allows an analogy with Markov chains):</li></ul><p>$$
\mathbf{L}_{t}=\mathbf{D}^{-1} \mathbf{A}
$$</p><ul><li>The random-walk graph Laplacian:</li></ul><p>$$
\mathbf{L}_{r}=\mathbf{D}^{-1} \mathbf{L}=\mathbf{I}-\mathbf{L}_{t}
$$</p><ul><li>These matrices are similar:</li></ul><p>$$
\mathbf{L}_{r}=\mathbf{D}^{-\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} \mathbf{D}^{\frac{1}{2}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L}_{n} \mathbf{D}^{\frac{1}{2}}
$$</p><h1 id=eigenvalues-and-eigenvectors-of-mathrml_n-and-mathrml_r>Eigenvalues and eigenvectors of $\mathrm{L}_{n}$ and $\mathrm{L}_{r}$<a hidden class=anchor aria-hidden=true href=#eigenvalues-and-eigenvectors-of-mathrml_n-and-mathrml_r>#</a></h1><ul><li>$\mathbf{L}_{r} \boldsymbol{w}=\lambda \boldsymbol{w} \Longleftrightarrow \mathbf{L} \boldsymbol{w}=\lambda \mathbf{D} \boldsymbol{w}$, hence:</li></ul><p>$$
\mathbf{L}_{r}: \quad \lambda_{1}=0 ; \quad \boldsymbol{w}_{1}=\mathbf{1}
$$</p><ul><li>$\mathbf{L}_{n} \boldsymbol{v}=\lambda \boldsymbol{v}$. By virtue of the similarity transformation between the two matrices:</li></ul><p>$$
\mathbf{L}_{n}: \quad \lambda_{1}=0 \quad \boldsymbol{v}_{1}=\mathbf{D}^{\frac{1}{2}} \mathbf{1}
$$</p><ul><li>More generally, the two matrices have the same eigenvalues:</li></ul><p>$$
0=\lambda_{1} \leq \ldots \leq \lambda_{i} \ldots \leq \lambda_{n}
$$</p><ul><li>Their eigenvectors are related by:</li></ul><p>$$
\boldsymbol{v}_{i}=\mathbf{D}^{\frac{1}{2}} \boldsymbol{w}_{i}, \forall i=1 \ldots n
$$</p><h1 id=spectral-embedding-using-the-random-walk-laplacian-mathbfl_r>Spectral embedding using the random-walk Laplacian $\mathbf{L}_{r}$<a hidden class=anchor aria-hidden=true href=#spectral-embedding-using-the-random-walk-laplacian-mathbfl_r>#</a></h1><ul><li>The $n \times k$ matrix contains the first $k$ eigenvectors of $\mathbf{L}_{r}$ :</li></ul><p>$$
\mathbf{W}=\left[\begin{array}{lll}
\boldsymbol{w}_{2} & \ldots & \boldsymbol{w}_{k+1}
\end{array}\right]
$$</p><ul><li>It is straightforward to obtain the following expressions, where $\boldsymbol{d}$ and $\mathbf{D}$ are the degree-vector and the degree-matrix:</li></ul><p>$$
\begin{gathered}
\boldsymbol{w}_{i}^{\top} \boldsymbol{d}=0, \forall i, 2 \leq i \leq n \\
\mathbf{W}^{\top} \mathbf{D W}=\mathbf{I}_{k}
\end{gathered}
$$</p><ul><li>The isometric embedding using the random-walk Laplacian:</li></ul><p>$$
\mathbf{Y}=\mathbf{W}^{\top}=\left[\begin{array}{lll}
\boldsymbol{y}_{1} & \ldots & \boldsymbol{y}_{n}
\end{array}\right]
$$</p><h1 id=the-normalized-additive-laplacian>The normalized additive Laplacian<a hidden class=anchor aria-hidden=true href=#the-normalized-additive-laplacian>#</a></h1><ul><li>Some authors use the following matrix:</li></ul><p>$$
\mathbf{L}_{a}=\frac{1}{d_{\max }}\left(\mathbf{A}+d_{\max } \mathbf{I}-\mathbf{D}\right)
$$</p><ul><li>This matrix is closely related to L:</li></ul><p>$$
\mathbf{L}_{a}=\frac{1}{d_{\max }}\left(d_{\max } \mathbf{I}-\mathbf{L}\right)
$$</p><ul><li>and we have:</li></ul><p>$$
\mathbf{L}_{a} \boldsymbol{u}=\mu \boldsymbol{u} \Longleftrightarrow \mathbf{L} \boldsymbol{u}=\lambda \boldsymbol{u}, \mu=1-\frac{\lambda}{d_{\max }}
$$</p><h1 id=the-graph-partitioning-problem>The graph partitioning problem<a hidden class=anchor aria-hidden=true href=#the-graph-partitioning-problem>#</a></h1><ul><li>The graph-cut problem: Partition the graph such that:</li></ul><p>(1) Edges between groups have very low weight, and</p><p>(2) Edges within a group have high weight.</p><p>$\operatorname{cut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} W\left(A_{i}, \bar{A}_{i}\right)$ with $W(A, B)=\sum_{i \in A, j \in B} w_{i j}$</p><ul><li>Ratio cut:</li></ul><p>$$
\operatorname{RatioCut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\left|A_{i}\right|}
$$</p><ul><li>Normalized cut:</li></ul><p>$$
\operatorname{NCut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\operatorname{vol}\left(A_{i}\right)}
$$</p><h1 id=what-is-spectral-clustering>What is spectral clustering?<a hidden class=anchor aria-hidden=true href=#what-is-spectral-clustering>#</a></h1><p>See my <a href=https://jhuow.fun/posts/2019-09-07-spectral-clustering/>Blog</a> of Spectral Clustering (in Chinese).</p><ul><li><p>Both ratio-cut and normalized-cut minimizations are NP-hard problems</p></li><li><p>Spectral clustering is a way to solve relaxed versions of these problems:</p></li></ul><p>(1) The smallest non-null eigenvectors of the unnormalized Laplacian approximate the RatioCut minimization criterion, and</p><p>(2) The smallest non-null eigenvectors of the random-walk Laplacian approximate the NCut criterion.</p><h1 id=spectral-clustering-using-the-random-walk-laplacian>Spectral clustering using the random-walk Laplacian<a hidden class=anchor aria-hidden=true href=#spectral-clustering-using-the-random-walk-laplacian>#</a></h1><ul><li><p>For details see (von Luxburg &lsquo;07)</p></li><li><p>Input: Laplacian $\mathbf{L}_{r}$ and the number $k$ of clusters to compute.</p></li><li><p>Output: Cluster $C_{1}, \ldots, C_{k}$.</p></li></ul><p>(3) Compute W formed with the first $k$ eigenvectors of the random-walk Laplacian.</p><p>(2) Determine the spectral embedding $\mathbf{Y}=\mathbf{W}^{\top}$</p><p>(3) Cluster the columns $\boldsymbol{y}_{j}, j=1, \ldots, n$ into $k$ clusters using the K-means algorithm.</p><h1 id=k-means-clustering>K-means clustering<a hidden class=anchor aria-hidden=true href=#k-means-clustering>#</a></h1><p>See Bishop'2006 (pages 424-428) for more details.</p><ul><li><p>What is a cluster: a group of points whose inter-point distance are small compared to distances to points outside the cluster.</p></li><li><p>Cluster centers: $\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k}$.</p></li><li><p>Goal: find an assignment of points to clusters as well as a set of vectors $\mu_{i}$.</p></li><li><p>Notations: For each point $\boldsymbol{y}_{j}$ there is a binary indicator variable $r_{j i} \in{0,1}$.</p></li><li><p>Objective: minimize the following distorsion measure:</p></li></ul><p>$$
J=\sum_{j=1}^{n} \sum_{i=1}^{k} r_{j i}\left|\left|\boldsymbol{y}_{j}-\boldsymbol{\mu}_{i}\right|\right|^{2}
$$</p><h1 id=the-k-means-algorithm>The K-means algorithm<a hidden class=anchor aria-hidden=true href=#the-k-means-algorithm>#</a></h1><p>(1) Initialization: Choose initial values for $\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k}$.</p><p>(2) First step: Assign the $j$-th point to the closest cluster center:</p><p>$$
r_{j i}= \begin{cases}1 & \text { if } i=\arg \min_{l}\left|\left|\boldsymbol{y}_{j}-\mu_{l}\right|\right|^{2} \\ 0 & \text { otherwise }\end{cases}
$$</p><p>(3) Second Step: Minimize $J$ to estimate the cluster centers:</p><p>$$
\boldsymbol{\mu}_{i}=\frac{\sum_{j=1}^{n} r_{j i} \boldsymbol{y}_{j}}{\sum_{j=1}^{n} r_{j i}}
$$</p><p>(4) Convergence: Repeat until no more change in the assignments.</p><h1 id=the-laplacian-and-the-rayleigh-quotient>The Laplacian and the Rayleigh quotient<a hidden class=anchor aria-hidden=true href=#the-laplacian-and-the-rayleigh-quotient>#</a></h1><p>As usual, for a graph $G=(V, E)$, let $A$ be its adjacency matrix and $D$ be the diagonal matrix with $D(v, v)=d_{v}$. Then, the random walk on $G$ will be taken according to the transition matrix $P=D^{-1} A$. We also define the stationary distribution $\pi$ with $\pi(x)=d_{x} / \operatorname{vol} G$.</p><p>Our discussion of random walks on $G$ left off with the result</p><p>$$
\left|\left|f P^{t}-\pi\right|\right|_{2} \leq \max_{i \neq 0}\left|\rho_{i}\right|^{t} \frac{\max_{x} \sqrt{d_{x}}}{\min_{y} \sqrt{d_{y}}}
$$</p><p>where $f$ is a probability distribution (i.e. $f \geq 0$ and $\sum_{x} f(x)=1$ ) and $1=\rho_{0} \geq \rho_{1} \geq \ldots \geq \rho_{n-1}$ are the eigenvalues of $P$. This inequality implies that convergence to the stationary distribution $\pi$ will follow if $\max \left\{\left|\rho_{1}\right|,\left|\rho_{n-1}\right|\right\}&lt;1$.</p><p>The transition probability matrix $P$ is similar to the matrix $M=D^{\frac{1}{2}} P D^{-\frac{1}{2}}$, so $P$ and $M$ have the same eigenvalues. We previously introduced the Laplacian of the graph as $\mathcal{L}=I-M$, so it has eigenvalues $0=\lambda_{0} \leq \lambda_{1} \leq \ldots \leq \lambda_{n-1}$ (where $\lambda_{i}=1-\rho_{i}$ ).</p><p>The main tool we&rsquo;ll use to study the spectrum of $\mathcal{L}$ is the Rayleigh quotient $R(f)$ of $\mathcal{L}$, defined (for our purposes) as</p><p>$$
R(f)=\frac{f L f^{*}}{f D f^{*}}
$$</p><p>where $L=D-A$ is the combinatorial Laplacian. This is the same as the usual sense of the Rayleigh quotient $g \mathcal{L} g^{*} / g g^{*}$ with the subtitution $f=g D^{-\frac{1}{2}}$. Following this equivalence, if the $\phi_{i}$ are the eigenvectors of $\mathcal{L}$, we&rsquo;ll call the $\psi_{i}=\phi_{i} D^{-\frac{1}{2}}$ the harmonic eigenvectors of $\mathcal{L}$.</p><p>Employing the Rayleigh quotient, we see that the eigenvalue $\lambda_{1}$ can be written as</p><p>$$
\lambda_{1}=\inf_{\substack{f \\ \sum_{x} f(x) d_{x}=0}} R(f) .
$$</p><p>Since the eigenvector associated with $\lambda_{0}$ is $\phi_{0}=1 D^{\frac{1}{2}}$, the condition $\sum_{x} f(x) d_{x}=0$ is an orthogonality condition. Such variational characterizations can also be made for the other eigenvalues:</p><p>$$
\lambda_{n-1}=\sup _{f} R(f)
$$</p><p>and, in general,
$$
\lambda_{i}=\sup_{h_{0}, h_{1}, \ldots, h_{i-1}}
\inf_{\substack{f: \\ \sum_{x} f(x) h_{j}(x) d_{x}=0 \\ \forall j \in{0, \ldots, i-1}}} R(f)
$$
The following characterization of the Rayleigh quotient (demonstrated last time) will be useful later:
$$
R(f)=\frac{\sum_{x \sim y}(f(x)-f(y))^{2}}{\sum_{x} f^{2}(x) d_{x}} .
$$</p><p>To this point, we have done a lot of linear algebra. We are not here to teach linear algebra; we are here to take linear algebra one step further to understand what is happening in the graph.</p><h1 id=the-cheeger-ratio-and-the-cheeger-constant>The Cheeger Ratio and The Cheeger Constant<a hidden class=anchor aria-hidden=true href=#the-cheeger-ratio-and-the-cheeger-constant>#</a></h1><p>In many areas of mathematics the questions of &ldquo;best&rdquo; comes into play. What is the best bound for a given constant? What is the best way of row reducing a certain matrix? In this section, we will describe a way to make the &ldquo;best possible cut&rdquo; of a graph $G=(V, E)$, where a cut may be either an edge-cut or a vertex-cut, and this cut will split $G$ into two disconnected pieces.</p><p>We would like a way to measure the quality of a cut that is made to $G$. That is, would it be better to cut 4 edges which cause us to lose 20 vertices, or is it better to cut 10 edges which would result in the removal of 120 vetices?</p><p>Suppose we are given a graph $G=(V, E)$ and a subset $S \subseteq V$. We wish to define the folling two sets:</p><p>$$
\partial S={{u, v} \mid u \in S, v \notin S}
$$</p><p>and</p><p>$$
\delta S={v \notin S \mid v \sim u, u \in S} .
$$</p><p>Definition 1 For any vertex set $W$, the volume of $W$ is given by</p><p>$$
\operatorname{vol}(W)=\sum_{x W} d_{x},
$$</p><p>where $d_{x}$ is the degree of $\mathrm{x}$ in $W$.</p><p>Definition 2 The Cheerger Ratio for a vertex set $S$ is</p><p>$$
h(S)=\frac{|\partial S|}{\min {\operatorname{vol}(S), \operatorname{vol}(\bar{S})}},
$$</p><p>where $\bar{S}=V-S$.</p><p>It is first worth noting that in terms of this defintion of the Cheeger ratio, we are gauging the quality of our cut by taking a measure of what&rsquo;s been cut off of $G$. There are other forms of the Cheeger ratio as well. For example, we can use $|\delta S|$ instead of $|\partial S|,|S|($ or $\bar{S})$ instead of $\operatorname{vol}(S)$ (or $\operatorname{vol}(\bar{S}))$, or $|S||\bar{S}|$ instead of $\min {\operatorname{vol}(S), \operatorname{vol}(\bar{S})}$.</p><p>Definition 3 For any graph $G=(V, E)$, the Cheeger Constant of $G$ is given by</p><p>$$
h_{G}=\min_S h(S) .
$$</p><p>Now, if we consider the case where $\operatorname{vol}(S) \leq \frac{1}{2} \operatorname{vol}(G)$, then we can see that</p><p>$$
|\partial S| \geq h_{G}(\operatorname{vol}(S)) .
$$</p><h1 id=the-cheeger-inequality>The Cheeger Inequality<a hidden class=anchor aria-hidden=true href=#the-cheeger-inequality>#</a></h1><p>Given a graph $G$, we can define $\lambda_{1}$ to be the first nontrivial eignevalue of the Laplacian, $\mathcal{L}$, of $G$.</p><p>For any graph $G$,</p><p>$$
2 h_{G} \geq \lambda_{1} \geq \frac{h_{G}^{2}}{2}
$$</p><h1 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h1><p><a href=https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf>https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf</a></p><p><a href=https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p><p><a href=https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf>https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf</a></p><p><a href=https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf>https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://JhuoW.github.io/tags/laplacian/>Laplacian</a></li></ul><nav class=paginav><a class=prev href=https://JhuoW.github.io/posts/pgd/><span class=title>« Prev Page</span><br><span>Proximal Gradient Descent</span></a>
<a class=next href=https://JhuoW.github.io/posts/2022-04-02-tinl/><span class=title>Next Page »</span><br><span>NeurIPS2021 《Topology-Imbalance Learning for Semi-Supervised Node Classification》 Reading Notes</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Everything about Graph Laplacian on twitter" href="https://twitter.com/intent/tweet/?text=Everything%20about%20Graph%20Laplacian&url=https%3a%2f%2fJhuoW.github.io%2fposts%2flaplacian%2f&hashtags=Laplacian"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Everything about Graph Laplacian on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fJhuoW.github.io%2fposts%2flaplacian%2f&title=Everything%20about%20Graph%20Laplacian&summary=Everything%20about%20Graph%20Laplacian&source=https%3a%2f%2fJhuoW.github.io%2fposts%2flaplacian%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Everything about Graph Laplacian on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fJhuoW.github.io%2fposts%2flaplacian%2f&title=Everything%20about%20Graph%20Laplacian"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Everything about Graph Laplacian on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fJhuoW.github.io%2fposts%2flaplacian%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Everything about Graph Laplacian on whatsapp" href="https://api.whatsapp.com/send?text=Everything%20about%20Graph%20Laplacian%20-%20https%3a%2f%2fJhuoW.github.io%2fposts%2flaplacian%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Everything about Graph Laplacian on telegram" href="https://telegram.me/share/url?text=Everything%20about%20Graph%20Laplacian&url=https%3a%2f%2fJhuoW.github.io%2fposts%2flaplacian%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://giscus.app/client.js data-repo=JhuoW/WebComments data-repo-id=R_kgDOHHz8Ug data-category=Announcements data-category-id=DIC_kwDOHHz8Us4COa5e data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>Copyright &copy; 2022 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span></span><br><script>function siteTime(){var u=1e3,r=u*60,a=r*60,n=a*24,x=n*365,e=new Date,d=2019,O=1,w=16,_=19,y=15,m=11,l=e.getFullYear(),C=e.getMonth()+1,f=e.getDate(),p=e.getHours(),g=e.getMinutes(),v=e.getSeconds(),b=Date.UTC(d,O,w,_,y,m),j=Date.UTC(l,C,f,p,g,v),s=j-b,o=Math.floor(s/x),t=Math.floor(s/n-o*365),i=Math.floor((s-(o*365+t)*n)/a),c=Math.floor((s-(o*365+t)*n-i*a)/r),h=Math.floor((s-(o*365+t)*n-i*a-c*r)/u);d==l?document.getElementById("sitetime").innerHTML="本站已运行 "+t+" 天 "+i+" 小时 "+c+" 分钟 "+h+" 秒":document.getElementById("sitetime").innerHTML="本站已运行 "+o+" 年 "+t+" 天 "+i+" 小时 "+c+" 分钟 "+h+" 秒"}setInterval(siteTime,1e3)</script><span id=sitetime>载入运行时间...</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>