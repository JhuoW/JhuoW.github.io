<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GNN on JhuoW‘s Notes</title>
    <link>https://JhuoW.github.io/tags/gnn/</link>
    <description>Recent content in GNN on JhuoW‘s Notes</description>
    <image>
      <url>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Jul 2025 15:19:07 +0800</lastBuildDate><atom:link href="https://JhuoW.github.io/tags/gnn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NeurIPS2023《Prodigy：Enabling In-context Learning Over Graphs》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/prodigy-icl/</link>
      <pubDate>Mon, 28 Jul 2025 15:19:07 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/prodigy-icl/</guid>
      <description>NeurIPS2023 &amp;#34;Prodigy：Enabling In-context Learning Over Graphs&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>在llm里面，In-Context Learning就是用一些新任务的问答例子来提示LLM，使得LLM不需要更新参数，也能基于这些例子来对新的任务生成输出。</p>
<p><img loading="lazy" src="/posts/2025-07-28-prodigy/1.png#center" alt="icl"  />
</p>
<p>为LM的预训练任务实际上是next token prediction的任务，由于prompt和回答只是相邻的tokens，因此 Transformer 的自注意力可以计算要补全的句子和之前token之间的关系，从而让模型动态推断出新的决策规则。In-Context Learning实际上也是一种next token prediction的任务，所以In-Context Learning是和预训练任务相关的。</p>
<p><img loading="lazy" src="/posts/2025-07-28-prodigy/2.png#center" alt="icl"  />
</p>
<p>Prodigy和All in One (KDD 2023)是同期工作，目的都是为了去提示Pretrained graph model。 All in One 是已经有一个Pretrained graph model，然后用meta learning基于冻结的pretrained model，去学一个prompt graph，使得这个prompt graph和原图结合起来之后可以使预训练模型泛化到新的任务上。</p>
<p>All in One 存在的问题是，All in one要对测试图做finetune。但实际上我们知道foundation model是不需要fine tune的。遇到一个新任务，因为支持in-context learning，可以基于给出的prompt example 来对新任务直接做出预测。而Prodigy就是不需要fine tune的graph prompt方法，它是直接以in-context learning的方式来预训练模型。Prodigy在预训练图上构造一堆prompt example和queries，预训练的优化目标是使在给定prompt examples和query set的情况下，query set的预测损失最小，也就是说<strong>模型的参数由prompt examples来调整，优化目标是使得query set的损失最小</strong>。</p>
<p><img loading="lazy" src="/posts/2025-07-28-prodigy/3.png#center" alt="icl"  />
</p>
<h2 id="prompt-graph-representation">Prompt Graph Representation</h2>
<p>首先要基于预训练的图构造一个支持in-context learning的prompt graph。然后在这个prompt graph上以in-context learning的方式来预训练模型。具体来说，在预训练图上构造一个 $m$-way $k$-shot的分类任务。预训练图为MAG240M，该图中有122M个节点，1.3B条边，153个类。 在每次训练epoch中，采样 $m=30$个class，每个class选择 $k=3$个样本作为prompt examples $\mathcal{S}$，也就是每个epoch有90个prompt examples。每个类别取4个样本作为query set $\mathcal{Q}$。也就是 $\mathcal{S} = \{(x_i, y_i)\}^{mk}_{i = 1}$ 作为预训练阶段的support set。 $\mathcal{Q} = \{x_i\}^n_{i = 1}$作为预训练阶段的query set，也就是预训练阶段的优化目标是要在query set上的损失最小。</p>
<p><strong>Data Graphs</strong>：若预训练任务是node-level的任务，那么每个数据点集 $\mathcal{V}_i$是一个目标节点，如果是edge level的任务，那么每个数据点集 $\mathcal{V}_i$是一对节点。然后，通过采样每个数据点集的 $k$跳局部子图来contextualize每个数据点集，从而为所有prompt examples和query set构造Data Graphs：
$$
\mathcal{G}_i^{\mathrm{D}}=\left(\mathcal{V}_i^{\mathrm{D}}, \mathcal{E}_i^{\mathrm{D}}\right) \sim \bigoplus_{i=0}^k \operatorname{Neighbor}\left(\mathcal{V}_i, \mathcal{G}, i\right)
$$</p>
<p><strong>Message Passing on Data Graphs</strong>：然后每个Data Graph有一个Super Node $v_{x_i}$，来得到上下文化的目标节点表示。具体来说，在Data Graph上做message，然后把目标节点的embedding拿出来作为super node的node feature。</p>
<p><strong>Task Graph</strong>：在每次预训练迭代中，有30个class参与预训练，那么就有30个label nodes $v_y$。然后将所有super nodes和label nodes 连接起来，用边来反映数据点和label之间的关系，具体来说，每条边的特征有2个binary value构成，如果目标数据点是来自prompt set的节点，那么第一个属性为 $0$，如果是来自query set的数据点，那么第一个属性为 $1$。在task graph中，每个超节点和所有label node连接，如果prompt example的超节点连接到正确的label node，那么第二个属性为 $1$，如果连到错误的label，那么边的第二个属性为 $-1$。</p>
<p><img loading="lazy" src="/posts/2025-07-28-prodigy/4.png#center" alt="icl"  />
</p>
<p><strong>Message Passing on Task Graph</strong>：在得到每条边的feature $e_{ij}$后，在Task Graph上做attention-based message passing。也就是每个label node要聚合所有super nodes，每个super nodes也要聚合所有label nodes。
$$
\begin{aligned}\beta_{i j} &amp; =M L P\left(W_q^T H_i^l\left|\left|W_k^T H_j^l\right|\right| e_{i j}\right) \\ \alpha_{i j} &amp; =\frac{\exp \left(\beta_{i j}\right)}{\sum_{k \in \mathcal{N}(i) \cup{i}} \exp \left(\beta_{i k}\right)} \\ H_i^{l+1} &amp; =\operatorname{ReLU}\left(B N\left(H_i^l+W_o^T \sum_{j \in \mathcal{N}(i) \cup{i}} \alpha_{i j} W_v^T H_j^l\right)\right)\end{aligned}
$$</p>
<h2 id="pretraining-task-generation">Pretraining Task Generation</h2>
<p>2个预训练任务：<strong>Neighbor Matching</strong>和<strong>Multi-Class Classification</strong>。</p>
<p><strong>Neighbor Matching</strong>：在每次training epoch，随机采样 $m=30$个节点作为 $30$个way，然后每个采样的节点选择 $k=3$个邻域内节点作为这个类的prompt examples。然后所有label nodes的特征从Uniform distribution上初始化：</p>
<p>$$
\begin{gathered}\mathcal{C}=\left\{c_i\right\}_{i=1}^m \quad c_i \sim \operatorname{Uniform}\left(\mathcal{V}_{\text {pretrain }}\right) \\ N_i=\text { Neighbor }\left(c_i, \mathcal{G}_{\text {pretrain }}, l\right) \\ \mathcal{S}_i=\left\{\left(x_j, y_j=c_i\right)\right\}_{j=1}^k \quad x_j \sim \operatorname{Uniform}\left(N_i\right) \\ \mathcal{Q}_i=\left\{\left(x_j, y_j=c_i\right)\right\}_{j=1}^{\left\lceil\frac{n}{m}\right\rceil} \quad x_j \sim \operatorname{Uniform}\left(N_i\right)\end{gathered}
$$</p>
<p><strong>Multi-Class Classification</strong>：用原始label来构造预训练任务，label node的feature依旧从uniform distribution中采样：</p>
<p>$$
\begin{gathered}\mathcal{C}=\left\{c_i\right\}_{i=1}^m \quad c_i \sim \operatorname{Uniform}(\mathcal{Y}) \\ \mathcal{S}_i=\left\{\left(x_j, y_j=c_i\right)\right\}_{j=1}^k \quad x_j \sim \operatorname{Uniform}\left(\left\{x_i \mid f\left(x_i\right)=c_i\right\}\right) \\ \mathcal{Q}_i=\left\{\left(x_j, y_j=c_i\right)\right\}_{j=1}^{\left\lceil\frac{n}{m}\right\rceil} \quad x_j \sim \operatorname{Uniform}\left(\left\{x_i \mid f\left(x_i\right)=c_i\right\}\right)\end{gathered}
$$</p>
<p>通过计算每个super node和所有label node的cosine similarity 来得到预测的logits：</p>
<p>$$
O_i=\left[\operatorname{cosine} \_ \text {similarity }\left(H_{x_i}, H_y\right), \forall y \in \mathcal{Y}\right]
$$</p>
<p>模型的优化目标是在NM和MT这两个预训练任务的query set上的损失最小：</p>
<p>$$
\mathcal{L}=\underset{x_i \in \mathcal{Q}_{\mathrm{NM}}}{\mathbb{E}} \operatorname{CE}\left(O_{\mathrm{NM}, i}, y_{\mathrm{NM}, i}\right)+\underset{x_i \in \mathcal{Q}_{\mathrm{MT}}}{\mathbb{E}} \operatorname{CE}\left(O_{\mathrm{MT}, i}, y_{\mathrm{MT}, i}\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2024《LLaGA：Large Language and Graph Assistant》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/llaga/</link>
      <pubDate>Mon, 28 Jul 2025 15:09:11 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/llaga/</guid>
      <description>ICML2024 &amp;#34;LLaGA：Large Language and Graph Assistant&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>目的：将图结构调整为适配LLM的输入。具体来说，将图节点表示为<strong>图结构感知</strong>和<strong>属性感知</strong>的序列，然后将序列映射到token embedding space中，从而使LLM可以用处理text tokens的方式来对Graph token embeddings进行处理。为了实现这个目标，就要求node sequence必须充分保存中心节点的结构信息。</p>
<h2 id="structure-aware-graph-translation">Structure-Aware Graph Translation</h2>
<p>LLaGA的目的是将graph翻译成可被LLM理解的token embedding sequence的形式。同时，这可以利用LLM固有的推理能力来处理图的任务，并且无需改变LLM的参数（foundation model）。为了实现这个目标，LLaGA将图结构转化为node embedding sequences，这些sequences融合了图的局部和更大范围的结构信息，然后将node embedding sequences通过一个projector转化为LLM可以处理的token embedding sequence。</p>
<p>第一步是将图转换为node embedding sequences。由于图分析的基本单位是节点，所以本工作开发了2个节点级templates，这些templates是多功能的，不仅可以用于节点级任务，也可以用于边级任务。分别是一个<strong>Neighborhood Detail Template</strong>，提供对中心节点及其周围环境的深入观察；<strong>Hop-Field Overview Template</strong>，提供了一个节点邻居的总结视角，可以拓展到更大的域。<strong>这两个模板都旨在编码节点周围的结构信息</strong>，为分析提供不同的视角。</p>
<h3 id="neighborhood-detail-template"><strong>Neighborhood Detail Template</strong></h3>
<p>Neighborhood Detail Template用于描述节点和其周围邻居的详细信息。给定一个中心节点 $v$，需要构造一个形状固定的tree。对于中心节点 $v$的每跳邻居，定义一组邻居采样的size： $n_1$, $n_2$, …，其中 $n_i$表示第 $i$跳邻居的采样数量。对于 $1$-hop邻居集合 $\mathcal{N}_v^1$，从其中随机采样 $n_1$个邻居，表示为 $\widetilde{\mathcal{N}}_v^1$。如果 $\left|\mathcal{N}_v^1\right|&lt;n_1$，那么用placeholder nodes来补全 $n_1$个邻居。注意，这里定义的每跳邻居的采样数量 $n_1, n_2, \cdots$是应用于所有节点的。得到的 $v$-centered tree如下图所示：</p>
<p><img loading="lazy" src="/posts/2025-07-28-llaga/0.png#center" alt="icl"  />
</p>
<p>这里定义 $n_2 = 3$，如果2跳邻居不满3个节点，那么用 placeholder node  $[\text{pad}]$来占位。然后我们从tree的中心节点开始遍历，可以把这棵计算树转换为一个固定长度的node sequence。这个node sequence描述了以 $A$为中心的局部邻域内的节点相对结构位置（从近到远）。</p>
<p>上面的步骤将中心节点和它的结构信息编码成一个节点序列，然后我们需要把这个节点序列映射到embedding space中。对于TAG（Text-Attributed Graph），使用现成的语言模型 $\phi$，例如SBERT，RoBERTa或SimTeG来编码文本信息，placeholder node的特征被编码为 $0$向量。然后进一步融合结构信息，每个节点在tree中的结构信息用Laplacian Embedding来表示，也就是拉普拉斯矩阵的特征向量在node id的位置来表示这个node在Tree中的结构信息，也就是相邻的节点有相似的embedding：</p>
<p>$$
L=I-\mathcal{D}^{-\frac{1}{2}} \mathcal{A}_{\text {tree }} \mathcal{D}^{-\frac{1}{2}}=U^T \Lambda U
$$</p>
<p>其中 $U$的每行表示对应节点的Laplacian Embedding。注意到，我们预训练的时候，只需要计算一次Laplacian Embedding，以为对于任何图中的任何节点，都为它构造一样的computational tree，所以Laplacian Embedding是不变的。对于计算树induced node sequence $v_1, v_2, \cdots, v_n$，那么这个计算树中的节点 $v_i$的最终node embedding表示为：</p>
<p>$$
h_{v_i}= \begin{cases}\mathbf{0} || U_i, &amp; \text { if } v_i=[p a d] ; \ \phi\left(x_{v_i}\right) || U_i, &amp; \text { otherwise }\end{cases}
$$</p>
<p>它结合了文本信息和节点与领域内其他节点的相对位置信息。</p>
<h3 id="hop-field-overview-template">Hop-Field Overview Template</h3>
<p><img loading="lazy" src="/posts/2025-07-28-llaga/1.png#center" alt="icl"  />
</p>
<p>Hop-Field Overview Template提供的是一个关于中心节点和其邻居的总结视角。**Neighborhood Detail Template中sequence的每个元素是一个节点，Hop-Field Overview Template中每个元素是一跳节点的总结。**首先，用LM来初始话每个节点的特征，节点 $v$的初始特征为 $h_x^0=\phi\left(x_v\right)$。定义parameter-free message passing on encoded text features：</p>
<p>$$
h_v^i=\frac{1}{\left|\mathcal{N}_v^1\right|} \sum_{v^{\prime} \in \mathcal{N}_v^1} h_{v^{\prime}}^{i-1}
$$</p>
<p>$h_v^1=\frac{1}{\left|\mathcal{N}_v^1\right|} \sum_{v^{\prime} \in \mathcal{N}_v^1} h_{v^{\prime}}^{0}$表示节点 $v$的 $1$-hop邻居的summarized embedding，即直接聚合邻居的原始特征。 $h_v^i$表示聚合邻居的 $i-1$阶特征，也就是 $v$的 $i$hop以内邻居的summarized embedding。那么可以依据中心节点和hop顺序关系构造一个序列 $h_v^1, h_v^2,\cdots$，用来表示中心节点和它的相对结构信息。这个方法牺牲了更多细节，但是保留了更广的感受野。</p>
<h3 id="mapping-node-embeddings-into-llms-token-space">Mapping Node Embeddings into LLM’s token space</h3>
<p>用于对齐节点的embedding space和LLM的token space，使得节点序列可以作为LLM的输入。定义一个可训练的projector $f_\theta$：</p>
<p>$$
e_i=f_\theta\left(h_i\right)
$$</p>
<p>用来将序列中的每个节点embedding映射到token space。然后，node embedding sequence $h_1, h_2,\cdots, h_n$可以被转换成token embeddings $e_2, e_2,\cdots, e_n$。LLaGA中唯一的训练参数就是这个projector $f_\theta$。</p>
<h3 id="alignment-tuning">Alignment Tuning</h3>
<p><img loading="lazy" src="/posts/2025-07-28-llaga/2.png#center" alt="icl"  />
</p>
<p>LLaGA采用3个预训练任务：节点分类，链路预测和<strong>节点描述（Node description）。<strong>其中，Node description任务用于将node embedding和特定文本描述对齐。这个特殊任务能够提供图的丰富语义解释，从而更深入地了解基于图的预测背后的逻辑。将这些预训练projector的任务统一成</strong>Questions and Answers</strong>的形式：</p>
<p>Questions: Please describe the center node: <!-- raw HTML omitted -->.</p>
<p>Answers: The center node represents a [paper / products /&hellip;], it’s about [node description].</p>
<p>在训练过程中，以chat的形式在组织问答，Vicuna-v1.5 (Chiang et al., 2023) 作为LLaGA的LLM模型。如上图step 2所示，在处理Freezed LLM的输入阶段，将SYSTEM MESSAGE，USER: [Give you a node] 都tokenize，然后将node sequence 替换为projected node embedding $e_1, e_2, \cdots$，然后将后面描述任务的prompt也tokenize，训练的优化目标是最大化生成正确答案的概率：</p>
<p>$$
\underset{\theta}{\operatorname{maximize}} \quad p\left(X_{\text {answer }} \mid X_{\text {graph }}, X_{\text {question }}, X_{\text {system }}\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2024《GFT：Graph Foundation Model with Transferable Tree Vocabulary》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gft/</link>
      <pubDate>Mon, 28 Jul 2025 14:49:00 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gft/</guid>
      <description>NeurIPS2024 &amp;#34;GFT：Graph Foundation Model with Transferable Tree Vocabulary&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>本文的目的是在预训练图上训练一组通用的图词汇表（vocabulary）作为图上可迁移的通用pattern，这些词汇表可应用于跨领域的多任务中。</p>
<h2 id="gnn的可迁移性">GNN的可迁移性</h2>
<p>可迁移性指的是模型从source task中提取模式，这些模式的知识可以增强对目标任务的预测。近期的工作通过识别与各种任务相关的关键子结构，来研究图上可迁移的词汇表。例如，三元闭包（triadic closure），同质性，异质性对于节点分类至关重要；一些motif，比如三角形，k-cliques，和star，是图分类的基本组成成分，这些子结构可以作为通用可迁移的pattern，基于这些子结构在不同图中的形式，或者研究图具有怎样的子结构组成，可以推断图在各种任务上的表现。</p>
<p>拿自然语言来类比，positive sub-vocabulary 可以是 [happy, nice, great, … ]；negative sub-vocabulary 可以是[upset, bad, worse, … ]，对于一个句子，只需要用它来检索这个词汇表，就可以对这个句子是积极或者消极做出分类。</p>
<p>对于Graph也是一样，构建一些通用的子结构，下游graph去检索这些子结构，基于子结构的性质来对图的性质做出推理。</p>
<h2 id="计算树computation-tree作为可迁移的graph-pattern">计算树（computation tree）作为可迁移的graph pattern</h2>
<p>Computation Tree：对于一个图 $\mathcal{G}(\mathcal{V}, \mathcal{E})$，关于节点 $v$，它的 $L$ 层计算树为 $\mathcal{T}_v^L$，且 $\mathcal{T}_v^1 = v$。这个tree通过递归融合邻居的子树得到。对于图 $\mathcal{G}$，它的 $L$层子树的集合可以表示为一个multiset： $\mathcal{T}_{\mathcal{G}}^L:=\left\{\mathcal{T}_v^L\right\}_{v \in \mathcal{V}}$ 。</p>
<h3 id="为什么计算树比其他子结构如motif更适合做通用的vocabulary">为什么计算树比其他子结构（如motif）更适合做通用的vocabulary？</h3>
<p>首先，节点级、边级、图级任务都可以表示为计算树的分类任务。如下图所示：</p>
<p><img loading="lazy" src="/posts/2025-07-28-GFT/0.png#center" alt="icl"  />
</p>
<p>计算树可以捕获图中重要的局部子树模式。如果两个节点的 $L$层计算树相似（同时考虑节点和特征），那么表明这两个节点共享相似的邻域信息，则这两个节点具有类似现象（analogous phenomena）。</p>
<p><strong>两个具有相似计算树的图，是否以为这模型在这两个图上的迁移性更好？（Yes）</strong></p>
<p>两个图具有相似的其他子结构（如motif），并<strong>不一定</strong>意味着两个图之间模型的迁移性更好</p>
<p>Theorem 2.2：</p>
<p>$$
\Delta \triangleq\left|\left|\phi\left(\mathcal{T}_{v_1}\right)-\phi\left(\mathcal{T}_{v_2}\right)\right|\right|_2 \leq \mathcal{C}_1 \left| \left| \mathbf{x}_{v_1}-\mathbf{x}_{v_2}\right|\right|_2+\mathcal{C}_2 \sum_{j \in \mathcal{N}(v)} \Delta_{v_1, v_2, j}^{L-1} \leq 2 \mathcal{B}_{\mathbf{x}}\left(\mathcal{C}_1+\sum_{l=1}^L \mathcal{C}_2^l D_l\right)
$$</p>
<p>上面定理表示，一个GNN模型 $\phi$对两个计算树计算embedding的相似度，一定会被两个计算树的子树相似度bound，也就是说，<strong>如果两个计算树相似，那么GNN一定可以为他们计算相似的embedding，从而做出相似度预测，所以GNN在这两个子树上具有较好的可迁移性</strong>。如下图所示，motif的相似度并不意味着更高的准确率，99.01的motif相似度，实际上的准确率很低，但计算树的相似度可以反映相似度，所以可以作为图上可迁移的vocabulary，下游图可以检索vocabulary计算树来推测性质。</p>
<p><img loading="lazy" src="/posts/2025-07-28-GFT/1.png#center" alt="icl"  />
</p>
<h2 id="pre-training-with-computation-tree-reconstruction">Pre-training with Computation Tree Reconstruction</h2>
<p>GFT采用Vector Quantization (VQ, 参考VQ-VAE)来开发计算树词汇表。对于一个图数据库 $\mathcal{D} = \{\mathcal{G}_i\}^n_{i = 1}$，从中提取一个集合的计算树 $\mathcal{T}=\left\{\mathcal{T}_i\right\}_{i=1}^m$。通过GNN $\phi$可以得到 $m$个computation tree的embedding： $\mathcal{Z}=\left\{\mathbf{z}_i\right\}_{i=1}^m$。然后定义一组codebooks为可学习的tokens $\mathbf{C}=\left\{\mathbf{c}_1, \ldots, \mathbf{c}_K\right\}$，那么tree embedding space被量化为将每个tree embedding分配给最近的codebook token（类似于k-means，但没有明确的k）。那么对于tree embedding $\mathbf{z}_i$，它所属的codebook token 为 $\mathbf{q}_i=\mathbf{c}_j$，其中 $j=\arg \min _j\left|\left|\mathbf{z}_i-\mathbf{c}_j\right|\right|_2 .$ 那么对于 $m$个数，可以将他们分配給 $K$个codebook tokens 作为长度为 $K$的词汇表。通过vocabulary loss和commitment loss来优化tree embedding和codebook tokens，可以得到预训练目标函数：</p>
<p>$$
\mathcal{L}_{\text {pretrain }}=\mathcal{L}_{\text {tree }}+\underbrace{\frac{1}{m} \sum_{i=1}^m\left|\left|\operatorname{sg}\left[\mathbf{z}_i\right]-\mathbf{c}_i\right|\right|_2^2}_{\text {vocabulary loss }}+\beta_1 \cdot \underbrace{\frac{1}{m} \sum_{i=1}^m\left|\left|\mathbf{z}_i-\operatorname{sg}\left[\mathbf{c}_i\right]\right|\right|_2^2}_{\text {commitment loss }}
$$</p>
<p>先看后面两项，目的是可微地将 $m$个tree embedding压缩为 $K$个向量， $\mathrm{sg}[\mathbf{z}_i]$是stop-gradient的意思，即前向传播得到 $\mathbf{z}_i$后，block $\mathbf{z}_i$回传的梯度，也就是把 $\mathrm{sg}[\mathbf{z}_i]$视为constant来优化 $\mathbf{c}_j$。commitment loss同理。</p>
<p><strong>Computation Tree Reconstruction.</strong> 除了vocabulary loss和commitment loss联合训练codebooks和tree embedding外，本文还引入了一个树重建任务使得学习到的词汇表可以深入理解计算树的结构和语义属性。</p>
<ol>
<li>对于每个计算树 $\mathcal{T}_i$所属的token $\mathbf{q}_i \in \mathbf{C}$，它应当具备重构计算树root node 特征 $\mathbf{x}_i$的能力（codebook token要充分学习它包含的计算树的结构和属性信息）：</li>
</ol>
<p>$$
\mathcal{L}_{\text {feat }}=\frac{1}{m} \sum_{i=1}^m\left|\left|\hat{\mathbf{q}_i^2}-\mathbf{x}_i\right|\right|_2^2
$$</p>
<ol>
<li>计算树token $\mathbf{q}_i$ 要能够重构树 $\mathcal{T}_i$的总体语义信息 $\mathbf{z}_i$：</li>
</ol>
<p>$$
\mathcal{L}_{s e m}=\frac{1}{m} \sum_{i=1}^m\left(1-\frac{\hat{\mathbf{q}}_i^{1^T} \hat{\mathbf{z}_i}}{\left|\left|\hat{\mathbf{q}_i^1}\right|\right|\left|\left|\hat{\mathbf{z}}_i\right|\right|}\right)^\gamma
$$</p>
<ol>
<li>对于有边连接的节点，他们的两个计算树 $\mathcal{T}_i$ 和 $\mathcal{T}_j$所属的codebook token $\mathbf{q}_i$和 $\mathbf{q}_j$要相似：</li>
</ol>
<p>$$
\mathcal{L}_{\text {topo }}=\sum_{(i, j) \in \mathcal{E},\left(i, j^{\prime}\right) \in \hat{\mathcal{E}}}-\frac{1}{|\mathcal{E}|} \log \left(\sigma\left(\hat{\mathbf{q}}_i^{\mathbf{3}^T} \hat{\mathbf{q}}_j^3\right)\right)-\frac{1}{|\hat{\mathcal{E}}|} \log \left(1-\sigma\left(\hat{\mathbf{q}}_i^{\mathbf{3}^T} \hat{\mathbf{q}}_{j^{\prime}}^3\right)\right)+\frac{1}{|\mathcal{E}|}\left|\left|\left[\mathbf{q}_i^4 || \mathbf{q}_j^4\right]-\mathbf{e}_{i j}\right|\right|_2^2
$$</p>
<p>所以预训练loss $\mathcal{L}_{\text {pretrain }}$中的 $\mathcal{L}_{\text {tree }}$定义为如下形式：</p>
<p>$$
\mathcal{L}_{\text {tree }}=\beta_2 \cdot \mathcal{L}_{\text {feat }}+\beta_3 \cdot \mathcal{L}_{\text {sem }}+\beta_4 \cdot \mathcal{L}_{\text {topo }}
$$</p>
<p>为了增强tree vocabulary的质量，不同的词汇应具备一定的区分性，所以regularize the tree vocabulary space by intentionally <strong>increasing the distance between distinct tokens</strong>：</p>
<p>$$
\mathcal{L}_{\text {ortho }}=\lambda \frac{1}{K^2}\left|\left|\mathbf{C C}^T-\mathbf{I}_K\right|\right|_F^2, \quad \mathbf{C}=\left[\mathbf{c}_1, \ldots, \mathbf{c}_K\right]^T \in \mathbb{R}^{K \times d^{\prime}}
$$</p>
<h2 id="fine-tuning-with-computation-tree-classification">Fine-tuning with Computation Tree Classification</h2>
<p>预训练阶段将通用知识编码到Tree vocabulary中，Fine-tuning阶段将这些知识应用于特定的预测任务。首先将节点级、边级、图级任务全部统一成tree-level task。</p>
<ul>
<li>节点分类：任务特定的树表示为 $\mathcal{T}_{node}  =\mathcal{T}_i$，embedding为 $\mathbf{z}=\phi\left(\mathcal{T}_i\right)$。</li>
<li>链接预测：任务特定的树表示为 $\mathcal{T}_{\text {link }}=\operatorname{Combine}\left(\mathcal{T}_s, \mathcal{T}_t\right)$， embedding为 $\mathbf{z}=\operatorname{mean}\left(\phi\left(\mathcal{T}_s\right), \phi\left(\mathcal{T}_t\right)\right)$。</li>
<li>图级分类：任务特定的树表示为 $\mathbf{z}=\operatorname{mean}\left(\phi\left(\mathcal{T}_s\right), \phi\left(\mathcal{T}_t\right)\right)$。</li>
</ul>
<p>然后，任务特定的计算树通过查询tree vocabulary来做出预测，从而将词汇表中的通用知识用于各种任务和领域。</p>
<p><strong>Prototype Classifier</strong>. 给定一组任务特定的计算树 $\left\{\left(\mathcal{T}_i, y_i\right)\right\}_{i=1}^n$包含 $|C|$个类别，需要预测这个计算树的类别。对于一个预训练好的GNN $\phi$，可以为这些计算树生成embeddings $\mathcal{Z}=\left\{\mathbf{z}_i\right\}_{i=1}^n$。这些embedding用于查询tree vocabulary 然后可以得到每个计算树所属的codebook token $\mathcal{Q}=\left\{\mathbf{q}_i\right\}_{i=1}^n$。然后，对于每个类别有一定数量的计算树用于训练，对于训练计算树，构造一个memory bank $\mathbb{S}=\left\{\mathbb{S}^1, \ldots, \mathbb{S}^{|C|}\right\}$用来保存构构成每个类别的codebook token，其中 $\mathbb{S}^k = \{\mathbf{q}_i \in \mathcal{Q} | y_i = k\}$，表示那些用于构成类别 $k$的codebook tokens。那么，class prototype就是用于表示这些类的tokens的结合：
$$
\mathbf{p}_k=\frac{1}{ \left|\mathbb{S}^k\right|} \sum_{\mathbf{q}_i \in \mathbb{S}^k} \mathbf{q}_i
$$</p>
<p>这些类prototype可以用于对测试计算树的预测。若测试计算树的GNN embedding为 $\mathbf{z}$，那么它的第 $k$个类别的预测结果如下：</p>
<p>$$
p(y=k \mid \mathbf{z})=\frac{\exp \left(-\operatorname{sim}\left(\mathbf{z}, \mathbf{p}_k\right) / \tau\right)}{\sum_c \exp \left(-\operatorname{sim}\left(\mathbf{z}, \mathbf{p}_c\right) / \tau\right)}
$$</p>
<p>通过上面的概率在下游图的训练集上fine-tuning模型。为什么要fine tuning，因为对于不同的图，同一个计算树可能的作用也是不一样的，所以要通过这种方式来让计算树的embedding变得task specific，使得每个类别有最适合它的计算树。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2025《GOFA：A Generative One-For-All Model for Joint Graph Language Modeling》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gofa/</link>
      <pubDate>Mon, 28 Jul 2025 14:35:16 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gofa/</guid>
      <description>ICLR2025 &amp;#34;GOFA：A Generative One-For-All Model for Joint Graph Language Modeling&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>GOFA是OFA（ICLR 2024）的后续工作，用来解决OFA中存在的一些问题。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/6.png#center" alt="icl"  />
</p>
<p>OFA用NOI Prompt Node来统一不同的分类任务，也就是用不同的NOI Prompt Node来表示不同的分类任务，然后用任务的文本描述作为这些表示任务的节点的文本特征，LLM 编码来数值化这个节点的初始特征，任务对应的label也用节点（Class Node）来表示，然后用每个class的文本描述作为这个Class Node的特征。GNN的训练目标就是训练NOI Prompt node的embedding 和class node的embedding，对于某个节点，如果我们要预测它关于某个任务的标签，只需要把它和这个任务对应的NOI Prompt Node连接起来，根据输出就可以知道这个节点在该任务下的标签。</p>
<p>但是，OFA存在以下问题：必须是已知任务，无法泛化到新的任务，因为新的任务NOI Prompt Node 未知，也不知道新任务的Class Node是什么， 因此下游任务必须得是预训练阶段见过的任务，不能是一个全新的任务。此外，可以看出由于OFA是一个supervised foundation model，所以他和参与训练的任务标签是强相关的，很难泛化到训练阶段没有见过的任务，GOFA就是用来解决OFA存在的问题，GOFA认为foundation model的训练过程应该不能有标签信息加入的，也就是需要是自监督的。</p>
<h2 id="unified-task-formats">Unified Task Formats</h2>
<p>模型的输入必须要统一，所以和过去的很多方法一样，GOFA用TAG来统一所有graph：</p>
<p>$$
G=\left\{V, E, X_V, X_E\right\}
$$</p>
<p>自监督语言模型里面，模型训练的目标叫next-token prediction 通常是补全句子，比如给定一个句子，自监督语言模型的训练目标是基于这个句子来预测生下来的tokens，假设剩下来的token是apple，参数优化的目标就是使生成apple的likelihood要最大， 如下图所示：</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/1.png#center" alt="icl"  />
</p>
<p>GOFA提出的自监督图模型，也继承这种做法，模型学习的目标是补全TAG图，有个目标节点，叫node of generation，也就是自监督学习要补全的节点，把它的文本属性截断，然后自监督学习的目标就是要生成这个NOG node 的剩下内容。但是，如果要生成这个节点剩下的内容的话，不能无视这种图中的边信息，因为图的结构可以帮助这个自监督任务生成正确的信息，所以自监督图模型需要充分理解图结构。比如上图中的Target就需要在理解边关系的情况下补全句子，因此，模型需要充分学习图结构。</p>
<h2 id="gofa-generative-one-for-all-model">GOFA: Generative One-For-All Model</h2>
<p>怎么才能在充分学习图结构的情况下，补全目标节点的text，是GOFA的主要目标。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/2.png#center" alt="icl"  />
</p>
<p>GOFA主要由2个模块组成，一个是Graph Language Encoder，还有一个是LLM Decoder。其中Graph Language Encoder的目的是用来学习节点的表示向量， 其中交替训练了LLM compressor和GNNlayer，LLM compressor用来学习TAG graph中的语义信息，GNN用来学习结构关系信息（也就是NOG Node和其他节点的交互）。因为GNN已经可以学到图的结构信息了，LLM compressor的作用就是把文本信息压缩到GNN学习的节点表示中。 第二个模块是一个LLM decoder，它的训练目标是，基于节点的向量表示，预测下一个token。</p>
<h3 id="graph-language-encoder">Graph Language Encoder</h3>
<p>Graph Language Encoder 包含LLM compressors和GNN Layers。对于一个NOG Node的text attribute，若它由 $l$个tokens组成 $\{x_i\}_{i = 1}^l$，通过一个pretrained LLM 比如Mistral-7B，可以将每个token映射为一个token embedding $q(x_i)$。那么将NOG Node的所有token $\left\{q\left(x_i\right)\right\}_{i=1}^l$ pooling成一个向量可以作为NOG Node的node feature vector。但是这种做法丢失太多semantic information。</p>
<p><strong>Solution:</strong> 将NOG Node的text attribute压缩为 $K$个向量，且 $K &laquo;l$。具体来说，首先定义 $K$个memory tokens $\{m_1, \cdots. m_K \}$ with their lookup initial embeddings $\left\{q\left(m_j\right)\right\}_{j=1}^K$用于接收压缩后的text tokens。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/3.png#center" alt="icl"  />
</p>
<p>然后把 $l$个text tokens和 $K$个memory tokens 拼起来输入到LLM Compressor Layer1，由于LLM的是<strong>multiple transformer layers</strong> (self-attention mechanisms)，因此可以捕获memory tokens和text tokens之间的dependencies。所以通过这种方式，text tokens的信息可以被压缩到memory tokens中。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/4.png#center" alt="icl"  />
</p>
<p>此时，这个目标节点（NOG Node）的特征向量就被压缩到 $K$个token embeddings，但是这个时候 $K$个memory tokens并没有学到图的结构信息。为了编码图的结构信息，将这 $K$个memory token embeddings作为node features输入GNN中，做一个 $K$通道的聚合GNN。通过上图中的Token-level Message Passing，可以将图的结构信息编码到 NOG Node的 $K$个memory tokens中，然后将GNN处理过的memory token embeddings再和text token embeddings拼起来，用LLM compressors将文本信息压缩到 memory token embeddings中。经过 $L$次的交替LLM压缩和训练GNN后，这里得到的 $K$个memory token可以认为是充分节点的text 信息和图的结构信息。</p>
<h3 id="llm-decoder">LLM Decoder</h3>
<p><strong>The memory tokens $Q_{m,x}$ ( $m$:memory; $x$: 节点 $x$) of every node contain information about the text on the node, the surrounding node text, and the graph structure due to the message-passing process in the GNN layers.</strong> 然后，对于NOG节点 $v$，它的completion target 是 $y$，GOFA将NOG Node的memory tokens $Q_{m,x}$插入到target text tokens的前面，然后输入LLM Decoder中来生成下一个token。优化目标是最大化completion target 的log likelihood，也就是生成正确tokens的概率。</p>
<p>$$
\operatorname{CrossEntropy}\left(\left\{l\left(m_K\right), l\left(x_1\right), \ldots, l\left(x_{l-1}\right)\right\},\left\{x_1, \ldots, x_l\right\}\right) .
$$</p>
<p>在GOFA的influence阶段，输入一个新图和每个节点的text attribute，并且给出目标节点NOG Node，就可以补全该NOG Node。</p>
<p><img loading="lazy" src="/posts/2025-07-28-Gofa/5.png#center" alt="icl"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Fair Graph Learning</title>
      <link>https://JhuoW.github.io/posts/fairnessgnn/</link>
      <pubDate>Wed, 21 May 2025 12:13:13 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fairnessgnn/</guid>
      <description>Fair Graph Learning, Fair GNNs相关论文阅读笔记</description>
      <content:encoded><![CDATA[<h1 id="overview">Overview</h1>
<p>The following two works reduce prediction discrimination by optimizing adjacency matrices, which can improve fairness for link prediction tasks:</p>
<ul>
<li>On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections （ICLR 2021）</li>
<li>All of the Fairness for Edge Prediction with Optimal Transport (AISTATS 2021)</li>
</ul>
<p>通过修改原图的敏感属性，使用对比学习来实现模型对敏感属性的鲁棒性，即敏感属性的修改不会影响模型的输出:</p>
<ul>
<li>Towards a Unified Framework for Fair and Stable Graph Representation Learning (UAI 2021)</li>
</ul>
<p>使用对抗训练策略来增强图，使得增强图与敏感属性的关系（MI）最小，基于增强图训练的representation可以实现fairness:</p>
<ul>
<li>Learning Fair Graph Representations via Automated Data Augmentations (ICLR 2023)</li>
</ul>
<p>证明了message passing的neighbor aggregation会使得拓扑偏差积累到node representation中，在GNN的signal denoising优化框架中加入fairness regularization，使得学习到的节点表示向量要满足，不同敏感group具有相同的期望logits:</p>
<ul>
<li>FMP: Toward Fair Graph Message Passing against Topology Bias (Arxiv 2022)</li>
</ul>
<p>跨group的属性联合分布在model处理后的分布差异最小化:</p>
<ul>
<li>EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks (WWW 22)</li>
</ul>
<h1 id="1-on-dyadic-fairness-exploring-and-mitigating-bias-in-graph-connections-iclr-2021">1. On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections (ICLR 2021)</h1>
<h2 id="introduction">Introduction</h2>
<p>为了减轻算法的歧视性，涉及公平的算法至关重要。具体来说本文关注二元公平性（Dyadic fairness），即<strong>2个实例间的预测关系独立于敏感属性（敏感属性不会对节点间的预测关系分数产生影响）</strong>。本文揭示了调节现有边的权重有益于实现二元公平，并且进一步提出<strong>FairAdj</strong>来学习一个<strong>具有适当图结构约束的公平邻接矩阵</strong>，从而实现公平的链路预测。</p>
<p>在推荐系统中，敏感属性可能会主导带有偏见的推荐，比如用户的宗教信可能会对用户的相互推荐偏向产生影响，导致用户更可能被推荐具有相同宗教信仰的其他用户，从而导致社会关系产生隔离。因此，对于两个实例而言，算法应该在不受他们敏感属性影响的情况下执行链路预测。二元公平性的概念来自于统计中的group fairness。首先，图中的节点根据敏感属性的取值分为几个组，二元公平要求<strong>一些统计数据在组内和组间大致相等</strong>。</p>
<h2 id="dyadic-fairness">Dyadic Fairness</h2>
<p>对于某一个敏感属性，每个节点有一个该属性的值，用$S(v)$表示节点 $v$的敏感属性， $\Gamma(v)$表示节点 $v$的一阶邻居集合。如果节点 $u$和 $v$的敏感属性值相同，那么Edge $(u,v)$为<strong>intra (Group内边)</strong>；如果 $u$和 $v$的敏感属性值不同，即 $S(u) \neq S(v)$则Edge $(u,v)$为<strong>inter(Group间边)</strong>。对于一个binary sensitive attribute，即sensitive attribute有两个可选值（如男/女），可以将图中的节点分为不同敏感属性值的两个组 $S_0$和 $S_1$。 $\widetilde{S_0}:=\left\{v \in S_0 \mid \Gamma(v) \cap S_1 \neq \varnothing\right\}$表示 $S_0$中与 $S_1$有边连接的节点，同理 $\widetilde{S_1}$表示 $S_1$中与 $S_0$ 有边连接的节点。也就是说， $\widetilde{S_0}$和 $\widetilde{S_1}$表示<strong>具有跨group边的节点集合</strong> 。链路预测模型 $g(\cdot, \cdot): \mathbb{R}^D \times \mathbb{R}^D \rightarrow \mathbb{R}$表示将两个节点的embeddings映射为一个分数来判断是否有边连接。</p>
<p><em>Definition 2.1 如果link prediction的预测分数满足：</em></p>
<p>$\operatorname{Pr}(g(u, v) \mid S(u)=S(v))=\operatorname{Pr}(g(u, v) \mid S(u) \neq S(v))$</p>
<p><em>那么该link prediction算法满足Dyadic fairness。</em></p>
<p>也就是两个节点是否处在同一个group并不会模型对他们的预测概率。换句话说，任意调整两个节点的敏感属性值，模型对他们之间是否存在边的预测结果不变。</p>
<p><strong>Demographic Parity</strong>:  <strong>A fairness metric that is satisfied if the results of a model&rsquo;s classification are not dependent on a given sensitive attribute</strong>.</p>
<h2 id="图连通如何影响公平性">图连通如何影响公平性</h2>
<p><strong>Proposition 4.1</strong>：若链路预测函数 $g(\cdot, \cdot)$定义为一个内积函数 $g(v, u)=v^{\top} \Sigma u$，其中 $\Sigma$是一个正定矩阵，那么 $\exists Q&gt;0$， $\forall v \sim \mathcal{V}$， $||v||_2 \leq Q$，对于 $\mathbb{E}_{v \sim U}[v] \in \mathbb{R}^M$（ $U$是节点集 $\mathcal{V}$上的离散均匀分布），如果 $\left|\left|\mathbb{E}_{v \sim U}\left[v \mid v \in S_0\right]-\mathbb{E}_{v \sim U}\left[v \mid v \in S_1\right]\right|\right|_2 \leq \delta$ （即由敏感属性划分的两个集合的期望特征差异小于 $\delta$），则有：
$$
\Delta_{\mathrm{DP}}:=\left|\underbrace{\mathbb{E}_{(v, u) \sim U \times U}[g(v, u) \mid S(v)=S(u)]}_{u和v属于同一个group时，模型预测的期望分数}-\underbrace{\mathbb{E}_{(v, u) \sim U \times U}[g(v, u) \mid S(v) \neq S(u)]}_{u和v属于不同group时，模型预测的期望分数}\right| \leq Q||\Sigma||_2 \cdot \delta
$$</p>
<p><em><strong>Proof:</strong> 令 $p:=\mathbb{E}_{v \sim U}\left[v \mid v \in S_0\right] \in \mathbb{R}^M$表示group $S_0$中节点特征的期望， $q:=\mathbb{E}_{v \sim U}\left[v \mid v \in S_1\right] \in \mathbb{R}^M$表示group $S_1$中节点特征的期望。那么对于两个节点 $u$和 $v$，<strong>他们是跨group节点时表示向量的期望相似度</strong> 与 <strong>他们是同一个group时表示向量之间的期望相似度</strong> 之间的差异应该越小越好，表示模型的预测是独立于划分group的敏感属性。这个差异可以表示为以下形式：</em></p>
<p><em>若 $u$和 $v$的敏感属性不同，即他们属于不同的group，那么他们通过模型输出的embeddings相似度的期望表示为如下形式：</em></p>
<p>$$
\begin{aligned}
\mathbb{E}_{\text {inter }} &amp;= \mathbb{E}\left[v^{\top} \Sigma u \mid v \in S_0, u \in S_1\right] \\
&amp;=\frac{1}{|S_0||S_1|} \sum_{v \in S_0} \sum_{u \in S_1} v^\top \Sigma u \\
&amp;=\frac{1}{|S_0|} \sum_{v \in S_0} v^\top \Sigma \frac{1}{|S_1|}\sum_{u \in S_1} u \\
&amp; = p^\top \Sigma q
\end{aligned}
$$</p>
<p><em>若 $u$和 $v$的敏感属性相同，即他们属于同一个敏感group，那么他们通过模型输出的embeddings相似度期望可以表示为如下形式：</em></p>
<p>$$
\begin{aligned}
\mathbb{E}_{{\text {inter }}} &amp;= \mathbb{E}\left[v^{\top} \Sigma u \mid (v \in S_0, u \in S_0) \vee (v \in S_1, u \in S_1)\right]  \\
&amp;= \frac{1}{|S_0|^2 + |S_1|^2}(\sum_{v \in S_0}v^\top \Sigma \sum_{u \in S_0}u + \sum_{v \in S_1}v^\top \Sigma \sum_{u \in S_1}u) \\
&amp;=  \frac{1}{|S_0|^2 + |S_1|^2} (|S_0|p^\top\Sigma|S_0|p + |S_1|q^\top\Sigma|S_1|q)
\\
&amp;=\frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} p^{\top} \Sigma p+\frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} q^{\top} \Sigma q
\end{aligned}
$$</p>
<p><em>那么跨group节点表示之间的相似度的期望与group内节点表示之间相似度的期望 之间的差异可以表示为如下形式</em></p>
<p>$$
\begin{aligned}
\left|\mathbb{E}_{\text {inter }}-\mathbb{E}_{\text {intra }}\right| &amp; =\left|\mathbb{E}\left[v^{\top} \Sigma u \mid v \in S_0, u \in S_1\right]-\mathbb{E}\left[v^{\top} \Sigma u \mid v \in S_0, u \in S_0 \vee v \in S_1, u \in S_1\right]\right| \\
&amp; =\left|p^{\top} \Sigma q-\left(\frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} p^{\top} \Sigma p+\frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} q^{\top} \Sigma q\right)\right| \\
&amp; =\left|(q-p)^{\top}\left(\frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} \Sigma p-\frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} \Sigma q\right)\right|
\end{aligned}
$$</p>
<p><em>令 $\alpha = \frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2}$， $\beta = \frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2}$，那么上式可以改写为：</em>
$$
\left|(q-p)^{\top}\left(\frac{\left|S_0\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} \Sigma p-\frac{\left|S_1\right|^2}{\left|S_0\right|^2+\left|S_1\right|^2} \Sigma q\right)\right| = \left|(q-p)^{\top} (\alpha \Sigma p-\beta\Sigma q)\right|
$$</p>
<p><em>|根据柯西-施瓦茨不等式 (Cauchy–Schwarz inequality)，有 $|\langle\mathbf{u}, \mathbf{v}\rangle|^2 \leq\langle\mathbf{u}, \mathbf{u}\rangle \cdot\langle\mathbf{v}, \mathbf{v}\rangle$，那么下式成立：</em>
$$
\left|(q-p)^{\top} (\alpha \Sigma p-\beta\Sigma q)\right| \leq ||q-p||_2 \cdot||\alpha \Sigma p-\beta\Sigma q||_2
$$</p>
<p><em>因为敏感group期望的差异被 $\delta$ upper bounded所以下式成立：</em>
$$
||q-p||_2 \cdot||\alpha \Sigma p-\beta\Sigma q||_2 \leq \delta \cdot ||\alpha \Sigma p-\beta\Sigma q||_2 = \delta \cdot ||\Sigma (\alpha p - \beta q)||_2
$$</p>
<p><em>根据matrix norm，有 $||AB|| \leq ||A||\cdot ||B||$，因此下面不等式成立：</em></p>
<p>$$
\delta \cdot ||\Sigma (\alpha p - \beta q)|| \leq \delta ||\Sigma||_2  \cdot ||(\alpha p - \beta q)||_2 = \delta ||\Sigma||_2  \cdot ||(\alpha p + (- \beta q))||_2
$$</p>
<p><em>由于 $||A+B|| \leq ||A||+ ||B||$，所以下式成立：</em></p>
<p>$$
\delta ||\Sigma||_2  \cdot ||(\alpha p + (- \beta q))||_2  \leq \delta ||\Sigma||_2  \cdot (||\alpha p||_2 + ||\beta q||_2) = Q\delta ||\Sigma||_2
$$</p>
<p><em>因此，Proposition 4.1得证。</em></p>
<p><strong>由Proposition 4.1可以得出，模型在link prediction上实现demographic parity的充分条件是模型可以为敏感属性划分的group学习到期望相似的embeddings。</strong> 下面证明图结构如何影响demographic parity。</p>
<p>令由敏感属性划分的group中节点表示的期望为 $\mathbb{E}_{v \sim U}\left[v \mid v \in S_0\right] = \mu_0$； $\mathbb{E}_{v \sim U}\left[v \mid v \in S_1\right] = \mu_1$。用 $\sigma$表示节点表示和所在group期望之间的最大偏差，即 $\forall v \in S_0$， $\left|\left|v-\mu_0\right|\right|_{\infty} \leq \sigma$ 且 $\forall v \in S_1$， $\left|\left|v-\mu_1\right|\right|_{\infty} \leq \sigma$。图中节点的最大加权度为 $D_{\max }:=\max_{v \in \mathcal{V}} \operatorname{deg}_w(v)$， $m_w = \sum_{S(v)\neq S(u)} a_{vu}$表示所有跨group边的权重和。下面定理展示了单次GNN平滑后，两个group的期望的差异会如何变化：</p>
<p><strong>Theorem 4.1.</strong> 对于一个没有负边权重的图，在执行一次mean aggregation后，2个sensitive group中的表示差异 $\Delta_{\mathrm{DP}}^{\mathrm{Aggr}}:=\left|\left|\mathbb{E}_{v \sim U}\left[\operatorname{Agg}(v) \mid v \in S_0\right]-\mathbb{E}_{v \sim U}\left[\operatorname{Agg}(v) \mid v \in S_1\right]\right|\right|_2$会被下式bounded:<br>
$$
\max \left\{\alpha_{\min }\left|\left|\mu_0-\mu_1\right|\right|_{\infty}-2 \sigma, 0\right\} \leq \Delta_{\mathrm{DP}}^{\mathrm{Aggr}} \leq \alpha_{\max }\left|\left|\mu_0-\mu_1\right|\right|_2+2 \sqrt{M} \sigma
$$</p>
<p>其中 $\alpha_{\min }=\min \left\{\alpha_1, \alpha_2\right\}$， $\alpha_{\max}=\max \left\{\alpha_1, \alpha_2\right\}$， $\alpha_1=\left|1-\frac{m_w}{D_{\max }}\left(\frac{1}{\left|S_0\right|}+\frac{1}{\left|S_1\right|}\right)\right|$， $\alpha_2=\left|1-\frac{\left|\widetilde{S_0}\right|}{\left|S_0\right|}-\frac{\left|\widetilde{S_1}\right|}{\left|S_1\right|}\right|$ ， $M$是输入特征的维度。 $\alpha_2$只和图相关，所以是定值。</p>
<p>Theorem 4.1 中，只有跨group边的总权重 $m_w$和图中最大加权度 $D_{\max}$是可调节的，因此通过调节图中边的权重，可以调节一层GNN后得到的两个group期望表示差异，通过最小化这个差异 $\Delta_{\mathrm{DP}}^{\mathrm{Aggr}}$的上界，可以得到关于sensitive group公平的representations。</p>
<p>只看右边的upper bound， $\alpha_{\max}$中 $|S_0|$和 $|S_1|$是定值，对于 $\alpha_1$，如果跨group边权重很小（ $m_w \rightarrow 0$）会使得 $\alpha_1$趋近于1，如果跨group边的权重很大（一个group中的所有边都连到另一个group： $m_w \rightarrow D_w \cdot \min \left\{\left|S_0\right|,\left|S_1\right|\right\}$）也会使得 $\alpha_1$趋近于1。（这里不理解？？？为什么 $m_w$的上限是这种形式？） 因此，增加跨group边的权重并不能够达到demographic parity，因为增得太大会使 $\alpha_{\max}$增大，导致无法取到最小上界。</p>
<h2 id="learning-fair-graph-connections">Learning Fair Graph Connections</h2>
<p>上面的分析说明了调整图结构有助于模型有条件地获得公平性。然而，在多层GNN中优化图结构并不容易，因此FairAdj在保留原始图结构的基础上更新 $\widetilde{A}$，即更新已有边的权重来实现学习表示的公平。</p>
<p>因为是链路预测任务，GNN编码器用于重构原始图中的边，使得GNN可以预测原图中的连接关系，本文采用VGAE作为GNN编码器：</p>
<p>$$
\max _\theta \quad \mathcal{L}_{\text {util }}:=\mathbb{E}_{\mathrm{GNN}_\theta(Z \mid X, \widetilde{A})}[\log p(A \mid Z)]-K L\left[\operatorname{GNN}_\theta(Z \mid X, \widetilde{A}) || \mathcal{N}(0,1)\right]
$$</p>
<p>即 GNN编码器基于$\widetilde{A}$和 $X$生成高斯分布，再基于reparameterization trick把构造decoder来重构原图结构 $A$，这样的GNN具备链路预测能力。接下来为模型注入关于敏感属性公平性，公平性要求GNN对具有跨group边的节点间（inter 边）预测的期望分数 与group内边（intra边）预测的期望分数的差异要尽可能小，表示模型的预测独立于敏感属性：</p>
<p>$$
\begin{array}{cl}
\min_{\widetilde{A}} &amp; \mathcal{L}_{\text {fair }}:=\left|\left|\mathbb{E}_{v, u \sim U \times U}\left[\hat{a}_{v u} \mid S(v)=S(u)\right]-\mathbb{E}_{v, u \sim U \times U}\left[\hat{a}_{v u} \mid S(v) \neq S(u)\right]\right|\right|^2 \\
\text { s.t. } &amp; (1) .[\widetilde{A}]_{v u}=0, \text { if }[A]_{v u}=0, \quad(2) . \widetilde{A} \mathbb{1}=\mathbb{1}, \widetilde{A} \geq 0,
\end{array}
$$</p>
<h1 id="2-all-of-the-fairness-for-edge-prediction-with-optimal-transport-aistats-2021">2. All of the Fairness for Edge Prediction with Optimal Transport (AISTATS 2021)</h1>
<p>给定两个节点 $(V, V^\prime)$，用 $\mathbb{P}_1(h)=\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S \neq S^{\prime}\right)$表示两个节点敏感属性不同时，模型 $h$为他们预测的边概率；</p>
<p>$\mathbb{P}_0(h)=\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=S^{\prime}\right)$表示两个节点敏感属性相同时，模型 $h$为他们预测的边概率。</p>
<p>Definition (<strong>Disparate Impact (DI)</strong>) ：</p>
<p>$$
D I\left(h, \mathbb{V}, S \oplus S^{\prime}\right)=\frac{\mathbb{P}_1(h)}{\mathbb{P}_0(h)}
$$</p>
<p>其中 $S \oplus S^{\prime}$是XOR操作，不同为1，相同为0。分子 $\mathbb{P}_1(h)$表示 $h$为跨group节点预测有边的概率，分布表示 $h$为group内节点预测有边的概率。如果模型 $h$是完美平衡的，那么DI等于1，也就是无论两个节点的敏感属性如何设置， $h$为他们预测的边概率都相同。</p>
<p>Definition (<strong>Balanced Error Rate (BER)</strong>):</p>
<p>$$
\operatorname{BER}\left(h, \mathbb{V}, S \oplus S^{\prime}\right)=\frac{\mathbb{P}_1(h)-\mathbb{P}_0(h)+1}{2}
$$</p>
<p>BER表示模型预测两个节点敏感属性相同的概率，BER的最佳值为 $\frac{1}{2}$，即模型预测两个节点敏感属性相同的概率和不同的概率一样，都为 $\frac{1}{2}$。</p>
<p>Remark 1: DI 的定义可以改写成关于单个节点fairness的形式：</p>
<p>$$
D I(h, \mathbb{V}, S)=\frac{\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=0\right)}{\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=1\right)}
$$</p>
<p>分子 $\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=0\right)$表示某节点敏感属性是0时， $h$预测它与其他节点产生边的概率;</p>
<p>分母 $\mathbb{P}\left(h\left(V, V^{\prime}\right)=1 \mid S=1\right)$表示某节点敏感属性是1时， $h$预测它与其他节点产生边的概率。</p>
<p>Theorem 1: 给定一个图 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ 和一个边预测函数 $h: \mathbb{V} \times \mathbb{V} \rightarrow{0,1}$，假设 $D I(h, \mathbb{V}, S) \leq \tau$，那么下式成立：</p>
<p>$$
D I\left(h, \mathbb{V}, S \oplus S^{\prime}\right) \leq D I(h, \mathbb{V}, S) \leq \tau
$$</p>
<p>Corollary 1: 在Theorem 1成立的前提下，下式成立：</p>
<p>$$
\begin{aligned}
B E R\left(h, \mathbb{V}, S \oplus S^{\prime}\right) &amp; \leq \frac{1}{2}-\frac{\mathbb{P}_1(h)}{2}\left(\frac{1}{\tau}-1\right) \\
\min_{h \in \mathcal{H}} B E R\left(h, \mathbb{V}, S \oplus S^{\prime}\right) &amp; =\frac{1}{2}\left(1-\frac{1}{2} W_{1 . \neq \cdot}\left(\gamma_0, \gamma_1\right)\right)
\end{aligned}
$$</p>
<p>其中 $W_{1 . \neq \cdot}$是Wasserstein Distance on the joint distributions $\gamma_0$ and $\gamma_1$. 其中 $\gamma_0$ 和 $\gamma_1$分别是group $S=0$和group $S=1$的节点分布。因此邻接矩阵的优化目标是两个group节点的分布要相似（Wasserstein distance）。</p>
<h2 id="group-graph-fairness-with-ot">Group Graph Fairness with OT</h2>
<p>从Corollary 1可以看出，使得模型 $h$学习到的节点表示在敏感属性划分的group上达到fairness需要两个group上节点的分布 $\gamma_0$ 和 $\gamma_1$之间的Wasserrstein distance尽可能小。</p>
<p>对于邻接矩阵 $\mathcal{A} \in \mathbb{R}^{N \times N}$，分布 $\gamma_0$ 是由 $\mathcal{A}$中敏感属性 $S=0$的行 $\mathcal{A}_0 \in \mathbb{R}^{N_0 \times N}$构成，分布 $\gamma_1$ 是由 $\mathcal{A}$中敏感属性 $S=1$的行 $\mathcal{A}_1 \in \mathbb{R}^{N_1 \times N}$构成。 $\gamma \in \Pi\left(\frac{1}{N_0}, \frac{1}{N_1}\right)$表示两个节点分布之间的transport plan。 $\frac{1}{N_0}, \frac{1}{N_1}$分别表示有 $N_0$和 $N_1$个元素的均匀分布，表示2个group中节点的分布。 两个group中节点到节点的运输距离用矩阵 $M \in \mathbb{R}^{N_0 \times N_1}$表示，其中 $M_{i j}=l\left(a_0^{(i)}, a_1^{(j)}\right)$表示两个跨group节点间的距离， $a_0^{(i)}$分别表示 $\mathcal{A}_0$的第 $i$行， $a_1^{(j)}$表示 $\mathcal{A}_1$的第 $j$列。 $l$是距离度量。因此，两个group中节点分布之间的距离（Wasserstein distance）是以下OT问题的解：</p>
<p>$$
\mathrm{inf}_{\gamma \in \Pi\left(\frac{1}{N_0}, \frac{1}{N_1}\right)} \langle\gamma, M\rangle
$$</p>
<p>要使上式达到最小，需要优化 $M$，也就是图的邻接矩阵，使得两个group分布的Wasserstein distance达到最小:</p>
<p>$$
\tilde{\mathcal{A}}_{\text {bary }}=\underset{\mathcal{A} \in \mathbb{R}^{N \times N}}{\operatorname{argmin}} \frac{1}{|S|} \sum_{i=1}^{|S|} \min _{\gamma_i \in \Pi\left(\frac{1}{N}, \frac{1}{N_i}\right)} \langle\gamma, M\rangle
$$</p>
<h1 id="3-towards-a-unified-framework-for-fair-and-stable-graph-representation-learning-uai-2021">3. Towards a Unified Framework for Fair and Stable Graph Representation Learning (UAI 2021)</h1>
<h2 id="notations">Notations</h2>
<p>令图 $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathbf{X})$表示无向图，其中 $\mathbf{x}_v \in \mathbf{X}$是节点 $v$的一个 $M$维属性向量。令 $\mathbf{I}_u \in{0,1}^N$表示节点 $u$的局部邻居，即如果 $v$与 $u$有边连接，那么 $\mathbf{I}_{uv}=1$，否则 $\mathbf{I}_{uv}=0$。令 $\mathbf{b}_u=\left[\mathbf{x}_u ; \mathbf{I}_u\right]$表示节点 $u$所有信息（局部邻居和属性特征）。</p>
<h2 id="fairness-and-stability">Fairness and Stability</h2>
<p><em><strong>Counterfactual Fairness （模型关于敏感属性的改变是robust的）:</strong></em> 对于节点 $u$和它的增强 $\tilde{u}^s$（将 $u$的敏感属性翻转后得到），编码器 $\text { ENC }$若满足下面条件，那么$\text { ENC }$满足counterfactual fairness:</p>
<p>$$
\operatorname{ENC}(u)=\operatorname{ENC}\left(\tilde{u}^s\right)
$$</p>
<p><em><strong>Stability via Lipschitz Continuity:</strong></em> 对于节点 $u$和它的增强 $\tilde{u}$（对$u$扰动节点特征/边），如果编码器 $\text { ENC }$满足以下条件：</p>
<p>$$
||\operatorname{ENC}(\tilde{u})-\operatorname{ENC}(u)||_p \leq L\left|\left|\tilde{\mathbf{b}}_u-\mathbf{b}_u\right|\right|_p
$$</p>
<p>则编码器 $\text { ENC }$关于Lipschitz continuity是稳定的。 $L$ is the Lipschitz constant，对于固定的编码器函数，比如ReLU是一个1-Lipschitz连续函数 $L=1$。对于一个有固定激活函数的编码器， $L$的值是固定的。</p>
<h2 id="图增强">图增强</h2>
<p>a) <strong>节点属性扰动</strong>：给定一个 $M$维的attribute masking向量 $\mathbf{r} \in\{0,1\}^M$用于选择需要被扰动的属性， $\mathbf{r}$中的每个值从Bernoulli distribution  $p_n$中采样，表示每个属性（除了敏感属性）是否要被扰动，而扰动量从normal distribution中采样 $\delta \in \mathbb{R}^M$。那么节点 $u$特征的扰动量为 $\mathbf{r} \circ \delta$，扰动后的节点特征为： $\tilde{\mathbf{x}}_u=\mathbf{x}_u+\mathbf{r} \circ \delta$。</p>
<p>b) <strong>敏感属性的反事实扰动</strong>：将节点的敏感属性值翻转，这里考虑二值敏感属性分布，即 $s \in\{0,1\}$。</p>
<p>c) <strong>图结构扰动</strong>：Bernoulli distribution $\mathcal{B}(p_e)$ 中每条边以 $p_e$概率被drop，由此生成扰动后的图结构 $\tilde{\mathbf{A}}$。</p>
<h2 id="contrastive-objective">Contrastive Objective</h2>
<p>为了使模型可以对属性/结构的扰动，以及敏感属性的修改鲁棒，则需要在原图和增强后的图上，GNN可以学习到相似的embeddings，这样等价于学习扰动/敏感属性鲁棒的节点表示：</p>
<p>$$
\mathcal{L}_s=\mathbb{E}_u\left[\frac{1}{2}\left(D\left(t\left(\mathbf{z}_u\right), \operatorname{sg}\left(\tilde{\mathbf{z}}_u\right)\right)+D\left(t\left(\tilde{\mathbf{z}}_u\right), \operatorname{sg}\left(\mathbf{z}_u\right)\right)\right)\right]
$$</p>
<h2 id="stability">Stability</h2>
<p>若模型要满足稳定性，那么编码器应满足<em><strong>Lipschitz Continuity</strong></em>。将第 $k$层GNN编码器定义为 $\mathbf{h}^k_u = \sigma\left(\mathbf{W}_a^k \mathbf{h}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{k-1}\right)$，那么下式成立：</p>
<p>$$
||\operatorname{ENC}(\tilde{u})-\operatorname{ENC}(u)||_p \leq \prod_{k=1}^K\left|\left|\mathbf{W}_a^k\right|\right|_p\left|\left|\left(\tilde{\mathbf{b}}_u-\mathbf{b}_u\right)\right|\right|_p
$$</p>
<p>Proof: 第 $k$层编码器关于节点 $u$的扰动后差异可以写为：</p>
<p>$$
\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k=
\sigma\left(\mathbf{W}_a^k \tilde{\mathbf{h}}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(\bar{u})} \mathbf{h}_v^{k-1}\right)-\sigma\left(\mathbf{W}_a^k \mathbf{h}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{k-1}\right)
$$</p>
<p>其中 $\sigma(\cdot)$为非线性激活函数。<strong>假设 $\sigma(\cdot)$满足1-Lipschitz continuity（例如ReLU）</strong>，那么下式成立：</p>
<p>$$
||\sigma(b)-\sigma(a)||_p \leq||b-a||_p     \quad \quad \text{(1-Lipschitz, Lip_Constant=1)}
$$</p>
<p>所以下式成立：</p>
<p>$$
\begin{aligned}
||\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k||_p&amp;=
\left|\left|\sigma\left(\mathbf{W}_a^k \tilde{\mathbf{h}}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(\bar{u})} \mathbf{h}_v^{k-1}\right)-\sigma\left(\mathbf{W}_a^k \mathbf{h}_u^{k-1}+\mathbf{W}_n^k \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{k-1}\right)\right|\right|_p\\
&amp;\leq ||\mathbf{W}_a^k(\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1}) + \mathbf{W}_n^k (\sum_{v \in \mathcal{N}(\bar{u})} \mathbf{h}_v^{k-1}- \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{k-1})||_p
\end{aligned}
$$</p>
<p>由于对图中的边做扰动，边drop概率 $p_e$通常来说非常小，本文中设置为0.001，因此结构改变不大，所以上式的第二项可以近似为0，那么上式可以改写为如下形式：</p>
<p>$$
||\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k||_p \leq  ||\mathbf{W}_a^k(\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1})||_p
$$</p>
<p>根据Cauchy-Schwartz inequality $|\langle\mathbf{u}, \mathbf{v}\rangle|^2 \leq\langle\mathbf{u}, \mathbf{u}\rangle \cdot\langle\mathbf{v}, \mathbf{v}\rangle$，上式可以改写成：</p>
<p>$$
||\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k||_p \leq  ||\mathbf{W}_a^k(\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1})||_p \leq  ||\mathbf{W}_a^k||_p ||\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1}||_p
$$</p>
<p>进而得到下式：</p>
<p>$$
||\tilde{\mathbf{h}}_u^k-\mathbf{h}_u^k||_p \leq ||\mathbf{W}_a^k||_p ||\tilde{\mathbf{h}}_u^{k-1}-\mathbf{h}_u^{k-1}||_p \leq ||\mathbf{W}_a^k||_p ||\mathbf{W}_a^{k-1}||_p ||\tilde{\mathbf{h}}_u^{k-2}-\mathbf{h}_u^{k-2}||_p \cdots
$$</p>
<p>因此下式成立：</p>
<p>$$
\begin{aligned}
||\operatorname{ENC}(\tilde{u})-\operatorname{ENC}(u)||_p &amp; =\left|\left|\tilde{\mathbf{z}}_u-\mathbf{z}_u\right|\right|_p=\left|\left|\tilde{\mathbf{h}}_u^K-\mathbf{h}_u^K\right|\right|_p
\leq \prod_{k=1}^K\left|\left|\mathbf{W}_a^k\right|\right|_p\left|\left|\left(\tilde{\mathbf{b}}_u-\mathbf{b}_u\right)\right|\right|_p,
\end{aligned}
$$</p>
<p>这里考虑 $p=2$的情况，根据spectral norm，有 $||A||_2 =\sqrt{\lambda_{\max}(A^\star A)} =  \sigma_{\max}(A)$，即最大 $A$的最大奇异值。所以下式成立：</p>
<p>$$
\begin{aligned}
||\operatorname{ENC}(\tilde{u})-\operatorname{ENC}(u)||_p &amp; =\left|\left|\tilde{\mathbf{z}}_u-\mathbf{z}_u\right|\right|_p=\left|\left|\tilde{\mathbf{h}}_u^K-\mathbf{h}_u^K\right|\right|_p
\leq \prod_{k=1}^K \sigma_{\max} (\mathbf{W}_a^k)\left|\left|\left(\tilde{\mathbf{b}}_u-\mathbf{b}_u\right)\right|\right|_p,
\end{aligned}
$$</p>
<p>因为GNN的激活函数为ReLU，严格满足1-Lipschitz，因此 $\prod_{k=1}^K \sigma_{\max} (\mathbf{W}_a^k)$应等于1，为了是模型每层权重的最大奇异值为1，需要对权重矩阵做spectral normalization：</p>
<p>$$
\tilde{\mathbf{W}}_a^k=\mathbf{W}_a^k / \sigma\left(\mathbf{W}_a^k\right)
$$</p>
<h1 id="4-learning-fair-graph-representations-via-automated-data-augmentations-iclr-2023">4. Learning Fair Graph Representations via Automated Data Augmentations (ICLR 2023)</h1>
<p><a href="https://openreview.net/pdf?id=xgGS6PmzNq6">FairAdj</a>和<a href="http://proceedings.mlr.press/v130/laclau21a/laclau21a.pdf">FairGraph</a>对所有图均使用统一的邻接矩阵增强策略。FairAdj要求优化图中边的权重，使得模型预测两个节点无论他们是否在同一个group，他们之间有边连接的概率相同（对所有图均使用这一原则来增强图结构）；而FairGraph要求优化图结构，使得不同group节点的分布Wasserstein distance最小。这些方法通过添加**启发式的fairness regularization （上面提到的原则就是这些fairness regularization的启发式规则）**来对表示学习过程施加公平性约束。然而对所有图均使用统一的公平性策略来做数据增强并不合适，不同图的敏感属性不同，敏感属性的分布也不同，因此不同的图可能需要不同的图结构或者节点特征增强策略来适应这些多样化的敏感属性。因此，针对不同图的敏感属性，采用自适应的图结构和节点特征的增强策略，而不是局限在固定的启发式的公平性规则中，是highly desirable solution。</p>
<h2 id="automated-graph-augmentations">Automated Graph Augmentations</h2>
<p>给定一个GNN encoder $g_{\text {enc }}:(A, X) \rightarrow Z \in \mathbb{R}^{n \times d_r}$将每个节点映射为表示向量 $Z$。基于 $Z$可以自适应生成增强图。</p>
<p><strong>Edge Perturbation</strong>: 基于 $Z$生成增强图的结构：</p>
<p>$$
Z_A=\operatorname{MLP}_A(Z), \quad \widetilde{A^{\prime}}=\sigma\left(Z_A Z_A^T\right), \quad A_{i j}^{\prime} \sim \operatorname{Bernoulli}\left({\widetilde{A^{\prime}}}_{i j}\right) \text { for } i, j=1, \cdots, n
$$</p>
<p>其中 $\operatorname{MLP}_A$是结构增强器的训练参数， $\sigma(\cdot)$是sigmoid函数。增强图的邻接矩阵从Bernoulli分布 $\operatorname{Bernoulli}\left({\widetilde{A^{\prime}}}\right)$中采样。</p>
<p><strong>Node Feature Masking:</strong> 基于 $Z$来自适应mask节点特征矩阵中的元素：</p>
<p>$$
\begin{aligned}
&amp;Z_X =\operatorname{MLP}_X(Z), \\ &amp;\widetilde{M}=\sigma\left(Z_X\right), \\
&amp;M_{i j} \sim \operatorname{Bernoulli}\left(\widetilde{M}_{i j}\right) \text { for } i, j=1, \cdots, n, \\
&amp;X^{\prime}=M \odot X
\end{aligned}
$$</p>
<p>$\operatorname{MLP}_A$是特征增强器的参数，node feature mask矩阵从Bernoulli分布 $M_{i j} \sim \operatorname{Bernoulli}\left(\widetilde{M}_{i j}\right)$采样，然后与原特征矩阵 $X$。</p>
<p>在端到端的模型优化中，从Bernoulli distribution中采样邻接矩阵和featre masking矩阵是不可微的。为了使 $g$是端到端可训练的，Bernoulli distribution的采样操作可以使用Gumbel-Softmax reparameterization trick来模拟。具体来说，给定一个由模型 $\varphi$输出的概率 $\widetilde{P}$，那么可以通过添加一个从Gumbel分布中采样的随机变量 $G$来作为概率 $\widetilde{P}$的连续近似（continuous approximation）： $\hat{P}=\frac{1}{1+\exp (-(\log \widetilde{P}+G) / \tau)}$。那么该概率下的伯努利分布采样值为 $P=\left\lfloor\hat{P}+\frac{1}{2}\right\rfloor$，这个从 $\hat{P}$中得到采样值可以近似作为 $\widetilde{P}$的采样值。在反向传播计算梯度的时候，计算的使 $\hat{P}$关于可训练参数 $\varphi$的梯度。</p>
<h2 id="adversarial-training">Adversarial Training</h2>
<p>由于图中那些边以及节点的哪些属性会导致出现不公平的预测是未知的，因此使用对抗训练的方法来学习一个图增强器 $g$，用上文提到的方式来生成增强图的结构和节点特征，所得到的增强要尽可能与敏感属性无关，也就是<strong>判别器 $k$要无法建立增强图与敏感属性之间的关系</strong>。此时可以认为图增强器 $g$生成的增强图与敏感属性无关。</p>
<p>具体来说，判别器 $k:\left(A^{\prime}, X^{\prime}\right) \rightarrow \hat{S} \in[0,1]^n$用来基于增强图来预测每个节点的敏感属性值，预测损失越低说明增强图和敏感属性之间的关系越弱。因此对抗训练来学习图增强器的过程为以下优化问题：</p>
<p>$$
\begin{aligned}
\min_g \max_k L_{\mathrm{adv}}&amp;=\min_g \max_k \frac{1}{n} \sum_{i=1}^n\left[S_i \log \hat{S}_i+\left(1-S_i\right) \log \left(1-\hat{S}_i\right)\right] \\
&amp;= \min_g \max_k -\operatorname{CE}\left(S, \hat{S}\right)
\end{aligned}
$$</p>
<p>上面的优化目标可以改写为：</p>
<p>$$
\max_g \min_k \operatorname{CE}\left(S, \hat{S}\right) = \max_g \min_k \operatorname{CE} \left(S, k(g(A, X))\right)
$$</p>
<p>上式的优化过程可以总结如下：固定判别器 $k(\cdot, \cdot)$，优化z</p>
<p>固定判别器 $k(\cdot, \cdot)$，优化图增强器 $g$，使得CE loss最大化，即通过 $g$来生成的增强图 $g(A,X) = A^\prime, X^\prime$要使得 $k$无法利用它们来预测每个节点的敏感属性值，也就是 $g$需要生成与敏感属性无关的增强图。</p>
<p>固定图增强器 $g$，优化判别器 $k(\cdot, \cdot)$，使得CE loss最小化，即对于当前得到的增强图 $\left(A^{\prime}, X^{\prime}\right)$，需要一个更强的判别器  $k(\cdot, \cdot)$来建立增强图与敏感属性的关系，也就是用增强图来预测每个节点的敏感属性。</p>
<p>这种对抗训练的过程可以让增强器 $g$生成尽可能难以被用来预测敏感属性的图结构，而判别器 $k$则要尽可能建立增强图与敏感属性之间的关联。当对抗训练收敛时，可以认为 $g$足以生成无法与敏感属性建立起关系的增强图 $\left(A^{\prime}, X^{\prime}\right)$。</p>
<p><strong>这种图增强的方法来减轻敏感属性对预测的影响是自适应的，并没有基于确定的规则regularization（比如intra/inter group相似度，distribution相似度），对于不同的图会自适应地学习与敏感属性关系最低的对应增强图。因此相对于基于fairness regularization的方法，更加通用。</strong></p>
<h1 id="5-fmp-toward-fair-graph-message-passing-against-topology-bias-arxiv-2022">5. FMP: Toward Fair Graph Message Passing against Topology Bias (Arxiv 2022)</h1>
<p>Topology bias 指的是在许多现实世界graph中，具有相同敏感属性（除了节点特征外的敏感属性，如性别，国籍等）的节点更可能相互连接。本文证明了message passing中的聚合过程会在node representation中积累偏差。</p>
<p>给定一个有 $n$个节点的图 $\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$，节点的（binary）敏感属性是一个 $n$维向量 $\mathbf{s} \in\{-1,1\}^n$，表示每个节点的敏感属性取值。图中节点敏感属性的指示向量（incident vector）定义为：</p>
<p>$$
\Delta_{\mathbf{s}}=\frac{\mathbb{1}_{&gt;0}(\mathbf{s})}{\left|\left|\mathbb{1}_{&gt;0}(\mathbf{s})\right|\right|_1}-\frac{\mathbb{1}_{&gt;0}(-\mathbf{s})}{\left|\left|\mathbb{1}_{&gt;0}(-\mathbf{s})\right|\right|_1}
$$</p>
<p>其中 $\left|\left|\mathbb{1}_{&gt;0}(\mathbf{s})\right|\right|_1$等于敏感属性为1的节点数 $n_1$， $\left|\left|\mathbb{1}_{&gt;0}(-\mathbf{s})\right|\right|_1$等于敏感属性为-1的节点数。若 $s_i=1$，那么 $\mathbb{1}_{&gt;0}(\mathbf{s}_i)=1$， $\mathbb{1}_{&gt;0}(-\mathbf{s}_i)=0$，则 $\Delta_{\mathbf{s}_i} = \frac{1}{n_1}$；同理，若 $s_i=0$，那么 $\Delta_{\mathbf{s}_i} = -\frac{1}{n_{-1}}$，其中 $n_{-1}$表示敏感属性为-1的节点数。因此normalized incident vector $\Delta_{\mathbf{s}} = \{\frac{1}{n_1}, -\frac{1}{n_{-1}}\}^n$。</p>
<p><strong>Definition 1 (Label and Sensitive Homophily Coefficient)</strong> Label homophily coefficient:
$$
\epsilon_{\text {label }}(\mathcal{G}, \mathbf{y})=\frac{1}{|\mathcal{E}|} \sum_{(i, j) \in \mathcal{E}} \mathbb{1}\left(\mathbf{y}_i=\mathbf{y}_j\right)
$$</p>
<p>表示连接相同label节点的边占总边数的比例。</p>
<p>Sensitive homophily coefficient:</p>
<p>$$
\epsilon_{\text {sens }}(\mathcal{G}, \mathbf{s})=\frac{1}{|\mathcal{E}|} \sum_{(i, j) \in \mathcal{E}} \mathbb{1}\left(\mathbf{s}_i=\mathbf{s}_j\right)
$$</p>
<p>表示连接相同敏感属性节点的边占总边数的比例。</p>
<h2 id="gnns-as-graph-signal-denoising">GNNs as Graph Signal Denoising</h2>
<p>现有的GNN如GCN, SGC, APPNP等均可以看作以不同的方式（直接求导或梯度下降）来优化signal denoising函数：</p>
<p>$$
\min_\mathbf{F} h_s(\mathbf{F}) = \min_\mathbf{F}\frac{\lambda_s}{2} \operatorname{tr}\left(\mathbf{F}^T \tilde{\mathbf{L}} \mathbf{F}\right)+\frac{1}{2}\left|\left|\mathbf{F}-\mathbf{X}_{\text {trans }}\right|\right|_F^2
$$</p>
<p>用来enforce smoothnes以及保持和输入特征的相似度。</p>
<h2 id="the-optimization-framework">The Optimization Framework</h2>
<p>在优化 $\mathbf{F}$的同时， $\mathbf{F}$要满足公平性约束fairness objective $\left|\left|\boldsymbol{\Delta}_s S F(\mathbf{F})\right|\right|_1$:</p>
<p>$$
\min_{\mathbf{F}} \underbrace{\frac{\lambda_s}{2} \operatorname{tr}\left(\mathbf{F}^T \tilde{\mathbf{L}} \mathbf{F}\right)+\frac{1}{2}\left|\left|\mathbf{F}-\mathbf{X}_{\text {trans }}\right|\right|_F^2}_{h_s(\mathbf{F})}+\underbrace{\lambda_f\left|\left|\boldsymbol{\Delta}_s S F(\mathbf{F})\right|\right|_1}_{h_f\left(\boldsymbol{\Delta}_s S F(\mathbf{F})\right)}
$$</p>
<p>其中 $S F(\mathbf{F})_{ij} = \hat{P}\left(y_i=j \mid \mathbf{X}\right)$表示将节点 $v_i$预测label为 $j$的概率，而 $S F(\mathbf{F})_{i:}$ 表示节点 $v_i$的预测logits向量，所以 $S F(\mathbf{F}) = \operatorname{Softmax}(\mathbf{F})$。</p>
<p>由于$\Delta_{\mathbf{s}} = \{\frac{1}{n_1}, -\frac{1}{n_{-1}}\}^n$用来指示每个节点的敏感属性，因此 $\boldsymbol{\Delta}_s S F(\mathbf{F}) \in \mathbb{R}^n$。那么 $\boldsymbol{\Delta}_s S F(\mathbf{F})$的第 $j$个元素 $\boldsymbol{\Delta}_s S F(\mathbf{F})_j = \boldsymbol{\Delta}_s \cdot \operatorname{Softmax}(\mathbf{F})_{:,j}$，因此， $\boldsymbol{\Delta}_s S F(\mathbf{F})_j$等于敏感属性$s=1$的节点预测为label $j$的平均概率 与 $s=-1$的节点预测为label  $j$的平均概率 之差：</p>
<p>$$
\left(\Delta_s S F(\mathbf{F})\right)_j = \hat{P}\left(y_i=j \mid \mathbf{s}_i=1, \mathbf{X}\right)-\hat{P}\left(y_i=j \mid \mathbf{s}_i=-1, \mathbf{X}\right)
$$</p>
<p>其中 $\hat{P}\left(y_i=j \mid \mathbf{s}_i=1, \mathbf{X}\right)$表示敏感属性为 $1$的节点预测为标签 $j$的期望概率。如果 $\left(\boldsymbol{\Delta}_s S F(\mathbf{F})\right)_j$越小，说明模型学习到的node representations $\mathbf{F}$ is equivalent to demographic parity，也就是模型对通过敏感属性划分的sensitive group的预测不会产生偏差，即模型的预测与sensitive attribute无关。</p>
<h1 id="6-edits-modeling-and-mitigating-data-bias-for-graph-neural-networks-www-22">6. EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks (WWW 22)</h1>
<p>Bias in attribute：敏感属性划分的demographic group的attribute distribution （除了敏感属性之外的node feature）是不同的</p>
<p>Bias in network structure：敏感属性划分的demographic groups之间的节点邻居分布存在差异</p>
<p>上面两个bias意味着图的<strong>feature信息</strong>或图的<strong>结构信息</strong>与敏感属性高度相关</p>
<p><strong>Case 1: 有偏属性和无偏结构。</strong> 对于两个demographic group （例如通过敏感属性性别来划分group），bias in attribute意味着两个group中节点的属性分布（除了敏感属性之外的node feature）不同，但是图的连通性与敏感属性无关。这里将male节点的node feature从分布 $\mathcal{N}\left(-1.5,1^2\right)$采样，female节点的node feature从分布 $\mathcal{N}\left(1.5,1^2\right)$中采样。通过这种方式可以让敏感属性和节点特征保持强相关。为了实现无偏网络结构，图中的节点对以 $2\times 10^{-3}$的概率连接。</p>
<p><img loading="lazy" src="/posts/2025-05-21-Fairness/1.png#center" alt=""  />
</p>
<p>上图(a)是在propagation之前的两个敏感group的node feature分布，(b)可以看出图连通与group无关，(c) 可以看出propagation之后的node representations与(a)中的输入特征相比，2个敏感group节点之间的表示差异变小，说明无偏的结构可以使GNN学到的node representations与敏感属性的相关性降低。</p>
<p><strong>Case 2: 无偏属性和有偏结构。</strong> 这里假设所有节点的特征均采样自同一个分布$\mathcal{N}\left(0,1^2\right)$来构造无偏属性。为了构造关于敏感属性的有偏结构，对于每个节点，它的邻居的敏感属性与中心节点的敏感属性相关。首先，为每个节点累加它们的属性值，并且按照降序排列所有节点。取排名最高的前 $t$个male和最低的后 $t$个female构造两个community，在每个community内，节点以 $5\times 10^{-2}$的概率构造边。对于其它节点，以 $1\times 10^{-2}$的概率构造边。如下图所示：</p>
<p><img loading="lazy" src="/posts/2025-05-21-Fairness/2.png#center" alt=""  />
</p>
<p>从上图中可以看出，虽然(a)中的原始node feature是与sensitive attribute无关的，但是有偏的network structure会导致propagation之后的node representation会产生与sensitive attribute相关的聚类。</p>
<h2 id="bias">Bias</h2>
<p>Definition 1 (<strong>Attribute bias</strong>) 敏感属性 $i$将节点分为2个demographic groups。如果node feature中的任意属性在2个groups间存在分布差异，那么属性偏差存在。也就是node feature的分布与敏感属性相关。</p>
<p>Definition 2 (<strong>Structural bias</strong>) Propagate后2个group的节点属性分布出现差异，即 $AX$会导致representation在group间出现分布差异。因此图结构与敏感属性相关。</p>
<h2 id="objective-function">Objective Function</h2>
<p>考虑到节点特征有 $M$个属性，那么用 $P_{0,m}$表示group 0的第 $m$个属性的值分布，用 $P_{1,m}$表示group 1的第 $m$个属性的值分布。</p>
<p>若在structural debiasing阶段，模型propagate $H$次，并计算每次propagate的跨group属性差异。那么第 $h$层（ $1 \leq h \leq H$）group 0中节点的第 $m$个属性的值分布为 $P_{0,m}^{(h)}$，group 1中节点的第 $m$个属性值的分布为 $P_{1,m}^{(h)}$。那么对于所有 $H+1$层，group 0 的第 $m$个属性的联合概率分布 $P_{0,m}^{Joint} = P_{0,m}^{(0)} \cdot  P_{0,m}^{(1)} \cdot P_{0,m}^{(2)} \cdots P_{0,m}^{(H)}$ ，同理，group 1的第 $m$个属性的联合概率分布 $P_{1,m}^{Joint} = P_{1,m}^{(0)} \cdot  P_{1,m}^{(1)} \cdot P_{1,m}^{(2)} \cdots P_{1,m}^{(H)}$。这两个分布可以看作 $H+1$维的随机变量，每个随机变量服从一个分布。模型的参数可以分为2个部分，分别是 $g_{\theta_m}: \mathbb{R} \to \mathbb{R}$，用于mitigate每个节点第 $m$个属性的bias。参数化的邻接矩阵 $\tilde{A}$ 用于mitigate bias from structure。因此，对于第 $m$个attribute，模型的目标函数定义为如下形式：</p>
<p>$$
\min_{\theta_m, \tilde{A}} W(P_{0,m}^{Joint}, P_{1,m}^{Joint})
$$</p>
<p>优化目标是优化第 $m$个属性的“去偏器”参数 $\theta_m$和图结构 $\tilde{A}$，使得2个group原始节点属性分布以及propagate之后的属性分布之间的差异最小，即原始属性分布要在group间无差异，且propagate后的属性分布也要在group间无差异。</p>
<p>联合分布 $P_{0,m}^{Joint}$中的一个样本 $\mathrm{x}_{0, m} \sim P_{0,m}^{Joint}$  定义为每个概率分布的一个样本 $\mathrm{x}_{0, m}=\left[x_{0, m}^{(0)}, x_{0, m}^{(1)}, \ldots, x_{0, m}^{(H)}\right]$。而group 1的第 $m$个属性服从联合分布 $P_{1,m}^{Joint}$，那么它的一个样本可以视为从每个分布中采样一个样本的组合 $\mathrm{x}_{1, m}=\left[x_{1, m}^{(0)}, x_{1, m}^{(1)}, \ldots, x_{1, m}^{(H)}\right]$。</p>
<p>上面目标函数用Wasserstein distance 来表示，其中 $\gamma$表示两个分布间的一种传输方案，Wasserstein distance则表示两个分布中样本的平均差异:</p>
<p>$$
W(P_{0,m}^{Joint}, P_{1,m}^{Joint})= \inf_{\gamma\in \Pi(P_{0,m}^{Joint}, P_{1,m}^{Joint})} \mathbb{E}_{(\mathrm{x}_{0, m},\mathrm{x}_{1, m}) \sim \gamma} \left[||\mathrm{x}_{0, m}-\mathrm{x}_{1, m}||_1\right]
$$</p>
<p>因此模型优化的目标函数是要使2个group联合概率分布中样本的平均差异最小。</p>
<p>考虑所有 $M$个属性，模型的目标函数为：</p>
<p>$$
\min_{\theta, \tilde{A}} \frac{1}{M} \sum_{1 \leq m \leq M} W\left(P_{0, m}^{Joint }, P_{1, m}^{Joint }\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2024《One for All：Towards Training One Graph Model for All Classification Tasks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/ofa/</link>
      <pubDate>Wed, 21 May 2025 11:57:35 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/ofa/</guid>
      <description>ICLR2024 &amp;#34;One for All：Towards Training One Graph Model for All Classification Tasks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><strong>What is in-context learning?</strong></p>
<p>“In-Context Learning is a way to use LLMs to learn tasks given only a few examples. During in-context learning, we give the LM a prompt that consists of a list of input-output pairs that demonstrate how to perform a task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens. For example, to answer the two prompts below, the model needs to examine the training examples to figure out the input distribution (financial or general news), output distribution (Positive/Negative or topic), input-output mapping (sentiment or topic classification), and the formatting. “</p>
<p><img loading="lazy" src="/posts/2025-05-21-OFA/1.gif#center" alt="icl"  />
</p>
<p>如上图所示，在prompt中包含一些上下文input-output pairs来演示（demonstrate）如何执行任务。在这些prompt的最后添加text input token来让LLM学习上下文中的演示来执行text input token的任务。</p>
<h2 id="challenge"><strong>Challenge</strong></h2>
<p>Foundation Model指用单一模型来解决多个任务。但是Graph Foundation Model面临以下挑战：（1）不同来源的图数据在feature表征上通常完全不同。比如molecular graph中的节点特征是其中原子nominal feature的索引，e-commerce networks中的节点特征通常用Bag-of-Word来编码。这些不同来源的图数据特征维度、语义信息和尺度的差别都很大，几乎不可能用同一个模型来学习他们的表示。（2）不同的下游任务涉及图的不同部分（节点级、边级、图级）需要不同的策略和方法来学习表示。（3）如何设计统一的模型来实现跨领域和in-context learning是不确定的。</p>
<h1 id="ofa">OFA</h1>
<h2 id="用tag来统一不同领域的图都转化为tag形式">用TAG来统一不同领域的图（都转化为TAG形式）</h2>
<p><strong>将不同Domian的图转化为统一的TAG形式</strong></p>
<p>用human-interpretable language来描述节点和边的属性，从而使LLM可以将这些text attribute编码到同一个空间。具体来说，通过以下方式来生成每个节点的text feature:
将不同domain的graph的节点属性用统一形式的文本来描述，如上图中，对于一个节点的特征，它的文本描述以<strong>Feature node</strong>开头，后面的每个</p>
<p><img loading="lazy" src="/posts/2025-05-21-OFA/node_text_feature.jpg#center" alt="node_text_feature.jpg"  />
</p>
<p>将不同domain的graph的节点属性用统一形式的文本来描述，如上图中，对于一个节点的特征，它的文本描述以<strong>Feature node</strong>开头，后面的每个 $&lt;\text{feature describe}&gt;:&lt;\text{feature content}&gt;$是该节点的一个type-content文本对。例如对于一个molecule graph，其中的一个节点是原子，那么它的type是<strong>Atom</strong>，它的content是该原子的属性文本描述<strong>Carbon, Atomic number 6, helix chirality</strong>。如果是Citation Network， type是“Paper title and abstract”，content是“Attention is all you need. balabala”  （该文章具体的标题和摘要）
同理，边的text feature以<strong>Feature edge</strong>开头进行文本描述。</p>
<p>在用以上方式得到节点 $v_i$的text feature $s_{v_i}$和边 $e_{ij}$的text feature $s_{e_{ij}}$后，用LLM（sentence transformer, e5-large-v2, or Llama2）来将每个节点和边的text feature编码为vector embeddings：</p>
<p>$$
x_i = \operatorname{LLM}(s_{v_i}), \quad x_{ij} = \operatorname{LLM}(s_{e_{ij}})
$$</p>
<h2 id="用nodes-of-interest-noi来统一不同的图任务">用Nodes-of-Interest (NOI)来统一不同的图任务</h2>
<p>图上的任务主要分为3中：node-level tasks，link-level tasks和graph-level tasks。如果要用一个模型来处理这些任务的话，<strong>需要将这些任务统一为一个任务以便于在图数据上训练</strong>。</p>
<p><img loading="lazy" src="/posts/2025-05-21-OFA/NOI.jpg#center" alt="NOI.jpg"  />
</p>
<p>Nodes-of-Interest (<strong>NOI</strong>)指的是一个任务的<strong>目标节点（任务感兴趣的节点）</strong>，如上图的蓝色节点所示，表示为 $\mathcal{T}$。</p>
<ol>
<li>对于节点级任务，NOI是待预测节点集合。</li>
<li>边级任务NOI是待预测是否有边的节点对。</li>
<li>图级任务NOI是待预测图中的所有节点。</li>
</ol>
<p>待训练的有很多图，有的图要做节点分类，有的图做链路预测，有的图做图分类，通过构造NOI subgraph的方式，无论是什么任务，都把每个图中所有的 <strong>与任务相关的节点</strong> （nodes-of-interest）提取出来。</p>
<p>若一个NOI节点 $v$的 $h$-hop局部子图表示为 $\mathcal{S}_h(v)$，那么NOI中所有目标节点的局部子图共同构成了一个<strong>NOI subgraph</strong>，表示为 $\mathcal{G}_h (\mathcal{T})$：</p>
<p>$$
\mathcal{G}_h(\mathcal{T})=\bigcup_{v \in \mathcal{T}} \mathcal{S}_h(v)=\left\{\bigcup_{v \in \mathcal{T}} \mathcal{V}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{E}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{R}_v^h\right\} \quad \text{NOI中所有节点的局部子图共同构成}
$$</p>
<p><strong>NOI prompt node</strong>：定义NOI prompt node，该节点的文本信息用来描述任务，比如某个数据集上的节点分类、图分类等。NOI prompt node的节点text feature以如下形式构建：</p>
<p><img loading="lazy" src="/posts/2025-05-21-OFA/NOI_prompt_node.jpg" alt="NOI_prompt_node.jpg"  />
</p>
<p>NOI prompt node的节点特征以 “Prompt node.” 开头，该节点的文本表述为需要在NOI上进行的task，如果需要对NOI做node classification，那么Prompt node的文本特征为：“Node classification on the literature category of the paper.”。通过这种方式，即使是图上任务有不同，只需要用不同的NOI prompt node就可以描述不同的任务。</p>
<h2 id="graph-in-context-learning的图提示范式graph-prompting-paradigm">Graph In-Context Learning的图提示范式（Graph Prompting Paradigm）</h2>
<p>LLM的一大特性就是它可以通过prompt来实现in-context learning，使得模型可以在不用fine-tuning的情况下适应于不同的任务。比如在few-shot场景下，目标是基于一篇paper的摘要和内容预测它的类别，我们可以为LLM提供每个类别的 $k$篇papers作为context加入prompt中，来指导模型基于这些提供的context来生成目标摘要和内容的类别预测。（通过一些<strong>任务相关的其他信息</strong>来指导模型对目标样本的预测）</p>
<p>本文发现实现图上in-context learning的核心在于操作输入图使其和下游任务对齐。<strong>Graph Prompting Paradigm (GPP)</strong> 即图提示范式旨在操作输入图使其可以从输入数据本身获得任务相关的信息。如图2中的虚线所示，用来描述任务的NOI prompt node与所有NOI node建立连接（任务的目标节点），表示在这些NOI nodes上执行对应prompt的任务（如节点分类，链路预测，图分类等）。</p>
<p><strong>Zero-shot Learning:</strong> 对于一个NOI $q$，若是节点分类任务 $q$是一个节点；若是边分类任务 $q$是一条边的两端节点；若是图分类任务， $q$是图中的所有节点，如Figure 2中的蓝色节点所示。 $q$中节点构成的局部子图，即NOI subgraph 表示为 $\mathcal{G}^q_h (\mathcal{T}^q) = (\mathcal{V}^h_q, \mathcal{E}^h_q, \mathcal{R}^h_q)$。</p>
<p>对于一个NOI subgraph $\mathcal{G}^q_h (\mathcal{T}^q)$, 该子图的NOI的任务可能是节点级任务也可能是边级或图级任务，根据该NOI subgraph的目标任务不同，将对应任务的NOI prompt node $p_q$与该NOI subgraph中的所有NOI节点 $q$相连。如Figure 2中的虚线所示，任务描述节点NOI prompt node与该任务的目标节点用<strong>虚线</strong>相连。</p>
<p>除此之外，定义class node与NOI prompt node相连，class node用于描述NOI prompt node的分类任务的类别信息，有多少个类别就有多少个class nodes。最终可以得到所有NOI的prompt graph，如Figure 2的左边框中3个prompt graph所示。用同一个GNN模型来训练不同分类任务的Prompt Graph （任务仅仅通过NOI prompt nodes 和class nodes来做区分）。在半监督训练完成后，我们可以得到一个GNN模型，3种任务的NOI prompt nodes的embedding，以及每种任务的class nodes的embedding，以及一个从embedding映射到label的MLP。对于一个新的节点/边，首先提取他的NOI subgraph，如Figure 2的彩色框中所示，然后将新的NOI nodes连接到NOI prompt nodes，再将NOI prompt nodes连接到任务的class nodes。先用训练好的GNN应用在这个新的prompt graph上，然后可以得到每个class node的embedding, 再用训练好的MLP将class node embeddings映射到label space，从而得到该NOI属于不同类别的概率。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2023《All in One：Multi-Task Prompting for Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/allinone/</link>
      <pubDate>Tue, 20 May 2025 14:22:08 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/allinone/</guid>
      <description>KDD2023 &amp;#34;All in One：Multi-Task Prompting for Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>图神经网络的预训练任务和下游任务之间可能存在较大gap，直接将预训练模型应用在下游任务上可能会产生负迁移现象（“negative transfer”）。例如，binary edge prediction经常用于pretrain graph model。这样的预训练模型使得有边连接的节点在representation space中接近。但是下游任务可能是node-level 或graph-level tasks，下游的任务如果是节点分类任务，那么预训练模型需要针对额外的节点类别标签搜索更高维度的参数空间。如果图中相连节点的类别不同（heterophilic），那么基于edge prediction pretrained的模型会对下游任务参数负面效果。</p>
<p>为了解决上述问题，一个潜在的方向是将“pretraining and fine-tuning”拓展为“pretraining, <strong>prompting</strong>, and fine-tuning”。例如在自然语言处理中，如果要赋予预训练语言模型预测句子情感的能力（sentiment analysis），可以通过prompt来完成，而不需要优化pretrained model。</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/image.png#center" alt="你想输入的替代文字"  />
</p>
<p>以上图为例，对于一个fronzen LLM（参数固定），如果要为这个模型赋予情感分析的能力，我们可以额外训练一个最佳的prompt，训练数据为prompt parameters，要求这个prompt tokens在tasker $\phi$的优化下，生成的下一个token是正确的情感（label为excited）。即训练得到一个最佳的prompt tokens，比如训练得到的最佳prompt tokens是“I feel so [mask]”，使得frozen LLM应用在“KDD2023 will witness many high-quality papers. I feel so ” 这个句子上时，可以将下一个词预测为情感词，这样LLM在输入包含prompt tokens的情况下，可以具备预测句子情感的能力。也就是说， <strong>Prompt Learning的目的是训练得到一堆tokens，使得这些tokens与原来的context拼起来可以使得LLM具备新的能力。</strong></p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/1.png#center" alt="你想输入的替代文字"  />
</p>
<p><strong>这篇文章的目的是在图上做Prompt Learning，也就是在训练一个prompt graph，使得现有图拼上这个prompt graph后，预训练的GNN可以在新的任务上（预训练阶段没有接触过的任务上）也表现的较好。</strong> 但是在graph上做Prompt Learning存在以下挑战：</p>
<ul>
<li>自然语言处理中，prompt tokens是一个一维线性的句子，可以放在content的开头或结尾，但是在graph中，节点是非欧结构，因此如何组织prompt tokens，以及如何将graph prompts与input graph结合是一个挑战。</li>
<li>在自然语言处理中，类似于情感分析，和问答任务，这些任务都可以简单的重构为next token prediction（单词预测）的任务，所以只需要使用单词预测来训练prompt就可以。但是在图中，节点级任务、边级任务和图级任务难以统一成一种形式。因此如何将各种prompt任务统一来训练graph prompt也是一个挑战。</li>
</ul>
<p>训练好prompt token的向量化信息、连接结构、以及插入到原图的方式，然后Frozen Pretrained Model应用在这个combined graph上后，就可以为Pretrained Model赋予处理新任务的能力。</p>
<h2 id="reformulating-downstream-tasks">Reformulating Downstream Tasks</h2>
<p>将节点级和边级的下游任务统一为induced graph的标签预测问题。</p>
<p>对于节点预测任务，将它重构为图分类任务：</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/2.png#center" alt="你想输入的替代文字"  />
</p>
<p>将节点标签设置为它的 $k$-hop诱导子图的标签。</p>
<p>将边预测（存在性预测和类别预测都可以，即二分类和多分类）任务也重构为图分类任务，如下：</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/3.png#center" alt="你想输入的替代文字"  />
</p>
<h2 id="prompt-graph-design">Prompt Graph Design</h2>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/4.png#center" alt="你想输入的替代文字"  />
</p>
<p><strong>Prompt Tokens</strong> 首先Prompt graph 定义为 $\mathcal{G}_p(\mathcal{P},\mathcal{S})$，其中每个token $p_i \in \mathcal{P}$是Prompt graph中的节点，它的特征是 $\mathbf{p}_i$，维度与node features相同。</p>
<p><strong>Token Structures</strong> 每个tokens的特征 $\mathbf{p}_i$是随机初始化可学习的，然后可以通过计算相似度并用sigmoid和相应的阈值来控制 $\mathcal{G}_p$的结构。然后可以计算 $\mathbf{p}_k$和节点 $\mathbf{x}_i$之间的相似度来确定该prompt node如何将自身信息与节点 $\mathbf{x}_i$的信息结合，来赋予预训练模型处理节点 $i$相应任务的能力。
$$
w_{i k}= \begin{cases}\sigma\left(\mathrm{p}_k \cdot \mathrm{x}_i^T\right) &amp; \sigma\left(\mathrm{p}_k \cdot \mathrm{x}_i^T\right)&gt;\delta \\ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>通过 $\hat{\mathbf{x}}_i=\mathbf{x}_i+\sum_{k=1}^{|\mathcal{P}|} w_{i k} \mathbf{p}_k$ 来为节点 $i$添加提示。</p>
<h2 id="如何训练-prompt-graph使其可以作为预训练模型的有效提示multi-task-prompting-via-meta-learning">如何训练 Prompt Graph，使其可以作为预训练模型的有效提示：Multi-task Prompting via Meta Learning</h2>
<p>学习最好的Prompt，使得该Prompt可以帮助Pretrained Model更好的适应下游任务。</p>
<p><strong>Phase 1</strong> 有一些 Source Task去训练Prompt 的初始值。</p>
<p><strong>Phase 2</strong> 对Target Task做测试。</p>
<p>具体来说，首先对于要训练的prompt graph $\theta$ （即所有token的特征）让它在多个training task上训练，如下图所示，又3个节点分类的tasks，每个tasks里又自己的训练集（support set）和测试集（query set）。每个task都有一个共同的起点，在几个tasks上训练上训练后，每个tasks上有个优化后的task-specific prompt (Prompt 1, Prompt 2, Prompt 3)。</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/5.png#center" alt="你想输入的替代文字"  />
</p>
<p>在更新了之后，评估组合起来的Prompt是否会让整体的性能更好，如下图所示，把所有task-specific prompt组合起来，来看组合后的prompt是不是足够好。</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/6.png#center" alt="你想输入的替代文字"  />
</p>
<p>如果当前的Initial prompt训练出来的模型在三个tasks上不够好，那么换一个Initial Prompt，直到某个Initial Prompt在所有training tasks经过训练后都表现的较好时，就说明这是一个 <strong>较好的初始prompt (训练起点)</strong>，它适合做training tasks的初始化prompt。</p>
<p>在Meta Training 结束后，我们可以得到一个较好的prompt初始化值，即上图中的Adapt prompt initialization。</p>
<p>在Meta Testing阶段，将我们通过Training Tasks学到的最佳初始Prompt在Test tasks的support set上做微调，然后在Test Task的query set上验证prompt initialization的效果。</p>
<p><img loading="lazy" src="/posts/2025-05-20-AllinOne/7.png#center" alt="你想输入的替代文字"  />
</p>
<p>通过上面方式得到的prompt可以作为graph prompt与input graph 结合，使其具备link prediction和node classification的能力。</p>
<p><strong>可以看作时学习一个图增强，使得预训练的图模型在这个增强图上可以适用于更多任务。</strong></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2023《Evaluating GNN Performance On Unseen Graphs Without Labels》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gnn-evaluator/</link>
      <pubDate>Tue, 20 May 2025 13:45:31 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gnn-evaluator/</guid>
      <description>NeurIPS2023 &amp;#34;Evaluating GNN Performance On Unseen Graphs Without Labels&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2310.14586">Paper</a></p>
<p>目的：在一个图上训练好的GNN，在未知的testing graph上的结果由于training和test数据分布的不同，可能存在很大的不确定性。通常来说，in-service的GNN在已知graph with labels的图上训练好后，需要部署在label未知的testing graph上，但是由于label未知，无法估计GNN在testing graph上的效果。</p>
<p><img loading="lazy" src="/posts/2025-05-20-GNNEvaluator/1.png#center" alt="你想输入的替代文字"  />
</p>
<p>如上图所示，对于一个已经在训练图 $\mathcal{S}=(\mathbf{X}, \mathbf{A}, \mathbf{Y})$上well-trained &amp; fixed GNN model  $\mathrm{GNN}_{\mathcal{S}}^\star$，并且将它deploy in service。对于一个不知道label的测试图 $\mathcal{T}$，由于不知道label，如何评估GNN在该测试图上的性能是一个挑战。由于可见的训练graph和不可见的test graph的分布差异可能很大，因此GNN评估器<strong>GNNEvaluator</strong>需要充分学习多样化的图结构with diverse node context and graph structure distributions，从而可以评估不同数据分布的测试图潜在的效果。</p>
<p>假设：<strong>Covariate shift</strong> between the training graph $\mathcal{S}$ and the label-unlabeled graph $\mathcal{T}$ with respect to the label space。 即无论输入图是什么样的，输出的label space相同。</p>
<h3 id="如何生成数量足够的graph-set来训练gnnevaluator-f_theta">如何生成数量足够的graph set来训练GNNEvaluator $f_\theta$?</h3>
<p>Solution：采样一个seed subgraph $\mathcal{S}_{seed}$ from the observed training graph $\mathcal{S}$。采样原则是seed graph 要和 training graph之间满足相同的label space，所以原图中采样可以使采样图$\mathcal{S}_{seed}$的分布尽可能少的偏离原图，从而满足Covariate shift。如下图左边所示。</p>
<p><img loading="lazy" src="/posts/2025-05-20-GNNEvaluator/2.png#center" alt="你想输入的替代文字"  />
</p>
<p>在得到采样seed graph $\mathcal{S}_{seed}$后，对 $\mathcal{S}_{seed}$做 $K$次增强，其中涉及的增强包括 $\texttt{EdgeDrop}$， $\texttt{NodeDrop(Subgraph)}$， $\texttt{AttrMask}$和 $\texttt{NodeMix}$。选择哪种增强有特定的概率 $\epsilon$。基于seed graph  $\mathcal{S}_{seed}$，可以得到一个 meta-graph集合 $\mathcal{G}_{\text{meta}}=\left\{g_{\text {meta }}^i\right\}_{i=1}^K$，其中的每个meta-graph都是由seed graph 扰动而来，和原图具有相同的label space。每个meta-graph $g_{\text {meta }}^i=\left\{\mathbf{X}_{\text {meta }}^i, \mathbf{A}_{\text {meta }}^i, \mathbf{Y}_{\text {meta }}^i\right\}$，分别表示meta-graph的节点特征，结构和标签。通过这种方式可以为原图生成label space相同，但结构/特征都不相同图，拓展了差异性，基于这些图学习到的GNNEvaluator可以评估不同分布图的效果。</p>
<h3 id="如何学习well-trained-gnn在不同分布图上的差异">如何学习well-trained GNN在不同分布图上的差异？</h3>
<p>给定一个在 training graph $\mathcal{S}$上 well-trained的GNN模型 $\mathbf{Z}_{\mathcal{S}}^\star=\operatorname{GNN}_{\mathcal{S}}^\star(\mathbf{X}, \mathbf{A})$， $\mathbf{Z}_{\mathcal{S}}^\star$是学习到的原图embeddings。将这个well-trained GNN  $\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$直接应用在每个meta graph $g_{\text {meta }}^i$上：</p>
<p>$$ \mathbf{Z}_{\text {meta }}^{(i, \star)}, \hat{\mathbf{Y}}_{\text {meta }}^{(i, \star)}=\operatorname{GNN}_{\mathcal{S}}^\star\left(\mathbf{X}_{\text {meta }}^i, \mathbf{A}_{\text {meta }}^i\right)$$</p>
<p>可以得到$\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$为meta graph $g_{\text{meta}}^i$学习到的节点embedding和预测值，可以用来评估在$\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$该分布的图上的效果。若 $g_{\text {meta }}^i$中有 $M_i$个节点，那么 $\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$对该图的准确率为：
$$
y_{\mathrm{disc}}^i=\operatorname{Acc}\left(g_{\text {meta }}^i\right)=\frac{\sum_{j=1}^{M_i}\left(\hat{y}_{\operatorname{meta}(i, *)}^j==y_{\operatorname{meta}(i)}^j\right)}{M_i},
$$
表示 $\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$为该图中节点预测的平均准确率。</p>
<p>那么对于meta graph集合  $\mathcal{G}_{\text {meta }}=\left\{g_{\text {meta }}^i\right\}_{i=1}^K$中的所有图，由于这些图来组training graph，有明确的节点标签信息，因此$\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$可以计算在每个meta graph上的准确率。由于不同的meta graph的数据分布不同，所以我们可以得到 $\operatorname{GNN}_{\mathcal{S}}^*(\cdot, \cdot)$对不同数据分布图的适用性。</p>
<p>此外，在well-trained模型下，meta graph $g_{meta}^i$的表示 $\mathbf{Z}_{\text {meta }}^{(i, \star)}$和训练图表示 $\mathbf{Z}_{\mathcal{S}}^\star$之间差异用下式衡量：
$$
\mathbf{X}_{\mathrm{disc}}^i=D\left(\mathbf{Z}_{\text {meta }}^{(i, \star)}, \mathbf{Z}_{\mathcal{S}}^\star\right)=\frac{\mathbf{Z}_{\text {meta }}^{(i, \star)} \mathbf{Z}_{\mathcal{S}}^{\star \mathrm{~T}}}{\left|\left|\mathbf{Z}_{\text {meta }}^{(i, \star)}\right|\right|_2 \cdot\left|\left|\mathbf{Z}_{\mathcal{S}}^\star\right|\right|_2}
$$</p>
<h3 id="gnnevaluator如何估计无标签图的准确率">GNNEvaluator如何估计无标签图的准确率？</h3>
<p>训练GNNEvaluator:
$$
\min_\phi \sum_{i=1}^K \mathcal{L}_{\mathrm{reg}}\left(f_{\boldsymbol{\phi}}\left(\mathbf{X}_{\mathrm{disc}}^i, \mathbf{A}_{\mathrm{disc}}^i\right), y_{\mathrm{disc}}^i\right)
$$
对于每个meta graph $g_{\text {meta }}^i=\left\{\mathbf{X}_{\text {meta }}^i, \mathbf{A}_{\text {meta }}^i, \mathbf{Y}_{\text {meta }}^i\right\}$，基于结构 $\mathbf{A}_{\text {meta }}^i$和差异属性 $\mathbf{X}_{\text {meta }}^i$使用逻辑回归来预测准确率 $y_{\mathrm{disc}}^i$。损失函数采用MSE回归损失。</p>
<p>测试阶段，对于一个label不可见的预测图 $\mathcal{T} = (\mathbf{X}^\prime, \mathbf{A}^\prime)$，将其带入 $\operatorname{GNN}_{\mathcal{S}}^\star(\cdot, \cdot)$得到它的表示 $\mathbf{Z}_{\mathcal{T}}^\star$，并与 $\mathbf{Z}_{\mathcal{S}}^\star$一同计算得到差异节点特征$\mathbf{X}_{\text {disc }}^{\mathcal{T}}$。再将$\mathbf{X}_{\text {disc }}^{\mathcal{T}}$和 $\mathbf{A}^\prime$带入 $f_\phi (\cdot,\cdot)$中，得到估计准确率 $\operatorname{Acc}(\mathcal{T})=\hat{y}_{\text {dist }}^{\mathcal{T}}$。该过程无需任何测试集标签参与评估。</p>
<p>图的差异与图的准确率匹配，所以可以用图差异来衡量与真实准确率的差异。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2023《Personalized Subgraph Federated Learning》Reading Notes</title>
      <link>https://JhuoW.github.io/posts/fedpub/</link>
      <pubDate>Mon, 19 May 2025 13:42:56 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fedpub/</guid>
      <description>ICML2023 &amp;#34;Personalized Subgraph Federated Learning&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>本文旨在解决不同子图作为不同的client，由于client是locally accessible，所以client间存在的missing edges无法被server捕获的情况。与FedSage仅仅基于自身连接来expand missing neighbors（并且最小化生成neighbor和其他所有子图的距离） 的missing neighbors生成方式不同，本文提出的FedPub考虑了子图的社区结构，即不同社区的子图相互之间的边连接关系应该较弱，处于同一个社区的子图应存在更强的边连接关系。</p>
<h2 id="personalized-subgraph-fl-overall">Personalized Subgraph FL (Overall)</h2>
<p>对于每个子图 $G_i \in \mathcal{G}$，传统的方法优化方式是学习global optimal parameters $\bar{\theta}$，使得该参数在所有clients上的总loss最小： $\min_{\overline{\theta}} \sum_{G_i \subset \mathcal{G}} \mathcal{L}\left(G_i ; \overline{\theta}\right)$。但是由于处在不同社区中的子图异质性很严重，并且不同社区的子图间的edges也很稀疏，这种强异质性很难学习到一个对所有子图都最优的 $\bar{\theta}$。因此本文提出，在同一个community的子图间共享参数，不同community的子图不共享参数，而不是使用全局通用参数 $\bar{\theta}$。因此，Personalized Subgraph FL的形式如下：</p>
<p>$$
\begin{aligned}
&amp; \min_{\left\{\boldsymbol{\theta}_i, \boldsymbol{\mu}_i\right\}_{i=1}^K} \sum_{G_i \subseteq \mathcal{G}} \mathcal{L}\left(G_i ; \boldsymbol{\theta}_i, \boldsymbol{\mu}_i\right), \quad \boldsymbol{\theta}_i \leftarrow \boldsymbol{\mu}_i \odot\left(\sum_{j=1}^K \alpha_{i j} \boldsymbol{\theta}_j\right) \\
&amp; \text { with } \alpha_{i k} \gg \alpha_{i l} \text { for } G_k \subseteq C \text { and } G_l \nsubseteq C
\end{aligned}
$$
其中 $K$是client数量， $\theta_i$是属于community $C$ 的client $G_i$的可训练权重， $\alpha_{ij}$是client $i$ 和 $j$的权重聚合coefficient。如果client $k$和 $i$属于同一个community，而client $l$和 $i$不属于同一个community，那么它们关于 $i$的参数聚合系数 $\alpha_{i k} \gg \alpha_{i l}$。通过这种方式模型可以隐式地考虑不同subgraph之间的关系。</p>
<p>虽然 $\alpha_{i,:}$指示了client之间的相关程度，但是未能指示那些参数与 $i$ 相关，这里用一个size和 GNN 权重相同的mask矩阵 $\mu_i$来过滤无关权重。</p>
<h2 id="fedpub-detailed">FedPub (Detailed)</h2>
<p>为了实现上式，首先要基于子图之间的相似度为图中的子图分配communities。由于每个子图是local accessibility的，服务器只能接收到每个client的GNN参数，所以需要从子图的GNN模型参数中获得辅助信息来计算子图之间的相似度。</p>
<h3 id="functional-embeddings-for-subgraph-similarities"><strong>Functional Embeddings for Subgraph Similarities</strong></h3>
<p>为了解决上述问题，本文通过向所有 local GNN 模型提供相同的输入，然后使用它们的输出计算相似度来测量 GNN 的functional similarity。基于stochastic block model构建一个有5个communities，每个community 100个节点的random community graph $\tilde{G}=(\tilde{\mathcal{V}}, \tilde{\mathcal{E}})$。令 $\tilde{\boldsymbol{h}}_i=\operatorname{AVG}\left(f\left(\tilde{G} ; \boldsymbol{\theta}_i\right)\right)$表示client $i$上的局部模型对 $\tilde{G}$的所有node embedding输出的均值。那么client $i$和 $j$之间的GNN functional similarity 可以用下式表示：</p>
<p>$$ S(i, j)=\frac{\tilde{\boldsymbol{h}}_i \cdot \tilde{\boldsymbol{h}}_j}{\left|\left|\tilde{\boldsymbol{h}}_i\right|\right|\left|\left|\tilde{\boldsymbol{h}}_j\right|\right|} $$</p>
<h3 id="personalized-weight-aggregation"><strong>Personalized Weight Aggregation</strong></h3>
<p>在得到子图之间的相似度衡量标准后，对于client $i$，可以计算client $j$与它的normalized subgraph similarity：</p>
<p>$$ \alpha_{i j}=\frac{\exp (\tau \cdot S(i, j))}{\sum_k \exp (\tau \cdot S(i, k))} $$</p>
<p>其中 $\tau$是一个相似度放缩参数。然后在同一个社区中的client尽可能共享更多的参数：</p>
<p>$$\overline{\boldsymbol{\theta}}_i \leftarrow \sum_{j=1}^K \alpha_{i j} \cdot \boldsymbol{\theta}_j$$</p>
<h3 id="adaptive-weight-masking">Adaptive Weight Masking</h3>
<p>上面加权聚合所有client local GNN参数的方式得到 $\overline{\boldsymbol{\theta}}_i$只考虑到how much each local model from other clients is relevant to $i$，因为 $\alpha_{i j}$仅用来衡量client $j$与client $i$在社区层面的相关程度，然后就依据相关性聚合来自client $j$的所有参数 $\boldsymbol{\theta}_j$。并没有考虑到聚合而来的参数 $\overline{\boldsymbol{\theta}}_i$中，哪些与client $i$是相关的。因此这里用一个可学习的参数mask $\boldsymbol{\mu}_i$来从 $\overline{\boldsymbol{\theta}}_i$中过滤出与client $i$有关的参数：</p>
<p>$$ \boldsymbol{\theta}_i=\overline{\boldsymbol{\theta}}_i \odot \boldsymbol{\mu}_i $$</p>
<p>这里得到的 $\boldsymbol{\theta}_i$作为client $i$上local GNN的参数。对于local client的训练过程，由于 $\boldsymbol{\mu}_i$仅在本地数据上训练，并不与其他client共享参数，因此容易与本地数据过拟合，即完全mask掉所有其他client的信息。为了缓解该问题，在局部损失函数上添加一个proximal term来使得 $\boldsymbol{\theta}_i$保留更多全局模型信息：
$$
\min _{\left(\boldsymbol{\theta}_i, \boldsymbol{\mu}_i\right)} \mathcal{L}\left(G_i ; \boldsymbol{\theta}_i, \boldsymbol{\mu}_i\right)+\lambda_1\left|\left|\boldsymbol{\mu}_i\right|\right|_1+\lambda_2\left|\left|\boldsymbol{\theta}_i-\overline{\boldsymbol{\theta}}_i\right|\right|_2^2
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021《Subgraph Federated Learning with Missing Neighbor Generation》Reading Notes</title>
      <link>https://JhuoW.github.io/posts/fedsage/</link>
      <pubDate>Sun, 18 May 2025 14:59:38 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fedsage/</guid>
      <description>NeurIPS2021 &amp;#34;Subgraph Federated Learning with Missing Neighbor Generation&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Subgraph Federated Learning旨在多个分布式的子图上训练一个图模型，并且子图之间没有数据共享。面临2个挑战：</p>
<p><strong>Q1:</strong> 如何将各个子图上的模型融合成一个全局可用的模型，使其可以处理来自全局图的任何请求？</p>
<p><strong>A1:</strong> 针对该挑战，本文提出FedSage将GraphSage和FedAvg结合。</p>
<p><strong>Q2:</strong> 如何处理local subgraph之间缺失的边？</p>
<p><strong>A2:</strong> 在FedSage的基础上添加一个<strong>邻居生成器</strong>。具体来说，为了得到missing neighbor generator，每个client首先通过随机移除一些节点和他们的边来<strong>损坏subgraph</strong>，然后基于损坏的图来和移除的节点来训练邻居生成器。邻居生成器不仅为子图中的每个节点生成missing neighbor number，也为每个节点生产missing neighbor features。然后用生成的节点邻居去修补subgraph，所谓模拟的跨子图交互。最后在修补好的子图上使用FedSage。</p>
<h2 id="collaborative-learning-on-isolated-subgraphsfedsage">Collaborative Learning on Isolated Subgraphs：FedSage</h2>
<p>对于一个被查询节点 $v\in V$，一个全局 $K$层GraphSage分类器 $F$通过融合 $v$和它的 $K$跳邻居来得到节点的预测值，其中每层的可学习参数为 $\phi = \{\phi^k\}^K_{k = 1}$，其中 $\phi^k$表示第 $k$层的权重矩阵。考虑一个子图 $G_i = \{V_i, E_i, X_i\}$， GraphSage通过一下式子计算节点 $v \in V_i$的第 $k \in [K]$层表示：</p>
<p>$$
h_v^k=\sigma\left(\phi^k \cdot\left(h_v^{k-1} || \operatorname{Agg}\left(\left\{h_u^{k-1}, \forall u \in \mathcal{N}_{G_i}(v)\right\}\right)\right)\right)
$$</p>
<p>其中 $\mathcal{N}_{G_i}(v)$表示节点 $v$在子图 $G_i$中的邻居，上式表示邻居节点聚合后和中心节点拼接，然后再用参数 $\phi^k$做变换后得到中心节点的表示。</p>
<p>在FedSage中，全局模型 $F$的参数 $\phi$通过 $e_c$次训练迭代得到。在每次训练迭代 $t$中，每个client $D_i$首先拷贝全局参数 $\phi$到本地，作为本地模型的初始参数，再基于自身的数据更新参数 $\phi_i$：</p>
<p>$$
\phi_i \leftarrow\phi-\eta \nabla \ell\left(\phi \mid\left\{\left(G_i(v), y_v\right) \mid v \in V_i^t\right\}\right)
$$</p>
<p>其中 $\eta$是学习率。然后中心服务器 $S$将所有 $M$个客户端上的最新参数 $\{\phi_i | i \in [M]\}$聚合并做平均，作为中心服务器上的模型参数。再下一次迭代时，将新的参数广播到所有客户端上进行新一轮的优化。</p>
<h2 id="fedsage">FedSage+</h2>
<h3 id="missing-neighbor-generator-neighgen">Missing Neighbor Generator (NeighGen)</h3>
<p><img loading="lazy" src="/posts/2025-05-18-Fedsage/image.png#center" alt=""  />
</p>
<p>邻居生成器NeighGen由编码器 $H^e$和生成器 $H^g$组成：</p>
<p><strong>编码器 $H^e$</strong>: 对于一个输入subgraph  $G_i = (V_i, E_i, X_i)$，一个由 $\theta^e$参数化的GNN模型为子图中的每个节点学习embeddings $Z_i = \{z_v | v \in V_i\}$ 。</p>
<p><strong>生成器 $H^g$</strong>: 基于node embedding生成它的missing neighbors。如图2所示， $H^g$包含两个模块dGen和fGen。其中dGen是一个由 $\theta^d$参数化的线性回归模型，用来预测给定节点的missing neighbors数量：
$$
\tilde{n}_v=\sigma\left(MLP_{\theta^d} ( z_v)\right) \in \mathbb{R}
$$</p>
<p>基于子图中的节点 $v$的embedding $z_v$生成它的预测missing neighbors数量 $\tilde{n}_v$。</p>
<p>fGen是一个由 $\theta^f$参数化的MLP用来基于节点 $v$的embedding $z_v$生成它的missing neighbor features。由于fGen为节点 $v$生成missing neighbor特征的个数取决于 $v$由多少个predicted missing neighbors，因此这里用变分的方式来基于 $z_v$生成它的missing neighbor features。具体来说，fGen有一个Gaussian noise生成器 $\mathbf{N}(0,1)$来生成随机噪声，以及一个随机采样器 $R$来从生成的邻居中采样 $\tilde{n}_v$个邻居特征：</p>
<p>$$
\widetilde{x}_v=R\left(\sigma\left(MLP_{\theta^f}\left(z_v+\mathbf{N}(0,1)\right)\right), \widetilde{n}_v\right)
$$</p>
<p>通过这种方式，可以基于 $z_v$定义一个随机邻居生成器。</p>
<p>图修补仿真：假设一个client中只有一个特定集合的节点具有跨subgraph missing neighbors，因此可以通过图损坏（impairing）和修复（mending）过程来模拟missing neighbor的情况。具体来说，为了训练NeighGen，对于每个subgraph $G_i$，随机取出 $h%$的节点和与这些节点相关联的边作为simulated missing neighbors of $G_i$，由此可以得到一个损坏后的图 $\bar{G}_i=\left\{\bar{V}_i, \bar{E}_i, \bar{X}_i\right\}$。对于 $\bar{G}_i$中的每个节点 $v$，它的真实被移除的邻居数量为 $n_v$， 使用NeighGen预测它的missing neighbor数量表示为 $\tilde{n}_v$，NeighGen的优化目标是使得预测的missing neighbors数量和被移除的neighbors数量相似：</p>
<p>$$
\mathcal{L}^d = \frac{1}{|\bar{V}_i|} \sum_{v \in \bar{V}_i} L_1^S(\tilde{n}_v - n_v)
$$</p>
<p>其中 $L_1^S (\cdot, \cdot)$是 L1 distance。对于特征，要求对于每个节点 $v \in \bar{V}_i$，它在 $G_i$中被移除的邻居表示为 $\mathcal{N}_{G_i}(v) \cap V_i^h$，其中 $\mathcal{N}_{G_i}(v)$表示 $v$在 $G_i$中的真实邻居集合， $V_i^h$表示impair后的邻居集合，因此 $\mathcal{N}_{G_i}(v) \cap V_i^h$共有 $n_v$个被移除邻居，对于节点 $v$的所有被移除邻居 $u \in \mathcal{N}_{G_i}(v) \cap V_i^h$， 要求真实被移除的邻居特征 $x_u$与为 $v$生成的邻居 $\widetilde{x}_v$之间的距离应该最小：</p>
<p>$$
\mathcal{L}^f = \frac{1}{\left|\bar{V}_i\right|} \sum_{v \in \bar{V}_i} \sum_{p \in\left[\widetilde{n}_v\right]} \min _{u \in \mathcal{N}_{G_i}(v) \cap V_i^h}\left(\left|\left|\widetilde{x}_v^p-x_u\right|\right|_2^2\right)
$$</p>
<p>其中 $\widetilde{x}_v^p$表示NeighGen为节点 $v$生成的第 $p$个missing neighbor。</p>
<p>为了使模型生成的邻居特征可以模拟跨subgraph的特性，因此将特征loss $\mathcal{L}^f$改写为如下形式：</p>
<p>$$
\mathcal{L}_i^f=\frac{1}{\left|\bar{V}_i\right|} \sum_{v \in \bar{V}_i} \sum_{p \in\left[\tilde{n}_v\right]}\left(\min_{u \in \mathcal{N}_{G_i}(v) \cap V_i^h}\left(\left|\left|\widetilde{x}_v^p-x_u\right|\right|_2^2\right)+\alpha \sum_{j \in[M] / i} \min_{u \in V_j}\left(\left|\left|H_i^g\left(z_v\right)^p-x_u\right|\right|_2^2\right)\right)
$$</p>
<p>其中后面一项 $\min _{u \in V_j}\left(\left|\left|H_i^g\left(z_v\right)^p-x_u\right|\right|_2^2\right)$中的 $x_u$表示另一个子图 $G_j$中的节点特征 $H_i^g\left(z_v\right)^p$表示NeighGen为 $G_i$中的节点 $v$生成的missing neighbor feature。<strong>因此这一项表示生成的新邻居要和其他子图中与它最相似的邻居尽可能一致，也就是生成的邻居要尽可能接近其他子图中的节点特征。</strong></p>
<p>但是这存在另一个问题， $x_u$在 $G_j$中，不符合FL的设定。为了解决该问题，这里将 $z_v$ send到client $D_j$中来计算 $\min _{u \in V_j}\left(\left|\left|H_i^g\left(z_v\right)^p-x_u\right|\right|_2^2\right)$，并计算生成器的梯度，然后将梯度send back到 $D_i$。因为编码器 $H_i^e$在 $D_i$上是local的，所以 $D_j$无法通过 $z_v$恢复出 $v$的原始特征。</p>
<p><strong>注意：<strong>从上面的 $\mathcal{L}^f$的式子第二项可以看出，由 $z_v$生成的邻居embedding $H_i^g\left(z_v\right)$要和</strong>所有其他子图中的所有节点</strong>的相似度尽可能高。也就是认为 $v$的missing neighbors需要来自于其他所有subgraphs。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>LLMs and Graphs</title>
      <link>https://JhuoW.github.io/posts/graphllm/</link>
      <pubDate>Sat, 25 May 2024 01:04:50 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/graphllm/</guid>
      <description>Large Language Model, GNNs, and Foundation Models</description>
      <content:encoded><![CDATA[<h1 id="1-harnessing-explanations-llm-to-lm-interpreter-for-enhanced-text-attributed-graph-representation-learning-tape">1. Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning (TAPE)</h1>
<p><strong>Shallow model:</strong> Encoding the textual attributes using shallow or hand-crafted features such as skip-gram or bag-of-words (BoW) which used in PgG and DGL <strong>are limited in the complexity of the semantic features they can capture.</strong></p>
<p><strong>LM:</strong> 指相对较小并且可以被fine-tune的模型。GIANT <strong>fine-tune</strong> an LM using neighborhood prediction task （在neighborhood prediction task上来微调LM模型）. GLEM <strong>fine-tune</strong> an LM to predict the label distribution from GNN&rsquo;s output (GLEM 用GNN预测的伪标签作为监督信号，让LM来fine tune 节点的文本表示)。这些工作需要大量的计算资源，并且由于要微调模型的参数，所以选取的LM相对较小，比如BERT和DeBERTa，因此缺乏LLM的推理能力。</p>
<p>LLMs refer to very large language models such as GPT-3/4.</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/TAPE.png#center" alt="TAPE"  />
</p>
<p><strong>The present work: LLM augmentation using explanations:</strong> 使用解释作为node feature。通过LLM来解释它的预测，这些解释传达了text与prediction的相关知识和LLM的推理步骤，这些解释信息更易于小LM模型吸收消化。如上图所示，首先绿框中的节点text属性通过自定义的prompt来询问LLM，比如GPT-3.5，让GPT来生成关于文本类别的<strong>预测排名list</strong>（黄色框所示），并且<strong>提供这些得到这些预测的解释理由</strong>。接着，原始text，LLM的预测，以及解释共同用来fine-tune LM，比如BERT或DeBERTa。然后，LM将他们转化为节点的features用于下游预测。</p>
<h2 id="formalization">Formalization</h2>
<p><strong>LM for text classification</strong>
$$
h_n=\mathbf{L M}\left(s_n\right) \in \mathbb{R}^d
$$
其中$s_n \in \mathcal{D}^{L_n}$是节点$n$的文本属性。LM是已经被预训练的模型如BERT和DeBERTa。LM可以将文本属性编码为文本的表示向量$h_n$。</p>
<p><strong>LLM and prompting</strong></p>
<p>输入token序列 $x=\left(x_1, x_2, \ldots, x_q\right)$，目标是输出token序列 $y=\left(y_1, y_2, \ldots, y_m\right)$。并且在输入token序列中加入prompt $p$来对输出施加约束。LLM旨在优化一下条件概率：
$$
p(y \mid \hat{x})=\prod_{i=1}^m p\left(y_i \mid y_{&lt;i}, \hat{x}\right)
$$
其中$\hat{x}=\left(p, x_1, x_2, \ldots, x_q\right)$是使用prompt约束的input token sequence。</p>
<h2 id="tape">TAPE</h2>
<h3 id="使用llms生成预测和解释">使用LLMs生成预测和解释</h3>
<p>LLMs的prompt包括文章的title和abstract，并要求LLMs预测paper的一个或多个类别标签，并且将这些类别标签按从高到低的概率排序，并且要求LLMs提供预测的解释理由，完整prompt如下图所示：</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/tape_prompt.png#center" alt="TAPE"  />
</p>
<p>Abstract输入节点对应paper的摘要，Title输入paper的题目，question则要求模型输出一个或多个paper的预测类别标签并按照可能性排序，同时给出预测的解释。</p>
<h3 id="fine-tuning-lm-interpreter-and-node-feature-extraction">Fine-Tuning LM Interpreter and Node Feature Extraction</h3>
<p>将LM作为LLM得到文本解释的“理解器”。给定两个预训练好的LMs $\mathrm{LM}_{\text {orig }}$ 和$\mathrm{LM}_{\text {expl }}$，他们的输入分别是原始文本特征$s^{\text {orig }}$和该文本特征的解释$s^{\text {expl }}$，这样我们可以分别得到原始文本和解释的text embeddings:
$$
h_{\text {orig }}=\mathrm{LM}_{\text {orig }}\left(s^{\text {orig }}\right) \in \mathbb{R}^{N \times d}, \quad h_{\text {expl }}=\mathrm{LM}_{\text {expl }}\left(s^{\text {expl }}\right) \in \mathbb{R}^{N \times d}
$$
然后要对LM进行Fine-tuning使其与下游任务。首先使用MLP将原始文本和解释的text embedding分别映射到标签空间：
$$
y_{\text {orig }}=\operatorname{MLP}_{\text {orig }}\left(h_{\text {orig }}\right) \in \mathbb{R}^{N \times C}, \quad y_{\text {expl }}=\operatorname{MLP}_{\text {expl }}\left(h_{\text {expl }}\right) \in \mathbb{R}^{N \times C}
$$
通过最小化分类的cross-entropy loss来对LM进行fine-tuning，从而使$\mathrm{LM}_{\text {orig }}$ 和$\mathrm{LM}_{\text {expl }}$可以分别学习到文本和解释与标签之间的关联（什么样的原始文本对应于什么样的标签、什么样的解释对应于什么样的标签）。</p>
<p><strong>Ranked prediction features</strong> LLM同时也给出了对于每个节点文本的类别可能性排名，同样也是有价值的信息。假设每个节点有$C=5$个可能的类别数，对这些类别分别做one-hot编码，节点$i$排名第1的类别为类别4，那么$p_{i,1} = [0,0,0,1,0]$，那么LLM为节点$i$预测的top-$k$个label可以拼接成一个$kC$维的向量。然后通过一个线性变换:
$$
h_{\text{pred}} = \operatorname{MLP}_{\text {pred }}(\operatorname{Concat}(p_{i,1}, \cdots, p_{i,k})) \in \mathbb{R}^{N\times d_P}
$$
可以得到每个节点的排序预测特征，如Figure 1中的$h_{\text{pred}}$所示。</p>
<p><strong>TAPE node feature: $\{[h_{\text {orig }}, h_{\text {expl }}, h_{\text{pred}}]\}$</strong>。其中$h_{\text {orig }}$和$h_{\text {expl }}$是通过LM 对下游任务标签做fine-tuning后的文本特征和解释特征，用来简历原始文本和解释与label之间的联系，$h_{\text{pred}}$是LLMs预测类别标签编码。</p>
<h3 id="gnn-training-with-tape-features">GNN Training with TAPE features</h3>
<p>对于TAPE的三种特征：LM fine-tuning的原始text embedding $h_{\text {orig }}$, LM fine-tuning的解释text embedding $h_{\text {expl }}$，以及LLM的prediction embedding $h_{\text{pred}}$，使用3个GNN模型分别预测labels:
$$
\begin{aligned}
\hat{y}_{\text{orig}} &amp;= \operatorname{GNN}_{\text{orig}}(h_{\text {orig }}, A) \in \mathbb{R}^{N \times C}, \\
\hat{y}_{\text{expl}} &amp;= \operatorname{GNN}_{\text{expl}}(h_{\text {expl }}, A) \in \mathbb{R}^{N \times C}, \\
\hat{y}_{\text{pred}} &amp;= \operatorname{GNN}_{\text{pred}}(h_{\text {pred }}, A) \in \mathbb{R}^{N \times C}
\end{aligned}
$$</p>
<p>然后基于不同特征得到的预测取平均可以得到模型最终的预测label：
$$
\hat{y}=\operatorname{mean}\left(\hat{y}_{\text {orig }}, \hat{y}_{\text {expl }}, \hat{y}_{\text {pred }}\right) \in \mathbb{R}^{N \times C}
$$</p>
<h1 id="2-exploring-the-potential-of-large-language-models-llms-in-learning-on-graphs">2. Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</h1>
<p><a href="https://arxiv.org/abs/2307.03393">paper</a></p>
<p>Q1: 能否利用LLMs来弥补图神经网络对contextualized knowledge和semantic comprehension理解不足的缺陷？</p>
<p>Q2: LLM能否独立运行于图结构任务？</p>
<h2 id="llms">LLMs</h2>
<h3 id="embedding-visible-llms">Embedding-visible LLMs</h3>
<p>可以获得words, sentences, documents的具体representations（embeddings），如BERT, Sentence-BERT和Deberta。</p>
<h3 id="embedding-invisible-llms">Embedding-invisible LLMs</h3>
<p>用户无法获取和操作embeddings，通常部署在web服务上，如ChatGPT，只能通过text来进行交互</p>
<h3 id="detailed-four-types-of-llms">Detailed four types of LLMs</h3>
<p><strong>Pre-trained Languagge Models (PLMs):</strong> 指相对较小的LLMs，比如Bert和Deberta，并且可以根据下游数据集进行fine-tuning，比如在下游数据集上fine-tune  Deberta然后取最后一个hidden state的embeddings作为text embedding。</p>
<p><strong>Deep Sentence Embedding Models:</strong> 使用PLMs作为base encoders，并且将训练好的PLMs进一步进行监督或对比学习<strong>预训练</strong>。这样的模型通常不需要Fine-tuning。</p>
<p><strong>Large Language Models:</strong> 与PLM相比，LLMs具有更强的能力和更多数量级的参数。</p>
<h2 id="llms-as-enhancers">LLMs-as-Enhancers</h2>
<p><strong>利用LLMs来增强节点的文本属性特征，然后用GNN生成预测。</strong></p>
<p>How LLMs can enhance GNNs by leveraging their <strong>extensive knowledge</strong> and <strong>semantic comprehension</strong> capability?</p>
<p>Challenge: 不同的LLMs能力不同，越强大的模型有更多使用限制，因此需要对不同的LLMs针对性设计使用策略来充分利用他们的能力。</p>
<h3 id="feature-level-enhancement">Feature-level Enhancement</h3>
<h4 id="1-cascading-structure-级联结构">1. Cascading Structure 级联结构</h4>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/cascading.png#center" alt="Cascading Structure"  />
</p>
<p>先试用embedding-visible的LLMs来对dataset中的text attribute做fine-tuning，然后生成每个节点的文本特征的embeddings，然后将这些embeddings作为node features，与图结构一起数据GNN中来训练GNN模型。</p>
<h4 id="2-iterative-structure-迭代结构">2. Iterative Structure 迭代结构</h4>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/iterative.png#center" alt="iterative Structure"  />
</p>
<p>如GLEM[1]，在E步根据真实标签和GNN预测的伪标签来训练PLM，在M步根据真实标签和PLM预测的伪标签和PLM学到的text embedding作为node feature来训练GNN模型，然后训练好的GNN模型和PLM模型都可以用来作为节点标签预测器。</p>
<h4 id="node-classification-comparison">Node Classification Comparison</h4>
<p>实验结果来看，</p>
<ol>
<li>在下游数据集上fine-tuned PLM模型取最后一层作为text embedding，然后GNN作为predictor的效果来看，fine-tuned PLM并不比简单点TF-IDF强。对于不同的text embedding方式（Fine-tuned PLM，PLM without fine-tuning，online sentence embedding）GNN的表现各不相同。</li>
<li>在监督训练数据较少的情况下，fine-tuned PLM和迭代结构的GLEM学习到的text embedding用到GNN后，得到的结果比普通的TF-IDF差。</li>
<li>使用Deep Sentence Embedding Models 如sentence-bert学习到的text embedding + GNN predictor的效果较好。</li>
<li>LLama的效果弱于deep sentence embedding models，说明简单的增加参数并不能生成对GNN有用的text embedding。</li>
</ol>
<h3 id="text-level-enhancement">Text-level Enhancement</h3>
<h4 id="1-tape">1. TAPE</h4>
<p>使用文本和LLM解释来fine-tuning PLM，LLM的分类解释和分类排序作为增强text embedding。</p>
<h4 id="2-knowledge-enhanced-augmentation-kea">2. Knowledge-Enhanced Augmentation (KEA)</h4>
<p>使用额外的knowledge来增强PLM。</p>
<h4 id="node-classification-comparison-1">Node Classification Comparison</h4>
<p>实验结果来看，</p>
<ol>
<li>TAPE的效果主要受益于LLMs生成的文本解释。</li>
<li>TAPE原文中采用PLM （Deberta）作为LM，将text attribute和explanation以及任务的label用来fine-tune PLM。而local sentence embedding model e5-large不fine-tune 直接用text attribute以及LLM的explanation来输出embeddings，相比于TAPE中使用的fine-tuned PLM，e5得到了更好的效果。</li>
</ol>
<h2 id="llms-as-predictors">LLMs-as-Predictors</h2>
<p><strong>LLMs作为独立的predictor</strong></p>
<p>How LLMs can be adapted to explicit graph structures as a <strong>predictor</strong>?</p>
<p>Challenge: 如何设计prompt使得LLM可以处理图中的structure和attribute信息。</p>
<p>直接使用LLM来预测节点类别标签可以使用以下几种prompt策略：</p>
<ol>
<li><strong>Zero-shot prompts:</strong> 给定单一节点的文本信息，让LLM预测它的类别标签，如下面的prompt所示。</li>
</ol>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/zero_shot_prompt.png#center" alt="zero-shot prompt"  />
</p>
<p>​       其中，Paper是一个节点的text attribute，Task提供可能的类别标签，然后prompt要求LLM输出一个最可能的类别。</p>
<ol start="2">
<li><strong>Few-shot prompts:</strong> 如下图所示，先按照zero-shot的形式给出一些node samples的 prompts，<strong>以及这些samples的ground-truth标签</strong>。最后，给出目标节点的prompt，要求LLMs输出节点的类别。</li>
</ol>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/few_shot_prompt.png#center" alt="few-shot prompt"  />
</p>
<ol start="3">
<li><strong>Zero-shot prompts with CoT (Chain-of-Thoughts):</strong> 基于zero-shot prompt，在prompt中进一步要求LLM生成思考过程，也就是<strong>Think it step by step and output the reason in one sentence</strong>  (用一句话来概括推理步骤)。</li>
</ol>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/zero_shot_cot.png#center" alt="zero-shot cot"  />
</p>
<ol start="4">
<li><strong>Few-shot prompts with CoT:</strong> 使用第三个prompt的zero-shot prompts with CoT来分别为多个sample生成推理步骤的sentences，然后将这些这些samples的文本内容、Ground-truth标签和CoT process共同作为prompt，并且要求LLM为当前的目标node输出预测标签以及CoT。</li>
</ol>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/few_shot_cot.png#center" alt="few-shot cot"  />
</p>
<h1 id="3-one-for-all-towards-training-one-graph-model-for-all-classification-tasks">3. One for All: Towards Training One Graph Model for All Classification Tasks</h1>
<p><strong>What is in-context learning?[2]</strong></p>
<p>&ldquo;In-Context Learning is a way to use LLMs to learn tasks given only a few examples. During in-context learning, we give the LM a prompt that consists of a  list of input-output pairs that demonstrate how to perform a task. At  the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next  tokens. For example, to answer the two prompts below, the model needs to examine the training examples to figure out the input distribution  (financial or general news), output distribution (Positive/Negative or  topic), input-output mapping (sentiment or topic classification), and  the formatting. &quot;</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/icl.gif#center" alt="ICL"  />
</p>
<p>如上图（from [2]）所示，在prompt中包含一些上下文input-output pairs来演示（demonstrate）如何执行任务。在这些prompt的最后添加text input token来让LLM学习上下文中的演示来执行text input token的任务。</p>
<h2 id="challenge">Challenge</h2>
<p>Foundation Model指用单一模型来解决多个任务。但是Graph Foundation Model面临以下挑战：（1）不同来源的图数据在feature表征上通常完全不同。比如molecular graph中的节点特征是其中原子nominal feature的索引，e-commerce networks中的节点特征通常用Bag-of-Word来编码。这些不同来源的图数据特征维度、语义信息和尺度的差别都很大，几乎不可能用同一个模型来学习他们的表示。（2）不同的下游任务涉及图的不同部分（节点级、边级、图级）需要不同的策略和方法来学习表示。（3）如何设计统一的模型来实现跨领域和in-context learning是不确定的。</p>
<h2 id="ofa-one-for-all">OFA (One-for-All)</h2>
<h3 id="将不同domian的图转化为统一的tag形式">将不同Domian的图转化为统一的TAG形式</h3>
<p>用human-interpretable language来描述节点和边的属性，从而使LLM可以将这些text attribute编码到同一个空间。具体来说，通过以下方式来生成每个节点的text feature:</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/node_text_feature.png#center" alt="text feature"  />
</p>
<p>将不同domain的graph的节点属性用统一形式的文本来描述，如上图中，对于一个节点的特征，它的文本描述以<strong>Feature node</strong>开头，后面的每个$&lt;\text{feature describe}&gt;:&lt;\text{feature content}&gt;$是该节点的一个type-content文本对。如对于一个molecule graph，其中的一个节点是原子，那么它的type是<strong>Atom</strong>，它的content是该原子的属性文本描述<strong>Carbon, Atomic number 6, helix chirality</strong>。同理，边的text feature以<strong>Feature edge</strong>开头进行文本描述。</p>
<p>在用以上方式得到节点$v_i$的text feature $s_{v_i}$和边$e_{ij}$的text feature $s_{e_{ij}}$后，用LLM（这里用sentence transformer）来将每个节点和边的text feature编码为vector embeddings：
$$
x_i = \operatorname{LLM}(s_{v_i}), \quad x_{ij} = \operatorname{LLM}(s_{e_{ij}})
$$</p>
<h3 id="用nodes-of-interest-noi来统一不同的图任务">用Nodes-of-Interest (NOI)来统一不同的图任务</h3>
<p>图上的任务主要分为3中：node-level tasks，link-level tasks和graph-level tasks。如果要用一个模型来处理这些任务的话，<strong>需要将这些任务统一为一个任务以便于在图数据上训练</strong>。</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/NOI.png#center" alt="NOI"  />
</p>
<p>Nodes-of-Interest (<strong>NOI</strong>)指的是一个任务的<strong>目标节点</strong>，如上图的蓝色节点所示，表示为$\mathcal{T}$。对于节点级任务，NOI是待预测节点集合，边级任务NOI是待预测是否有边的节点对，图级任务NOI是待预测图中的所有节点。若一个NOI节点$v$的$h$-hop局部子图表示为$\mathcal{S}_h(v)$，那么NOI中所有目标节点的局部子图共同构成了一个<strong>NOI subgraph</strong>，表示为$\mathcal{G}_h (\mathcal{T})$。
$$
\mathcal{G}_h(\mathcal{T})=\bigcup_{v \in \mathcal{T}} \mathcal{S}_h(v)=\left\{\bigcup_{v \in \mathcal{T}} \mathcal{V}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{E}_v^h, \bigcup_{v \in \mathcal{T}} \mathcal{R}_v^h\right\}   \quad \text{NOI中所有节点的局部子图共同构成}
$$
<strong>NOI prompt node</strong>：定义NOI prompt node，该节点的文本信息用来描述任务，比如某个数据集上的节点分类、图分类等。NOI prompt node的节点text feature以如下形式构建：</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/NOI_prompt_node.png#center" alt="NOI"  />
</p>
<p>即每个数据集的一个task对应于一个prompt node，该node用来描述task。这样，把不同的图任务都用节点的text feature这种统一的形式来表达。</p>
<h3 id="graph-in-context-learning的图提示范式graph-prompting-paradigm">Graph In-Context Learning的图提示范式（Graph Prompting Paradigm）</h3>
<p>LLM的一大特性就是它可以通过prompt来实现in-context learning，使得模型可以在不用fine-tuning的情况下适应于不同的任务。比如在few-shot场景下，目标是基于一篇paper的摘要和内容预测它的类别，我们可以为LLM提供每个类别的$k$篇papers作为context加入prompt中，来指导模型基于这些提供的context来生成目标摘要和内容的类别预测。（通过一些<strong>任务相关的其他信息</strong>来指导模型对目标样本的预测）</p>
<p>本文发现实现图上in-context learning的核心在于操作输入图使其和下游任务对齐。<strong>Graph Prompting Paradigm (GPP)</strong> 即图提示范式旨在操作输入图使其可以从输入数据本身获得任务相关的信息。如图2中的虚线所示，用来描述任务的NOI prompt node与所有NOI node建立连接（任务的目标节点），表示在这些NOI nodes上执行对应prompt的任务（如节点分类，链路预测，图分类等）。图中的$p2t$和$t2p$ edge表示NOI节点和prompt node之间的边。下一步建立NOI prompt node和具体类别之间的联系，如图2中的<strong>Class Node</strong>用来描述该分类任务的每个类别的类信息，任务有多少个类，就有多少个Class Node。Class Node的Text feature如下图所示：</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/class_node.png#center" alt="NOI"  />
</p>
<p><strong>Zero-shot Learning:</strong> 对于NOI中的一个节点$q$，用于描述它的任务的NOI prompt node $p_q$，以及Class Node $\{c_i | i \in [N]\}$ ，其中$N$为任务的类别数。prompt node，class node，以及和prompt node 连接的所有边（包括与NOI连接的边）共同构成了<strong>prompt graph</strong> $\mathcal{P}=\left(\mathcal{V}_p, \mathcal{E}_p, \mathcal{R}_p\right)$。</p>
<p>然后，prompt graph $\mathcal{P}=\left(\mathcal{V}_p, \mathcal{E}_p, \mathcal{R}_p\right)$和NOI subgraph (即NOI节点的局部子图)相结合为<strong>通用graph model</strong>的输入图$\mathcal{G}_m=\left(\mathcal{V}_q^h \cup \mathcal{V}_p, \mathcal{E}_q^h \cup \mathcal{E}_p, \mathcal{R}_q^h \cup \mathcal{R}_p\right)$。如图2中的（a）(b)（c）都是graph model的输入图。通过模型的学习，可以得到每个Class node 的embeddings，如类别$c_i$的Class node embedding为$h_{c_i}$。因为$c_i$与任务相关的prompt node 连接，而prompt node与目标NOI node 连接，因此可以用$h_{c_i}$来推断NOI node属于类别$c_i$的概率：
$$
P[\text { NOI belongs to class } i]=\sigma\left(\operatorname{MLP}\left(h_{c_i}\right)\right)
$$</p>
<h1 id="4-talk-like-a-graph-encoding-graphs-for-large-language-models">4. Talk like a Graph: Encoding Graphs for Large Language Models</h1>
<p>prompt engineering的目的是找到一个合适的方式使LLM $f$可以解析问题$Q$，并且得到他的Answer $\mathcal{A}$，即$\mathcal{A} = f(Q)$。本工作的目标是为LLM$f$提供图$G$，使得LLM可以对图做推理QA，即$\mathcal{A} = f(G,Q)$。在该工作中，固定LLM $f$的参数不变，并引入一个图编码函数（graph encoding function）$g(G): G \to W$用于将图结构数据编码成text，以及一个问题重解析函数$q(Q): W \to W$。训练数据$D$有图$G$，问题$Q$和回答$S$组成，即每个训练数据$(G,Q,S) \in D$。训练目标是固定大语言模型$f$不变，找到最佳的图编码函数$g$和问题重解析函数$q$，使得给定训练$G$和$Q$，得到回答$S$的分数最高：
$$
\max _{g, q} \mathbb{E}_{G, Q, S \in D} \operatorname{score}_f(g(G), q(Q), S)
$$</p>
<h3 id="graph-encoding-function-gg">Graph encoding function $g(G)$</h3>
<p>$g(G)$用于将$G$映射为LLM可以处理的token：首先，encode图中的节点，然后encode图中的边。具体编码技术如下图所示：</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/encoding_graph.png#center" alt="NOI"  />
</p>
<p><strong>Encoding Nodes</strong>:</p>
<ol>
<li>
<p>整型节点编码：$G$ describes a graph among nodes 0,1,2,3,4,5,6,7</p>
</li>
<li>
<p>使用well-known English first names：$G$ describes a friendship among James, Robert, Michael, Mary.</p>
</li>
<li>
<p>使用电视剧《权力的游戏》和《南方公园》中流行的角色名字。</p>
</li>
<li>
<p>包括美国政治家的名字。</p>
</li>
<li>
<p>用字母表示的。</p>
</li>
</ol>
<p><strong>Representing Edges</strong>:</p>
<ol>
<li>用括号表示边：The edge in $G$ given as (0,1), (0,2), &hellip;, (6,7), (7,8)</li>
<li>Friendship: source node and target node are friends. 如 We have the following edges in $G$: James and Robert are friends, &hellip; Jennifer and Linda are friends。对应于上面的well-known English first name表示节点</li>
<li>Coauthorship: 比如 James and Robert wrote a paper together 来表示一条边</li>
<li>Social network: James and Robert are connected来表示一条边</li>
<li>Arrows: A-&gt;B 来表示边</li>
<li>Incident: Node 8 is connected to nodes 3,7 来表示每个节点的<strong>邻域</strong>。</li>
</ol>
<h2 id="experiments">Experiments</h2>
<p>使用PaLM 62B作为LLM，在不同图任务以及不同图编码器下的准确率比较，最有效的prompt (zero-shot、zero-cot、few-show、cot、cot-bag)用下划线标出，最佳图编码器用加粗标出。实验使用ER Graph作为数据集，ER Graph的统计数据如下表所示，可以看出平均节点数为12.37，平均边数为39.79，平均度为5.70。对于边存在任务，有53.96%的情况不存在边，对于cycle check任务，有81.96%的情况存在cycle（因为ER graph很可能存在cycle）。</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/graphqa_dataset.png#center" alt="graphqa_datase"  />
</p>
<p>从下面的实验结果可以看出，LLM在所有prompt heuristic的所有graph encoding function上预测的最高边存在概率为44.5%，76%的情况存在cycle。而在Node degree 任务上，均与真实平均度5.7差距较大，平均边数也与真实情况差距较大。</p>
<p>简单的Prompt比如zero-shot 在简单的任务上比复杂的prompt比如zero-cot效果更好，因为简单的任务无需多跳推理。</p>
<p>graph encoding function对LLM影响巨大。</p>
<p>整型节点编码可以提升算数性能，比如node degree、node count和edge count的预测。</p>
<p><img loading="lazy" src="/posts/2024-05-26-graphllm/graphqa_exp.png#center" alt="graphqa_exp"  />
</p>
<h1 id="5-can-language-models-solve-graph-problems-in-natural-language">5. Can Language Models Solve Graph Problems in Natural Language?</h1>
<p>[1] Learning on Large-scale Text-attributed Graphs via Variational Inference.</p>
<p>[2] <a href="https://medium.com/@francescofranco_39234/in-context-learning-icl-a775cd8b7261">https://medium.com/@francescofranco_39234/in-context-learning-icl-a775cd8b7261</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2023《Ordered GNN：Ordering Message Passing to Deal with Heterophily and Over-smoothing》 Reading Nodes</title>
      <link>https://JhuoW.github.io/posts/orderedgnn/</link>
      <pubDate>Sun, 16 Jul 2023 17:47:14 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/orderedgnn/</guid>
      <description>ICLR2023 &amp;#34;Ordered GNN：Ordering Message Passing to Deal with Heterophily and Over-smoothing&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2302.01524">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>多层message passing后，GNN会导致Over-smoothing使得节点表示趋同。另一方面，标签不同的相邻节点特征会混合，导致不同标签节点边的难以区分，即Heterophily问题。在本文中，提出对传递到节点的message进行排序，即表示向量的特定neuron块编码特定hop的消息。通过将节点rooted computation tree的层次和表示向量neuron 块对齐。如下图所示，3层GNN对节点$v$的输出$h_v^{(3)}$的前$P_v^{(0)}$个neurons 编码1 hop邻居信息，$[P_v^{(0)}, P_v^{(1)}]$编码了第1层另据的信息。通过以确定的顺序编码邻居信息，来避免hops的特征融合，即一个节点的embedding的神经元要和计算书的层次对齐，不同层分配不同的neuron。也就是按照顺序将不同层的邻居信息编码到最终表示$h_v^{(k)}$的不同维度区间中。</p>
<p><img loading="lazy" src="/posts/2023-07-16-OrderedGNN/1.png" alt=""  />
</p>
<h1 id="approach">Approach</h1>
<h2 id="aligning-rooted-tree-with-node-embedding">Aligning Rooted-Tree with Node Embedding</h2>
<p>对于一个节点$v$，显然它的第$k-1$层rooted-tree $\mathcal{T}^{(k-1)}_v$是$k$层rooted-tree  $\mathcal{T}^{(k)}_v$的子树:
$$
\mathcal{T}_v^{(0)} \subseteq \mathcal{T}_v^{(1)} \subseteq \cdots \subseteq \mathcal{T}_v^{(k)} \subseteq \cdots \subseteq \mathcal{T}_v^{(K)}
$$
随着$k$的增加，$\mathcal{T}_v^{(K)}$会变得越来越大且复杂，且包含之前层的所有信息。所以$\mathcal{T}_v^{(K)}$需要更多neuron (最终表示向量$h_v^{(k)}$中的维度) 来编码信息。由于$\mathcal{T}_v^{(k-1)}$是$\mathcal{T}_v^{(k)}$的子树，所以在表示向量$h_v^{(k)}$中，编码tree $\mathcal{T}_v^{(k)}$信息的neurons要包含编码 tree$\mathcal{T}_v^{(k-1)}$信息的neurons。具体来说，关于节点$v$的$k-1$层rooted-tree $\mathcal{T}_v^{(k-1)}$，它的信息会被编码到$h_v^{(K)}$的前$P_v^{(k-1)}$个neuron中（维度），对于下一个层次的rooted-tree $\mathcal{T}_v^{(k)}$，它会被编码到$h_v^{(K)}$的前$P_v^{(k)}$个neuron（维度）中。因为$\mathcal{T}_v^{(k-1)}$是$\mathcal{T}_v^{(k)}$的子树，所以$P_v^{(k-1)} \leq P_v^{(k)}$。  $v$的$K$层最终表示$h_v^{(K)}$，它的每个维度是一个neuron，前$P_v^{(k-1)}$个neurons编码了前$k-1$ hop邻居的信息，前$P_v^{(k)}$个neurons编码了前$k$ hop邻居的信息， 在两个分割点$P_v^{(k-1)}$和$P_v^{(k)}$之间的neurons要编码的是<strong>第$k$ hop邻居</strong>的信息。所以节点$v$在$K$层GNN下的最终embedding$h_v^{(K)} \in \mathbb{R}^D$ 会被$K+1$个分割点分成$K+1$块，其中前$P_v^{(0)}$个neurons编码的是节点$v$的自身信息，$P_v^{(k)}$为$h_v^{(K)}$的split point，且：
$$
P_v^{(0)} \leq P_v^{(1)} \leq \cdots \leq P_v^{(k)} \leq \cdots \leq P_v^{(K)} = D
$$</p>
<h2 id="the-split-point">The Split Point</h2>
<p>分割点$P_v^{(k)}$是一个index，会将$D$维node embedding 分为2块，$[0, P_v^{(k)}-1]$的neurons编码了前$k$层邻居的信息。 定义一个$D$维gating向量$g^{(k)}_v = [1,1,1,1,1,1,0,0,0,0,0]$其中前$P_v^{(k)}$个entries是1， 后面为0，即筛选出前$k$层要编码进的neurons：
$$
h_v^{(k)}=g_v^{(k)} \circ h_v^{(k-1)}+\left(1-g_v^{(k)}\right) \circ m_v^{(k)}
$$
其中第$k$层的信息$h_v^{(k-1)}$保留在第$k+1$层embedding $h_v^{(k)}$的前$P_v^{(k)}$个neuron中，而$h_v^{(k)}$的后面部分neuron编码新聚合的邻居信息，通过这种方式，将每一个hop的信息分开。 再下一层时， $h_v^{(k)}$的信息就会被编码到$D$个neurons中的前$P_v^{(k+1)}$个neuron中，那么其实$P_v^{(k)}$到$P_v^{(k+1)}$之间的neuron实际上只包含了$m_v^{(k)}$的信息，即第$k$个hop的信息。以这种方式将每一个hop的邻居信息按顺序编码到最终表示向量$h_v^{(K)}$中。</p>
<p>然而binary gating vector $g^{(k)}_v$ 来自于离散操作，导致$g^{(k)}_v$的学习不可微，使得每个hop用多少神经元来保存不可自适应学习。 为了解决该问题，将$g^{(k)}_v$定义为关于root node $h_v^{(k-1)}$和messge $m_v^{(k)}$ 的函数，$g_v^{(k)}$的split point $P_v^{(k)}$分割了前面所有层的信息和第$k$ hop的邻居信息，这里将$\hat{g}_v^{(k)}$定义为一个从右向左的累加向量：
$$
\hat{g}_v^{(k)}=\operatorname{cumax}_{\leftarrow}\left(f_{\xi}^{(k)}\left(h_v^{(k-1)}, m_v^{(k)}\right)\right)=\operatorname{cumax}_{\leftarrow}\left(W^{(k)}\left[h_v^{(k-1)} ; m_v^{(k)}\right]+b^{(k)}\right)
$$
进一步 由于$\hat{g}_v^{(k)}$是可学习的，它的split point为$P_v^{(k)}$ ， 因此并不能保证$\hat{g}_v^{(k+1)}$的split point $P_v^{(k+1)}$会相对于$P_v^{(k)}$右移，因此本文提出了可微OR操作来确保$P_v^{(k+1)}\geq P_v^{(k)}$：
$$
\tilde{g}_v^{(k)}=\operatorname{SOFTOR}\left(\tilde{g}_v^{(k-1)}, \hat{g}_v^{(k)}\right)=\tilde{g}_v^{(k-1)}+\left(1-\tilde{g}_v^{(k-1)}\right) \circ \hat{g}_v^{(k)}
$$</p>
<h2 id="putting-it-all-together">Putting it All Together</h2>
<p>Ordered GNN的第$k$层message passing如下：</p>
<p>首先计算第$k$hop邻居的message：
$$
m_v^{(k)}=\operatorname{MEAN}\left(\left\{h_u^{(k-1)}: u \in \mathcal{N}(v)\right\}\right)
$$
计算当前层的split point $P_v^{(k)}$和门控向量$\tilde{g}_v^{(k)}$：
$$
\begin{aligned}
&amp; \hat{g}_v^{(k)}=\operatorname{cumax}_{\leftarrow}\left(f_{\xi}^{(k)}\left(h_v^{(k-1)}, m_v^{(k)}\right)\right) \
&amp; \tilde{g}_v^{(k)}=\operatorname{SOFTOR}\left(\tilde{g}_v^{(k-1)}, \hat{g}_v^{(k)}\right)
\end{aligned}
$$
利用门控向量分块聚合当前节点embedding和新加入的hop信息：
$$
h_v^{(k)}=\tilde{g}_v^{(k)} \circ h_v^{(k-1)}+\left(1-\tilde{g}_v^{(k)}\right) \circ m_v^{(k)}
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2023《MLPInit：Embarrassingly Simple GNN Training Acceleration with MLP Initialization》 Reading Nodes</title>
      <link>https://JhuoW.github.io/posts/mlpinit/</link>
      <pubDate>Sat, 15 Jul 2023 15:43:09 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/mlpinit/</guid>
      <description>ICLR2023 &amp;#34;MLPInit：Embarrassingly Simple GNN Training Acceleration with MLP Initialization&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2210.00102">paper</a></p>
<p>GNN中的层次叠加需要稀疏矩阵乘法计算带来较大的计算开销，而MLP仅使用node feature可以避免此问题。本文发现大多数message-passing通过将训练参数设置为相同shape，可以推导出等效的MLP（PeerMLP），而使用PeerMLP来作为GNN的初始化参数可以相较于仅使用PeerMLP，效果提升极大。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/1.png" alt=""  />
</p>
<p>从上图的蓝线可以看出，GNN通常需要更多的训练迭代次数才可以达到收敛，因为其中涉及复杂的稀疏矩阵乘法计算。而MLP不使用结构信息，训练速度更快，因此本文发现MLP和GNN可以有相同的训练权重空间，因此 <strong>Can we train GNNs more efficiently by leveraging the weights ofconverged MLPs?</strong>  本文进一步发现，对于一个GNN和它对应的PeerMLP （相同的weight），在PeerMLP上训练的权重可以优化GNN。基于该发现，图上训练好的PeerMLP作为GNN的权重矩阵$W$, 然后再考虑结构信息，可以发现GNN的效果相较于PeerMLP有很大的提升。 如表2所示，其中PeerMLP和GNN有相同的权重空间，首先在图上训练PeerMLP，得到收敛时的最有参数$w^\star_{mlp}$，PeerMLP的预测结果为$f_{m l p}\left(\mathbf{X} ; w_{m l p}^\star\right)$， 然后直接使用不训练而直接使用$w_{m l p}^\star$作为GNN的参数，即$f_{g n n}\left(\mathbf{X}, \mathbf{A} ; w_{m l p}^\star\right)$,可以看出，在考虑图结构后，GNN即使不训练，直接使用PeerMLP的权重矩阵，效果也有巨大提升。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/2.png" alt=""  />
</p>
<p>受此启发，本文提出了用收敛的PeerMLP最优权重矩阵，作为GNN的初始化权重。从图1的红线可以看出，相较于随机初始化的GNN，MLPInit初始化的GNN在更少的epoch到达收敛 并且可以达到和相似的准确率。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/3.png" alt=""  />
</p>
<p>从上表可以看出GNN的Propagation操作$AZ$的前向计算和反向梯度传播的耗时都远远超过Feature Transformation操作$WX$。Feature Tran的操作相对与Propagation，计算成本几乎可以忽略不计，所以如果预训练操作得到的$W$可以使得训练GNN时的epoch大幅下降，可以使模型更加高效。如下表所示，训练PeerMLP的时间再加上的权重迁移到GNN后的fine-tuning时间， 远少于在GNN上直接训练随机初始化参数的时间。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/5.png" alt=""  />
</p>
<p>从下图同样可以看出PeerMLP的参数$w_{mlp}$的训练趋势，PeerMLP训练过程中每个epoch的$w_{mlp}$直接迁移到GNN上计算CE损失，可以发现使得MLP 的CE Loss下降的$w_{mlp}$同样可以使得GNN以同样的趋势下降。</p>
<p><img loading="lazy" src="/posts/2023-07-16-MLPInit/4.png" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Fraud Detection based on Graph Neural Networks</title>
      <link>https://JhuoW.github.io/posts/fd/</link>
      <pubDate>Sat, 13 May 2023 16:14:56 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fd/</guid>
      <description>基于图神经网络的异常检测</description>
      <content:encoded><![CDATA[<h1 id="1-label-information-enhanced-fraud-detection-against-low-homophily-in-graphs-www-23">1. Label Information Enhanced Fraud Detection against Low Homophily in Graphs (WWW &lsquo;23)</h1>
<h2 id="introduction">Introduction</h2>
<p>GNN4FD存在问题： 大多数基于GNN的欺诈检测器难以泛化到low homophily网络中，除此之外，如何充分利用label信息也是Fraud detection的重要因素。即如果一个Fraud node的邻居都是benign nodes，那么这样的图就是heterophily or low homophily graph，由于GNN的neighborhood aggregation机制，target node的表示会和它的邻居相似，无论他们的label是否不同，这样会使得GNN难以区分位于异质邻域内的Fraud nodes。另外， 现有的GNN4FD方法利用label信息的能力有限，这些方法仅在训练阶段使用label信息作为监督信号，但是在设计message passing 机制的过程中并没有使用label信息。</p>
<p>为了解决上述2个挑战，本文提出GAGA: 基于分组聚合的Transformer。 GAGA首先提出了一种预处理策略Group Aggregation (GA, 分组聚合)，然后每个节点的原始邻居特特征被分组为序列数据。 然后提出一种科学系的编码方式来编码structural，relational 和label信息 （全局），即整个图的relational encoding，group encoding 和 hop encoding （图中又几个relation就有几个relational embedding，取几个hop就又几个hop embedding..）。 最后用多头attention为每个节点聚合embedding sequence.</p>
<h2 id="preliminaries">Preliminaries</h2>
<p><strong>Multi-relational fraud graph construction</strong>  Multi-relational fraud graph $\mathcal{G}(\mathcal{V}, \mathcal{E}, \mathcal{X}, \mathcal{Y})$, 其中节点集$\mathcal{V}=\left\{v_1, v_2, \ldots, v_N\right\}(N=|\mathcal{V}|)$，$R$ 个邻接矩阵$\mathcal{E}=\left\{\mathbf{A}_1, \mathbf{A}_2, \ldots, \mathbf{A}_R\right\}(R=|\mathcal{E}|)$的多关系图 （$R$个关系）。节点feature vectors $X=\left\{\mathbf{x}_1, \mathrm{x}_2, \ldots, \mathrm{x}_N\right\}$以及节点的label集合$\mathcal{Y}$。 对于一个relation的邻接矩阵$\mathbf{A}_r$，如果$\mathbf{A}_1[u,v]=1$，那么在关系$r$下节点$u$和$v$被连接。每个节点$v \in \mathcal{V}$ 有一个$d$维feature vector $\mathbf{x}_v \in \mathbb{R}^d$。 在基于Graph的fraud detection中，我们考虑半监督场景，其中一小部分节点 $\hat{\mathcal{V}} \supset \mathcal{V}$是有label的 （$y=1$表示该节点为fraud node，$y=0$表示该节点为benign node）所以对于fraud graph，节点class数为2。</p>
<h2 id="gaga">GAGA</h2>
<p><img loading="lazy" src="/posts/2023-05-13-FD/1.png#center" alt=""  />
</p>
<p>上图为GAGA的框架。第一步为Group Aggregation，为预处理过程，为每个节点计算多条邻居信息，并且每跳内的信息分组表示（一跳内 label=0，label=1，label=None的节点分别聚合）。这样会为每个节点生成一系列embeddings。第二步中，定义三种类型可学习的embeddings：hop embeddings, relation embeddings,group embeddings，即如果每个节点有$K$-hop邻居参与聚合，那么hop embeddings 是一个$K \times d_H$ 矩阵，每个hop（结构特征）用一个$d_H$维向量表示。 同理relational embedding是一个$R \times d_H$矩阵，每个relation 用一个$d_H$维向量表示。一共存在3种group（$y=1$的group， $y=0$的group， 无标签邻居的group），所以group embeddings是一个$3 \times d_H$的矩阵，每种group 表示为一个$1 \times d_H$的向量。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/2.png#center" alt=""  />
</p>
<p>对于第一步得到的某一个节点$v$在relation 0的邻接矩阵$\mathbf{A}_0$下的第<strong>2</strong>跳邻居的fraud neighbors ($y=1$)的group embedding $g_v (r=0, h=2, y=1)$，为这个embedding融合hop信息 + relation信息+ group 信息得到 $x_v = g_v (r=0, h=2, y=1) + E_h(1) + E_r(0) + E_g(1)$    关于节点$v$的某个group的融合结构，关系特征的表示向量。最后一步将一个节点的所有多关系多hop的group向量用transformer合并然后输入MLP种来预测节点label。</p>
<p>即一个节点会生成 $\#relation (\#hop * (\#class+1) + 1)$个group 向量，每个group向量属于某个relation下的某个hop，这个group向量属于那个relation就加上这个relation的一维encoding，属于那是个hop就加上这个hop的1维encoding，这个group是0/1/None group就再加上对应group的encoding，从而得到这个group的最终encoding。</p>
<h3 id="group-aggregation">Group Aggregation</h3>
<p>对于Fraud detection任务，每个节点的label有3种情况，分别为 benign node $y=0$, Fraud node $y=1$, unlabeled node $y = None$，所以对于每个节点，它的第$k$hop邻居可以被分为3个group，每个group的节点做聚合：
$$
\mathbf{H}_g^{(k)}=\left[\mathbf{h}^{-}, \mathbf{h}^{+}, \mathbf{h}^*\right]^{(k)} \text { given } \hat{\mathcal{N}}_k(v)
$$
表示节点$k$ hop内的3个group表示向量 （由每个group节点取平均得到）。那么对于关系$r$下的所有$K$个hop内的group embedding可以表示为：
$$
\mathbf{H}_r=||_{k=1}^K \mathbf{H}_g^{(k)},
$$
那么对于所有$R$个关系，所涉及的group embedding sequence表示为：
$$
\mathbf{H}_s=||_{r=1}^R \mathbf{H}_{v, r} .
$$
关于每个节点，共有$S=R \times(P \times K+1)$个group embeddings。其中$R$为relation数， $K$为hop数，$K = \#class +1$ 为label数+1 （有多少种group）。</p>
<h3 id="learnable-encoding-for-graph-transformer">Learnable Encoding for Graph Transformer</h3>
<p>先将每个节点的所有$S$个group embeddings过一下MLP得到$\mathbf{X}_s \in \mathbb{R}^ {S\times d_H}$。用nn.Embedding来定义一个$K \times d_H$的可训练的Hop encoding 矩阵$E_h(\cdot)$，每行表示一种hop的embedding。 对于节点的$S$个group 向量，每个group向量都属于一个hop种，那么这个group embedding 就+对应hop的embedding，从而融合结构特征。 比如$X_s[3]$是hop 2的 group embedding，那么这个group embedding 就要加上 $E_h(1)$ 来保留hop结构特征。所有$S$个group embedding 都要融合各自的hop特征，他们的hop 特征为：
$$
\begin{gathered}
\mathbf{X}_h=[\underbrace{\mathbf{E}_h(0), \overbrace{\mathbf{E}_h(1), \mathrm{E}_h(1), \mathrm{E}_h(1)}^{1 \text { st hop }}, \ldots, \overbrace{\mathrm{E}_h(K), \mathrm{E}_h(K), \mathbf{E}_h(K)}^{K-\text { th hop }}}_{1 \text { st relation }}, \\
\ldots, \underbrace{\mathbf{E}_h(0), \mathrm{E}_h(1), \mathrm{E}_h(1), \mathrm{E}_h(1), \ldots, \mathrm{E}_h(K), \mathrm{E}_h(K), \mathrm{E}_h(K)}_{R \text {-th relation }}]
\end{gathered}
$$
$S$中1-st到R-th relation的所有1hop group embedding都要加上hop 1 的encoding $E_h(1)$，对于其他hop的group embedding 同理。$\mathbf{X}_s$ 表示一个节点的所有$S$个group embedding，每个group embedding 要加上它所在的relation encoding $E_r(\text{relation of group})$，hop encoding $E_h(\text{hop of group})$ 以及它属于那个group $E_g (\text{label of group})$:
$$
\mathrm{X}_{i n}=\mathrm{X}_s+\mathrm{X}_h+\mathrm{X}_r+\mathrm{X}_g
$$
$\mathbf{X}_in$为一个节点新的group embeddings。每个节点的每个group embedding 都要融合它所在的hop 特征，所在的relation特征和所在的label特征（group 特征）然后用transformer将一个节点所有$S$个融合丰富特征的group embedding 做聚合，从而得到这个节点的最终embedding，用这个最终embedding来计算binary classification loss。</p>
<h1 id="2-gccad-graph-contrastive-coding-for-anomaly-detection-tkde">2. GCCAD: Graph Contrastive Coding for Anomaly Detection （TKDE）</h1>
<p>本文的目标：拉近normal nodes和global embedding的距离，拉远fraud nodes和global embedding的距离。inference阶段通过计算testing node和global embedding的距离来判断节点是否为fraud node。</p>
<h2 id="preliminary-observations">Preliminary Observations</h2>
<p><img loading="lazy" src="/posts/2023-05-13-FD/3.png#center" alt=""  />
</p>
<p>上图中N-N表示Normal nodes之间的相似度，AB-AB表示Abnormal nodes之间的相似度，N-AB表示Normal nodes和Abnormal nodes之间的相似度，从图（a）中可以发现N-N节点原始之间的相似度差别很大，即normal nodes之间的相似度差别很大，相似度范围在$[0.2,0.8]$， 而abnormal nodes之间的相似度差别也很大。从图(a)中还可以看出，normal nodes (N)和abnormal nodes （AB）原始特征之间有很大一部分是相似的。从图(b)中可以看出，当使用GCN学习到新的节点feature vectors后，normal nodes之间的相似度(N-N)得到了提升，即从$[0.2,0.8]$改善到$[0.4,1.0]$，但是N-N，AB-AB内部的相似度依然变化较大，并且依然存在大量高相似度的N-AB。</p>
<p>从图（c）可以看出normal nodes的原始特征和global embedding （N-GL）之间相似度较高，并且相似度变化范围小。而Abnormal nodes和global embedding (AB-GL)之间的相似度较低，并且N-GL相似度和AB-GL相似度更好区分。所以通过与global embedding之间的相似度来区分normal nodes和abnormal nodes可能更加有效。而本文提出的GCCAD会进一步提升normal nodes和global emb之间的相似度，并且更加容易区分N-GL相似度与AB-GL相似度。</p>
<h2 id="gccad-model">GCCAD Model</h2>
<p>GCCAD基于监督对比学习来优化node embeddings和global embeddings，目标函数如下：
$$
\mathcal{L}_{\text {con }}=\underset{\substack{i: y_i=0 \\ j: y_j=1}}{\mathbb{E}}\left[-\log \frac{\exp \left(\mathbf{q}^{\top} \boldsymbol{h}_i / \tau\right)}{\sum_j \exp \left(\boldsymbol{q}^{\top} \boldsymbol{h}_j / \tau\right)+\exp \left(\boldsymbol{q}^{\top} \boldsymbol{h}_i / \tau\right)}\right]
$$
其中$\boldsymbol{q}$为global embedding，$\boldsymbol{h}_i$为normal node $v_i$的embedding。基于上述supervised contrastive loss，训练目标为增大训练集中normal nodes和global embedding的相似度，减少abnormal nodes和global embedding的相似度。即使得global embedding尽可能不受abnormal nodes的影响。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/4.png#center" alt=""  />
</p>
<p>另外 本文不直接在原图上训练contrastive loss，而是先优化图结构，然后在优化的图结构上训练contrastive loss。由于message passing过程中会使得节点特征局部平滑，而abnormal node通常和位于normal node中，所以MPNN无论如何都会使得abnormal node和周围的normal node变得相似。所以在MPNN前先使用<strong>Edge Update</strong>模块对图更新，移除潜在的可以links，使得abnormal nodes尽可能少的接受到normal node的信息。本文提出Context-Aware Link Predictor来衡量原图中两个节点的边的保留概率：
$$
\begin{array}{r}
p_{i j}^{(l)}=\operatorname{MLP}\left(\left(\boldsymbol{h}_i^{(l-1)}-\boldsymbol{h}_j^{(l-1)}\right) \oplus\left(\boldsymbol{h}_i^{(l-1)}-\boldsymbol{q}^{(l-1)}\right)\right.
\left.\oplus\left(\boldsymbol{h}_j^{(l-1)}-\boldsymbol{q}^{(l-1)}\right)\right)
\end{array}
$$
两个节点间边的保留概率和两个节点间的embedding相似度有关（第1项），也和节点与global emb相似度有关。然后用训练集中标注好的normal nodes和abnormal nodes来训练$p_{i j}^{(l)}$：
$$
\mathcal{L}_{\text {link }}=\mathbb{E}\left[\sum_{i, j: y_i=y_j=0}-\log p_{i j}^{(l)}-\sum_{i, j: y_i \neq y_j=0}\left(1-\log p_{i j}^{(l)}\right)\right]
$$
通过这种方式，来将潜在的与abnormal nodes连接的边移除，从而使得abnormal node在message passing过程减少收到normal node的影响。基于每条边的保留概率，基于Bernoulli 分布来采样edges从而得到边mask 矩阵 $I^{(l)}$。新的图结构定义为：
$$
A_{i j}^{(l)}=\left(\alpha A_{i j}^{(l-1)}+(1-\alpha) p_{i j}^{(l)}\right) \odot I_{i j}^{(l)}
$$
反向传播时$I$视为常量，梯度从$p_{ij}$走。得到新的图结构后用GNN学习图的node embeddings。最后基于得到的node embeddings计算每个node embedding 和global embedding $\boldsymbol{q}$ 的相似度 （$\boldsymbol{q}$初始化为所有节点初始feature的均值）：
$$
s_i^{(l)}=\operatorname{cosine}\left(\boldsymbol{h}_i^{(l)}, \boldsymbol{m}\right)
$$
然后基于每个节点和global emb之间的相似度来计算聚合权重：
$$
\alpha_i^{(l)}=\frac{\exp \left(s_i^{(l)}\right)}{\sum_{j=1}^N \exp \left(s_j^{(l)}\right)}
$$
聚合node emb得到global emb:
$$
\boldsymbol{q}^{(l)}=\sum_{i=1}^N \alpha_i^{(l)} \cdot \boldsymbol{h}_i^{(l)}
$$
<strong>Training and Inference</strong></p>
<p>每个epoch 基于得到的global emb $\boldsymbol{q}$以及node embeddings $\boldsymbol{h}$ 来计算supervised contrastive loss，从而同时优化node embs和global embs。测试阶段，将测试节点的emb计算和global emb之间的相似度，相似度越低，测试节点是abnormal nodes的可能性越大。</p>
<h1 id="3-h2-fdetector-a-gnn-based-fraud-detector-with-homophilic-and-heterophilic-connections-www-22">3. H2-FDetector: A GNN-based Fraud Detector with Homophilic and Heterophilic Connections (WWW &lsquo;22)</h1>
<h2 id="introduction-1">Introduction</h2>
<p>Fraud graph通常包含2种类型的实体关联：1. homophilic connections: 相同label的节点被连接。 2. heterophilic connections: 不同label的节点被连接（fraudster 和 benign）。对于同时存在homophily 和heterophily的fraud graph，存在以下挑战：（1）如何学习一个边判别器，来判断图中的边是homophilic （两端都是benign 或 fraud） 还是 heterophilic (边一端是fraud一端是benign)。（2）如何为同时包含homophilic和heterophilic connections的fraud graph设计GNN。 (3) 如何利用整个类别的特征来判别新的fraud node? 即fraud node除了捕获与其邻居中benign nodes不相似的信息，还要捕获其他fraudster的信息，所以本方法让每个节点的表示和它所在类别的category feature相似，来捕获其他fraud 节点的特征。</p>
<h2 id="methodology">Methodology</h2>
<h3 id="h2-connection-identification">H$^2$-connection Identification</h3>
<p>训练一个边判别器来预测图中任意一条边是homophilic edge还是heterophilic edge，基于训练集中的节点label。对于第$l$层的node embedding $H^{(l-1)}=\left\{h_1^{(l-1)}, h_2^{(l-1)}, \ldots, h_N^{(l-1)}\right\}$。对于图中的每条边 $e_{uv}$，定义一个可训练的判别器来判断该边是homophilic还是heterophilic。 首先：
$$
\begin{aligned}
&amp; \bar{h}_u^{(l)}=\sigma\left(W_t^{(l)} h_u^{(l-1)}\right) \\
&amp; \bar{h}_v^{(l)}=\sigma\left(W_t^{(l)} h_v^{(l-1)}\right)
\end{aligned}
$$
其中$W_t^{(l)} \in \mathbb{R}^{d_l \times d_{l-1}}$是边判别器的可学习参数。然后基于$\bar{h}_u^{(l)}$和$\bar{h}_v^{(l)}$来计算边$e_{uv}$的homophilic分数。边的homophilic分数通过对两个节点的拼接 以及两个节点的不同来计算，$W_c^{(l)}$也是边判别器的参数，用于输出边分数：
$$
m_{u v}^{(l)}=\tanh \left(W_c^{(l)}\left[\bar{h}_u^{(l)}||\bar{h}_v^{(l)}||\left(\bar{h}_u^{(l)}-\bar{h}_v^{(l)}\right)\right]\right)
$$
其中 $\mathrm{tanh}(\cdot) \in (-1,1)$。根据$m_{u v}^{(l)}$的符号来判断$e_{uv}$是homo还是hetero：
$$
c_{u v}^{(l)}=\operatorname{SIGN}\left(m_{u v}^{(l)}\right)
$$
基于第$l$层的embedding输入边判别器中，可以得到所有边是homophilic还是heterophilic：
$$
C^{(l)}=\left\{c_{u v}^{(l)}\right\}_{e_{u v} \in \mathcal{E}}
$$
因为边判别器的输出是$\{-1,1\}$，所以对于训练集中的homophilic边，$y_{uv}=1$，那么$m_{u v}^{(l)}$要逼近1。同理对于heterophilic边，$y_{uv}=-1$，那么$m_{u v}^{(l)}$要逼近-1。即最小化以下目标：
$$
\mathcal{L}_{H I}^{(l)}=\frac{1}{\mathcal{E}_t} \sum_{e_{u v}}^{\mathcal{E}_t} \max \left(0,1-y_{u v} m_{u v}^{(l)}\right)
$$</p>
<h3 id="h2-connection-aggregation">H$^2$-connection Aggregation</h3>
<p>对于第$r$个relation下的图$\mathcal{G}_r=\left\{\mathcal{V}, X,\left\{\mathcal{E}_r\right\}, Y\right\}$，$\mathcal{N}_r(v)$表示关系$r$下节点$v$的邻居，$u \in \mathcal{N}_r(v)$。计算$u$对中心节点$v$重要性分数时考虑他们之间的边是homo边还是hetero边，所以在计算边$e_{vu}$间的重要性系数时考虑$c_{uv}^{(l)}$:
$$
e_{u v}^{(l), r}=a^{(l), r}\left[W_r^{{l}} h_v^{(l-1)} || c_{u v}^{(l)} W_r^{(l)} h_u^{(l-1)}\right]
$$
其中 attention mechanism权重向量$a^{(l), r} \in \mathbb{R}^{1 \times 2d_l}$。类似于GAT，邻居聚合的attention系数如下：
$$
\alpha_{u, v}^{(l), r}=\frac{\exp \left\{\operatorname{LeakyReLU}\left(e_{u v}^{(l), r}\right)\right\}}{\sum_{k \in \mathcal{N}_r(v)} \exp \left\{\operatorname{LeakyReLU}\left(e_{k v}^{(l), r}\right)\right\}}
$$
考虑多头attention，并且在邻居聚合的时候考虑边类型：
$$
h_v^{(l), r}=||_{k=1}^K \sigma\left(\sum_{u \in \mathcal{N}_r(v)} \alpha_{u, v}^{(l), r, k} c_{u v}^{(l)} W_r^{(l), k} h_u^{(l-1)}\right)
$$
对于$R$个relation，将每个节点每层输出$h_v^{(l), r}$的所有$R$个关系拼接后做特征变换，得到融合多关系的节点embedding：
$$
\begin{aligned}
&amp; h_v^{(l), \text { all }}=||_{r=1}^R h_v^{(l), r} \\
&amp; h_v^{(l)}=W_d^{(l)} h_v^{(l), \text { all }}
\end{aligned}
$$
其中$W_d^{(l)} \in \mathbb{R}^{d_l \times R d_l}$。最后一层输出维度为2，并做softmax：
$$
p_v=\operatorname{softmax}\left(h_v^{(L)}\right)
$$
用cross-entropy 训练GNN：
$$
\mathcal{L}_o=-\sum_{v \in \mathcal{V}_t}\left[y_v \log \left(p_v\right)+\left(1-y_v\right) \log \left(1-p_v\right)\right]
$$</p>
<h3 id="prototype-extraction">Prototype Extraction</h3>
<p>除了训练边类型判别器H$^2$-connection Identification $\mathcal{L}_{H I}$，节点embedding类型判别器$\mathcal{L}_o$外，节点的每层embedding要和该节点所属的类embedding（prototype embedding）相似。类的prototype embedding:
$$
\begin{aligned}
\operatorname{prototype}_{\text {fraud }}^{(l)} &amp; =\frac{1}{\left|\mathcal{V}_f\right|} \sum_{v \in \mathcal{V}_f} h_v^{(l)} \\
\operatorname{prototype}_{\text {benign }}^{(l)} &amp; =\frac{1}{\left|\mathcal{V}_b\right|} \sum_{v \in \mathcal{V}_b} h_v^{(l)}
\end{aligned}
$$
distance between node $v$ and two prototype:
$$
\begin{aligned}
&amp; \mathcal{D}_f^{{l}}(v)=|| h_v^{(l)}-\text { prototype }_{f r a u d}^{(l)} ||_2 \\
&amp; \mathcal{D}_b^{{l}}(v)=|| h_v^{(l)}-\text { prototype }_{\text {benign }}^{(l)} ||_2
\end{aligned}
$$
$v$到两个prototype 的距离可以用softmax来输出一个2维概率向量，用来匹配他的ground truth one-hot label:
$$
\begin{gathered}
\mathcal{L}_{P E}^{(l)}=-\sum_{v \in \mathcal{V}_t}\left[y_v \log \left(q_v^{(l)}\right)+\left(1-y_v\right) \log \left(1-q_v^{(l)}\right)\right] \\
q_v^{(l)}=\operatorname{softmax}\left(-\mathcal{D}_{C(v)}^{(l)}(v)\right)
\end{gathered}
$$
最终的训练目标为：
$$
\mathcal{L}=\mathcal{L}_o+\gamma_1 \sum_{l-1}^L \mathcal{L}_{H I}^{(l)}+\gamma_2 \sum_{l=1}^L \mathcal{L}_{P E}^{(l)}
$$</p>
<h1 id="4-care-gnn-enhancing-graph-neural-network-based-fraud-detectors-against-camouflaged-fraudsters-cikm-20">4. Care-GNN: Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters (CIKM &lsquo;20)</h1>
<p>Fraud nodes 在图中有2中类型的伪装（Camouflage）。（1）Feature Camouflage：通过添加一些特殊属性，从而骗过基于特征的一场检测器。（2）Relation Camouflage：Fraud nodes 隐藏在benign nodes中。为了解决两种伪装问题，对于<strong>特征伪装</strong>，提出一种标签感知的节点相似度衡量指标（label-aware similarity measure）用来为节点找到在特征层面和它最相似的邻居，节点特征基于它的label训练得到。（2）相似度感知的邻居提取器。基于强化学习，在GNN训练过程中自适应的寻找和他最相似的邻居。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/4.png#center" alt=""  />
</p>
<h2 id="label-aware-similarity-measure">Label-aware Similarity Measure</h2>
<p>在关系$r$下，中心节点$v$在第$l$层的表示为$\mathbf{h}_v^{(l-1)}$，对于该关系下的关于$v$的边$\left(v, v^{\prime}\right) \in \mathcal{E}_r^{(l-1)}$，他们在该层embeddings之间的$l_1$-distance为：
$$
\mathcal{D}^{(l)}\left(v, v^{\prime}\right)=||\sigma\left(M L P^{(l)}\left(\mathbf{h}_v^{(l-1)}\right)\right)-\sigma\left(M L P^{(l)}\left(\mathbf{h}_{v^{\prime}}^{(l-1)}\right)\right)||_1
$$
基于距离可以直接得到相似度：
$$
S^{(l)}\left(v, v^{\prime}\right)=1-\mathcal{D}^{(l)}\left(v, v^{\prime}\right)
$$
其中MLP输出的是一个scalar，然后输出到一个激活函数$\sigma = \tanh \in [-1,1]$中，通过衡量两个节点1维实数表示的距离来评价$v$和它邻居$v^{\prime}$的相似度。其中$M L P^{(l)}$是相似度评价器的参数，目标是基于node label $\{-1,1\}$来训练scalar embedding：
$$
\mathcal{L}_{\mathrm{Simi}}^{(l)}=\sum_{v \in \mathcal{V}}-\log \left(y_v \cdot \sigma\left(M L P^{(l)}\left(\mathbf{h}_v^{(l)}\right)\right)\right)
$$
$MLP$要使得训练集节点可以正确分类，在这种情况下计算两个节点的相似度。</p>
<h2 id="similarity-aware-neighbor-selector">Similarity-aware Neighbor Selector</h2>
<p>对于第$r$个relation，设置第$l$层的邻居采样阈值$p_r^{(l)} \in[0,1]$，表示当前epoch，在关系$r$下第$l$层每个fraud node仅采样和他相似度最高的前$p_r^{(l)}$比例个数的邻居参与聚合。这样可以尽可能为fraud node提取出和他相连的fraud nodes。注意，本文只针对fraud node计算$p_r^{(l)}$，但是该$p_r^{(l)}$会应用到关系$r$第$l$层的所有节点上。因为benign周围的同类型节点占比有绝对优势，所以对$p_r^{(l)}$的大小不敏感，$p_r^{(l)}$不管很大还是很小，都能为它聚合到同类的节点，所以基于fraud node计算的$p_r^{(l)}$来通用在所有节点上。</p>
<p>那么如何设置$p_r^{(l)}$，使得fraud node可以聚合到和它最相似的邻居，从而尽可能过滤掉和它不同类的邻居？由于在训练过程中$p_r^{(l)}$是一个采样概率，采样出的邻居参与聚合，所以$p_r^{(l)}$没有梯度，无法在端到端的训练过程中基于梯度优化。所以为了优化$p_r^{(l)}$，CARE-GNN采用一种基于强化学习的方式，在每个epoch中优化$p_r^{(l)}$。具体来说，代码中只设置了一层GNN，对于一个3relation的图，每个relation下有一个采样概率，$[p_1 ,p_2, p_3]$，初始化为$[0.5, 0.5, 0.5]$，reward初始话为$[0,0,0]$。对于关系$i$下的采样概率$p_i$，第一个epoch先基于初始化概率采样邻居，然后聚合采样出的邻居：
$$
\mathbf{h}_{v, r}^{(l)}=\operatorname{ReLU}\left(\mathrm{AGG}_r^{(l)}\left(\left\{\mathbf{h}_{v^{\prime}}^{(l-1)}:\left(v, v^{\prime}\right) \in \mathcal{E}_r^{(l)}\right\}\right)\right)
$$
然后用当前不同relation下的概率$[p_1 ,p_2, p_3]$加权聚合节点$v$在3个relation下的邻居embedding，然后和节点$v$的self-feature聚合，得到每个节点$v$在当前epoch的输出embedding：
$$
\mathbf{h}_v^{(l)}=\operatorname{ReLU}\left(\mathrm{AGG}_{a l l}^{(l)}\left(\left.\mathbf{h}_v^{(l-1)} \oplus\left\{p_r^{(l)} \cdot \mathbf{h}_{v, r}^{(l)}\right\}\right|_{r=1} ^R\right)\right)
$$
因为只有一层，所以$\mathbf{h}_v^{(l)} = z_v$。基于$z_v$构造cross-entropy loss来预测node label，其中$z_v$过MLP+softmax：
$$
\mathcal{L}_{\mathrm{GNN}}=\sum_{v \in \mathcal{V}}-\log \left(y_v \cdot \sigma\left(M L P\left(\mathbf{z}_v\right)\right)\right) .
$$
now current epoch end，当前epoch中，training fraud node 和采样出的邻居平均相似度为：
$$
G\left(\mathcal{D}_r^{(l)}\right)^{(e)}=\frac{\sum_{v \in \mathcal{V}_{\text {train }}} \mathcal{D}_r^{(l)}\left(v, v^{\prime}\right)^{(e)}}{\left|\mathcal{V}_{\text {train }}\right|}
$$
第一个epoch后 reward 变为$[1,1,1]$，因为当前的$p_i$使得fraud node采样出了更相似的邻居，那么下一个epoch中就要扩大$p_i$来探索更多相似的邻居，如果下一个epoch在扩大$p_i$后采样到的邻居与中心节点的平均相似度降低，那么reward为-1，下一个epoch要用小一些的$p_i$。通过这种方式，基于每个epoch中中心节点和邻居的相似度，来优化采样概率$p_i$。这个过程和GNN的训练以及Label-aware Similarity Measure的参数训练同时进行，最终的loss func为：
$$
\mathcal{L}_{\mathrm{CARE}}=\mathcal{L}_{\mathrm{GNN}}+\lambda_1 \mathcal{L}_{\mathrm{Simi}}^{(1)}+\lambda_2||\Theta||_2
$$</p>
<h1 id="5-rethinking-graph-neural-networks-for-anomaly-detection-icml-22">5. Rethinking Graph Neural Networks for Anomaly Detection (ICML &lsquo;22)</h1>
<p>由于GNN的neighborhood aggregation机制，位于benign nodes中的anomalies会变得难以区分。现有面向Anomaly Detection的GNN分发可以大概分为3类 （1）采用Attention机制从多个视图聚合不同的邻居；（2）对节点邻居重新采样；（3）设置额外的辅助loss来增强GNN在Fraud graph上的训练能力。但这些方法都是spatial methods，很少从spectral的角度设计模型。然而，选择定制的频谱滤波器是GNN设计的关键组成部分，因为频谱滤波器决定了GNN的表达能力。</p>
<p>因此本文研究如何为图上的异常检测任务设计谱图滤波器。本文首先分析了lens of the graph spectrum (图信号经过图傅里叶变换后的谱域表示)，即图信号在每个频率（特征值）上的响应强度。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/5.png#center" alt=""  />
</p>
<p>图1中（a）（c）：异常节点数量不变，异常节点和正常节点的差别增加，导致图的异常程度增加。（b）（d）：异常节点和正常节点的差别增加，异常节点的数量不变，导致图的异常程度增加。仅关注蓝色柱，表示图的异常程度很低，可以看出图信号在低频部分的energy高，而在高频部分的energy较低，即图信号在低频上的响应更多，在高频上的响应更少。随着异常程度的增加，关注红色柱，可以看出图上信号在低频上的响应降低，高频部分响应增加。可以看出<strong>异常数据会导致频谱能量的 “右移”</strong>。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/6.png#center" alt=""  />
</p>
<p>上图在一个fraud graph amazon上，对比原图，随机删除节点，删除异常节点 三种情况下在普通频率上的能量，可以看出，删除异常节点后低频能量上升，而高频能量（$\lambda = 1.0\sim1.2$）下降。所以信号的高频部分可能carry异常节点的性质，考虑高频部分可以帮助模型区分出异常节点。而保留信号的低频部分可以使得正常节点间平滑。因此设计图上的band-pass filter对于fraud graph很重要。<strong>现有的图神经网络大多属于低通滤波器或者自适应滤波器，它们无法保证带通性质。其中自适应滤波器虽然具有拟合任意函数的能力，但在异常检测中同样可能退化为低通滤波器。这是因为在整个数据集中，异常数据对应的高频信息占比较小（类不平衡），而大部分频谱能量仍然集中在低频。</strong></p>
<p>为了保留图上信号的从低频到高频的部分，本文选择使用Beta distribution作为graph kernel function $g(\Lambda)$。Beta distribution的概率密度函数为：
$$
\beta_{p, q}(w)= \begin{cases}\frac{1}{B(p+1, q+1)} w^p(1-w)^q &amp; \text { if } w \in[0,1] \\ 0 &amp; \text { otherwise }\end{cases}
$$
其中$p, q \in \mathbb{R}^{+}$，$B(p+1, q+1)=p ! q ! /(p+q+1) !$是一个常数。由于normalized graph Laplacian $L$ 的特征值$\lambda \in[0,2]$，所以convolution kernel function（用来给不同频率basis加权的函数）定义为$\beta_{p, q}^*(w)=\frac{1}{2} \beta_{p, q}\left(\frac{w}{2}\right)$使得$\beta_{p, q}^*(\lambda)$可解。将$\beta_{p, q}^*(\Lambda)$ 其中$\Lambda$是$L$的特征值对角阵作为convolution kernel function：
$$
\mathcal{W}_{p, q}=\boldsymbol{U} \beta_{p, q}^*(\boldsymbol{\Lambda}) \boldsymbol{U}^T=\beta_{p, q}^*(\boldsymbol{L})=\frac{\left(\frac{L}{2}\right)^p\left(I-\frac{L}{2}\right)^q}{2 B(p+1, q+1)}
$$
在不同的$p,q$设置下，$\mathcal{W}_{p, q}$可以倾向于不同的频率， 如下图所示，在$p=0,q=4$时，$\boldsymbol{U} \beta_{0, 4}^*(\boldsymbol{\Lambda}) \boldsymbol{U}^T$是一个low-pass filter，graph convolution kernel function $\beta_{p, q}^*(\boldsymbol{\Lambda})$为低频部分赋予更高权重。当$p=1, q=3$，以及$p=2, q=2$时，中频部分被赋予更高的权重，当$p=3,q=1$时，高频部分被赋予更高的权重。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/7.png#center" alt=""  />
</p>
<p>将不同$p,q$取值的graph filter结合起来可以得到一个band pass graph filter，然后将不同filter下提取的信号分量做拼接：
$$
\begin{aligned}
\boldsymbol{Z}_i &amp; =\mathcal{W}_{i, C-i}(\operatorname{MLP}(\boldsymbol{X})) \\
\boldsymbol{H} &amp; =\operatorname{AGG}\left(\left[\boldsymbol{Z}_0, \boldsymbol{Z}_1, \cdots, \boldsymbol{Z}_C\right]\right)
\end{aligned}
$$</p>
<h1 id="6-pick-and-choose-a-gnn-based-imbalanced-learning-approach-for-fraud-detection-www-21">6. Pick and Choose: A GNN-based imbalanced learning approach for fraud detection (WWW &lsquo;21)</h1>
<p><img loading="lazy" src="/posts/2023-05-13-FD/8.png#center" alt=""  />
</p>
<h2 id="pick-label-balanced-sampler">Pick: Label-balanced Sampler</h2>
<p>该方法类似于CAER-GNN。CAER-GNN中每个batch抽取的节点为所有fraud training nodes和一半的benign training nodes，然后每个batch node聚合他们相似度高的一阶邻居。和CARE-GNN固定一个batch中不同类节点数量不同的是，PC-GNN为每个training node 设置采样概率，小类（fraud node）更容易被采样到，大类节点被采样的概率较小，每个训练集节点$v$被采样的概率为：
$$
P(v) \propto \frac{||\hat{A}(:, v)||^2}{\operatorname{LF}(C(v))}
$$
其中$\operatorname{LF}(C(v))$为节点$v$所在类的训练节点数，小类的训练节点更容易被采样。分母为节点的度，表示越重要的节点越容易被采样到，这样一个batch中的训练节点可以避免原图中的类不平衡。</p>
<h2 id="choose-neighborhood-sampler">Choose: Neighborhood Sampler</h2>
<p>第二步为图中的choose过程，为每个节点采样要聚合的邻居。对于每个batch node $v$，它的predicted label probabiity embedding 为 $\mathrm{D}_r^{(\ell)}\left(\mathbf{h}_{v, r}^{(\ell)}\right)=\sigma\left(\mathbf{U}_r^{(\ell)} \mathbf{h}_{v, r}^{(\ell)}\right)$，$v$和它的邻居$u$相似度定义为他们之间的embedding的$\ell_1$距离：
$$
\mathcal{D}_r^{(\ell)}(v, u)=||\mathrm{D}_r^{(\ell)}\left(\mathbf{h}_{v, r}^{(\ell)}\right)-\mathrm{D}_r^{(\ell)}\left(\mathbf{h}_{u, r}^{(\ell)}\right)||_1
$$
对于batch node中的benign node，采样与其label probability embedding相似度最相似的一定数量的邻居来聚合，benign node $v$的采样聚合邻居为：
$$
\underline{\mathcal{N}_r^{(\ell)}}(v)=\{u \in \mathcal{V} \mid A_r(v, u)&gt;0 \text { and } \mathcal{D}_r^{(\ell)}(v, u)&lt;\rho_{-}\}
$$
对于batch中的fraud node，除了聚合上述一阶邻居外，还要将batch中其他和它相似度较高的fraud nodes加入它的邻居集合中：
$$
\overline{\mathcal{N}_r^{(\ell)}}(v)=\{u \in \mathcal{V} \mid C(u)=C(v) \text { and } \mathcal{D}_r^{(\ell)}(v, u)&lt;\rho_{+}\}
$$
所以对于fraud node $v$，他的聚合邻居集合为：$\mathcal{N}_r^{(\ell)}(v)=\underline{\mathcal{N}_r^{(\ell)}}(v) \cup \overline{\mathcal{N}_r^{(\ell)}}(v)$。计算节点何其邻居的距离是基于节点的label probability embeddings，基于cross-entropy loss来优化：
$$
\begin{gathered}
\mathcal{L}_{\text {dist }}=-\sum_{\ell=1}^L \sum_{r=1}^R \sum_{v \in \mathcal{V}}\left[y_v \log p_{v, r}^{(\ell)}+\left(1-y_v\right) \log \left(1-p_{v, r}^{(\ell)}\right)\right] \\
p_{v, r}^{(\ell)}=\mathrm{D}_r^{(\ell)}\left(\mathbf{h}_{v, r}^{(\ell)}\right)
\end{gathered}
$$</p>
<h2 id="aggregate-message-passing-architecture">Aggregate: Message Passing Architecture</h2>
<p>每个节点$v$ concat采样出的邻居：
$$
\mathbf{h}_{v, r}^{(\ell)}=\operatorname{ReLU}\left(W_r^{(\ell)}\left(\mathbf{h}_{v, r}^{(\ell-1)} \oplus \mathrm{AGG}_r^{(\ell)}\left\{\mathbf{h}_{u, r}^{(\ell-1)}, u \in \mathcal{N}_r^{(\ell)}(v)\right\}\right)\right)
$$
每个relation各自计算node embedding，然后拼接MLP后得到节点的embedding:
$$
\mathbf{h}_v^{(\ell)}=\operatorname{ReLU}\left(W^{(\ell)}\left(\mathbf{h}_v^{(\ell-1)} \oplus \mathbf{h}_{v, 1}^{(\ell)} \oplus \cdots \oplus \mathbf{h}_{v, R}^{(\ell)}\right)\right)
$$
GNN的loss为最后一层的输出变换为logits，然后计算cross-entropy：
$$
\begin{gathered}
\mathcal{L}_{\mathrm{gnn}}=-\sum_{v \in \mathcal{V}}\left[y_v \log p_v+\left(1-y_v\right) \log \left(1-p_v\right)\right] \\
p_v=\operatorname{MLP}\left(\mathbf{h}_v^{(L)}\right)
\end{gathered}
$$
模型的loss为：
$$
\mathcal{L}=\mathcal{L}_{\mathrm{gnn}}+\alpha \mathcal{L}_{\mathrm{dist}}
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2022 《Local Augmentation for Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/lagcn/</link>
      <pubDate>Wed, 25 Jan 2023 21:19:40 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/lagcn/</guid>
      <description>ICML2022 &amp;#34;Local Augmentation for Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2109.03856">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>在GNN的neighborhood aggregation中，对于拥有很少邻居的节点，在聚合过程中是否充分从邻居中获得了信息是一个问题。为解决该问题， 本文提出为每个节点做局部增强，即以中心节点为条件，学习邻居节点表示的分布。为了在局部邻域中生成一些样本来提升中心节点的neighborhood aggregation，本文提出一种数据增强框架：LA-GNNs， 以局部结构和中心节点特征为条件，生成neighborhood features。具体来说，在pre-training 阶段，通过一个生成模型，以中心节点的特征为条件来学习邻居特征的条件概率分布。然后利用这个邻居特征分布来生成中心节点的增强邻居特征。另外，通过pre-training来学习邻居增强特征生成器的过程是与下游任务无关的，所以该生成器生成的增强特征可以应用于其他GNN模型。</p>
<p><img loading="lazy" src="/posts/2023-01-28-LAGCN/1.png#center" alt=""  />
</p>
<h1 id="local-augmentation-for-graph-neural-networks-lagnn">Local Augmentation for Graph Neural Networks (LAGNN)</h1>
<h2 id="motivation">Motivation</h2>
<p>GNN在message passing的过程利用局部信息聚合来得到node representations。 但是对于邻居数量较少的节点，从邻居中得到的信息可能会不足。为了为节点$v$的邻域中$\mathcal{N}_v$生成更多样本，就需要知道邻居表示的分布。 由于一个节点邻居分布是与中心节点相关，所以我们要以中心节点$v$的representation为条件，学习它的邻居表示分布。</p>
<h2 id="approach">Approach</h2>
<p>本文利用Conditional Variational Auto-Encoder (CVAE) 来学习<strong>给定中心节点$v$，邻居$u \in \mathcal{N}_v$的节点特征的条件分布</strong>。给定中心节点特征$\boldsymbol{X}_v$，关于中心节点的邻居分布为$p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v)$。定义隐变量$\mathbf{z}$，则先验可以定义为$p_\theta(\mathbf{z}|\boldsymbol{X}_v)$。结合隐变量$\mathbf{z}$，邻居特征$\boldsymbol{X}_u$的分布可以改写为如下形式：
$$
\begin{aligned}
\log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v) &amp;= \log \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v)}{p_\theta( \boldsymbol{X}_v)}= \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}{p_\theta( \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} \\
&amp;=\log \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v, \mathbf{z})}{p_\theta( \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} \\
&amp;= \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}\\
\end{aligned}
$$
假设隐变量$\mathbf{z}$的分布为$q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)$， 左右两边对分布$q_\phi$计算期望，左边：
$$
\int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v) dz = \log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v)
$$
右边：
$$
\begin{aligned}
&amp;\int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)  \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} dz \\
=&amp; \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \left(\frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \cdot \frac{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}\right) dz \\
=&amp; \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} dz +  \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \frac{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}dz \\
=&amp; \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} dz + K L\left(q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) || p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)\right) \\
\geq&amp; \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} = ELBO
\end{aligned}
$$
Evidence Lower Bound (ELBO) 可以写为
$$
\begin{aligned}
L_{ELBO} &amp;= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\
&amp;= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \boldsymbol{X}_v, \mathbf{z}\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) p_\theta\left(\boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\
&amp;= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) p_\theta\left(\boldsymbol{X}_v, \mathbf{z}\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) p_\theta\left(\boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\
&amp;= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\
&amp;= -K L\left(q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) || p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)\right)+\int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) \mathrm{d} \mathbf{z} \\
&amp;=  -K L\left(\underbrace{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}_{Encoder} || \underbrace{ p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)}_{\text{Normal Distribution}}\right) + \mathbb{E}_{\mathbf{z} \sim q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) }\log p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right)
\end{aligned}
$$
在CVAE pre-training的过程中，第一项KL中CVAE Encoder 的一对邻接节点对，对于该节点对，输出一组分布参数均值$\mu$和方差$\sigma$，作为隐变量$z$的分布参数，第一项的优化目标使得编码器输出的分布接近Normal Distribution。然后利用reparameterization trick可微的从生成的$\mathbf{z}$分布中采样一个encoding:</p>
<pre tabindex="0"><code>def reparameterize(means, log_var):
    std = torch.exp(0.5 * log_var)
    eps = torch.randn_like(std)
    return means + eps * std   // z
</code></pre><p>若当前输入节点对为$(\boldsymbol{X}_v, \boldsymbol{X}_u)$，从输出的分布中采样一个encoding $\mathbf{z}$然后将$\mathbf{z}$输入decoding中，使得用$\mathbf{z}$和中心节点$\boldsymbol{X}_v$可以重构邻接节点$\boldsymbol{X}_u$。用所有邻接节点对训练encoder参数$\phi$和generator参数$\phi$。 这样在下游预测任务前，直接从Normal Distribution 随机采样$\mathbf{z}$，拼接中心节点$\boldsymbol{X}_v$输入generator中就可以为节点$v$生成增强邻居。过程如下图所示：</p>
<p><img loading="lazy" src="/posts/2023-01-28-LAGCN/2.png#center" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2022 《ProGCL：Rethinking Hard Negative Mining in Graph Contrastive Learning》 Reading Note</title>
      <link>https://JhuoW.github.io/posts/progcl/</link>
      <pubDate>Sun, 08 Jan 2023 22:28:43 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/progcl/</guid>
      <description>ICML2022 &amp;#34;ProGCL：Rethinking Hard Negative Mining in Graph Contrastive Learning&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2110.02027">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>Contrastive Learning 受益于区分hard negatives (最相似的negative pairs)， 但是其他领域的hard negative mining方法不适用于graph。 对于GCL来说大量embedding之后的hard negatives实际上是false negatives。如左图所示，对于CV上的SimCLR，它所学到的高相似度的negatives中，True negatives 和False negatives数量相当，那么从高相似度的negatives中采样到true negatives的概率更大。然而对于GCL方法GCA来说，是每个anchor节点将其他所有（inter/intra）节点作为negatives，使得在训练过程中与它同类的节点也变成anchor的negatives，这些negatives是false negatives。对于GCA，高相似度的negatives中false negatives的数量远多于true negatives，所以直接采样高相似度的negatives作为hard negatives来针对性的判别他们，会导致同类节点的embedding相互远离。这是传统的hard negatives mining方法在graph domain失效的原因。</p>
<p><img loading="lazy" src="/posts/2023-01-15-ProGCL/1.png#center" alt="你想输入的替代文字"  />
</p>
<p>为了解决这个问题， 本文提出利用Beta mixture model来估计对于一个anchor node，它的一个negatve是true negative的概率，结合相似度，来衡量该negative的hardness。即与anchor node相似度越高，且它是true negative的概率越大，那么该节点的hardness越高。</p>
<h1 id="methodology">Methodology</h1>
<h2 id="gcl">GCL</h2>
<p><img loading="lazy" src="/posts/2023-01-15-ProGCL/gcl.png#center" alt="你想输入的替代文字"  />
</p>
<p>如上图所示，InfoNCE将跨图same node视为positives，其他节点对视为negatives，GCL的目标函数如下：
$$
\begin{aligned}
\ell\left(\boldsymbol{u}_{i},\boldsymbol{v}_{i}\right)=
\log \frac{e^{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right) / \tau}}{\underbrace{e^{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right) / \tau}}_{\text{positive pair }}+\underbrace{\sum_{k\neq i}e^{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{k}\right) / \tau}}_{\text{inter-view negative pairs}}+\underbrace{\sum_{k\neq i}e^{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{u}_{k}\right) / \tau}}_{\text{intra-view negative pairs}}},
\end{aligned}
$$
Overall objective定义在所有跨图same node pairs上：
$$
\mathcal{J}=-\frac{1}{2 N} \sum_{i=1}^N\left[\ell\left(\boldsymbol{u}_{\boldsymbol{i}}, \boldsymbol{v}_{\boldsymbol{i}}\right)+\ell\left(\boldsymbol{v}_{\boldsymbol{i}}, \boldsymbol{u}_{\boldsymbol{i}}\right)\right]
$$
如果将GCA中的2层shared GNN替换为MLP，那么contrastive learning将不存在Message Passing，这样得到的true/false negative分布如(b)所示，可以看出Message Passing是GCL和CL之间产生区别关键因素。直观上，MP将anchor与相邻的negatives拉近，而相邻的negatives大多为False negatives（Homophily），所以GCL得到的高相似度negatives中false negatives要远多于True negatives。</p>
<p><img loading="lazy" src="/posts/2023-01-15-ProGCL/exp.png#center" alt="你想输入的替代文字"  />
</p>
<p><strong>Theorem 3.1</strong>: $\mathcal{G}$ is a non-bipartile and connected graph with $N$ nodes $\mathcal{V}=\left\{v_1, \ldots, v_N\right\}$ and $\boldsymbol{X}_i^{(\tau)}$ is the embedding of node $v_i$ after $\tau$ tims message passing. For large enough $\tau$,
$$
\left|\left|\boldsymbol{X}_i^{(\tau)}-\boldsymbol{X}_j^{(\tau)}\right|\right|_2 \leq\left|\left|\boldsymbol{X}_i^{(0)}-\boldsymbol{X}_j^{(0)}\right|\right|_2
$$
该定理说明了Message passing之后，不同节点间的距离会变小。</p>
<h2 id="progcl">ProGCL</h2>
<p><img loading="lazy" src="/posts/2023-01-15-ProGCL/exp2.png#center" alt="你想输入的替代文字"  />
</p>
<p>Figure 4描述了negatives相似度的直方图分布，即每个相似度下true/false negatives的相对数量。例如Figure 4（a），对于true negatives，高相似度的true negatives较少，中等相似度的true negatives较多；对于False negatives，大多相似度较高。所以直接采样高相似度的negatives作为hard negatives很容易采样到false negatives。那么随机采样一个相似度$s$的概率有该相似度在true negatives中的采样概率和在false negatives采样概率共同决定。因此相似度分布可以建模为mixture model。本文用Beta distribution来建模true/false negatives的相似度分布。beta distribution的pdf为：
$$
p(s \mid \alpha, \beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} s^{\alpha-1}(1-s)^{\beta-1},
$$
其中$\alpha, \beta &gt;0$ 是beta distribution的参数，$\Gamma(\cdot)$ 是gamma function。对于$C$个beta distribution的mixture model，相似度$s$在该mixture model中的概率为$s$在true negatives 相似度分布中的概率和false negatives相似度分布中的概率的加权：
$$
p(s)=\sum_{c=1}^{C} \lambda_{c} p(s \mid \alpha_c, \beta_c),
$$
其中$\lambda_c$是mixture coefficients。下面要做的就是通过EM算法来优化两beta mixture model的参数，包括两个beta distribution的$\alpha, \beta$参数以及mixture coefficient，<strong>使得GCL学习得到的negatives similarities从改mixture model中采样的概率最大</strong>，即优化mixture model这个混合概率分布，使其符合所有negatives similarity分布。例如总共有10000个negatives（true+false），这一万个similarity分布要符合分布$p(s)$。用EM算法来优化mixture model $p(s)$时，E-step要计算后验：
$$
p(c\mid s)=\frac{\lambda_{c} p\left(s \mid \alpha_{c}, \beta_{c}\right)}{\sum_{j=1}^{C} \lambda_{j} p\left(s \mid \alpha_{j}, \beta_{j}\right)}.
$$
其中$c$为latent variable。然而优化Beta Mixture Model使其你和所有negatives的similarity分布是计算量巨大的，为了解决这个问题每次迭代只采样$M$ ($M \ll N^2$)个相似度来的分布来优化BMM，使其你和这$M$个相似度的分布。首先计算$M$个相似度在每个beta distribution上的weighted average  $\bar{s}_c$以及variance $v_c^2$：
$$
\bar{s}_c=\frac{\sum_{i=1}^M p\left(c \mid s_i\right) s_i}{\sum_{i=1}^M p\left(c \mid s_i\right)}, \quad v_c^2=\frac{\sum_{i=1}^M p\left(c \mid s_i\right)\left(s_i-\bar{s}_c\right)^2}{\sum_{i=1}^M p\left(c \mid s_i\right)} .
$$
在M-step，如下优化BMM的参数 $\alpha_c, \beta_c, \lambda_c$使得BMM符合$M$个similarity的分布：
$$
\alpha_c=\bar{s}_c\left(\frac{\bar{s}_c\left(1-\bar{s}_c\right)}{v_c^2}-1\right), \quad \beta_c=\frac{\alpha_c\left(1-\bar{s}_c\right)}{\bar{s}_c}，\quad \lambda_c=\frac{1}{M} \sum_{i=1}^M p\left(c \mid s_i\right)
$$
这样通过E-step计算后验和average, variance，M-step基于average,variance和后验更新BMM的参数，循环迭代$I=10$次后，可以得到拟合输入negatives similarity的BMM分布的参数。最终，得到关于相似度$s$的BMM$p(s)$后，构成$p(s)$的两个分布即给定negative similarity $s$，该negative是true negative/false negative的概率为：
$$
p(c \mid s)=\frac{\lambda_c p\left(s \mid \alpha_c, \beta_c\right)}{p(s)}
$$</p>
<h2 id="progcl-weight">ProGCL-weight</h2>
<p>对于一对negative pair，$\boldsymbol{u}_i$为anchor和他的inter-view $\boldsymbol{v}_k$，他们的相似度$s_{ik}$在true negative分布中的概率为$p(c_t|s_{ik})$，概率越大越可能是true negative，同时$s_{ik}$越大越hardness。所以在Contrastive loss中，越hard的true negative要被赋予越大的权重，使得被判别区分。negative pair权重为：
$$
w(i, k)=\frac{p\left(c_t \mid s_{i k}\right) s_{i k}}{\frac{1}{N-1} \sum_{j \neq i}\left[p\left(c_t \mid s_{i j}\right) s_{i j}\right]}
$$
在Contrastive objective中对negatives加权：
$$
\ell_w\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right)=
\log \frac{e^{\frac{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right)}{ \tau}}}{\underbrace{e^{\frac{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right)}{\tau}}}_{\text{positive pair }}+\underbrace{\sum_{k\neq i}w(i,k)e^{\frac{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{k}\right)}{\tau}}}_{\text{inter-view negative pairs}}+\underbrace{\sum_{k\neq i}w(i,k)e^{\frac{\theta\left(\boldsymbol{u}_{i}, \boldsymbol{u}_{k}\right)} {\tau}}}_{\text{intra-view negative pairs}}},
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/clusterscl/</link>
      <pubDate>Thu, 17 Nov 2022 01:33:20 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/clusterscl/</guid>
      <description>WWW2022 &amp;#34;ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://xiaojingzi.github.io/publications/WWW22-Wang-et-al-ClusterSCL.pdf">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>对于<strong>监督对比学习</strong>（Supervised Contrastive Learning, SupCon）, SupCon loss旨在表示空间中拉近属于同一个class的数据点，分离不同类的数据点。 但是SupCon难以处理高类内方差，类间相似度较大的数据集。为了解决该问题，本文提出了Cluster-aware supervised contrastive learning loss (ClusterSCL)。什么是高类内方差，高跨类相似度问题？如图1(a)所示，节点$u_1$和$u_3$ 是同类节点，$u_2$和$u_4$是同类节点。他们是同类节点但在不同的社区中，所以类内方差较大，即同一个类内的节点跨越了多个community。 另外$u_1$和$u_2$， $u_3$和$u_4$，是不同类的节点对， 但他们处在同一个社区中，导致在MPNN过程中，这些处在同一个community中的不同类节点被拉近，导致跨类相似度较高的问题。</p>
<p>如果对节点$u_2$计算SupCon时，如图1(b)所示，SupCon会使得同类节点被拉近，如$u_2$和$u_4$会被拉近。但是$u_3$和$u_4$处在同一个社区中（structurally similar）那么MPNN会使得$u_3$和$u_4$被拉近，所以SupCon在拉近$u_2$和$u_4$的同时，会间接拉近不同类节点$u_2$和$u_3$。同时，对于构成negative pairs的不同类节点，例如$u_1$和$u_2$，SupCon会推远$u_1$和$u_2$，但是$u_1$和$u_5$ structurally similar, 因此会推远$u_1$和$u_2$会间接导致$u_2$和$u_5$这两个同类节点被推远。因此对于一个cluster内节点不同类，且不同cluster中存在同类节点的情况，会导致复杂的决策边界，即<strong>在拉近同类但不同社区的节点时，也会间接拉近不同类不同社区的节点</strong>。<strong>在推远不同类同社区的节点时，也可能间接推远同类同社区的节点</strong>。</p>
<p><img loading="lazy" src="/posts/2022-11-15-ClusterSCL/1.png#center" alt=""  />
</p>
<p>为了解决上述问题，最直接的方法是对于每个cluster，如图1(a)的Community 1，不考虑其他cluster，只对当前cluster内节点做SupCon。但是这么做忽略了跨cluster的同类节点交互，如$u_1$和$u_3$，$u_2$和$u_4$，这些跨cluster的positive pairs可能包含有益的信息。为了解决这个问题，本文提出<strong>cluster-aware data augmentation (CDA)</strong> 聚类感知的数据增强，来为每个节点生成augmented positives and negatives，如图1(b)中ClusterSCL所示。对于每个节点$u$，为它生成positive 和negative samples, 生成的samples 位于或接近$u$所在的cluster。Recall SupCon存在的问题：</p>
<ul>
<li>SupCon会使得$u_2$和$u_4$被拉的太近，从而间接导致$u_2$和$u_3$被拉近，所以对于high intra-class variances，要求不同cluster的同类节点如$u_2$和$u_4$不要被拉太近；</li>
<li>SupCon会使得$u_1$和$u_2$被推远，从而间接导致$u_2$和$u_5$被推远，所以对于high inter-class similarity，要求同一个cluster内的不同类节点如$u_1$和$u_2$不要被拉的太远。</li>
</ul>
<h1 id="method">Method</h1>
<h2 id="two-stage-training-with-supervised-contrastive-loss">Two stage training with Supervised Contrastive Loss</h2>
<p><strong>SupCon encourages samples of the same class to have similar representations, while pushes apart samples of different classes in the embedding space.</strong></p>
<p>First Stage: 计算node embeddings $H = g_\theta(G)$， 然后用SupCon Loss来训练 $g_\theta$ 。即已经知道训练集中的节点label，基于这些节点label，SupCon在embedding space中把同label的节点拉近，不同label的节点分开。$g_\theta$用于得到node embeddings.</p>
<p>Second Stage：基于学习好的$g_\theta$， 用$\hat{Y}=f_\phi\left(g_\theta(G)\right)$来得到logits/prediction。即用cross-entropy loss来训练$f_\theta$。</p>
<h2 id="supcon">SupCon</h2>
<p>从同一个类中采样的节点构成positive pairs。batch中随机采样的节点对为negative pairs。给定$N$个随机采样的节点，对于每个节点，从其对应的class中随机采样一个不为它的节点作为positive pairs。所以一个batch有$N$对positive pairs，共$2N$个节点。</p>
<p>用$I \equiv\{1,2, \ldots, 2 N\}$ 表示一个batch中的node indices。$s_i \in I$表示这$2N$个节点中与节点$v_i$属于同一类的节点的indices。如下式所示， 在一个batch中，令$S_i \subset I$表示$2N$个节点中可以与节点$v_i$构成positive pairs的节点集合。相比其他节点，SupCon的objective是拉近positive pairs。
$$
\max \sum_{i \in I} \frac{1}{\left|S_i\right|} \sum_{s_i \in S_i} \log \frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_{s_i} / \tau\right)}{\sum_{j \in I \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j / \tau\right)}
$$
但是如果batch中与$v_i$同类的节点$v_j$和它不属于同一个cluster，$v_j$所属的cluster不同类的节点较多，那么拉近他们的距离也会间接拉近$v_i$与不同类节点间的距离。</p>
<h2 id="clusterscl">ClusterSCL</h2>
<p>为了解决上述问题，本文提出ClusterSCL。</p>
<h3 id="cluster-aware-data-augmentation-cda">Cluster-aware Data Augmentation (CDA)</h3>
<p>定义隐变量 $c_i$，该隐变量取值范围为$c_i \in \{1,2, \ldots, M\}$ 表示节点$v_i$属于哪一个cluster。给定两个anchor node $v_i$,$v_j$，CDA使用线性插值法为$v_j$生成augmentation：
$$
\tilde{\mathbf{h}}_j=\alpha \mathbf{h}_j+(1-\alpha) \mathbf{w}_{c_i}     \tag{4}
$$
其中$c_i$指示了节点$v_i$所在的cluster。$\mathbf{w}=\left\{\mathbf{w}_m\right\}_{m=1}^M$ 表示每个cluster的cluster prototypes，即每个cluster的中心，serve to characterize the cluster。$\mathbf{w}_{c_i}$表示节点$v_i$所在<strong>cluster</strong>的中心表示。$\tilde{\mathbf{h}}_j$包含了节点$v_j$的信息。并且，由于$\mathbf{w}_{c_i}$是$v_i$所在cluster的prtotype，所以通过调整$\alpha$，可以使$\tilde{\mathbf{h}}_j$位于$v_i$所在cluster的附近或内部。</p>
<p>（1）如果$(v_i, v_j)$是一个batch中的positive pair，$v_i$是anchor节点，如果$v_j$位于$v_i$所在的cluster $c_i$内，那么就需要学到的$\mathbf{h}_j$与$\mathbf{h}_i$尽可能靠近。在SupCon中，需要设置较大的$\alpha$使得$\tilde{\mathbf{h}}_j$保留更多$\mathbf{h}_j$ 。对于anchor节点$\mathbf{h}_i$，将它与$\tilde{\mathbf{h}}_j$拉近的时候，由于$\tilde{\mathbf{h}}_j$保留了更多$v_j$特征，所以$\mathbf{h}_j$也会被和$\mathbf{h}_i$拉近。如图2(a)所示。</p>
<p>（2）如果$(v_i, v_j)$是positive pair，如果$v_j$位于$v_i$所在的cluster $c_i$外时，如果$v_j$周围有negative samples （不一定在该batch中），那么直接拉近$(v_i, v_j)$会间接导致潜在的negative samples也会被拉近，因此对于位于$v_i$所在cluster外的节点$v_j$，要求它最终的表示$\mathbf{h}_j$不能被拉的太近，此时就需要小一些的$\alpha$，使得$\tilde{\mathbf{h}}_j$保留少一些$v_j$的信息，那么在SupCon拉近$\mathbf{h}_i$和$\tilde{\mathbf{h}}_j$的过程不会导致$\mathbf{h}_j$被拉近太多。因为$ \mathbf{w}_{c_i}$占据了$\tilde{\mathbf{h}}_j$的大部分，且它与$\mathbf{h}_i$已经很接近。如图2(b)所示。</p>
<p><img loading="lazy" src="/posts/2022-11-15-ClusterSCL/2.png#center" alt=""  />
</p>
<p>对于negative pair $(v_i, v_j)$。如果$v_j$位于$v_i$所在的cluster $c_i$内，如果直接推远$\mathbf{h}_i$和$\mathbf{h}_j$，会导致如果$v_j$的邻居有$v_i$的positive sample，那么这个positive sample也会被间接推远。所以$\tilde{\mathbf{h}}_j$应该保留较少的$\mathbf{h}_j$，即$\alpha$应该小。但是本文不考虑negative pairs的这种情况了，直接套用posiive的CDA原则。</p>
<p>综合上面的（1）（2）即对于close positive pairs，要让他们尽可能接近，即$\alpha$要大，对于distant positive pairs，要让他们不要太接近，$\alpha$要小一些。所以$\alpha$应与positive pair之间的相似度相关。所以$\alpha$定义如下：
$$
\alpha=\frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j\right)}{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j\right)+\exp \left(\mathbf{h}_i^{\top} \mathbf{w}_{c_i}\right)}
$$
上式分子越大$\alpha$越大，说明对于anchor node $v_i$， 如果的positive sample $v_j$位于它的cluster内（$v_i$与$v_j$相似）,$\mathbf{h}_j$的augmentation $\tilde{\mathbf{h}}_j$要保留越多自身信息。</p>
<h3 id="integraging-clustering-and-cda-into-supcon-learning">Integraging Clustering and CDA into SupCon Learning</h3>
<p>目标是给定节点$v_i$以及它所在的cluster 隐变量$c_i$，$v_i$的positive samples $s_i$的cluster-aware SupCon定义为条件概率：
$$
\begin{aligned}
p\left(s_i \mid v_i, c_i\right) &amp;=\frac{\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)} \\
&amp;=\frac{\exp \left(\mathbf{h}_i^{\top}\left(\alpha \mathbf{h}_{s_i}+(1-\alpha) \mathbf{w}_{c_i}\right) / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top}\left(\alpha \mathbf{h}_j+(1-\alpha) \mathbf{w}_{c_i}\right) / \tau\right)}
\end{aligned} \tag{6}
$$
即对于positive pair $(v_i, s_i)$，最大化$\mathbf{h}_i$和$s_i$的augmentation $\tilde{\mathbf{h}}_{s_i}$之间的一致性，$\alpha$可以依据$s_i$和$c_i$的关系来调整$\mathbf{h}_{s_i}$对于SupCon的贡献，使得$\mathbf{h}_{i}$与$\mathbf{h}_{s_i}$在位于不同cluster的情况下不会被拉的太近。其中$c_i$是隐变量。</p>
<p>首先定义关于节点$v_i$的cluster分布，即$v_i$属于每个$c_i$的概率。给定anchor node $v_i$，它属于cluster $c_i \in \{1,2, \ldots, M\}$的概率定义为：
$$
p\left(c_i \mid v_i\right)=\frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{w}_{c_i} / \kappa\right)}{\sum_{m=1}^M \exp \left(\mathbf{h}_i^{\top} \mathbf{w}_m / \kappa\right)} \tag{7}
$$
$\mathbf{w}_{c_i}$是cluster $c_i$的prototype表示，$v_i$属于在embedding空间中与它相似的cluster的概率更高。ClusterSCL旨在最大化给定锚节点$v_i$，锚节点与其positive sample $s_i$的likelihood：
$$
p\left(s_i \mid v_i\right)=\int_{c_i} p\left(c_i \mid v_i\right) p\left(s_i \mid v_i, c_i\right) d c_i= \sum^M_{m=1} p(m|v_i)p(s_i|v_i,m)    \tag{8}
$$
即给定anchor $v_i$，$s_i$是$v_i$的postive sample的概率为 当$v_i$属于cluster $m$的情况下，$s_i$是其positive sample的概率， over all clusters $m \in M$。</p>
<p>在likelihood Eq.(8)中，anchor node $v_i$ 为待优化参数，它的positive sample $s_i$为观测数据，$c_i$为隐变量。</p>
<h3 id="maximize-likelihood-eq8-via-em">Maximize likelihood Eq.(8) via EM</h3>
<p>Objective: $\mathrm{maximize} \log p(s_i| v_i)$，即最大化positive pairs的条件概率 given anchor node $v_i$。</p>
<p>E-step：$\mathbb{E}_{p(c_i|s_i, v_i)} \log p(s_i, c_i|v_i)$</p>
<p>M-step: $\widehat{v}_i = \arg \max_{v_i} \mathbb{E}_{p(c_i|s_i, v_i)} \log p(s_i, c_i|v_i)$</p>
<p>可见，如果要通过EM算法来优化得到anchor node $v_i$的表示，需要计算后验 $p(c_i|s_i, v_i)$：
$$
\begin{aligned}
p(c_i|s_i, v_i) &amp; = \frac{p(c_i,v_i, s_i)}{p(s_i,v_i)} \\
&amp; = \frac{p(v_i)p(s_i,c_i | v_i)}{p(s_i |v_i) p(v_i)}\\
&amp; = \frac{p(s_i,c_i|v_i)}{p(s_i |v_i)} \\
&amp; = \frac{p(s_i,c_i|v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}\\
&amp; = \frac{\frac{p(s_i,c_i,v_i)}{p(v_i)} = \frac{p(c_i,v_i)}{p(v_i)} \frac{p(s_i,c_i,v_i)}{p(c_i,v_i)} = p(c_i | v_i)p(s_i|c_i,v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)} \\
&amp; = \frac{p(c_i | v_i)p(s_i|c_i,v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}<br>
\end{aligned}\tag{9}
$$
后验中，$p(c_i | v_i)$，$p(m|v_i)$是$v_i$的cluster 分布，在Eq.(7)中给出定义。但是，对于$p(s_i|c_i,v_i)$和$p(s_i | v_i, m)$，Eq.(6)中给出了它的定义，$p\left(s_i \mid v_i, c_i\right) =\frac{\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)}$， 可以看出分母部分需要计算$\mathbf{h}_i$与所有节点，并且还要over all $M$ cluster，因此后验难以计算。为了解决这个问题，我们可以maximize evidence lower bound (ELBO) of  $\log p(s_i | v_i)$：
$$
\begin{aligned}
\log p(s_i | v_i) &amp;= \log \frac{p(s_i,v_i)}{p(v_i)} = \log  \frac{p(s_i,v_i)p(c_i | v_i,s_i)}{p(v_i)p(c_i | v_i,s_i)} \\
&amp;= \log \frac{p(v_i,s_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} \\
&amp;= \log \frac{p(s_i|v_i,c_i)p(v_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} = \log \frac{p(s_i|v_i,c_i)p(c_i|v_i)p(v_i)}{p(v_i)p(c_i | v_i,s_i)} \\
&amp; = \log \frac{p(s_i|v_i,c_i)p(c_i|v_i)}{p(c_i | v_i,s_i)} \\
&amp; = \log p(s_i|v_i,c_i)-\log p(c_i | v_i,s_i) +  \log p(c_i|v_i) \\
&amp; \text{引进一个关于隐变量$c_i$的分布$q(c_i)$，可以是任意形式，这里定义为$q(c_i|v_i,s_i)$}\\
&amp; = \log p(s_i|v_i,c_i) - \log \frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} + \log \frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} \\
&amp;\text{左右两边分别对$q(c_i|v_i,s_i)$求期望} \\
\text{左边} &amp;= \int q(c_i|v_i,s_i) \log p(s_i | v_i) d c_i = \int q(c_i|v_i,s_i) d c_i \cdot \log p(s_i | v_i) = \log p(s_i | v_i) \\
\text{右边} &amp;= \int q(c_i|v_i,s_i) \log  p(s_i|v_i,c_i) dc_i - \underbrace{\int q(c_i|v_i,s_i) \log \frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} dc_i}_{-\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i))} + \underbrace{\int q(c_i|v_i,s_i) \log \frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} dc_i}_{-\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))} \\
&amp;= \int q(c_i|v_i,s_i) \log  p(s_i|v_i,c_i) dc_i + \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i)) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \\
&amp; \geq \underbrace{\mathbb{E}_{q(c_i|v_i,s_i)} \log  p(s_i|v_i,c_i) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))}_{ELBO}
\end{aligned}  \tag{10}
$$
因此， we have：
$$
\log p(s_i | v_i) \geq ELBO = \mathbb{E}_{q(c_i|v_i,s_i)} \log  p(s_i|v_i,c_i) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \tag{10}
$$
所以目标为找到node embeddings $\mathbf{h}_i, \forall i$， 最大化ELBO。接下来可以定义任意关于隐变量$c_i$的vartational distribution $q(c_i)$。 这里将关于$c_i$的variational distribution定义为用mini-batch近似后验的形式：
$$
q\left(c_i \mid v_i, s_i\right)=\frac{p\left(c_i \mid v_i\right) \tilde{p}\left(s_i \mid v_i, c_i\right)}{\sum_{m=1}^M p\left(m \mid v_i\right) \tilde{p}\left(s_i \mid v_i, m\right)}  \tag{11}
$$
其中$\tilde{p}\left(s_i \mid v_i, c_i\right)=\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right) / \sum_{j \in I \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)$，与Eq.(6)不同的是$\tilde{p}\left(s_i \mid v_i, c_i\right)$的分母只计算mini-batch $I$内的negative samples。并且用$\tilde{p}\left(s_i \mid v_i, c_i\right)$来替换了Eq.10 中的$ p(s_i|v_i,c_i)$，文中证明了这种替代的合理性。</p>
<p>在E-step，定义了variational distribution $q\left(c_i \mid v_i, s_i\right)$和likelihood objective的ELBO，在M-step最大化ELBO。给定一个mini-batch $I$，目标函数为最大化$I$中每对positive pairs 的ELBO：
$$
\max \quad \mathcal{L}_{\mathrm{ELBO}}(\theta, \mathbf{w} ; I) \approx \frac{1}{|I|} \sum_{i \in I} \frac{1}{\left|S_i\right|} \sum_{s_i \in S_i} \mathcal{L}_{\mathrm{ELBO}}\left(\theta, \mathbf{w} ; v_i, s_i\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2022 《Graph Condensation for Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gcond/</link>
      <pubDate>Thu, 01 Sep 2022 10:47:21 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gcond/</guid>
      <description>ICLR2022 &amp;#34;Graph Condensation for Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/forum?id=WLEx3Jo4QaB">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文提出图浓缩技术（Graph Condensation），旨在将大图浓缩为一个小图，使得在小图上训练的GNN可以得到和大图相当的效果。通过优化<strong>gradient matching loss</strong>来模拟GNN在原图上的<strong>训练轨迹</strong>，从而解决图浓缩问题。</p>
<p>通常有两个策略来简化图：Graph Sparsification(图稀疏化)和Graph Coarsening(图粗化)。图稀疏化通过减少边数来近似一个图； 图粗化旨在减少节点数量。（1）当节点具有属性特征时，由于稀疏化不会减少节点数量，因此属性量不会减少。 （2）图粗化的目的是保存一些图属性比如主特征值，这可能对下游任务不是最优的保存属性。</p>
<p>本文提出图浓缩，来学习生成图的结构和节点属性，从这两方面同时进行浓缩。对于Reddit数据集，GCond可以将节点数浓缩至0.1%，并且在浓缩图上可以得到和原图相当的效果。如下图所示：</p>
<p><img loading="lazy" src="/posts/2022-09-01-GCond/1.png#center" alt=""  />
</p>
<p>本文解决了图浓缩面临的两个挑战：1. 构建目标函数， 2. 参数化可学习的节点特征和图结构。为了解决上述挑战，本文使用gradient matching loss来匹配每一个training step上原图与浓缩图的GNN参数梯度，使得GNN在浓缩图上的训练趋势与原图相匹配。为了参数化节点特征和图结构，本文将浓缩图的Feature Matrix设为自由参数矩阵，<strong>将浓缩图结构设为关于Feature matrix 的 函数</strong>（基于结构与特征相关联假设），使得计算开销降低。</p>
<h1 id="methodology">Methodology</h1>
<p>A graph $\mathcal{T}=\{\mathbf{A}, \mathbf{X}, \mathbf{Y}\}$，其中$\mathbf{X} \in \mathbb{R}^{N \times d}$是$d$维节点特征，$\mathbf{Y} \in\{0, \ldots, C-1\}^N$ 表示$N$个节点的labels，共有$C$个class。图浓缩旨在学习一个小的生成图$\mathcal{S}=\left\{\mathbf{A}^{\prime}, \mathbf{X}^{\prime}, \mathbf{Y}^{\prime}\right\}$，其中$\mathbf{A}^{\prime} \in \mathbb{R}^{N^{\prime} \times N^{\prime}}$是浓缩图的邻接矩阵，$\mathbf{X}^{\prime} \in \mathbb{R}^{N^{\prime} \times D}$是浓缩图的特征矩阵，$\mathbf{Y}^{\prime} \in\{0, \ldots, C-1\}^{N^{\prime}}$是浓缩图的node labels 其中$N^{\prime} \ll N$，特征维度从$d$变为$D$。图浓缩的目标是基于原图训练过程<strong>学习浓缩图$\mathcal{S}$</strong>，使得在$\mathcal{S}$上训练的GNN应用在原图上的loss最小：
$$
\min_{\mathcal{S}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_{\mathcal{S}}}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right) \quad \text { s.t } \quad \boldsymbol{\theta}_{\mathcal{S}}=\underset{\boldsymbol{\theta}}{\arg \min } \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}}\left(\mathbf{A}^{\prime}, \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right),
$$
Outer：固定GNN参数，优化小图。 Inner: 固定小图，在小图上训练GNN参数。</p>
<p>由于如果就用一个固定的初始化参数来初始化GNN，小图的训练参数${\boldsymbol{\theta}_{\mathcal{S}}}$可能会过拟合一个特定初始化的GNN。 因此为了使得浓缩data可以泛化到随机初始化的GNN $P_{\boldsymbol{\theta}_0}$，上面的目标函数可以改写为：
$$
\min_{\mathcal{S}} \mathrm{E}_{\boldsymbol{\theta}_0 \sim P_{\theta_0}}\left[\mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_{\mathcal{S}}}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right)\right] \quad \text { s.t. } \quad \boldsymbol{\theta}_{\mathcal{S}}=\underset{\boldsymbol{\theta}}{\arg \min } \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}\left(\boldsymbol{\theta}_0\right)}\left(\mathbf{A}^{\prime}, \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right)
$$
具体实现就是在用一个GNN训练${\boldsymbol{\theta}_{\mathcal{S}}}$后，初始化GNN继续训练${\boldsymbol{\theta}_{\mathcal{S}}}$（${\boldsymbol{\theta}_{\mathcal{S}}}$不用初始化）。也就是在不同的初始化GNN情况下训练${\boldsymbol{\theta}_{\mathcal{S}}}$。</p>
<h2 id="graph-condensation-via-gradient-matching">Graph Condensation via Gradient Matching</h2>
<p>通过优化bi-level问题来求解参数过于困难，因此使用gradient matching方法来匹配在不同数据上每次迭代的参数梯度。通过这种方式，模型在浓缩图$\mathcal{S}$上的训练轨迹可以用来模拟原图$\mathcal{T}$上的训练轨迹。模型的参数匹配可以表示为：
$$
\begin{gathered}
\min_{\mathcal{S}} \mathrm{E}_{\boldsymbol{\theta}_0 \sim P_{\boldsymbol{\theta}_0}}\left[\sum_{t=0}^{T-1} D\left(\boldsymbol{\theta}_t^{\mathcal{S}}, \boldsymbol{\theta}_t^{\mathcal{T}}\right)\right] \quad \text { with } \\
\boldsymbol{\theta}_{t+1}^{\mathcal{S}}=\operatorname{opt}_{\boldsymbol{\theta}}\left(\mathcal{L}\left(\operatorname{GNN}_{\boldsymbol{\theta}_t^{\mathcal{S}}}\left(\mathbf{A}^{\prime}, \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right)\right) \text { and } \boldsymbol{\theta}_{t+1}^{\mathcal{T}}=\operatorname{opt}_{\boldsymbol{\theta}}\left(\mathcal{L}\left(\operatorname{GNN}_{\boldsymbol{\theta}_t^{\mathcal{T}}}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right)\right)
\end{gathered}
$$
表示第$t$次迭代时，原图上训练的GNN参数$\boldsymbol{\theta}_t^{\mathcal{T}}$和小图上训练的GNN参数$\boldsymbol{\theta}_t^{\mathcal{S}}$要接近。由于两个GNN初始化参数一致，如果将同一个GNN应用于两个图，要使他们的每一步训练轨迹一致，那么他们每一步的参数梯度应该一致，对于$\mathrm{GNN}_{\theta_t}$，所有$T$步的梯度匹配可以写为：
$$
\min_{\mathcal{S}}\mathrm{E}_{\boldsymbol{\theta}_0 \sim P_{\theta_0}}\left [\sum_{t=0}^{T-1} D\left(\nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_t}\left(\mathbf{A}^{\prime}, \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right), \nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_t}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right)\right)\right]
$$
其中$D$为参数梯度矩阵之间的距离，若$\mathbf{G}^{\mathcal{S}}, \mathbf{G}^{\mathcal{T}} \in \mathbb{R}^{d_1 \times d_2}$， 那么梯度矩阵的差异定义为：
$$
\operatorname{dis}\left(\mathbf{G}^{\mathcal{S}}, \mathbf{G}^{\mathcal{T}}\right)=\sum_{i=1}^{d_2}\left(1-\frac{\mathbf{G}_{\mathbf{i}}^{\mathcal{S}} \cdot \mathbf{G}_{\mathbf{i}}^{\mathcal{T}}}{\left|\left|\mathbf{G}_{\mathbf{i}}^{\mathcal{S}}\right|\right|\left|\left|\mathbf{G}_{\mathbf{i}}^{\mathcal{T}}\right|\right|}\right)
$$</p>
<h2 id="modeling-condensed-graph-data">Modeling Condensed Graph Data</h2>
<p>要使得浓缩图可学习，直接参数化三个矩阵$\mathbf{A}^{\prime}, \mathbf{X}^{\prime}, \mathbf{Y}^{\prime}$并优化是很困难的。 因此本文先确定浓缩图的label矩阵$\mathbf{Y}^{\prime}$，具体来说，对于每个class的<strong>训练节点</strong>，选取特定比例的节点。例如训练集有3个类，每个类选取一定比例的训练节点，这些节点label作为浓缩图的node labels，features作为浓缩图的初始化node features $\mathbf{X}^{\prime}$。注意，这里$\mathbf{X}^{\prime}$是自由可训练参数。 由于在社交网络中，结构通常与节点特征相关，因此将结构$\mathbf{A}^{\prime}$设置成关于特征$\mathbf{X}^{\prime}$的函数，这样减少了参数量：
$$
\mathbf{A}^{\prime}=g_{\Phi}\left(\mathbf{X}^{\prime}\right), \quad \text { with } \mathbf{A}_{i j}^{\prime}=\operatorname{Sigmoid}\left(\frac{\operatorname{MLP}_{\Phi}\left(\left[\mathbf{x}_i^{\prime} ; \mathbf{x}_j^{\prime}\right]\right)+\operatorname{MLP}_{\Phi}\left(\left[\mathbf{x}_j^{\prime} ; \mathbf{x}_i^{\prime}\right]\right)}{2}\right)
$$
带入gradient matching loss中：
$$
\min_{\mathbf{X}^{\prime}, \Phi} \mathrm{E}_{\boldsymbol{\theta}_0 \sim P_{\theta_0}}\left[\sum_{t=0}^{T-1} D\left(\nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_t}\left(g_{\Phi}\left(\mathbf{X}^{\prime}\right), \mathbf{X}^{\prime}\right), \mathbf{Y}^{\prime}\right), \nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\mathrm{GNN}_{\boldsymbol{\theta}_t}(\mathbf{A}, \mathbf{X}), \mathbf{Y}\right)\right)\right]
$$
其中：GNN的参数会重复初始化，增强$\theta_\mathcal{S}$的泛化效果。</p>
<p><img loading="lazy" src="/posts/2022-09-01-GCond/2.png#center" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>MLP and GNNs</title>
      <link>https://JhuoW.github.io/posts/glnn/</link>
      <pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/glnn/</guid>
      <description>Scalable GNN with MLP 相关论文总结</description>
      <content:encoded><![CDATA[<p>最近一些工作通过解耦Message-Passing 和 Feature Learning的方式来提升GNN的可拓展性，这里对一小部分相关工作做一个小总结。</p>
<h1 id="1-combining-label-propagation-and-simple-models-out-performs-graph-neural-networks-iclr2021">1. Combining Label Propagation and Simple Models Out-performs Graph Neural Networks （ICLR2021）</h1>
<p><img loading="lazy" src="/posts/2022-07-13/C_S.png#center" alt=""  />
</p>
<p>模型首先忽略图结构，用简单模型（MLP），只使用节点特征预测label：</p>
<p>$$
\min \sum_{i \in L_{t}} \ell\left(f\left(x_{i}\right), y_{i}\right)
$$
考虑一个inductive bias：预测误差与邻近度关系强相关，对图中所有节点的误差做校正。</p>
<p>具体来说，首先计算一个初始的误差矩阵$E$，其中训练集误差如下
$$
E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:}
$$
其他节点的误差未知：$E_{L_{v},:}=0, \quad E_{U,:}=0$。然后通过Label Propagation将误差矩阵在图上做平滑，使得相邻节点的误差相似：</p>
<p>$$
\hat{E}=\underset{W \in \mathbb{R}^{n \times c}}{\arg \min } \operatorname{trace}\left(W^{T}(I-S) W\right)+\mu||W-E||_{F}^{2}
$$
由此得到所有节点的误差矩阵$\hat{E}$。然后用$\hat{E}$对基础MLP预测做校正，这个post-processing过程不涉及训练参数，校正后的预测为：
$$
Z^{(r)} = Z + \hat{E}
$$
考虑homophily：校正的预测label要满足相邻节点label相似。 注意，这里不直接对$Z^{(r)}$做Label Propagation，而是构造了一个label矩阵$H \in \mathbb{R}^{n \times c}$，其中将训练集真实label和验证+测试集校正label加入$H$中，然后对$H$做label propagation：
$$
\begin{aligned}
H_{L_{t},:}&amp;=Y_{L_{t},:} \\
H_{L_{v} \cup U,:}&amp;=Z_{L_{v} \cup U,:}^{(r)}
\end{aligned}
$$
Label Prop:
$$
H^{(t+1)}=(1-\alpha) H+\alpha S H^{(t)}
$$
最后直接用收敛的$H$做预测，即$\hat{Y} = H^{\infty}$，node $i$ 的预测class为：
$$
y_i = \arg \max _{j \in\{1, \ldots, c\}} \hat{Y}_{i j}
$$</p>
<h1 id="2-graph-less-neural-networks-teaching-old-mlps-new-tricks-via-distillation-iclr2022">2. Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation (ICLR2022)</h1>
<p>MLPs相对于GNN更易于部署，并且避开了在线预测过程中的<strong>冷启动问题</strong>，即在线预测时，新节点的加入，它的邻居可能不能立即获得，MLP无需依赖于图结构，相比于GNN，MLP的延迟低。而GNN依赖于节点上下文，准确性更高。如何融合GNN和MLP的有点是本文解决的挑战。</p>
<p>本文的核心发现为：可以在不显著损失性能的情况下将知识从 GNN 提取到 MLP，从而大大减少节点分类的<strong>推理时间</strong>。 知识蒸馏（Knowledge Distillation, KD）可以离线完成，并且与模型训练相耦合，即<strong>将推理阶段的耗时迁移到训练阶段，因为训练阶段可以容忍高耗时，而推理通常需要较大的时间减少。</strong>  也就是 训练阶段不仅要训练GNN模型，还要将模型的知识迁移到MLP上，而推理阶段直接用MLP做预测。</p>
<p><strong>Motivation：</strong> GNN虽然可以取得较好的性能，但是由于它依赖于图结构，所以有较大的推理延迟。 每增加一层GNN，就要为每个节点多捕获一跳邻居。对于一个平均度为$R$的图，在用$L$层GNN预测一个节点时，需要先获取的邻居数量为$\mathcal{O}(R^L)$。 同时，由于层是sequential，所以要逐层获取邻居，总的延迟会随层数加深而增大，每层需要融合的邻居数也成指数上升趋势，是的层数越深，预测延迟越高。</p>
<p>相反，MLP不利用图结构，使得推理时间远小于GNN，但是损害了节点分类的预测性能，因此 如何同时兼顾GNN和MLP的优势，是的模型以获得高精度和低延迟是一个待解决问题，因此本文提出了GNN和MLP的跨模型Knowledge Distillation。</p>
<p><img loading="lazy" src="/posts/2022-07-13/GLNN.png#center" alt=""  />
</p>
<p>本文提出了GLNN，如上图所示，训练一个“Boosted” MLP，他的Knowledge来自于一个Teacher GNN。如上图所示，首先训练好一个GNN模型（这里用GraphSAGE+GCN Aggregation）作为Teacher，GNN的为节点集$v \in V$的预测输出为$\boldsymbol{z}_{v}$。 然后训练一个student MLP，predictions为$\hat{\boldsymbol{y}}_{v}$。loss由两部分组成，第一部分直接用MLP做半监督预测的损失；第二部分为KD，使得MLP对所有节点的预测与GNN的预测接近：
$$
\mathcal{L}=\lambda \Sigma_{v \in \mathcal{V}^{L}} \mathcal{L}_{\text {label }}\left(\hat{\boldsymbol{y}}_{v}, \boldsymbol{y}_{v}\right)+(1-\lambda) \Sigma_{v \in \mathcal{V}} \mathcal{L}_{\text {teacher }}\left(\hat{\boldsymbol{y}}_{v}, \boldsymbol{z}_{v}\right)
$$
其中$\mathcal{L}_{\text {label }}$为cross-entropy loss， $\mathcal{L}_{\text {teacher }}$为KL-divergence。在推理阶段直接使用MLP来预测测试集节点label，无需依赖图结构。预测阶段，直接用MLP做预测。实做中直接去掉了第一部分，只保留KD部分。</p>
<h1 id="3-node-dependent-local-smoothing-for-scalable-graph-learning-neurips2021">3. Node Dependent Local Smoothing for Scalable Graph Learning （NeurIPS2021）</h1>
<p>以往的工作已经证明了，简单的MLP+Label Smoothing的性能可以超过vanilla GCN。但是如何控制模型的平滑程度（extent of smoothness）依然是个问题。 太少的平滑迭代会造成欠平滑（under-smoothing）问题，而太多的迭代会造成过平滑（oversmoothing）问题。另外，不同节点应有特定的平滑程度。大多数现有的GNN使用一个统一的迭代次数$k$，即每个节点都聚合它的$k$阶邻居。 这种统一的聚合方式存在问题，因为迭代次数应与每个节点的度和局部结构相关。如下图所示，两个红色节点有完全不同的局部结构。 左边的红色节点位于Dense region中，因此它的传播速度更快，即很少的step就可以扩散到很多节点，因此，对于这类节点，需要较少次的propagation，因为小的iteration足以聚合足够多的节点。而右边红色节点需要更多次的propagation来聚合足够的信息。</p>
<p><img loading="lazy" src="/posts/2022-07-13/NDLS_1.png#center" alt=""  />
</p>
<p>本文提出了Node-dependent Local Smoothing (NDLS)， 计算每个节点的特定迭代次数（LSI），使得节点只会聚合它特定LSI以内的节点。</p>
<p><strong>Over-Smoothing issue:</strong> <em>The convolution matrix is defined as $\widetilde{\mathbf{D}}^{r-1} \tilde{\mathbf{A}} \widetilde{\mathbf{D}}^{-r}$.  By continually smoothing the node feature with infinite number of propagation in SGC, the final smoothed feature $\mathbf{X}^{(\infty)}$ is：</em>
$$
\mathbf{X}^{(\infty)}=\hat{\mathbf{A}}^{\infty} \mathbf{X}, \quad \hat{\mathbf{A}}_{i, j}^{\infty}=\frac{\left(d_{i}+1\right)^{r}\left(d_{j}+1\right)^{1-r}}{2 m+n}
$$
可以看到，无限多层的SGC，卷积矩阵$\hat{\mathbf{A}}_{i, j}^{\infty}$每个元素只和两个节点的度，节点数$n$以及边数$m$有关，即$\hat{\mathbf{A}}^{\infty} \mathbf{X}$表示节点聚合图中所有节点，聚合权重只与两个节点的度有关，与节点位置，距离根节点距离无关。因此度越大的节点被赋予更大的聚合权重，无论两个节点的相对位置如何。</p>
<p><strong>Local Smoothing Iteration (LSI)：</strong> 对于SGC，第$k$次迭代为$\mathbf{X}^{(k)}=\hat{\mathbf{A}}^{k} \mathbf{X}$。对于第$h$个feature，定义Influence matrix $I_{h}(k)$：
$$
I_{h}(k)_{i j}=\frac{\partial \hat{\mathbf{X}}_{i h}^{(k)}}{\partial \hat{\mathbf{X}}_{j h}^{(0)}}
$$
$I_{h}(k)_{i j}$ 表示在第$h$个feature处，节点$j$的变化对于<strong>节点$i$第$k$层输出的影响</strong>。因为$\mathbf{X}^{(k)}_{ih}=\hat{\mathbf{A}}^{k}_i \mathbf{X}_{:,h}$， 所以$\frac{\partial \hat{\mathbf{X}}_{i h}^{(k)}}{\partial \hat{\mathbf{X}}_{j h}^{(0)}} = \hat{\mathbf{A}}^{k}_{ij}$， 与特征$h$无关，因此节点$j$的输入特征对于节点$i$的第$k$层表示的影响为$\hat{\mathbf{A}}^{k}_{ij}$。那么第$k$次迭代的影响力矩阵可以写为：
$$
I(k)=\hat{\mathbf{A}}^{k}
$$</p>
<p>$$
\tilde{I} = I(\infty)=\hat{\mathbf{A}}^{\infty}
$$</p>
<p>影响力矩阵$I(\infty)$收敛于稳态分布，即无限多层GNN时，节点$j$对$i$的representation的影响只与两个节点的度有关，与他们之间的结构关系无关。</p>
<p>$\tilde{I}_i$为节点$v_i$的over-smoothing stationarity，LSI衡量了其他节点对$v_i$的影响力到达over-smoothing所需最少的迭代次数：
$$
K(i, \epsilon)=\min \left\{k:\left|\left|\tilde{I}_{i}-I(k)_{i}\right|\right|_{2}&lt;\epsilon\right\}
$$
上式可以通过迭代次数来控制节点$v_i$的平滑程度，是node-specific的。</p>
<p><strong>NDLS Pipeline</strong> 1). 节点依赖的局部平滑 （NDLS-F） 2). 基于平滑特征的base prediction 3). 节点依赖的标签平滑（NDLS-L）。其中，第一步为pre-processing，第三部为post-processing。图结构仅用于第一部和第三部，参数的训练过程不涉及图结构，因此更加scalable。</p>
<p>LSI和参数$\epsilon$ 使得每个节点可以与oversmoothing保持一个合适的距离。NDLS-F和NDLS-L利用label smoothing和node smoothing, 具体如下:</p>
<ul>
<li>
<p>NDLS-F</p>
<p>对于节点$i$， 计算它的LSI$K(i,\epsilon)$，对节点$i$做$K(i,\epsilon)$次propagation，然后做multi-scale features residual connection:
$$
\widetilde{\mathbf{X}}_{i}(\epsilon)=\frac{1}{K(i, \epsilon)+1} \sum_{k=0}^{K(i, \epsilon)} \mathbf{X}_{i}^{(k)}
$$
上式的矩阵形式可写为：
$$
\tilde{\mathbf{X}}(\epsilon)=\sum_{k=0}^{\max_{i} K(i, \epsilon)} \mathbf{M}^{(k)} \mathbf{X}^{(k)}, \quad \mathbf{M}^{(\mathbf{k})}{ }_{i j}=\left\{\begin{array}{l}
\frac{1}{K(i, \epsilon)+1}, \quad i=j \quad \text { and } \quad k \leq K(i, \epsilon) \\
0, \quad \text { otherwise }
\end{array}\right.
$$</p>
</li>
<li>
<p>Base Prediction</p>
<p>基于NDLS-F的得到的smoothed features $\tilde{\mathbf{X}}$训练一个base predictor，$\hat{\mathbf{Y}}=f(\widetilde{\mathbf{X}})$。 $\hat{\mathbf{Y}}$ 为模型预测的soft label (softmax output)。</p>
</li>
<li>
<p>NDLS-L</p>
<p>将预测的soft label $\hat{\mathbf{Y}}$ 再做Label Propagation，得到最终的预测结果。依然先计算在做Label Prop时，每个节点的LSI。 $\hat{\mathbf{Y}}^{(k)}=\hat{\mathbf{A}}^{k} \hat{\mathbf{Y}}$， 同理，影响力矩阵为$J_h(k) = I_h(k)$，LP for node $i$：
$$
\tilde{\mathbf{Y}}_{i}(\epsilon)=\frac{1}{K(i, \epsilon)+1} \sum_{k=0}^{K(i, \epsilon)} \hat{\mathbf{Y}}_{i}^{(k)}
$$</p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2021 《Combining Label Propagation and Simple Models Out-performs Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/c_and_s/</link>
      <pubDate>Mon, 11 Jul 2022 09:42:15 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/c_and_s/</guid>
      <description>ICLR2021 &amp;#34;Combining Label Propagation and Simple Models Out-performs Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=8E1-f3VhX1o">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文研究了结合更简单的模型来处理transductive node classification任务。 主要包括1个预测模块和两个后处理（post-processing）模块：</p>
<ul>
<li>Base predictor：忽略图结构，用简单模型（如MLP或线性模型）使用节点特征预测label</li>
<li>Error correction：校正步骤，将训练数据中的不确定性（误差）传播到图上，来校正Base predictor的预测</li>
<li>Smoothing：在图上平滑预测</li>
</ul>
<p>其中只有第一步base predictor的参数是可学习的，即涉及图结构的操作（Correction和Smoothing）无需参数学习，这种简单的模型使得参数数量减少了几个数量级，训练时间也减少了几个数量级，并且可以轻松扩展到大规模图。</p>
<p>相比于过去的GNN+LP的方法，C&amp;S更加高效：1）C&amp;S首先只使用节点特征进行低成本的base prediction；2）然后再使用标签传播对基础预测进行校正 ；3）最后对最终预测进行平滑。 第一步是预测操作，后两部是后处理操作，也就是第一步为一个独立的端到端模型，后两部基于一个inductive bias来调整节点的表示。即homophily假设：相连节点的误差和label是相似的（正相关）。训练节点的误差和它相连节点的误差应相似，那么就用训练节点的误差来校正邻居节点。</p>
<p>因此，将标签更加直接的整合到GNN的学习算法中是本文性能的关键，并且发现LP与node features是相互互补的信号。实验表明，在OGB-Products上，参数量比GNN少了2个数量级，训练时间也减少2个数量级。</p>
<h1 id="correct-and-smooth-cs-model">Correct and Smooth (C&amp;S) Model</h1>
<p>给定无向图$G=(V,E)$，$A$为邻接矩阵，$S=D^{-1 / 2} A D^{-1 / 2}$为归一化邻接矩阵。节点集划分为labeled nodes $V_L$和unlabeled nodes $V_U$，其中$V = V_L \cup V_U$。进一步，labeled nodes可以分为训练节点集$V_{L_t}$和验证节点集$V_{L_v}$。训练集和验证集的label分别为$Y_{L_t:}$和$Y_{L_v:}$， 每行为label的one-hot向量。</p>
<p><img loading="lazy" src="/posts/2022-07-11-CS/1.png#center" alt=""  />
</p>
<h2 id="simple-base-predictor">Simple Base Predictor</h2>
<p>$$
\min \sum_{i \in L_{t}} \ell\left(f\left(x_{i}\right), y_{i}\right)
$$</p>
<p>$f(\cdot)$为简单的训练模型+softmax，如浅层MLP， $\ell$为cross-entropy loss。 基于训练节点$V_{L_t}$特征的模型$f$可以得到输出预测$Z \in \mathbb{R}^{n\times c}$， 其中$Z$的每行是softmax得到的分类概率分布。Simple Base Predictor是一个独立训练的端到端模型。</p>
<h2 id="correcting-base-prediction-with-error-correlation-使用邻居误差关联来纠正基础预测">Correcting Base Prediction with Error Correlation (使用邻居误差关联来纠正基础预测）</h2>
<p>通过融合标签信息来提高base prediction $Z$的准确率。 本文期望base prediction中的误差沿着图中的边正相关，即节点$i$出的预测误差在它的邻居处也会出现相似的误差。为了实现这个目的，首先定义一个误差矩阵$E \in \mathbb{R}^{n \times c}$用来保存每个节点的预测误差，其中误差为训练数据集上的残差（只有训练节点由误差）其他没有训练过程中不知道label的节点误差设为0：
$$
E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:}  \quad 为训练集节点 V_{L_t}的误差
$$</p>
<p>$$
E_{L_{v},:}=0, \quad E_{U,:}=0  \quad 验证集和测试集节点的误差设为0
$$</p>
<p>若base predictor做出perfect prediction时，$E$将是一个全0矩阵。</p>
<p>然后要在$E$中填补图中其他节点（验证集和测试集节点）的误差。依据homophily假设，相邻接节点的误差相似，因此使用标签扩散技术来平滑误差，即优化一下函数：
$$
\hat{E}=\underset{W \in \mathbb{R}^{n \times c}}{\arg \min } \operatorname{trace}\left(W^{T}(I-S) W\right)+\mu||W-E||_{F}^{2}
$$
实际上就是Laplacian Smoothing，$W \in \mathbb{R}^{n \times c}$表示$c$个信号，最小化第一项用来保证$W$每一列在图上平滑，即相邻的节点的误差向量$W_i \in \mathbb{R}^c$相似。第二项要求$W$要尽量接近$E$。最优的$W$表示为$\hat{E}$，上式可以通过：
$$
E^{(t+1)}=(1-\alpha) E+\alpha S E^{(t)}
$$
迭代求解，其中$\alpha = 1/(1+\mu)$。得到的$\hat{E}$称为smoothed errors。Base predictor中得到的$E_{L_{t},:}$只包含训练节点的误差，而通过在图上的误差平滑后，基于homophily 假设可以得到图中所有的误差（平滑误差）。已知图中所有节点在base predictor中的预测为$Z$，它的误差矩阵为$\hat{E}$，然后用误差矩阵来校正预测结果：
$$
Z^{(r)} = Z + \hat{E}
$$
通过这种方式对图中所有节点的误差做校正会存在一个问题，已知训练集的总误差为$||E||_2$， 通过迭代计算得到的总误差为$||E^{(t)}||_2$，且$||S||_2 = 1$, 所以下式成立：
$$
||E^{(t+1)}||_2 = ||(1-\alpha)E + \alpha S E^{(t)}||_2 \leq (1-\alpha)||E||_2+\alpha ||S||_2||E^{(t)}||_2 = (1-\alpha)||E||_2+\alpha ||E^{(t)}||_2
$$
因为 $||E^{(1)}||_2 \leq (1-\alpha)||E||_2 + \alpha ||E||_2 = ||E||_2$， 可以推出$||E^{(2)}||_2\leq (1-\alpha)||E||_2 + \alpha ||E^{(1)}||_2 \leq (1-\alpha)||E||_2 + \alpha ||E||_2 = ||E||_2$。因此，可以得到：
$$
||E^{(t)}||_2 \leq ||E||_2
$$
可以看出，传播之后的总error小了，因此不能完全纠正所有节点上的error。并且实验发现，对残差做放缩可以取得实质上的帮助。因此，本文提出两种方式对误差（残差）做放缩。</p>
<ul>
<li>
<p><strong>Autoscale.</strong> 希望平滑后的总误差$\hat{E}$可以放缩到和训练集误差$E$差不多大小。 由于我们只知道训练节点上的真实误差，所以用训练集节点上的平均误差来缩放。形式上，令$e^T_j \in \mathbb{R}^c$表示$E$的第$j$行，即节点$j$的误差，用$\hat{e}^T_j \in \mathbb{R}^c$表示平滑之后的节点$j$误差，即$\hat{E}$的第$j$行。 定义：
$$
\sigma=\frac{1}{\left|L_{t}\right|} \sum_{j \in L_{t}}\left|\left|e_{j}\right|\right|_{1}
$$
$\sigma$为训练集节点的平均误差，对于每个unlabeled node $i \in  V_U$，它的校正prediction为：
$$
Z_{i,:}^{(r)}=Z_{i,:}+\frac{\sigma}{\left|\left|\hat{e}_{i}\right|\right|_{1}} \cdot \hat{e}_{i}^{T}
$$
其中校正误差为 $\frac{\hat{e}_{i}^{T}}{\left|\left|\hat{e}_{i}\right|\right|_{1}} \cdot \sigma$， 表示对校正的误差做放缩，使得unlabeled node每个节点的校正误差为训练集节点的平均校正误差。</p>
</li>
<li>
<p><strong>Scaled Fixed Diffusion (FDiff-scale).</strong> 每次传播完，把training node的误差设为真实误差再进行下一次传播。另外，本文发现用超参数来放缩误差校正也是有效的：$Z^{(r)}=Z+s \hat{E}$。</p>
</li>
</ul>
<h2 id="smoothing-final-predictions-with-prediction-correlation">Smoothing Final Predictions with Prediction Correlation</h2>
<p>在用$\hat{E}$校正base prediction $Z$后得到校正预测矩阵$Z^{(r)}$。为了得到最后的预测，本文进一步对校正预测做平滑处理。 动机是：图中相邻的顶点可能具有相似的标签，即homophily假设。而在对base prediction做校正后，仅是的相邻的节点具有相似的误差（误差正相关），为了使其进一步满足homophily假设（即标签正相关），本文通过另一个LP来使得label在图上是平滑的：定义一个预测矩阵$H \in \mathbb{R}^{n \times c}$, 将训练集ground-truth label赋值给对应位置：
$$
H_{L_{t},:}=Y_{L_{t},:}
$$
然后将验证集和测试集节点的位置赋值为校正预测 （平滑误差后的预测）：
$$
H_{L_{v} \cup U,:}=Z_{L_{v} \cup U,:}^{(r)}
$$
然后对矩阵$H$做LP:
$$
H^{(t+1)}=(1-\alpha) H+\alpha S H^{(t)}
$$
其中 $H^{(0)} = H$，直到收敛，即收敛的$H^{(T)}$会尽可能保持平滑，并且和$H^{(0)}$接近，即最优$H$会依据训练集label使得相邻节点的label尽可能一样的同时，对图中节点做校正。</p>
<h3 id="与appnp的关系">与APPNP的关系</h3>
<p>APPNP也可以视为先特征变换，再平滑的过程，但APPNP是端到端的过程，label信息没有被加入平滑过程中。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2022 《GLASS：GNN with Labeling Tricks for Subgraph Representation Learning》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/glass/</link>
      <pubDate>Thu, 09 Jun 2022 23:01:07 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/glass/</guid>
      <description>ICLR2022 &amp;#34;GLASS：GNN with Labeling Tricks for Subgraph Representation Learning&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/forum?id=XLxhEjKNbXj">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>SubGNN在学习子图representation时保留子图的三种属性：Position，Neighborhood，Structure，每种属性包含Internal 和Border两方面，并且要精心设计不同的anchor patch，所以过于复杂。通过分析SubGNN和普通GNN，作者发现子图表示的核心可能是区分子图内部和外部节点。基于此发现，本文提出了一种labeling trick, 即max-zero-one，来提升子图GNN的表达能力和可拓展性。</p>
<p><img loading="lazy" src="/posts/2022-06-09-GLASS/1.png#center" alt=""  />
</p>
<p>Subgraph representation task 如上图所示，目标子图$\mathbb{S}$被嵌入在整个图中，并且可能拥有多个连通分量，目标是学习子图的表示向量，使其可以预测子图的属性。NeurIPS2020文章SubGNN提出子图级message-passing来代替节点级的message passing，并且设计了三个message passing通道，每个通道分为内部和边界模块，分别捕获子图分量间的交互，以及子图与图的其他部分之间的交互。尽管取得了比普通GNN更好的效果，但是SubGNN需要繁琐的预计算，因为SubGNN通过不同采样规则的anchor patch来传递子图分量之间，以及子图分量与图其他部分之间的相关性，而三个通道共6个aspects需要不同的采样规则，以及各自的message passing，计算十分冗长（<a href="https://jhuow.fun/posts/subgnn/">这里</a>有解读）。另外 SubGNN对每个aspects需要使用不同的anchor patch随机采样策略，无法保证采样的anchor patch是最优的，因此效果的方差较大，使得鲁棒性堪忧。</p>
<p>通过对比SubGNN相较于普通GNN的优势，作者发现对于<strong>子图任务来说，区分子图内部节点和外部节点非常重要</strong>。基于这个发现，本文提出了一种<em>labeling trick</em>，即max-zero-one labeling trick，来标注每个节点是否在子图外或者子图内。</p>
<p><strong>Labeling Trick [1]:</strong> 使用GNN生成multi-node representations （即，为一组节点，例如子图生成表示向量），该方法说明了为高阶结构生成有表达能力的representation，需要捕获结构内不同节点间的交互。在实现上，labeling trick通过一个专门设计的label来表示节点的结构特征，这个label与node feature 结合作为新的feature输入GNN中。</p>
<p>注： 本文只考虑诱导子图，即每个子图的每个连通分量保留原图中的所有边。</p>
<h1 id="plain-gnn-and-subgnn">Plain GNN and SubGNN</h1>
<p><img loading="lazy" src="/posts/2022-06-09-GLASS/2.png#center" alt=""  />
</p>
<p>如上图所示，$G$是一个regular graph, 所以在没有节点feature的情况下，每个节点的embedding相同，所以GNN无法区分子图$\mathcal{S}$和$\mathcal{S}^\prime$。如下图所示，Plain GNN 在message passing中子图$\mathcal{S}$内部节点1同时接收来自子图内和子图外的邻居信息，并不会加以区分。同样$\mathcal{S}^\prime$中节点3也同时接收子图内外节点，因此对于Plain GNN ，它无法区分节点1和3，因此无法区分两个子图。</p>
<p><img loading="lazy" src="/posts/2022-06-09-GLASS/3.png#center" alt=""  />
</p>
<p>而SubGNN引入了3个通道：position (P)，neighborhood (N), 和structure (S) 每个通道分别学习Internal 和Border两方面，共6个属性融入子图表示学习中。对于子图$\mathcal{S}$，为了捕获某个通道$i$的属性，SubGNN首先随机采样$n_A$个anchor patches： $\mathbb{A}_{i}=\left\{\mathcal{A}_{i}^{(1)}, \ldots, \mathcal{A}_{i}^{\left(n_{A}\right)}\right\}$，然后学习$\mathcal{S}$中的每个连通分量在这个属性$i$下的表示向量，通过子图内部连通分量和anchor patches之间的消息传递，来捕获子图内部连通分量的相对位置/邻域/结构信息，以及子图连通分量相对于子图外部分的位置/邻域/结构信息。如图2右边所示。对于通道$i$，它的Internal和border两方面采样的anchor patches表示为$\mathbb{A}_{i}=\left\{\mathcal{A}_{i}^{(1)}, \ldots, \mathcal{A}_{i}^{\left(n_{A}\right)}\right\}$，对于子图$\mathcal{S}$的一个连通分量$\mathcal{S}^{(c)}$，要学习该连通分量的表示，可使用一下subgraph-level message passing layer:
$$
\begin{aligned}
&amp;\boldsymbol{a}_{i, \mathcal{S}^{(c)}}=\sum_{\mathcal{A}_{i} \in \mathbb{A}_{i}} \gamma_{i}\left(\mathcal{S}^{(c)}, \mathcal{A}_{i}\right) \boldsymbol{g}_{\mathcal{A}_{i}}, \\
&amp;\boldsymbol{h}_{i, \mathcal{S}^{(c)}}^{(k)}=\sigma\left(W_{i} \cdot\left[\boldsymbol{a}_{i, \mathcal{S}^{(c)}}, \boldsymbol{h}_{i, \mathcal{S}^{(c)}}^{(k-1)}\right]\right)
\end{aligned}
$$
其中$\gamma_{i}\left(\mathcal{S}^{(c)}, \mathcal{A}_{i}\right)$是子图分量$\mathcal{S}^{(c)}$和一个anchor patch $\mathcal{A}_{i}$的相似度。即每个子图分量依照与anchor patch 的相似度聚合来自anchor的信息。由于相似度函数的存在，SubGNN实际上是使用与子图分量$\mathcal{S}^{(c)}$接近或结构相似的anchor patch对$\mathcal{S}^{(c)}$的representation做平滑，即$\mathcal{S}^{(c)}$聚合更多与它结构相似的anchor patches的信息。通过精心设计的anchor和subgraph-level message passing，6个属性可以被各自保留，然后在融合。</p>
<p>Plain GNN 存在的问题在于不能很好的表示内部结构和外部结构，即Plain GNN在message passing过程中不能为子图中的节点判断它的邻居是在子图内还是子图外。 而SubGNN如Figure 2右边所示， 子图内节点1接收Internal消息和border消息在两个独立的message passing中，回味每个节点生成2个表示向量，分别表示内部MP和外部MP，因为节点1和3内外部节点不一样，所以SubGNN可以为这两个节点生成不同的representations。</p>
<h1 id="glassgnns-with-labeling-tricks-for-subgraph">GLASS：Gnns with LAbeling trickS for Subgraph</h1>
<p>首先介绍zero-one label trick：</p>
<p><strong>Definition 1 （zero-one label trick）:</strong> 给定一个图$\mathcal{G}$和它的一个子图$\mathcal{S}$，对与子图$\mathcal{S}$， 图$\mathcal{G}$中的任意一个节点$v$的zero-one标签为：
$$
l_{v}^{(\mathcal{S})}= \begin{cases}1 &amp; \text { if } v \in \mathbb{V}_{\mathcal{S}} \\ 0 &amp; \text { if } v \notin \mathbb{V}_{\mathcal{S}}\end{cases}
$$
即对于一个子图$\mathcal{S}$，对图中所有节点赋予一个node label，用来区分节点在$\mathcal{S}$内外。</p>
<h2 id="max-zero-one-labeling-trick">Max-Zero-One Labeling Trick</h2>
<p>对每个节点做zero-one labeling trick可以区分<strong>一个</strong>子图的内外节点，因此zero-one labeling trick难以做batch training。因为为一个子图标记内部节点和外部节点，可以得到一个<strong>labeled graph</strong> （子图内节点为1，子图外节点为0），也就是对于一个graph $\mathcal{G}$，每个子图都需要专门生成一个该子图的labeled graph, 不同子图的labeled graph 也不同。如果要为每个子图都构造labeled graph, 再各自在每个labeled graph上做独立的message passing，得到每个labeled graph对应的子图embedding，这样过于耗时。</p>
<p>为了减轻上述每个组图对应一个特定的labeled graph 问题，本文认为可以为一个batch 子图生成一个labeled graph，这样的话，通过一次message passing就可以计算一个batch subgraphs的representations。为了结合一个batch 子图的zero-one labels，从而生成一个公共的labeled graph， 本文进一步提出了<strong>Max-Zero-One</strong> Labeling Trick。具体来说，一个batch的所有子图只生成一个labeled graph，其中，该batch内所有子图内节点都被赋予label 1, 所有子图外节点都被赋予label 0， 然后在labeled graph 上做GNN，就可以一次性学习一个batch子图的representations。</p>
<p>作者认为如果目标子图稀疏的分布在图中的话，一个子图外有其他节点被赋予1标签的影响是微不足道的，因为浅层GNN也不会聚合到远距离的节点。另外这么做还可以避免对一个子图的过拟合。</p>
<h2 id="implementation">Implementation</h2>
<p>Input: Graph $\mathcal{G}$ ,  所有子图<code>subG_node = [[subgraph 1], [subgraph 2], [subgraph 3], ...]</code>， $z=[0,0,1,0,1,1,0,0,1, \cdots]$ 为batch subgraphs对应的labeled graph, 在batch subgraphs中的节点为1，不在的为0。</p>
<p>以下为一层GLASS：</p>
<ul>
<li>
<p>对于每个节点特征，分别做两个线性变换</p>
<pre tabindex="0"><code>x1 = MLP_1(x)  # 节点充当batch subgraphs内节点时的embedding
x0 = MLP_0(x)  # 节点充当batch subgraphs外节点时的embedding
</code></pre></li>
<li>
<p><strong>对于label=1的节点 （batch 子图内的节点）</strong></p>
<p>特征 $x = \alpha x_1 + (1-\alpha)x_0$， 对于ppi_bp数据集，$\alpha = 0.95$为超参数，若节点是batch子图内的节点，保留更多$x_1$。</p>
<p><strong>对于label=0的节点 （batch 子图外的节点）</strong></p>
<p>$x = (1-\alpha)x_1 + \alpha x_0$，对于不在batch子图中的节点，保留更多$x_0$。</p>
<p>通过这种方式，子图内外的节点得以区分</p>
</li>
<li>
<p>Message Passing:</p>
<p>$x = (D^{-1}A)X$</p>
<p>GraphNorm:</p>
<p>$x = \mathrm{GraphNorm}(x)$</p>
<p>Residual:</p>
<p>$x = \mathrm{cat}(x_, x)$    //和初始特征拼接</p>
</li>
<li>
<p>再次区分batch subgraph 内外节点：</p>
<pre tabindex="0"><code>x1 = MLP_2(x)  
x0 = MLP_3(x) 
</code></pre></li>
<li>
<p>再对子图内外节点做不同的组合</p>
<p>$x = \alpha x_1 + (1-\alpha)x_0$： 子图内节点</p>
<p>$x = (1-\alpha)x_1 + \alpha x_0$： 子图外节点</p>
</li>
</ul>
<p>可以发现，如果$\alpha = 1$，那么相当于子图内节点用$x_1$， 子图外节点用$x_0$，这样就彻底区别了子图内外的节点。即， 对于邻居聚合操作来说，如果聚合到了子图外邻居，那么子图外邻居使用$\mathrm{MLP}_0$变换过的特征，如果聚合到子图内的节点，使用$\mathrm{MLP}_1$变换过的特征。</p>
<h2 id="一点理论">一点理论</h2>
<p><strong>Proposition 1:</strong>  <em>给定图$G$，$\mathcal{S}$和$\mathcal{S}^\prime$，如果Plain GNN可以区分的子图，GLASS也一定可以区分。但是存在Plain GNN不能区分但GLASS可以区分的子图。</em></p>
<p><strong>Proof:</strong> 首先证明给定任意Plain GNN  model $m_1$，存在一个GLASS模型$m_2$，使得对于目标子图$\mathcal{S}$，$m_1$和$m_2$的输出相同，也就是GLASS至少可以和Plain GNN一样expressive。</p>
<p>假设Plain GNN  $m_1$ 的第$k$层 $\mathrm{AGGREGATE}$函数为$f^{(k)}_1$，$\mathrm{COMBINE}$函数为$g^{(k)}_1$， 第$k$层$\mathrm{READOUT}$函数为$\phi_1$，那么Plain GNN可以表达为：
$$
\begin{aligned}
&amp;\boldsymbol{a}_{v}^{(k)}=\operatorname{AGGREGATE}^{(k)}\left(\left\{\boldsymbol{h}_{u}^{(k-1)} \mid u \in N(v)\right\}\right) = f^{(k)}_1 \left(\left\{\boldsymbol{h}_{u}^{(k-1)} \mid u \in N(v)\right\}\right) \\
&amp;\boldsymbol{h}_{v}^{(k)}=\operatorname{COMBINE}^{(k)}\left(\boldsymbol{h}_{v}^{(k-1)}, \boldsymbol{a}_{v}^{(k)}\right) = g^{(k)}_1\left(\boldsymbol{h}_{v}^{(k-1)}, \boldsymbol{a}_{v}^{(k)}\right) \\
&amp;\boldsymbol{h}_{\mathcal{S}}=\operatorname{READOUT}\left(\left\{\boldsymbol{h}_{u} \mid u \in \mathbb{V}_{\mathcal{S}}\right\}\right) = \phi_1\left(\left\{\boldsymbol{h}_{u} \mid u \in \mathbb{V}_{\mathcal{S}}\right\}\right)
\end{aligned}
$$
接下来设计GLASS，将每层节点特征定义为$\operatorname{CONCATENATE}\left(\boldsymbol{h}_{u}^{(k-1)}, \boldsymbol{l}^{(\mathcal{S})}\right)$，即每层拼接该节点的label （是否在子图中），基于universal approximation theorem，一定存在一个函数$\theta$, 使得$\theta\left(\operatorname{CONCATENATE}\left(\boldsymbol{h}_{u}^{(k-1)}, \boldsymbol{l}^{(\mathcal{S})}\right)\right)=\boldsymbol{h}_{u}^{(k-1)}$，那么GLASS可以定义为：
$$
\begin{aligned}
\boldsymbol{h}_{u}^{\prime(k-1)} &amp;=\operatorname{CONCATENATE}\left(\boldsymbol{h}_{u}^{(k-1)}, \boldsymbol{l}^{(\mathcal{S})}\right), \\
\boldsymbol{a}_{v}^{(k)} &amp;=f_{1}^{(k)}\left(\left\{\theta \left(\boldsymbol{h}_{u}^{\prime (k-1)}\right) \mid u \in N(v)\right\}\right) \\
\boldsymbol{h}_{v}^{(k)} &amp;=g_{1}^{(k)}\left(\boldsymbol{h}_{v}^{(k-1)}, \boldsymbol{a}_{v}^{(k)}\right)
\end{aligned}
$$
因为$\theta \left(\boldsymbol{h}_{u}^{\prime (k-1)}\right) = \theta\left(\operatorname{CONCATENATE}\left(\boldsymbol{h}_{u}^{(k-1)}, \boldsymbol{l}^{(\mathcal{S})}\right)\right)=\boldsymbol{h}_{u}^{(k-1)}$， Plain GNN 是上述特定形式GLASS的特例，所以GLASS使得至少与Plain GNN 表达能力相同。</p>
<p>Figure 2给出了Plain GNN不能区分但GLASS可以区分的子图实例。</p>
<p><strong>Theorem 1：</strong> <em>给定任意图$\mathcal{G}$，存在一个GLASS model，可以准确预测$\mathcal{G}$中任意子图的density 和cut ratio。</em></p>
<p><strong>Proof:</strong> 一个图的Density定义为：
$$
D = \frac{2 |E|}{|V| \cdot|V-1|}
$$
即图中实际存在的边数，占左右节点对可能构成的总边数的比例</p>
<p>一个子图$\mathcal{S}$的cut ratio定义为：
$$
\mathrm{CR}(\mathcal{S}) = \frac{|B_{\mathcal{S}}|}{|\mathcal{S}| \cdot |\mathcal{G} \backslash \mathcal{S}|}
$$
为子图和其他部分之间的边数$|B_{\mathcal{S}}|$, 占子图和其他部分可能存在的总边数的比例。其中$B_{S}=\{(u, v) \in E \mid u \in S, v \in G \backslash S\}$。
$$
\begin{aligned}
\boldsymbol{a}_{v}^{(1)} &amp;=\sum_{u \in N(v)}\left(\boldsymbol{l}_{u}^{(\mathcal{S})}\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]+\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]\right)  = \left[\begin{array}{l}
N(v)中存在于\mathcal{S}的节点数量\quad m \\
N(v)中\mathcal{S}以外节点数量 \quad n\\
0
\end{array}\right]  \\
\boldsymbol{h}_{v}^{(1)} &amp;=\left[\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
-1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{array}\right] \boldsymbol{a}_{v}^{(1)}+\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right] = \left[\begin{array}{l}
m \\
n-m \\
1
\end{array}\right]  \\
\boldsymbol{h}_{\mathcal{S}} &amp;=\sum_{v \in \mathbb{V}_{\mathcal{S}}} \boldsymbol{h}_{v}^{(1)} = \left[\begin{array}{l}
子图内边数 \\
边界边数-子图内边数\\
子图内节点数
\end{array}\right]
\end{aligned}
$$
其中$l_{u}^{(\mathcal{S})}$为节点$u$的zero-one label，如果$u$在子图$\mathcal{S}$中，那么为1不在为0。 因此子图$\mathcal{S}$的Density $d$和 cut ratio $c$可以由上述定义的GLASS模型推导出：
$$
\begin{aligned}
&amp;d\left(\boldsymbol{h}_{\mathcal{S}}\right)=\left(\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}\right) /\left[\left(\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}\right) \cdot\left(\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}-1\right)\right] \\
&amp;c\left(\boldsymbol{h}_{\mathcal{S}}\right)=\left(\left[\begin{array}{c}
0 \\
0.5 \\
0
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}\right) /\left[\left(\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}\right) \cdot\left(n-\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]^{T} \boldsymbol{h}_{\mathcal{S}}-1\right)\right] .
\end{aligned}
$$</p>
<h1 id="reference">Reference</h1>
<p>[1] Labeling trick: A theory of using graph neural networks for multi-node representation learning. NeurIPS2021</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2020 《Subgraph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/subgnn/</link>
      <pubDate>Fri, 27 May 2022 17:13:33 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/subgnn/</guid>
      <description>NeurIPS2020 &amp;#34;Subgraph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2006.10538">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>GNN通常关注节点级任务和图级任务，缺少针对子图级预测任务的方法。针对这个问题，本文提出SubGNNs用于解耦子图在不同结构aspect的表示。为了学习准确的子图表示，SubGNN在子图的连通分量和随机采样的anchor patches之间进行消息传递，从而学习高准确度的子图表示。SubGNN指定了三个通道，每个通道捕获子图不同的拓扑结构属性。</p>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/1.png#center" alt="图1"  title="123"  />
</p>
<p>从拓扑的角度来看，子图是非常具有挑战性的结构，对子图的预测存在以下挑战：</p>
<ul>
<li>如要对更大且size不同的子图做联合预测，挑战在于<strong>如何表征含有多个分量，甚至分量间间隔较远的子图</strong>。</li>
<li>子图包含了高阶连通模式（connectivity patterns），这些连通模式不仅存在于子图内节点之间，也存在与子图内节点与子图外部节点之间， 挑战在于<strong>如何将子图边界信息和子图外部信息注入GNN中</strong>。</li>
<li>子图可能存在于图中的一个特定区域，也可能它的连通分量分布于多个局部邻域，挑战在于<strong>如何学习子图在图中的位置</strong>。</li>
<li>子图间共享边（sharing edges）和非边（non-edges）存在相关性，挑战在于<strong>如何将这种子图间的依赖融合进模型中，同时任然能够将特征信息考虑在内进行辅助归纳推理</strong>。</li>
</ul>
<p>本文提出SubGNN以解决上述挑战， SubGNN的核心原则是子图级的消息传递，可以捕获子图位置、邻域、结构三种特征</p>
<h1 id="formulating-subgraph-prediction">Formulating Subgraph Prediction</h1>
<p>给定无向图$G=(V,E)$，它的一个子图表示为$S=\left(V^{\prime}, E^{\prime}\right)$，每个子图$S$有一个label $y_{S}$，并且子图$S$可能包含多个连通分量，连通分量表示为$S^{(C)}$。</p>
<p><strong>Problem (Subgraph Representations and Property Prediction)</strong> 给定子图集合 $\mathcal{S} = \left\{S_{1}, S_{2}, \ldots, S_{n}\right\}$，SubGNN $E_S$为每个子图$S\in \mathcal{S}$生成一个$d_s$维的表示向量$\mathbf{Z}_S \in \mathbb{R}^{d_{s}}$， 然后用这些子图的表示向量学习一个子图分类器 $f: \mathcal{S} \rightarrow\{1,2, \ldots, C\}$，使得输入子图得到预测label: $f(S)=\hat{y}_{S}$。</p>
<p>本文针对子图分类任务，所提出的模型为一个可学习的embedding函数$E_{S}: S \rightarrow \mathbb{R}^{d_{s}}$， 将每个子图映射为低维表示向量，这些表示向量可以捕获子图拓扑对预测重要的aspects。具体来说，对于一个子图，message再它的连通分量之间传递，这使得我们可以对多个连通分量的子图学习有意义的表示。</p>
<h2 id="subgnn-properties-of-subgraph-topology">SUBGNN: Properties of subgraph topology</h2>
<p>子图拥有独特的内部结构，边界连通性，邻域概念，以及和图其他部分的相对位置。直觉上，我们的目标是以最大的似然保存保存特定的图属性。本文设计模型以考虑<strong>6</strong>种特定的图结构属性：</p>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/2.png#center" alt="图1"  title="123"  />
</p>
<p>具体来说：</p>
<p><strong>(1) Position.</strong></p>
<p>Border Position: 该属性保留子图和图的其他部分之间的距离，通过这种距离关系，可以区分两个同构但处于不同位置的子图。</p>
<p>Internal Position：子图自己连通分量之间的距离。</p>
<p><strong>(2) Neighborhood.</strong></p>
<p>Border Neighborhood：为子图的边界邻域，表示子图$S$中任意节点的$k$跳邻域中（不属于子图$S$）的节点集合。</p>
<p>Internal Neighborhood：子图内每个连通分量的边界邻域，每个连通分量$S^{(c)}$中任意节点的$k$跳邻域中（不属于子图$S^{(c)}$）的节点集合。</p>
<p><strong>(3) Structure.</strong></p>
<p>Border Structure：子图内部节点和边界邻居之间的连通性。</p>
<p>Internal Structure：每个连通分量的内部连通性。</p>
<p>本文旨在将上述属性学习到子图表示向量中。</p>
<h1 id="subgnn-subgraph-neural-network">SubGNN： Subgraph Neural Network</h1>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/3.png#center" alt="图1"  title="123"  />
</p>
<p>SubGNN以层次的方式学习子图表示，将神经消息从anchor patch传递到子图分量中，并将所有子图分量的表示聚合为最终的子图表示。如图2（a）所示，独立考虑子图的每个连通分量的每个属性，从anchor patch中获得相应的属性信息，对于每个分量，聚合它的所以属性表示，得到该分量的表示，然后聚合子图所有分量的表示，得到最后的子图表示。</p>
<h2 id="property-aware-routing">Property-Aware Routing</h2>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/4.png#center" alt="图1"  title="123"  />
</p>
<p>通过anchor patch向子图分量传递消息，使得子图分量可以实现结构属性感知，如图2（b）所示，每个通道$\mathbf{z}_{i}$表示子图的第$i$个分量的输出表示，它由3部分构成，分别为该分量的位置属性（绿色），邻居属性（蓝色），结构属性（橙色）。将所有通道（分量）聚合，可以得到最终的子图表示。</p>
<p>对于每个属性，我们定义一个anchor patch 采样函数$\phi_{\mathrm{X}}:\left(G, S^{(c)}\right) \rightarrow A_{\mathrm{X}}$，即对于子图$S$的一个连通分量$S^{(c)}$，$\phi_{\mathrm{X}}$为该子图分量输出属性$X$的anchor patch。</p>
<h3 id="position">Position</h3>
<p>为了捕获Internal Position, $\phi_{\mathrm{P}_{\mathrm{I}}}$返回anchor patch $A_{P_{I}}$，每个anchor patch为 子图内的单个节点，所有的针对Internal Position属性的anchor patch集合为$\mathcal{A}_{P_{I}} = \{A^{(1)}_{P_{I}},A^{(2)}_{P_{I}}, \cdots\}$， 为子图$S$内随机采样的节点集合，即$A^{(i)}_{P_{I}}$为一个子图内的节点。由于Internal Position的目的是捕获子图连通分量之间的距离，因此不同的连通分量共享anchor，通过共享的anchor和不同连通分量之间的相似度，来将anchor的信息聚合到不同的连通分量中，这将允许子图中不同连通分量相互定位。 例如$S$有两个连通分量，有一个anchor $A^{(i)}_{P_{I}}$存在于其中一个分量中，那么两个连通分量分别依据和anchor的相似度来聚合anchor的表示，那么如果有很多anchor的情况下，可以较为准确的为这两个分量区分相对位置。子图$S$的anchor patch集合$\mathcal{A}_{P_{I}}$对$S$的<strong>所有分量</strong>共享。</p>
<p>为了捕获Border Position， 由于Border Position是子图整体和图的其他部分的距离，所以$\phi_{\mathrm{P}_{\mathrm{B}}}$采样的节点在所有<strong>子图</strong>间共享，anchor集$\mathcal{A}_{P_{B}} = \{A^{(1)}_{P_{B}},A^{(2)}_{P_{B}}, \cdots\}$中每个anchor patch $A^{(i)}_{P_{B}}$是随机采样的节点，并且集合$\mathcal{A}_{P_{B}}$所有子图都共享，即所有子图的所有分量都聚合来自$\mathcal{A}_{P_{B}}$的消息，例如子图$S_1$和$S_2$， $S_1$的所有分量依据和anchor的相似度聚合所有anchor的信息，同样$S_2$的所有分量依据和anchor的相似度聚合所有anchor的信息，那么如果anchor数量足够多，并且在子图间共享，所以为子图$S_1$和$S_2$学习到的emb可以分别反映两个子图和原图中其他部分的相似度，从而区分两个子图的position。</p>
<p>综上所述，对于Internal Positon，anchor在子图中采样，且在同一个子图的不同分量间共享，从而捕获同一个子图不同分量间的位置距离。</p>
<p>对于Border Position， anchor在整个图中采样，且在所有子图间共享，从而捕获不同子图在原图中的位置距离。</p>
<h3 id="neighborhood">Neighborhood</h3>
<p>为了捕获Internal Neighborhood，$\phi_{\mathrm{N}_{\mathrm{I}}}$是从子图分量$S^{(c)}$中采样anchor patch，对于每个子图分量，它的anchor patch来自自己内部节点，聚合来自自己内部节点的消息，从而捕获内部邻域信息。</p>
<p>而$\phi_{\mathrm{N}_{\mathrm{B}}}$是从子图分量$S^{(c)}$的border neighborhood中采样anchor，即子图分量$S^{(c)}$中任意节点$k$-hop以内邻居（不在$S^{(c)}$）中的节点采样anchor，子图$S^{(c)}$聚合这些$k$-hop border neighborhood从而为每个子图分量捕获边界邻域信息。</p>
<h3 id="structure">Structure</h3>
<p>$\phi_{\mathrm{S}}$采样anchor patch用于捕获子图的内部结构信息和边界结构信息，针对内部结构信息采样的anchor集合$\mathcal{A}_{\mathrm{S}_{1}}$以及针对边界结构信息采样的anchor集合$\mathcal{A}_{\mathrm{S}_{B}}$对所有子图也是共享的，具体来说，$\phi_{\mathrm{S}}$返回的是根据三角随机游走从图中抽取的连通部分，通过计算与这些anchor<strong>图</strong>的相似度来聚合这些anchor图，从而区分不同子图在结构上的相似度。具体来说，每个子图根据与共享anchor图之间的相似度，聚合anchor图，那么不同子图如果与共享的anchor子图集相似度越高，那么这这些子图的结构相似度就越高。例如，$S_1$和$S_2$为$G$中的两个子图，要使两个子图的embedding可以反映两个子图结构上的相似性，那么给定一大堆anchor子图，$S_1$基于它和这些anchor的相似性聚合所有anchor，$S_2$同样基于它和anchor的相似性聚合所有anchor，anchor set相当于一个相似度中介，两个子图的embedding分别反映了两个子图和共享anchor set之间的相似度 $Sim_1$和$Sim_2$，那么如果，如果$Sim_1$和$Sim_2$的差别较大，那么说明两个子图与anchor set的相似度相差太大，所以两个子图的结构差别较大。</p>
<h2 id="neural-encoding-of-anchor-patches">Neural Encoding of Anchor Patches</h2>
<p>对提取出的anchor进行编码，对于Position anchor patches 和 Neighbor anchor patches，由于每个anchor patch是单一节点，所以节点特征就是anchor的representation。而对于 Structure属性的anchor patches，它是一个个子图，为了将他们编码，本文首先在每个anchor patch上进行长度为$w$参数为$\beta$的三角随机游走，得到节点序列$\left(u_{\pi_{w}(1)}, \ldots, u_{\pi_{w}(n)}\right)$，然后将节点序列输入双向LSTM中得到最终的patch 表示$\mathbf{a}_{S}$。</p>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/7.png#center" alt="图1"  title="123"  />
</p>
<h2 id="subgraph-level-message-passing">Subgraph-Level Message Passing</h2>
<p>SubGNN的消息传递定义在<strong>子图分量级</strong>。首先为每个结构属性采样一对anchor patches：$\mathcal{A}_{\mathrm{X}}=\left\{A_{\mathrm{X}}^{(1)}, \ldots, A_{\mathrm{X}}^{\left(n_{A}\right)}\right\}$，每个属性的采样规则如Property-Aware Routing 中所示，即针对Position和Neighborhood属性，采样的anchor patch是节点，针对Structure，采样的anchor patch是子图。$\mathcal{A}_{\mathrm{P}}$、$\mathcal{A}_{\mathrm{N}}$和$\mathcal{A}_{\mathrm{S}}$分别为三个属性的anchor patch 集合。对于子图$S$的第$c$个分量$S^{(c)}$，定义从anchor patch $A_{\mathrm{X}}$到$S^{(c)}$的消息：
$$
\mathrm{MSG}_{\mathrm{X}}^{A \rightarrow S}=\gamma_{\mathrm{X}}\left(S^{(c)}, A_{\mathrm{X}}\right) \cdot \mathbf{a}_{\mathrm{X}}
$$
其中$X$是结构属性，$\gamma_{\mathrm{X}}$是该结构属性的相似度函数，用于衡量anchor patch 和子图embedding之间的相似度值。将一个属性下所有anchor patches的消息聚合，然后与子图分量$S^{(c)}$结合成该子图分量在$X$属性上的表示：
$$
\begin{aligned}
&amp;\mathbf{g}_{\mathrm{X}, c}=\mathrm{AGG}_{M}\left(\left\{\mathrm{MSG}_{\mathrm{X}}^{A_{\mathrm{X}} \rightarrow S^{(c)}} \forall A_{\mathrm{X}} \in \mathcal{A}_{\mathrm{X}}\right\}\right) \\
&amp;\mathbf{h}_{\mathrm{X}, c} \leftarrow \sigma\left(\mathbf{W}_{\mathrm{X}} \cdot\left[\mathbf{g}_{\mathrm{X}, c} ; \mathbf{h}_{\mathrm{X}, c}\right]\right),
\end{aligned}
$$
<img loading="lazy" src="/posts/2022-05-29-SubGNN/5.png#center" alt="图1"  title="123"  />
</p>
<p>以上图为例，给定一个子图$S$的第一个连通分量$S^{(1)}$在Position属性下的输入embedding为$\mathbf{h}_{P,1}$， Position属性在Internal和Border方面一共有4个anchor patches，每个anchor patches将消息聚合到$S^{(1)}$中，得到4个Messages,即 $\mathbf{m}_{P,1,1}$，$\mathbf{m}_{P,1,2}$，$\mathbf{m}_{P,1,3}$，$\mathbf{m}_{P,1,4}$，分别表示4个Position属性的anchor patch聚合到子图$S$第1个连通分量的消息。</p>
<p>$\mathbf{h}_{\mathrm{X}, c}$的顺序不变性是层到层消息传递的重要属性，但是它会限制捕捉子图结构和位置的能力。因此这里构造了property-aware的输出表征$\mathbf{Z}_{X, c}$，通过将属性$X$在分量$c$上的所有anchor message拼接，得到anchor-set message 矩阵$\mathbf{M}_{\mathrm{X}}$， 如图二所示，$\mathbf{M}_{\mathrm{X}}$的每行都是anchor-set message （集合消息），然后传递给非线性激活函数（如算法1所示）。输出的表示向量的每一维都编码了anchor patch 的结构信息和位置信息，对于邻域通道，设定$\mathbf{Z}_{\mathrm{N}, c}=\mathbf{h}_{\mathrm{N}, c}$。最后SubGNN为连通分量拼接每个属性的消息，得到分量表示$\mathbf{z}_c$。最后所有的分量表示通过$\mathrm{READOUT}$聚合成最后的子图表示$\mathbf{z}_{S}$。</p>
<p>概括来说，先得到子图每个分量$S^{(c)}$在属性$X$上的表示$\mathbf{z}_{X,c}$，然后1）对于每层SubGNN，将分量$S^{(c)}$的所有属性表示向量聚合，得到该层$c$分量的表示。2）将所有层的$S^{(c)}$分量的表示向量聚合，得到子图分量$S^{(c)}$的最终表示$\mathbf{z}_c$。3）子图$S$所有分量的表示聚合，得到最终的子图表示$\mathbf{z}_S$。</p>
<p><img loading="lazy" src="/posts/2022-05-29-SubGNN/6.png#center" alt="图1"  title="123"  />
</p>
<h1 id="conclusion">Conclusion</h1>
<p>有点复杂~</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Decoupling the Depth and Scope of Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/decouplinggcn/</link>
      <pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/decouplinggcn/</guid>
      <description>NeurIPS2021 &amp;#34;Decoupling the Depth and Scope of Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2201.07858">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>现有的GNN在图和模型的size方面可拓展性有限。 对于大图，增加模型的深度对导致scope(感受野)大小成指数放大。深层model主要面临两个基本挑战： 1. oversmoothing导致表达能力下降，2. 邻域爆炸导致计算成本高昂。</p>
<p>本文旨在结构GNN的depth 和 scope，首先提取子图作为有限大小（bounded-size）的scope, 然后将任意深度的GNN用于子图上。 由于提取出的局部子图是由少量<strong>关键</strong>邻居组成，且排除了不相关的邻居，所以深层GNN也可以学到informative representations。</p>
<p>增加GNN层数会造成以下基本障碍：</p>
<ul>
<li>Expresivity (oversmoothing): 邻居的迭代混合导致不同节点的切入向量收敛到一个固定的低维子空间</li>
<li>Scalability (neighbor explosion): 多跳邻居递归导致感受野大小呈指数级增长</li>
</ul>
<p>为了研究导致表达能力和可拓展性缺陷的根本原因，本文提出了以下insight:</p>
<p><strong>Two views on the graph:</strong>  如果从全局视角来看两个节点，如果两个节点在同一个图的同一个连通分量中，那么这两个节点在随机游走中存在到达概率，无论他们间隔多远。而本文给出了图的局部视角，具体来说， 给定节点$v$的局部子图$\mathcal{G}_{[v]}$，将$\mathcal{G}_{[v]}$仅包含节点$v$的特性，整个图看一看做所有子图$\mathcal{G}_{[v]}$的集合。那么$v$的邻域不在是所有节点$\mathcal{V}$，而它的邻域只存在于$\mathcal{V}_{[v]}$中。 如果节点$u$不在$\mathcal{V}_{[v]}$中，$u$将永远不会被考虑为$v$的邻居，无论GNN有多深。</p>
<p><strong>Scope of GNNs:</strong> 加深GNN层次所造成的的表达能力和可拓展性问题都和GNN不断扩大的感受野（scope）有关。随着层次变深，<strong>感受野不断变大</strong>，使得每个节点包含的信息重叠越多，最终收敛到同一个子空间，导致oversmoothing; 另外 <strong>感受野变大</strong>，导致每个节点的邻居数呈指数级上升，导致邻居爆炸。所以GNN的层数加深会导致感受野变大（耦合），即$L$层GNN的感受野为全部$L$-hop以内的邻居，层数深度（depth）和感受野大小(scope)的强耦合限制了GNN的设计。</p>
<p><strong>Decoupling the GNN depth and scope:</strong> 为了解耦GNN的深度（depth）与感受野(scope)，使得加层数与感受野无关。对于节点$v$，首先为它提取一个小的子图$\mathcal{G}_{[v]}$，然后在小的子图上应用任意层数的GNN。若GNN的层数$L^\prime$大于感受野的跳数，那么子图中的每对节点会交换多次信息，额外的消息传递有助于GNN更好的融合scope内的信息，从而增强表达能力。</p>
<h1 id="decoupling-the-depth-and-scope-of-gnns">Decoupling the Depth and Scope of GNNs</h1>
<p><strong>Definition (Depth of subgraph)</strong> ：假设子图$\mathcal{G}_{[v]}$是连通的，$\mathcal{G}_{[v]}$的depth定义为$\max _{u \in \mathcal{V}_{[v]}} d(u, v)$, 其中$d(u, v)$表示$u$到$v$的最短路径。</p>
<p>本文提出shaDow-GNN，它包含了一个子图提取器$\text { EXTRACT}$。 shaDow-GNN的过程如下：</p>
<ol>
<li>用子图提取器$\operatorname{EXTRACT}(v, \mathcal{G})$为节点$v$提取一个连通子图$\mathcal{G}_{[v]}$，子图的深度（距离$v$最远的节点和$v$之间的跳数）为$L$。</li>
<li>构建一个$L^\prime$层的GNN并应用在$\mathcal{G}_{[v]}$上。 如果 $L^\prime &gt; L$那么可以反映decoupling，因为GNN层数此时与scope无关，层数加深不会影响感受野。</li>
</ol>
<p>本文从三个不同的角度理论证明了shaDow-GNN可以提升GNN的表达能力。</p>
<h2 id="graph-signal-processing-perspective">Graph Signal Processing Perspective</h2>
<ol>
<li>oversmoothing by <strong>deep GCN</strong>。 2. oversmoothing by <strong>repeated GCN-style propagation</strong>。</li>
</ol>
<p>对于deep GCN, 包含了非线性激活，权重和bias。而带有bias参数的deep GCN 不会导致oversmoothing[1]， 但任然存在准确率下降的问题，这说明GCN的这种传播形式是导致学习困难的根本原因，而不是来自于激活函数或者bias。</p>
<p>即，repeat-GCN-style propagation表示为$\boldsymbol{M}=\lim _{L \rightarrow \infty} \widetilde{\boldsymbol{A}}^{L} \boldsymbol{X}$这种邻居迭代聚合的传播形式会导致学习困难。因此，这里在忽略激活函数和bias的情况下分析聚合矩阵的渐近性。</p>
<p>对于子图$\mathcal{G}_{[v]}$， 无限次特征聚合表示为：$\boldsymbol{M}_{[v]}=\lim_{L \rightarrow \infty} \widetilde{\boldsymbol{A}}^{L}_{[v]} \boldsymbol{X}_{[v]}$。$\boldsymbol{M}_{[v]}$为节点$v$的子图$\mathcal{G}_{[v]}$的embedding矩阵。 由于$\widetilde{\boldsymbol{A}}_{[v]} = (\boldsymbol{D}_{[v]} + \boldsymbol{I}_{[v]})^{-\frac{1}{2}} (\boldsymbol{A}_{[v]} + \boldsymbol{I}_{[v]}) (\boldsymbol{D}_{[v]} + \boldsymbol{I}_{[v]})^{-\frac{1}{2}}$，$\widetilde{\boldsymbol{A}}_{[v]}$是实对称阵， 所以可以做特征分解， 即$\widetilde{\boldsymbol{A}}_{[v]}=\boldsymbol{E}_{[v]} \boldsymbol{\Lambda} \boldsymbol{E}_{[v]}^{-1}=\boldsymbol{E}_{[v]} \boldsymbol{\Lambda} \boldsymbol{E}_{[v]}^{\top}$。那么:
$$
\widetilde{\boldsymbol{A}}_{[v]}^{L \to \infty} =\boldsymbol{E}_{[v]} \boldsymbol{\Lambda}^L \boldsymbol{E}_{[v]}^{\top} = \boldsymbol{E}_{[v]} 	\begin{bmatrix}
\lambda_1^L &amp;   &amp; &amp;  \\
&amp;  \ddots &amp;  &amp; \\
&amp;   &amp; \lambda_N^L&amp;
\end{bmatrix}\boldsymbol{E}_{[v]}^{\top} = e_{[v]}e_{[v]}^\top
$$
因为归一化邻接矩阵$\widetilde{\boldsymbol{A}}_{[v]}$的最大特征值一定为1，且半正定，那么当$L \to \infty$时， 若$1 = \lambda_1 &gt; \cdots &gt; \lambda_N$, 所以$\lambda_1^L = 1$，$\lambda_2^L,\cdots,\lambda_N^L \to 0$。所以上式成立，其中$e_{[v]}$为$\widetilde{\boldsymbol{A}}_{[v]}$最大特征值对应特征向量。所以：
$$
\lim_{L\to \infty} \widetilde{\boldsymbol{A}}_{[v]}^{L} X_{[v]} = e_{[v]}e_{[v]}^\top X_{[v]} = e_{[v]}(e_{[v]}^\top X_{[v]}) = \boldsymbol{M}
$$
因此在子图$\mathcal{G}_{[v]}$上使用无限多层GNN的 shaDow-GNN得到的节点emb矩阵为$\boldsymbol{M} = e_{[v]}(e_{[v]}^\top X_{[v]})$, 那么 shaDow-GNN为节点$v$学习到的embedding为 节点$v$在其子图$\mathcal{G}_{[v]}$中的对应embedding。即：
$$
\boldsymbol{M}_{[v]}=\left[e_{[v]}\right]_{v} \cdot\left(\boldsymbol{e}_{[v]}^{\top} \boldsymbol{X}_{[v]}\right)  \tag{1}
$$
考虑到这是无向图，$\widetilde{\boldsymbol{A}}_{[v]}$的最大特征值对应的特征向量表示中的每个元素表示$\mathcal{G}_{[v]}$中对应节点的度。所以$\left[e_{[v]}\right]_{u}$是$\mathcal{G}_{[v]}$中节点$u$的度$\delta_{[v]}(u)$。所以$e_{[v]} = [\delta_{[v]}(u)]$是$\mathcal{G}_{[v]}$中节点的度向量。由于这里的粗体$e_{[v]}$是对$e_{[v]}$的normalization:<br>
$$
\left[e_{[v]}\right]_{u}=\sqrt{\frac{\delta_{[v]}(u)}{\sum_{w \in \mathcal{V}_{[v]}} \delta_{[v]}(w)}}
$$
上面的$\boldsymbol{M}_{[v]}$为 shaDow-GNN为节点$v$学到的embedding。</p>
<p>对于普通的GCN propagation，可以看做是一个有足够大的hop $L$的$\mathcal{G}_{[v]}$ 和一个有足够大hop $L$的$\mathcal{G}_{[u]}$，使得在无限多层GCN时，  $\mathcal{G}_{[v]} = \mathcal{G}_{[u]}$。即对于无限多层的普通GCN，两个不同节点的scope是一样的，这就说明  $e_{[v]} = e_{[u]}$，并且$\boldsymbol{X}_{[u]}=\boldsymbol{X}_{[v]}=\boldsymbol{X}$。即对于普通的无限多层GNN来说，两个不同节点的局部子图就是整个图，无法保存节点特有的特征信息，根据公式一，由于$e_{[v]} = e_{[u]}$只和两个节点的度有关，特征信息又是一样的，所以在整个图上使用无限多层GNN的到的两个节点的embedding $\boldsymbol{M}_{[v]}$和$\boldsymbol{M}_{[u]}$只和两个节点的度有关。</p>
<p>而对于shaDow-GNN，它实现一种局部平滑（local-smoothing）,由于是在子图上做特征聚合，那么无论多少层，都不会聚合到子图范围以外的邻居。从公式（1）可以看出，在子图内，目标节点的embedding实际上就是子图内所有节点embedding的线性组合，无限增加层数值只会使得线性组合的系数$e_{[v]}$收敛到一个固定值，即$\widetilde{\boldsymbol{A}}_{[v]}$的稳态分布向量。而稳态分布向量$e_{[v]}$的具体取值只与对应子图的度有关，与GNN层数无关。直观来看，子图提取器$\text { EXTRACT}$会为不同的节点提取不同的子图，若$u$,$v$的子图不同，那么对应的$e_{[v]}$与$e_{[v]}$也不同，$\boldsymbol{X}_{[u]}$和$\boldsymbol{X}_{[v]}$也不同，所以shaDow-GNN在无限多层下依旧捕获局部特征信息。</p>
<h2 id="function-approximation-perspective">Function Approximation Perspective</h2>
<p>GraphSAGE的每层定义为：
$$
\boldsymbol{h}_{v}^{(\ell)}=\sigma\left(\left(\boldsymbol{W}_{1}^{(\ell)}\right)^{\top} \boldsymbol{h}_{v}^{(\ell-1)}+\left(\boldsymbol{W}_{2}^{(\ell)}\right)^{\top}\left(\frac{1}{\mid \mathcal{N}_{v}\mid } \sum_{u \in \mathcal{N}_{v}} \boldsymbol{h}_{u}^{(\ell-1)}\right)\right)
$$
对于$L^\prime$层的shaDow-SAGE，，若$\text { EXTRACT}$为节点提取$L$-hop的邻居，并且，对于shaDow-SAGE的$L+1 \leq \ell \leq L^{\prime}$层，令$\boldsymbol{W}_{1}^{(\ell)}=\boldsymbol{I}$，$\boldsymbol{W}_{2}^{(\ell)}=\mathbf{0}$， 此时$L^\prime$层的shaDow-SAGE等价于$L$层的GraphSAGE。所以$L^\prime$层的shaDow-SAGE可以表达GraphSAGE能表达的所有函数。</p>
<p>要证明shaDow-SAGE可以表达一些GraphSAGE无法表达的函数，首先对于每个邻域子图$\mathcal{G}_{[v]}$考虑一个目标函数：$\tau\left(\boldsymbol{X}, \mathcal{G}_{[v]}\right)=C \cdot \sum_{u \in \mathcal{V}_{[v]}} \delta_{[v]}(u) \cdot \boldsymbol{x}_{u}$，其中$C$是一个scaling常数，$\delta_{[v]}(u)$是节点$u$在子图$\mathcal{G}_{[u]}$中的度。</p>
<p>GraphSAGE准确的学习函数$\tau$，而shaDow-SAGE可以。令子图$\mathcal{G}_{[v]}$的深度为$L$，对于GraphSAGE，只有它在原图上做$L$层的Message Passing，或者它做$L^\prime$次MP的同时，$L^{\prime}-L$层的$\boldsymbol{W}_{2}$为0时，GraphSAGE才会遍历$\mathcal{G}_{[v]}$中的节点，否则，$L^\prime$层GraphSAGE将受到$v^{\prime} \notin \mathcal{V}_{[v]}$的影响，那么它无法近似$\tau$。那么问题就变为比较$L^\prime$层shaDow-SAGE在$\mathcal{G}_{[v]}$上的表达能力 和 $L$层GraphSAGE在原图上的表达能力， 因为他们的感受野都是$\mathcal{G}_{[v]}$。接下来，假设GraphSAGE可以在一个局部子图$\mathcal{G}_{[v]}^{\prime}$学习一个函数$\zeta$， 使得$\zeta\left(\mathcal{G}_{[v]}^{\prime}\right)=\tau\left(\mathcal{G}_{[v]}^{\prime}\right)$， 那么如果我们为子图$\mathcal{G}_{[v]}^{\prime}$添加一条边$e$来连接子图第$L$层的两个节点。 那么边$e$将会改变子图的度分布$\delta_{[v]}(\cdot)$，因此$\tau\left(\mathcal{G}_{[v]}^{\prime}\right) \neq \tau\left(\mathcal{G}_{[v]}^{\prime \prime}\right)$一定成立。对于在原图上$L$层GraphSAGE来说，第$L$-hop节点没有可能进行互相之间的message passing，除非GraphSAGE的层数增加，也就是GraphSAGE至少需要$L+1$层才有可能区分关于两个不同图的函数$\tau$, $\tau\left(\mathcal{G}_{-}[v]^{\prime}\right)$ 和$\tau\left(\mathcal{G}_{-}[v]^{\prime \prime}\right)$。所以原图上的$L$层GraphSAGE难以区分函数$\tau$的不同输入，会将不同的输入图输出同一个值，导致表达能力下降。而对于子图上的$L^\prime$层shaDow-SAGE， 直观上来看，它在最后一层后还有Message Passing，所以可以区分不同的输入。因此表达能力强于GraphSAGE。</p>
<h2 id="topological-learning-perspective">Topological Learning Perspective</h2>
<p><img loading="lazy" src="/posts/2022-05-21-DGCN/1.png#center" alt=""  />
</p>
<p>对于Regular Graph, 总所周知它无法被1-WL test 区分， 因为“regular” property describes a global topological symmetry among nodes, 节点之间是全局拓扑对称的，任意两个节点的任意阶邻居数量完全一样，所以1-WL test无法区分regular graph 中的任意两个节点， 如图一中$\mathcal{G}$所示。而对于shaDow-GNN来说，每个节点的局部子图不一定regular，如上图中$\mathcal{G}_{[v]}^{1}$ 和$\mathcal{G}_{[u]}^{1}$所示，使用$\text { EXTRACT}$为节点提取一阶邻居后，可以区分两个节点。</p>
<p>另外$L^\prime$层shaDow-GNN 可以近似$L$层GIN。基于Universal Approximation Theorem, 先做$L^\prime-L$次MLP，使得输入等于输出：
$$
\definecolor{energy}{RGB}{114,0,172}
\definecolor{freq}{RGB}{45,177,93}
\definecolor{spin}{RGB}{251,0,29}
\definecolor{signal}{RGB}{18,110,213}
\definecolor{circle}{RGB}{217,86,16}
\definecolor{average}{RGB}{203,23,206}
\definecolor{red}{RGB}{255,0,0}
\definecolor{black}{RGB}{0,0,0}
\begin{aligned}
\boldsymbol{h}_{v}^{(\ell)} &amp;=f_{1}^{(\ell)}\left(\color{red}\boldsymbol{h}_{v}^{(\ell-1)}, \color{black}\sum_{u \in \mathcal{N}_{v}} f_{2}^{(\ell)}\left(\boldsymbol{h}_{v}^{(\ell-1)}, \boldsymbol{h}_{u}^{(\ell-1)}\right)\right) \\
&amp;=\color{red}\boldsymbol{h}_{v}^{(\ell-1)}\color{black}, \quad \forall 1 \leq \ell \leq L^{\prime}-L
\end{aligned}
$$</p>
<p>在此基础上在做$L$层GIN。 所以$L^\prime$层shaDow-GNN 可以表达$L$层GIN。</p>
<h2 id="子图提取算法">子图提取算法</h2>
<p>启发式$\text { EXTRACT}$： 1. 根据最短路径，随机选取或者选取全部$L$-hop内的邻居作为$\mathcal{G}_{[v]}$。 2. 基于PPR score为每个节点选取top-K 节点，构造诱导子图。同理 Katz index， SimRank都可以作为分数指标。</p>
<h2 id="architecture">Architecture</h2>
<h3 id="subgraph-pooling">Subgraph Pooling</h3>
<p>对于shaDow-GNN，通过READOUT子图中的所有节点embedding来作为target node的embedding、对于normal GNN, 则是直接将第$L$层的输出作为embedding。考虑一个问题，节点的$L$-hop邻域内的节点也可能存在交互，比如对于节点$v$来说，它的邻域内有两个节点$u$和$w$，这两个节点之间的距离可能有$2L$ hop。 但是$L$层GNN无法捕获$u$和$w$之间的交互，除非将层数扩大到$2L$，但这将引入相当多无关节点。 而shaDow-GNN在$L$-hop子图$\mathcal{G}_{[v]}$中直接应用深层（$&gt;2L$）GNN，此时可以捕获邻域内节点互相之间的交互，获得更加有意义的embedding。</p>
<h3 id="subgraph-ensemble">Subgraph Ensemble</h3>
<p>单一的$\text { EXTRACT}$不一定能够选取有意义的邻居， 所以可以使用多个候选$\text { EXTRACT}$，若有$R$个候选$\text { EXTRACT}$，最后每个节点将得到$R$个embedding，然后可以用一个基于attention的聚合函数将每个$\text { EXTRACT}$输出的node embedding组合目标节点最终的node embedding。</p>
<h1 id="reference">Reference</h1>
<p>[1]Tackling Over-Smoothing for General Graph Convolutional Networks.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《From Canonical Correlation Analysis to Self-supervised Graph Neural Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/cca-ssg/</link>
      <pubDate>Thu, 14 Apr 2022 22:54:10 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/cca-ssg/</guid>
      <description>NeurIPS2021 &amp;#34;From Canonical Correlation Analysis to Self-supervised Graph Neural Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2106.12484">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文提出了一种新型的Graph Contrastive Learning构造Contrastive pairs的方式，即将跨图的同维度特征作为positive pairs， 不同维度特征作为negative pairs。 和过去的GCL方法相比，本文无需互信息估计器（MI Estimator），映射头（Projector），不对称结构（asymmetric structures）。 并且理论证明了该方法可以看做Information Bottleneck 原则在自监督设置下的实例。</p>
<p>具体来说，受典型相关分析（From Canonical Correlation Analysis）的启发，本文提出了一种简单有效的GCL框架，从而是模型避免复杂难以理解多模块设计。 和过去的方法<strong>相同</strong>的是，为输入图以随机增强的方式生成两个view， 目的是为两个view学习共享的 node representations 通过共享的GNN Encoder。<strong>不同</strong>在于，本文利用了典型相关分析（CCA），具体来说，新目标旨在最大化同一输入的两个增强views之间的相关性，同时对单个视图表示的不同（特征）维度进行去相关（避免不同维度捕获相同信息，即同一个view内的不同维度channel互为negative pairs）。 这么做的目的是 1）本质上追求的是丢弃增强后变化的信息，同时保留增强后不变的信息，以及 2）防止维度崩溃（即不同维度捕获相同的信息）。</p>
<p><img loading="lazy" src="/posts/2022-04-15-CCA-SSG/1.png#center" alt=""  />
</p>
<p>和其他方法的对比如上图所示， 本文提出的CCA-SSG无需negative pairs， 参数化的互信息估计器， projection head或者不对称结构。对比对的数量仅为$O(D^2)$, 其中$D$为输出维度。</p>
<h1 id="canonical-correlation-analysis">Canonical Correlation Analysis</h1>
<p>CCA: Identify and Quantify the associations between  two sets of variables， 即CCA用来衡量两组随机变量的相关性，每组可能有很多Random Variables.</p>
<p>从相关系数引入：</p>
<p>Pearson 相关系数： 给定两组数据集$X$， $Y$。 其中$X \in \mathbb{R}^{N \times 1}$ 表示只有一个随机变量（属性），样本数为$N$。 $Y \in  \mathbb{R}^{M \times 1}$: 一个随机变量，样本量为$M$。那么Pearson 相关系数$\rho$定义为：
$$
\rho(X,Y)=  \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}
$$
其中$\sigma_X$，$\sigma_Y$分别为$X$和$Y$的标准差。$\mathrm{Cov}(X,Y)$为$X$, $Y$的协方差。$\rho \in [-1,1]$。 $\rho$越接近1， $X$和$Y$的线性相关性越高。$\rho$越接近0，$X$和$Y$的线性相关性月底。</p>
<p><strong>相关系数存在问题</strong>：相关系数不适用于高维数据。 如果$X$是2维的（2个属性，例如身高和体重）， $Y$也是2维的，属性为(跑步，跳远)， $X \in \mathbb{R}^{N \times 2}$, $Y \in  \mathbb{R}^{M \times 2}$。此时，相关系数$\rho$無法計算2維隨機變量的相關程度。</p>
<h2 id="cca-基本思想">CCA 基本思想</h2>
<p>$X$和$Y$ 为两个变量集合， 例如$X$中有两个随机变量（2维）， $Y$中也有两个随机变量。 要衡量变量间的相关性： 现将高维随机变量（即多个随机变量）降到一维（一个随机变量），再用相关系数计算相关性。</p>
<p>令$X = \{\boldsymbol{x}_1,\boldsymbol{x}_2\} \in \mathbb{R}^{n_1\times m}$， 表示$n_1=2$个随机变量，$m$个样本。 $Y = \{\boldsymbol{y}_1,\boldsymbol{y}_2\} \in \mathbb{R}^{n_2\times m}$表示$n_2=2$个随机变量，$m$个样本。</p>
<p>$U$为随机变量集合$X$的线性组合：
$$
U = a_1 \boldsymbol{x}_1 + a_2 \boldsymbol{x}_2 = [a_1, a_2]\begin{bmatrix} \boldsymbol{x}_1 \\ \boldsymbol{x}_2\end{bmatrix} = a^\top X
$$
$V$为随机变量集合$Y$的线性组合：
$$
V = b_1 \boldsymbol{y}_1 + b_2\boldsymbol{x}_2 = b^\top Y
$$
<strong>CCA</strong>的优化目标： 找到一组最优解$a$和$b$， 使得$\rho_{U,V}$最大：
$$
\arg \max_{a,b} \rho_{U,V} = \frac{\mathrm{Cov}(U,V)}{\sigma_U \sigma_V}
$$
得到的$a$, $b$是使得$X$与$Y$有最大关联的权重。</p>
<h2 id="cca的表示与求解">CCA的表示与求解</h2>
<p>输入：两个随机变量集合$X = \{\boldsymbol{x}_1 , \cdots, \boldsymbol{x}_n\}$, $Y= \{\boldsymbol{y}_1 , \cdots, \boldsymbol{y}_m\}$。 分别有$n$个和$m$个随机变量。</p>
<p>$X$是一个$n \times L$的矩阵， 即有$L$个样本， $n$个属性（$n$个随机变量）。</p>
<p>$Y$是一个$m \times L$的矩阵， $L$个样本， $m$个属性。</p>
<p>$U = a^\top X \in \mathbb{R}^{1 \times L}$, $V= b^\top Y \in \mathbb{R}^{1\times L}$, 分别将组高维随机变量转为一维。 目标函数为
$$
\arg \max_{a,b} \rho_{U,V} =\arg \max_{a,b}  \frac{\mathrm{Cov}(U,V)}{\sigma_U \sigma_V}
$$
设 $\Sigma_{XX} = \mathrm{Cov}(X,X) = \mathrm{Var}(X)$， $\Sigma_{YY} = \mathrm{Cov}(Y,Y) = \mathrm{Var}(Y)$， $\Sigma_{XY} = \mathrm{Cov}(X,Y)$， $E[X] = \mu_X \in \mathbb{R}^{n \times 1}$ （样本均值）， $E[Y] = \mu_Y \in \mathbb{R}^{m \times 1}$。</p>
<p>定义$X$ 为一个$n$个随机变量stack成的列向量：
$$
X= \begin{bmatrix} \boldsymbol{x}_1 \\ \cdots \\ \boldsymbol{x}_n\end{bmatrix} \in \mathbb{R}^{n \times L}
$$
$C$ 为$n$个scalars $c_1, \cdots, c_n$ stack成的列向量：
$$
C= \begin{bmatrix} \boldsymbol{c}_1 \\ \cdots \\ \boldsymbol{c}_n\end{bmatrix}
$$
$C^\top X$是这$n$个Random Variables的线性组合。 $C^\top X$的方差为：
$$
\mathrm{Var}(C^\top X) = C^\top \Sigma_{XX} C = C^\top \mathrm{Var}(X) C
$$
那么$\mathrm{Var}(U) = \mathrm{Var}(a^\top X) = a^\top \mathrm{Var}(X) a$。</p>
<p>每个随机变量$\boldsymbol{x}_i$为数据的第$i$个特征，每列为一个样本$X \in \mathbb{R}^{n \times L}$。 有$L$个样本， 对特征维度做标准化，也就是对每个维度$\boldsymbol{x}_i$做标准化， 可得$E(\boldsymbol{x}_i) = 0$, $\mathrm{Var}(\boldsymbol{x}_i) = 1$。
$$
\begin{aligned}
\mathrm{Var}(X) &amp;= E(X-E(X))^2 \\
&amp;= E(\begin{bmatrix} \boldsymbol{x}_1 \\ \cdots \\ \boldsymbol{x}_n\end{bmatrix} -\begin{bmatrix} \boldsymbol{\mu}_1 \\ \cdots \\ \boldsymbol{\mu}_n\end{bmatrix} )^2   \\
&amp;= E (\begin{bmatrix} \boldsymbol{x}_1 \\ \cdots \\ \boldsymbol{x}_n\end{bmatrix}^2) \\
&amp;= E(XX^\top)
\end{aligned}
$$
所以$\mathrm{Var}(U) = a^\top E(XX^\top) a$， 同理$\mathrm{Var}(V) = b^\top E(YY^\top) b$。另外：
$$
E(a^\top X) = E(a_1\boldsymbol{x}_1 + \cdots + a_n\boldsymbol{x}_n) = a_1E(\boldsymbol{x}_1) + \cdots + a_n E(\boldsymbol{x}_n) = 0
$$
那么：
$$
\begin{aligned}
\mathrm{Cov}(U,V) &amp;= \mathrm{Cov}(a^\top X, b^\top Y) \\
&amp;= E\left[ \langle a^\top X - E(a^\top X), b^\top Y- E(b^\top Y) \rangle  \right] \\
&amp;= E[\langle a^\top X,  b^\top Y \rangle] \\
&amp;= E[(a^\top X)(b^\top Y)^\top] \\
&amp;= E[a^\top X Y^\top b] \\
&amp;= a^\top E[XY^\top]b
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathrm{Var}(X) &amp;= \mathrm{Cov}(X,X) = E[XX^\top] \\
\mathrm{Var}(Y) &amp;= \mathrm{Cov}(Y,Y) = E[YY^\top] \\
\mathrm{Cov}(X,Y) &amp;= E[\langle X-\mu_X, Y-\mu_Y \rangle] = E[XY^\top] = \Sigma_{XY}\\
\mathrm{Cov}(Y,X) &amp;=E[YX^\top]
\end{aligned}
$$</p>
<p>优化目标转化为：
$$
\begin{aligned}
\arg \max_{a,b} \rho_{U,V} &amp;=\arg \max_{a,b}  \frac{\mathrm{Cov}(U,V)}{\sigma_U \sigma_V}  \\
&amp;=\arg \max_{a,b} \frac{a^\top \Sigma_{XY}b}{\sqrt{a^\top \Sigma_{XX} a} \sqrt{b^\top \Sigma_{YY}b}}
\end{aligned}
$$
若对$a$， $b$同时放缩， 即$a$放缩$k$倍， $b$放缩$l$倍， 公式的值不会改变：
$$
\frac{ka^\top \Sigma_{XY}lb}{\sqrt{ka^\top \Sigma_{XX} ka} \sqrt{lb^\top \Sigma_{YY}lb}} =  \frac{a^\top \Sigma_{XY}b}{\sqrt{a^\top \Sigma_{XX} a} \sqrt{b^\top \Sigma_{YY}b}}
$$
所以， 可以直接对$a$做放缩，使得$a^\top \Sigma_{XX} a=1$, 对$b$做放缩，使得$b^\top \Sigma_{YY}b=1$（类似于SVM）。 那么优化目标转化为：
$$
\begin{aligned}
&amp;\max_{a, b} a^{\top} \Sigma_{X Y} b, \\ &amp;\text{ s.t. } a^{\top} \Sigma_{X X} a=b^{\top} \Sigma_{Y Y} b=1
\end{aligned}
$$
对于两个向量集合$X_1$和$X_2$， CCA 寻求两组向量最大化它们的相关性，并受到它们彼此不相关的约束。 后来的研究通过用神经网络代替线性变换，将 CCA 应用于具有深度模型的多视图学习。 具体来说，假设 $X_1$和$X_2$作为输入数据的两个视图，CCA的优化目标为：
$$
\max_{\theta_{1}, \theta_{2}} \operatorname{Tr}\left(P_{\theta_{1}}^{\top}\left(X_{1}\right) P_{\theta_{2}}\left(X_{2}\right)\right) \quad \text { s.t. } P_{\theta_{1}}^{\top}\left(X_{1}\right) P_{\theta_{1}}\left(X_{1}\right)=P_{\theta_{2}}^{\top}\left(X_{2}\right) P_{\theta_{2}}\left(X_{2}\right)=I \text {. }  \tag{1}
$$
其中， $P_{\theta_{1}}$和$P_{\theta_{2}}$为两个Neural Network。尽管上式很精确，但这种计算确实很昂贵。Soft CCA 通过采用以下拉格朗日松弛, 消除了hard decorrelation constraint：
$$
\min_{\theta_{1}, \theta_{2}} \mathcal{L}_{\text {dist }}\left(P_{\theta_{1}}\left(X_{1}\right), P_{\theta_{2}}\left(X_{2}\right)\right)+\lambda\left(\mathcal{L}_{S D L}\left(P_{\theta_{1}}\left(X_{1}\right)\right)+\mathcal{L}_{S D L}\left(P_{\theta_{2}}\left(X_{2}\right)\right)\right)
$$
其中$\mathcal{L}_{\text {dist }}$用于衡量两个view的representations之间的相关性，$\mathcal{L}_{S D L}$ (stochastic decorrelation loss)计算$P_{\theta_{i}}\left(X_{i}\right)$和identity matrix之间的$L_1$距离。</p>
<h1 id="approach">Approach</h1>
<p><img loading="lazy" src="/posts/2022-04-15-CCA-SSG/2.png#center" alt=""  />
</p>
<p>模型包含3个模块 1. 随机图增强器$\mathcal{T}$，2. GNN encoder $f_\theta$, 3. 基于CCA的feature-level对比损失。</p>
<h2 id="graph-augmentations">Graph Augmentations</h2>
<p>本文利用 edge droping和 node feature masking两种graph corruption方式来对输入图做增强。 $\mathcal{T}$是所有可能的转换操作，$t \sim \mathcal{T}$表示图$G$的一种特定的转换。比如删除一条边的操作$t_r$就是$\mathcal{T}$中的一个变换。</p>
<h2 id="training">Training</h2>
<p>从$\mathcal{T}$随机采样两种图变换 $t_A$和$t_B$。 生成两个View: $\tilde{\mathbf{G}}_{A}=\left(\tilde{\mathbf{X}}_{A}, \tilde{\mathbf{A}}_{A}\right)$和$\tilde{\mathbf{G}}_{B}=\left(\tilde{\mathbf{X}}_{B}, \tilde{\mathbf{A}}_{B}\right)$，经过共享的GNN后，得到输出$\mathbf{Z}_{A}=f_{\theta}\left(\tilde{\mathbf{X}}_{A}, \tilde{\mathbf{A}}_{A}\right)$，$\mathbf{Z}_{B}=f_{\theta}\left(\tilde{\mathbf{X}}_{B}, \tilde{\mathbf{A}}_{B}\right)$。然后对feature dimensionzuo normalization (列标准化)， 是的每个特征维度均值为0， 标准差为$1 / \sqrt{N}$：</p>
<p>$$
\tilde{\mathbf{Z}}=\frac{\mathbf{Z}-\mu(\mathbf{Z})}{\sigma(\mathbf{Z}) * \sqrt{N}}
$$</p>
<h1 id="inference">Inference</h1>
<p>基于公式（1）,使用公式(1)中的CCA目标函数，将向量集定义为输出$\tilde{\mathbf{Z}}$的列向量， 最终CCA-SSG的目标函数定义如下：
$$
\mathcal{L}=\underbrace{\left|\left|\tilde{\mathbf{Z}}_{A}-\tilde{\mathbf{Z}}_{B}\right|\right|_{F}^{2}}_{\text {invariance term }}+\lambda \underbrace{\left(\left|\left|\tilde{\mathbf{Z}}_{A}^{\top} \tilde{\mathbf{Z}}_{A}-\mathbf{I}\right|\right|_{F}^{2}+\left|\left|\tilde{\mathbf{Z}}_{B}^{\top} \tilde{\mathbf{Z}}_{B}-\mathbf{I}\right|\right|_{F}^{2}\right)}_{\text {decorrelation term }}
$$
第二项中，要求不同特征之间的相似度尽可能低， 从而使得不同特征捕获不同的语义信息。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2020 《Contrastive Multi-View Representation Learning on Graphs》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/mvgrl/</link>
      <pubDate>Tue, 12 Apr 2022 22:21:29 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/mvgrl/</guid>
      <description>ICML2020 &amp;#34;Contrastive Multi-View Representation Learning on Graphs&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2006.05582">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文旨在通过多视图Contrastive Learning 来学习节点表示和图表示。其中对比视图为结构视图（structural view）。本文发现，两个以上的对比视图不会提升性能（我觉得仅是针对本文的Diffusion-based view吧~）。 本文实验性的表明了基于一阶邻居和图扩散视图做CL可以达到最好的效果。</p>
<p>为了将对比学习应用到图表示学习任务，本文提出通过最大化图的不同结构视角的互信息来作为监督信号。通过对提出框架的系统研究，本文展示了一些GCL和visual CL上的不同： （1）将view数量（即增强）增加到两个以上不会提高性能，最好的性能是通过对比来自一阶邻居view的embedding和graph diffusion的embedding，(2) 与对比图编码或多尺度编码相比，跨视图对比节点和图编码在node classification 和 graph classification上都能获得更好的结果。 (3) 与分层图池化方法（例如DiffPool相比）一个简单的Readout在这node classification 和 graph classification上实现了更好的性能，以及 (4) 应用正则化（early stopping除外） 或归一化层对性能有负面影响。</p>
<h1 id="method">Method</h1>
<p><img loading="lazy" src="/posts/2022-04-13-MVGRL/1.png" alt=""  />
</p>
<p>MVGRL通过最大化一个view的node embedding和另一个view的graph embedding之间的 互信息来学习节点和图表示。如上图所示，MVGRL由以下几个部分构成</p>
<ul>
<li><strong>增强机制</strong>：将样本图转化为同一个图的相关view， 这个view只是structural view， 不会改变原图中的node feature，然后对两个增强图中的相同节点（identical node）进行子采样，类似于CV中的域剪裁。</li>
<li><strong>两个专用的GNNs</strong>， 每个view一个GNN，再接一个共享的MLP作为projection head，来为两个view学习representation。</li>
<li><strong>图池化层</strong>， 在MLP后学习两个图的graph-level representation。</li>
<li><strong>判别器</strong> 来对比一个图的embedding和另一个图的节点embedding,并对他们的一致性（agreement）评分。</li>
</ul>
<h2 id="augmentations">Augmentations</h2>
<p>考虑两种类型的图增强：(1) 对初始节点特征进行操作的特征空间增强，例如，mask或添加高斯噪声，以及 (2) 通过添加或删除连通性、子采样或使用最短路径或diffusion matrix生成全局视图来对做图结构增强。 前一种增强可能是有问题的，因为许多数据集不带有初始节点特征。 此外，观察到在任一空间上屏蔽或添加噪声都会降低性能。 因此，本文选择生成全局视图，然后进行子采样。</p>
<p>实验表明，在大多数情况下，最好的结果是通过将邻接矩阵转化为扩散矩阵，并将这两个矩阵视为同一图的结构的两个一致view。因为<strong>邻接矩阵和扩散矩阵分别提供了图结构的局部和全局视图</strong>，从这两种view中学习到的表示之间最大一致性，从而鼓励模型同时编码的局部和全局信息。</p>
<p>Diffusion matrix从全局角度提供了任意节点对之间的相关性，其中$\mathbf{T} \in \mathbb{R}^{n \times n}$是通用转移矩阵，$\Theta$是权重系数，决定了全局和局部信息的比例，即对于每个节点，不同层次信息的比重， $\Theta_{k}$越大，表示全局信息权重越大。 令$\sum_{k=0}^{\infty} \theta_{k}=1, \theta_{k} \in[0,1]$，$\lambda_{i} \in[0,1]$,其中$\lambda$是$\mathbf{T}$的特征向量，  这样来保证$\mathbf{S}$可以收敛到一个固定矩阵。扩散用快速近似值和稀疏化方法计算：
$$
\mathbf{S}=\sum_{k=0}^{\infty} \Theta_{k} \mathbf{T}^{k} \in \mathbb{R}^{n \times n}
$$
给定一个邻接矩阵$\mathbf{A} \in \mathbb{R}^{n \times n}$和一个对角度矩阵$\mathbf{D} \in \mathbb{R}^{n \times n}$, Personalized PageRank (PPR)和Heat Kernel分别为两种不同的Diffusion matrix实例。对于PPR和HK，转移概率矩阵定义为$\mathbf{T}=\mathbf{A} \mathbf{D}^{-1}$。PPR将第$k$层的权重系数设置为$\theta_{k}=\alpha(1-\alpha)^{k}$, 而HK将第$k$层的权重系数设置为$\theta_{k}=e^{-t} t^{k} / k !$。</p>
<p>PPR的封闭阶如下所示：
$$
\mathbf{S}^{\mathrm{PPR}}=\alpha\left(\mathbf{I}_{n}-(1-\alpha) \mathbf{D}^{-1 / 2} \mathbf{A} \mathbf{D}^{-1 / 2}\right)^{-1}
$$
HK的封闭解如下所示：
$$
\mathbf{S}^{\text {heat }}=\exp \left(t \mathbf{A} \mathbf{D}^{-1}-t\right)
$$</p>
<h2 id="sub-sampling">Sub-Sampling</h2>
<p>从一个view中<strong>随机采样节点及其边</strong>，并从另一个view中<strong>选择exact的的节点和边</strong> (如示意图所示， 从第一个图中采样节点和边的子图作为一个view，从第二个图中采样相同节点以及这些节点之间的边作为另一个view，来做对比学习)。这个过程允许MVGRL应用于具有图数据不适合GPU内存的inductive任务，也可以通过将子样本视为独立的图来考虑transductive任务。</p>
<h2 id="encoder">Encoder</h2>
<p>和其他GCL方法不同的是，这里不同视图使用的是各自的GNN编码器， 邻接矩阵和Diffusion matrix是同一个图的两个一致视角，分别反映了局部和全局性质。首先，为两种view采样之后的子图分别定义GNN encoder：$g_{\theta}(.), g_{\omega}(.): \mathbb{R}^{n \times d_{x}} \times \mathbb{R}^{n \times n} \longmapsto \mathbb{R}^{n \times d_{h}}$， 使用最简单的GCN，传播矩阵分别为normalized adjacency matrix $\sigma(\tilde{\mathbf{A} }X \boldsymbol{\Theta})$ 和 Diffusion Matrix:  $\sigma(\mathbf{S} X \boldsymbol{\Theta})$。学习到的embedding输入projection head （MLP）$f_{\psi}(.): \mathbb{R}^{n \times d_{h}} \longmapsto \mathbb{R}^{n \times d_{h}}$中， 得到两个view的输出node embedding matrix: $\mathbf{H}^{\alpha}, \mathbf{H}^{\beta} \in \mathbb{R}^{n \times d_{h}}$。</p>
<p>接下来使用pooling $\mathcal{P}(.): \mathbb{R}^{n \times d_{h}} \longmapsto \mathbb{R}^{d_{h}}$ 输出两个view的graph representations。 本文采用JKnet中的跳连机制，即GNN的每层输出做sum pooling, 然后将所有层拼起来做特征变换：
$$
\vec{h}_{g}=\sigma\left(||_{l=1}^{L}\left[\sum_{i=1}^{n} \vec{h}_{i}^{(l)}\right] \mathbf{W}\right) \in \mathbb{R}^{h_{d}}
$$
其中$\vec{h}_{i}^{(l)}$是节点$i$的第$l$层输出，$||$是concatenation， $\mathbf{W} \in \mathbb{R}^{\left(L \times d_{h}\right) \times d_{h}}$是特征变换参数，$\sigma$是PReLU非线性激活。最终，将图表示输入到一个projection head $f_{\phi}(.): \mathbb{R}^{d_{h}} \longmapsto \mathbb{R}^{d_{h}}$ 中，得到最终的图表示：$\vec{h}_{g}^{\alpha}, \vec{h}_{g}^{\beta} \in \mathbb{R}^{d_{h}}$。</p>
<p>在推理阶段， 由于两个view来自同一个图，可以把两个view的表示结合起来作为原图的表示：两个view的graph embedding直接相加，作为原图 embedding.。 两个view的node embedding 直接相加，作为原图的node embedding $\vec{h}=\vec{h}_{g}^{\alpha}+\vec{h}_{g}^{\beta} \in \mathbb{R}^{n}$ 。 $\mathbf{H}=\mathbf{H}^{\alpha}+\mathbf{H}^{\beta} \in \mathbb{R}^{n \times d_{h}}$。 这里得到的原图embedding可以应用于下游任务。</p>
<h2 id="training">Training</h2>
<p>为了端到端训练encoder并学习与下游任务无关的丰富节点和图级表示，本文利用 Deep InfoMax 方法并通过对比一个视图的节点表示与图表示来最大化两个视图之间的 互信息。 实验表明，这种方法在节点和图分类上始终优于对比图-图或多尺度编码。 目标定义如下：
$$
\max_{\theta, \omega, \phi, \psi} \frac{1}{|\mathcal{G}|} \sum_{g \in \mathcal{G}}\left[\frac{1}{|g|} \sum_{i=1}^{|g|}\left[\operatorname{MI}\left(\vec{h}_{i}^{\alpha}, \vec{h}_{g}^{\beta}\right)+\operatorname{MI}\left(\vec{h}_{i}^{\beta}, \vec{h}_{g}^{\alpha}\right)\right]\right]
$$
其中$\theta, \omega, \phi, \psi$为是GNN encoder和projection head的参数, $|\mathcal{G}|$是图数量，$|\mathcal{g}|$是图中节点数， $\vec{h}_{i}^{\alpha}, \vec{h}_{g}^{\beta}$分别表示view $\alpha$中的节点$i$的representation， 和view $\beta$的 graph representation。</p>
<p>互信息判别器： $\mathcal{D}(., .): \mathbb{R}^{d_{h}} \times \mathbb{R}^{d_{h}} \longmapsto \mathbb{R}$简单的设置为表示向量间的内积相似度：
$$
\mathcal{D}\left(\vec{h}_{n}, \vec{h}_{g}\right)=\vec{h}_{n} \cdot \vec{h}_{g}^{T}
$$
作者发现当判别器和projection head集成到双线性层中时，节点分类基准略有改进。 为了确定 MI 估计器，实验中调查了四个估计器并为每个基准选择了最好的一个。</p>
<p>正样本采样自联合分布$x_{p} \sim p\left(\left[\mathbf{X}, \tau_{\alpha}(\mathbf{A})\right],\left[\mathbf{X}, \tau_{\beta}(\mathbf{A})\right]\right)$， 从边际乘积中采样负样本 $x_{p} \sim p\left(\left[\mathbf{X}, \tau_{\alpha}(\mathbf{A})\right]\right) p\left(\left[\mathbf{X}, \tau_{\beta}(\mathbf{A})\right]\right)$。利用小批量随机梯度下降法对模型参数进行优化。 MVGRL算法如下：</p>
<p><img loading="lazy" src="/posts/2022-04-13-MVGRL/2.png" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>WWW2022 《SimGRACE:A Simple Framework for Graph Contrastive Learning without Data Augmentation》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/simgrace/</link>
      <pubDate>Sat, 09 Apr 2022 14:47:57 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/simgrace/</guid>
      <description>WWW2022 &amp;#34;SimGRACE:A Simple Framework for Graph Contrastive Learning without Data Augmentation&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2202.03104">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>图对比学习（GCL）已经成为图表示学习的主要技术，它最大化了共享相同语义的成对图增强之间的互信息。鉴于图数据的多样性，在增强过程中很难很好地保留语义。目前，GCL 中选择图增强方式的途径通常有以下三种。 1.  适用于不同数据集的图增强方式可能是不同的，需要在每个数据集上做验证，手动选择最适用于每个数据集的增强。2. 通过繁琐的搜索来选择增强方式。3. 通过邻域只是来选择增强方式。所有这些都限制了现有 GCL 方法的效率和通用性。为了解决该问题，本文提出了一种不需要对图做编辑， 而是对GNN编码器做扰动的增强方式： <strong>SimGRACE</strong>。并且对SimGRACE设计了对抗训练的方案：<strong>AT-SimGRACE</strong>。</p>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/1.png#center" alt="图1"  title="123"  />
</p>
<p>上图的实验中，两类图用不同的颜色标出，三种GCL模型分别在三个数据集上训练，训练完成后的分类效果如第一行所示。 对于GraphCL, 对边做扰动后再输入GraphCL 训练好的encoder,可以看出GraphCL的encoder对于扰动后的图数据集无法很好的保留分类语义。而对于SIMGRACE，不对图做扰动，而对训练好的encoder做扰动，扰动后的encoder对数据集的分类效果可以很好地保留语义信息。由此实验性表明了对encoder扰动可以保留比直接对图扰动更多的语义信息。</p>
<p>GraphCL 表明 GNN 可以使用他们提出的框架获得鲁棒性。 但是，（1）他们没有解释为什么 GraphCL 可以增强鲁棒性； (2) GraphCL 似乎对随机攻击具有很好的免疫力，而对对抗性攻击的表现却不尽如人意。为了弥补这些缺陷，本文基于SimGRACE提出了一种新的算法 AT-SimGRACE通过对抗的方式来扰动编码器，从而是实现对抗训练的效果，它引入了更少的计算开销，同时显示出更好的鲁棒性。</p>
<h1 id="method">Method</h1>
<h2 id="simgrace">SimGRACE</h2>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/2.png#center" alt="图1"  />
</p>
<h3 id="编码器扰动encoder-perturbation">编码器扰动（Encoder perturbation）</h3>
<p>给定一个GNN编码器$f(\cdot;\theta)$,它的参数扰动版本表示为$f(\cdot;\theta^\prime)$。如图中所示，参数扰动版本的编码器不需要梯度反传训练参数，每次训练过程更新$f(\cdot;\theta)$，而$f(\cdot;\theta^\prime)$的参数$\theta^\prime$只通过对$\theta$扰动得到。第$l$层GNN的参数表示为$\theta_l$，那么它的扰动后参数$\theta^\prime_l$有下式得到：
$$
\theta_{l}^{\prime}=\theta_{l}+\eta \cdot \Delta \theta_{l} ; \quad \Delta \theta_{l} \sim \mathcal{N}\left(0, \sigma_{l}^{2}\right)
$$
其中$\eta$用来控制扰动的缩放，$\Delta \theta_{l}$是扰动项，扰动值采样自0均值$\sigma_{l}^{2}$的Gaussian Distribution。$f(\cdot;\theta)$和$f(\cdot;\theta^\prime)$的输出分别为$\mathbf{h}$和$\mathbf{h}^{\prime}$：
$$
\mathbf{h}=f(\mathcal{G} ; \boldsymbol{\theta}), \mathbf{h}^{\prime}=f\left(\mathcal{G} ; \boldsymbol{\theta}^{\prime}\right)
$$
从下图可以看出，如果不对编码器施加扰动，即超参数$\eta=0$，效果会很差，扰动太多效果也会很差。</p>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/3.png#center" alt="图1"  />
</p>
<h3 id="映射头-projection-head">映射头 （Projection Head）</h3>
<p>和其他大多数GCL方法一样，该方法也要一个projection head来对GNN的output representation做一次变换，通常就是个MLP，得到输出$z$和$z^\prime$：
$$
z=g(\mathbf{h}), z^{\prime}=g\left(\mathbf{h}^{\prime}\right)
$$</p>
<h3 id="对比损失contrastive-loss">对比损失（Contrastive loss）</h3>
<p>和GraphCL一样，使用NT-Xent作为损失函数。具体来说，用$z_n$和$z_n^\prime$分别表示表示图$n$在$f(\cdot;\theta)$和$f(\cdot;\theta^\prime)$两个编码器下的输出， 用$z_n$和$z_{n^\prime}$表示一个batch中两个不同图$n$和图$n^\prime$在未扰动编码器$f(\cdot;\theta)$下的输出。在一个batch内，最大化同一个图的两个编码器（$f(\cdot;\theta)$和$f(\cdot;\theta^\prime)$）输出间的相似度，同时最小化不同图在未扰动编码器$f(\cdot;\theta)$下输出的相似度：
$$
\ell_{n}=-\log \frac{\left.\exp \left(\operatorname{sim}\left(z_{n}, z_{n}^{\prime}\right)\right) / \tau\right)}{\sum_{n^{\prime}=1, n^{\prime} \neq n}^{N} \exp \left(\operatorname{sim}\left(z_{n}, z_{n^{\prime}}\right) / \tau\right)}
$$
即同一个图的两个输出为positive pair, 不同图的$f(\cdot;\theta)$输出为negative pair.</p>
<h2 id="why-can-simgrace-work-well">Why can SimGRACE work well?</h2>
<p>[1] 提供了两个属性来衡量对比学习学到的representation的质量： <em>Alignment和Uniformity</em>。其中Alignment metric直接定义为positive pairs之间的距离：
$$
\ell_{\text {align }}(f ; \alpha) \triangleq \underset{(x, y) \sim p_{\text {pos }}}{\mathbb{E}}\left[||f(x)-f(y)||_{2}^{\alpha}\right], \quad \alpha&gt;0
$$
其中$p_{\text {pos }}$为positive pairs的分布，也就是positive pairs之间的距离越小，说明CL越好。 基于SimGRACE构造contrastive pairs的方式，alignment metric 可以定义为如下形式：
$$
\ell_{\text {align }}(f ; \alpha) \triangleq \underset{x \sim p_{\text {data }}}{\mathbb{E}}\left[\left|\left|f(x ; \theta)-f\left(x ; \theta^{\prime}\right)\right|\right|_{2}^{\alpha}\right], \quad \alpha&gt;0
$$
另一个衡量指标是Uniformity， 定义为成对高斯势函数（Gaussian Potential）：
$$
\ell_{\text {uniform }}(f ; \alpha) \triangleq \log \underset{x, y_{\sim}^{i . i . d .} p_{\text {data }}}{\mathbb{E}}\left[e^{-t||f(x ; \theta)-f(y ; \theta)||_{2}^{2}}\right] . \quad t&gt;0
$$
它要求随机样本的embedding应尽可能分散在hypersphere上， 即随机采样两个图在未扰动编码器输出的embedding距离要尽可能大。从下图可以看出，随着training epoch的增加，三种方法都呈现出正确的趋势。</p>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/4.png#center" alt="图1"  />
</p>
<h2 id="at-simgrace">AT-SimGRACE</h2>
<p>通过对抗训练（Adversarial Training， AT）来提升SimGRACE的鲁棒性。 对抗训练的优化问题定义如下：
$$
\min_{\theta} \mathcal{L}^{\prime}(\theta), \quad \text { where } \quad \mathcal{L}^{\prime}(\theta)=\frac{1}{n} \sum_{i=1}^{n} \max_{ | |\mathrm{x}_{i}^{\prime}-\mathrm{x}_{i} | |_{p} \leq \epsilon} \ell_{i}^{\prime}\left(f\left(\mathrm{x}_{i}^{\prime} ; \theta\right), y_{i}\right)
$$
其中$n$是训练样本数，$\mathrm{x}_{i}^{\prime}$是对抗样本， 其中对抗样本在训练样本的$\epsilon$-ball中，即$| |\mathrm{x}_{i}^{\prime}-\mathrm{x}_{i} | |_{p} \leq \epsilon$, 表示对抗样本和原样本的变化不能超过$\epsilon$。Adversarial Training: 优化$\theta$，使得$f$可以在$\mathrm{x}_{i}$的对抗样本$\mathrm{x}_{i}^{\prime}$上可以预测准确。其中$\ell^{\prime}(\cdot)$为监督分类损失，$\mathcal{L}^{\prime}(\theta)$为对抗损失。AT不能直接应用于CL上，因为（1）CL任务无标签，（2）对数据集中的每个样本扰动计算量太大。 为了解决这个问题，本文将AT loss中的损失函数部分换成NT-Xent对比学习损失，然后用对抗的方式来扰动encoder，从而无需对数据集中的所有样本扰动。</p>
<p>假设$\Theta$为GNN的权重空间(weight space)， 对于任意$\mathbf{w}$任意正实数$\epsilon$, 为$\theta$定义半径为$\epsilon$,中心为$\mathbf{w}$的norm ball:
$$
\mathbf{R}(\mathbf{w} ; \epsilon):=\{\boldsymbol{\theta} \in \boldsymbol{\Theta}:||\boldsymbol{\theta}-\mathbf{w}|| \leq \epsilon\}
$$
$\theta \in \Theta$表示权重空间$\Theta$中任意一组可能的GNN权重$\theta$, $\mathbf{R}(\mathbf{w} ; \epsilon)$表示GNN所有与$\mathbf{w}$相似的权重，即所有与$\mathbf{w}$的差距小于$\epsilon$的权重。</p>
<p>那么AT-SimGRACE的优化问题定义如下：
$$
\begin{gathered}
\min_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}+\Delta) \\
\text { where } \mathcal{L}(\boldsymbol{\theta}+\Delta)=\frac{1}{M} \sum_{i=1}^{M} \max_{\Delta \in \mathrm{R}(0 ; \epsilon)} \ell_{i}\left(f\left(\mathcal{G}_{i} ; \boldsymbol{\theta}+\Delta\right), f\left(\mathcal{G}_{i} ; \boldsymbol{\theta}\right)\right)
\end{gathered}
$$
这里$\mathrm{R}(0 ; \epsilon)=\{\Delta \in \Theta: ||\Delta|| \leq \epsilon\}$ ，$\mathcal{L}(\boldsymbol{\theta}+\Delta)$表示在对GNN参数施加扰动$\Delta$，使得GNN的效果最差，换句话说，找到一个扰动$\Delta$，使得GNN的参数在被$\Delta$扰动后（变为$\theta+\Delta$）两个图最不匹配（对比学习损失达到最大）。 $min_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}+\Delta)$表示训练GNN参数，使得对比学习可以适应该扰动。算法如下：</p>
<p><img loading="lazy" src="/posts/2022-04-12-SimGRACE/5.png#center" alt="图1"  />
</p>
<p>对抗训练：</p>
<p>内层： 固定GNN参数，训练扰动参数$\Delta$，使得GNN的对比学习loss上升</p>
<p>外层： 固定扰动参数$\Delta$， 训练GNN参数$\theta$， 使得$\theta$加上扰动$\Delta$后的对比学习loss最小化。</p>
<h1 id="reference">Reference</h1>
<p>[1] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. ICML (2020)</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Graph Neural Networks with Adaptive Residual》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/airgnn/</link>
      <pubDate>Fri, 08 Apr 2022 21:23:41 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/airgnn/</guid>
      <description>NeurIPS2021 &amp;#34;Graph Neural Networks with Adaptive Residual&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=hfkER_KJiNw">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>在Deeper GNN中，residual connections通常可以缓解oversmoothing问题，但是，若图中的存在abnormal node features, 那么residual connections会放大abnormal features的影响。本文旨在设计AirGNN， 在自适应调整残差连接的权重，是的可以弹性适应存在abnormal node features的图。太多聚合（deep layers）会导致oversmoothing，但residual 对深层GNN有益，但是对于abnormal features是脆弱的。</p>
<h1 id="preliminary">Preliminary</h1>
<p>Frobenius norm: $||\mathbf{X}||_{F}=\sqrt{\sum_{i j} \mathbf{X}_{i j}^{2}}$</p>
<p>$\ell_{21}$ norm: $||\mathbf{X}||_{21}= \sum_{i}\left|\left|\mathbf{X}_{i}\right|\right|_{2}=\sum_{i} \sqrt{\sum_{j} \mathbf{X}_{i j}^{2}}$  表示对每行算$\ell_2$ norm 再对所有行算$\ell_1$ norm。</p>
<h2 id="study">Study</h2>
<p><img loading="lazy" src="/posts/2022-04-05-AirGNN/1.png#center" alt="你想输入的替代文字"  />
</p>
<p>如图Figure 1所示， 对于具有abnormal node feature 的图，添加residual（蓝线）会导致性能巨大下降，因为abnormal node feature是与任务无关的，residual相对于无residual 保留了更多original abnormal features。</p>
<p><img loading="lazy" src="/posts/2022-04-05-AirGNN/2.png#center" alt="你想输入的替代文字"  />
</p>
<p>对于node feature 都是normal的图， 没有residual的话，随着层次加深，GNN的性能会下降。</p>
<p>综上，residual可以是GNN层次加深（容忍更多聚合），但是对abnormal features的鲁棒性较差。</p>
<h2 id="understandings-i-feature-aggregation-as-laplacian-smoothing">Understandings I: Feature aggregation as Laplacian smoothing</h2>
<p>对于Laplacian Smoothing problem:
$$
\underset{\mathbf{X} \in \mathbb{R}^{n \times d}}{\arg \min } \mathcal{L}_{1}(\mathbf{X}):=\frac{1}{2} \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)=\frac{1}{2} \sum_{\left(v_{i}, v_{j}\right) \in \mathcal{E}}\left|\left|\frac{\mathbf{X}_{i}}{\sqrt{d_{i}+1}}-\frac{\mathbf{X}_{j}}{\sqrt{d_{j}+1}}\right|\right|_{2}^{2}  \tag{1}
$$
其目标是找到最佳的$X$,使得$X$在图上最平滑。而对于GCN, GCNII (w/o residual)和APPNP (w/o residual), 他们的每一层都可以看做是如下的特征聚合方式：
$$
\mathbf{X}^{(k+1)}=\tilde{\mathbf{A}} \mathbf{X}^{(k)} = \tilde{\mathbf{A}}=(\hat{\mathbf{D}}^{-\frac{1}{2}} \hat{\mathbf{A}} \hat{\mathbf{D}}^{-\frac{1}{2}})\mathbf{X}^{(k)} \tag{2}
$$
实际上，迭代多层GNN可以看做是以 step size =1的条件下，以梯度下降的方式求解Laplacian Smoothing问题，即以梯度下降的方式找到在图上最平滑的信号：
$$
\begin{equation}
\begin{aligned}
\mathbf{X}^{(k+1)} &amp;= \mathbf{X}^{(k)}-\left.\gamma \frac{\partial \mathcal{L}_{1}}{\partial \mathbf{X}} \right|_{\mathbf{X}=\mathbf{X}^{(k)}}\\
&amp;= \mathbf{X}^{(k)}-\left.\gamma \frac{\partial \frac{1}{2} \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)}{\partial \mathbf{X}} \right|_{\mathbf{X}=\mathbf{X}^{(k)}} \\
&amp;= \mathbf{X}^{(k)} - (\mathbf{I}-\tilde{\mathbf{A}})\mathbf{X}^{(k)} \\
&amp;= \tilde{\mathbf{A}} \mathbf{X}^{(k)}<br>
\end{aligned}\tag{3}
\end{equation}
$$
其中令$\gamma = 1$, 所以，迭代多层GCN相当于以step size=1的方式迭代求解Laplacian Smoothing 问题。GCNII (w/o residual)和APPNP (w/o residual) 同理。</p>
<p>堆叠GCN层来求解Laplacian smoothing问题可以被解释为图上信号的低通filter，即对于相邻节点，保留邻居节点间相似的特征(低频信号)，remove相邻节点间不同的特征（高频信号）。abnormal feature会导致图上的信号不平滑， 也就是若$v_i$的feature是abnormal，$v_j$的feature是normal, 且$v_i$与$v_j$相邻， 那么可能导致信号$k$的两个分量$X_{ik}$和$X_{jk}$ 差异较大， 所以abnormal feature可以看做图上的高频信号。 作为高频信号，它会被GCN等低筒滤波器过滤，即随着层数的加深，网络上的信号会越来越平滑，$X_{ik}^{out}$和$X_{jk}^{out}$的差距会变小，所以加深GCN可以缓解abnormal带来的问题。</p>
<h2 id="understandings-ii-residual-connection-maintains-feature-proximity">Understandings II: Residual connection maintains feature proximity</h2>
<p>含有residual的APPNP形式如下：
$$
\mathbf{X}^{k+1}=(1-\alpha) \tilde{\mathbf{A}} \mathbf{X}^{k}+\alpha \mathbf{X}_{\text {in }}  \tag{4}
$$
它也可以看做是对Laplaican Smoothing问题的求解，只不过加上的正则化项。 APPNP可以看做迭代求解如下regularized Laplacian smoothing problem:
$$
\underset{\mathbf{X} \in \mathbb{R}^{n \times d}}{\arg \min} \mathcal{L}_{2}(\mathbf{X}):=\frac{1}{2} \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right) + \frac{\alpha}{2(1-\alpha)}\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{F}^{2} \tag{5}
$$
其中 step size $\gamma = 1-\alpha$:
$$
\mathbf{X}^{k+1}=\mathbf{X}^{k}-(1-\alpha)\left(\frac{\alpha}{1-\alpha}\left(\mathbf{X}^{k}-\mathbf{X}_{\text {in }}\right)+(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}^{k}\right)=(1-\alpha) \tilde{\mathbf{A}} \mathbf{X}^{k}+\alpha \mathbf{X}_{\text {in }} \tag{6}
$$
上式的求解过程和公式(3)差不多。 其中$\frac{\alpha}{2(1-\alpha)}\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{F}^{2}$为正则化项， 上式要求 求得的$X$在图上尽可能平滑的同时， 要与输入尽可能接近。这样对平滑加以限制后，可以缓解深层GNN产生的oversmoothing 问题，因为保留了一些必要的高频信号。但是这些残差连接也携带了有害的异常特征，导致在含有abnormal feature的图上性能较差。</p>
<h1 id="the-proposed-model">The Proposed Model</h1>
<h2 id="design-motivation">Design Motivation</h2>
<p>更多的feature aggregation 可以缓解abnormal feature但是oversmoothing。residual 可以缓解深层GNN，但是受abnormal feature影响。 <strong>如何设计MPNN使得node 可以自适应的特征聚合和residual?</strong></p>
<p>公式（5）中的正则化项$\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{F}^{2} $决定了GNN的更新中含有residual，这样可以保持深层GNN的稳定性。虽然保持输入与每层输出之间的proximity对于加深层次很重要，但是，用Frobenius norm来惩罚偏差可能过于激进，即会使得输入和输出过于接近，从而削弱了Laplacian Smoothing项$\operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)$去除abnormal feature的能力。因此本文提出用$\ell_{21}$ norm 来替换Frobenius norm作为输入输出之间proximity的保留项。 相比于Frobenius norm，不那么激进，即最小化$\ell_{21}$ norm 不会让输出过于接近输入。It also allows large deviations because the penalty on large values is less aggressive, leading to the potential removal of abnormal features：意思是$\mathbf{X}-\mathbf{X}_{\text {in}}$ 如果很大的话，它的$\ell_{21}$ norm在目标函数中不会占据过于大的分量，即 $\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{F}^{2} &gt; \left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{21} $。这样，最小化regularized Laplacian smoothing problem的目标函数时，不会过度倾向于最小化邻近度项。 所以regularized Laplacian smoothing 问题定义如下：
$$
\underset{\mathbf{X} \in \mathbb{R}^{n \times d}}{\arg \min } \mathcal{L}(\mathbf{X}):=\underbrace{(1-\lambda) \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)}_{可微 g(X)} + \underbrace{\lambda\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{21}}_{不可微 h(X)}   \tag{7}
$$
其中$\lambda \in [0,1]$。</p>
<h2 id="adaptive-massage-passing">Adaptive Massage Passing</h2>
<p>公式（7）中，$ \mathcal{L}(\mathbf{X})$由可微和不可微凸函数组成，可以使用Proximal Gradient Descent（PGD）来优化 （PGD可以见<a href="https://jhuow.fun/posts/pgd/">这篇文章</a>）。公式（7）中可微部分 记为$g(X) = (1-\lambda) \operatorname{tr}\left(\mathbf{X}^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \mathbf{X}\right)$ ， 不可微部分记为 $h(X) = \lambda\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{21}$。 根据PGD, 是的(7)最小的$\mathbf{X}$可以通过迭代方式求解：
$$
\definecolor{energy}{RGB}{114,0,172}
\definecolor{freq}{RGB}{45,177,93}
\definecolor{spin}{RGB}{251,0,29}
\definecolor{signal}{RGB}{18,110,213}
\definecolor{circle}{RGB}{217,86,16}
\definecolor{average}{RGB}{203,23,206}
\definecolor{red}{RGB}{255,0,0}
\boldsymbol{X}^{k+1}=\operatorname{prox}_{\color{signal}h\color{energy}\gamma}\left(\boldsymbol{X}^{k}-\gamma \nabla g\left(\boldsymbol{X}^{k}\right)\right) \tag{8}
$$
其中，
$$
\begin{equation}
\begin{aligned}
\nabla g\left(\boldsymbol{X}^{k}\right) &amp;= \nabla \left[ (1-\lambda) \operatorname{tr}\left((\boldsymbol{X}^{k})^{\top}(\mathbf{I}-\tilde{\mathbf{A}}) \boldsymbol{X}^{k}\right)\right] \\
&amp;=(1-\lambda)(2(I-\tilde{\mathbf{A}})\boldsymbol{X}^{k}) \\
&amp;= 2\boldsymbol{X}^{k} - 2 \tilde{\mathbf{A}}\boldsymbol{X}^{k} - 2\lambda \boldsymbol{X}^{k}+ 2\lambda \tilde{\mathbf{A}}\boldsymbol{X}^{k} \\
&amp;=2(1-\lambda) (\mathbf{I}-\tilde{\mathbf{A}}) \boldsymbol{X}^{k}
\end{aligned} \tag{9}
\end{equation}
$$
令
$$
\begin{aligned}
\boldsymbol{Y}^{k} = \boldsymbol{X}^{k}-\gamma \nabla g\left(\boldsymbol{X}^{k}\right) &amp;= \boldsymbol{X}^{k} - 2\gamma(1-\lambda) (\mathbf{I}-\tilde{\mathbf{A}}) \boldsymbol{X}^{k} \\ &amp;= (1-2 \gamma(1-\lambda)) \mathbf{X}^{k}+2 \gamma(1-\lambda) \tilde{\mathbf{A}} \mathbf{X}^{k}<br>
\end{aligned}\tag{10}
$$
那么：
$$
\begin{equation}
\begin{aligned}
\boldsymbol{X}^{k+1}&amp;=\operatorname{prox}_{\color{signal}h\color{energy}\gamma}\left(\boldsymbol{Y}^{k}\right) \\
&amp;= \underset{\mathbf{X}}{\arg \min }\left\{\lambda\left|\left|\mathbf{X}-\mathbf{X}_{\text {in }}\right|\right|_{21}+\frac{1}{2 \gamma}\left|\left|\mathbf{X}-\mathbf{Y}^{k}\right|\right|_{F}^{2}\right\}
\end{aligned} \tag{11}
\end{equation}
$$
这样，<strong>迭代公式（11）只和不可微函数$||\cdot||_{21}$有关</strong>， 上式的第二项$\left|\left|\mathbf{X}-\mathbf{Y}^{k}\right|\right|_{F}^{2}$是proximity operator的固定计算，优化目标中的$g(\cdot)$无关。</p>
<p>接下来，对公式（11）做一个换元， 令$\mathbf{Z}=\mathbf{X}-\mathbf{X}_{\mathrm{in}}$， 那么公式（11）可以重写为：
$$
\begin{aligned}
\mathbf{Z}^{k+1} &amp;=\underset{\mathbf{Z}}{\arg \min }\left\{\lambda||\mathbf{Z}||_{21}+\frac{1}{2 \gamma}\left|\left|\mathbf{Z}-\left(\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}\right)\right|\right|_{F}^{2}\right\} \\
&amp;=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}\right) \\
\mathbf{X}^{k+1} &amp;=\mathbf{X}_{\text {in }}+\mathbf{Z}^{k+1}
\end{aligned}
$$
下面就是求解$\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}\right)$。</p>
<p>先看看$\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{X}\right)$怎么算的, 参考[1,2]:
$$
\left[\operatorname{prox}_{\color{signal}||\cdot||_{2}}\left(\mathbf{X}\right)\right]_i =
\begin{cases}\mathbf{X}_i-\frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}} &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2}&gt;1 \\ 0 &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} \leq 1\end{cases}
$$</p>
<p>$$
\begin{equation}
\begin{aligned}
\left[\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{X}\right)\right]_i &amp; = \operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left(\operatorname{prox}_{\color{signal}\lambda||\cdot||_{2}\color{energy}\gamma }\left(\mathbf{X}\right)_i\right) \\
&amp;= \begin{cases}
\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left(\mathbf{X}_i-\frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}}\right) &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2}&gt;1 \\
\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left(0\right)=\color{red}0 &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} \leq 1\end{cases}
\end{aligned}
\end{equation}
$$
其中：
$$
\begin{equation}
\begin{aligned}
\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left(\mathbf{X}_i-\frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}}\right) &amp;= \frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}} \operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left( \left|\left|\mathbf{X}_i\right|\right|_{2}-\underbrace{1}_{constant, 不影响结果}\right) \\
&amp;= \frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}} \operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left( \left|\left|\mathbf{X}_i\right|\right|_{2}\right) \\
&amp;= \frac{\mathbf{X}_i}{\left|\left|\mathbf{X}_i\right|\right|_{2}} \cdot \underbrace{\boxed{\begin{cases}
\left|\left|\mathbf{X}_i\right|\right|_{2} - \lambda\gamma &amp;
\text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} &gt; \lambda\gamma \\
0 &amp; \text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} = \lambda\gamma \\
\left|\left|\mathbf{X}_i\right|\right|_{2} + \lambda\gamma &amp;
\text { if }\left|\left|\mathbf{X}_i\right|\right|_{2} \leq -\lambda\gamma
\end{cases}}}_{\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left( \left|\left|\mathbf{X}_i\right|\right|_{2}\right)}
\end{aligned} \tag{12}
\end{equation}
$$
公式（12）中，第三种情况不存在，所以
$$
\begin{equation}
\begin{aligned}
\operatorname{prox}_{\color{signal}\lambda||\cdot||_{1}\color{energy}\gamma }\left( \left|\left|\mathbf{X}_i\right|\right|_{2}\right) = \max \left(\left|\left|\mathbf{X}_{i}\right|\right|_{2}-\gamma \lambda, 0\right)
\end{aligned}
\end{equation}
$$
所以：
$$
\left(\operatorname{prox}_{\gamma \lambda||\cdot||_{21}}(\mathbf{X})\right)_{i}=\frac{\mathbf{X}_{i}}{\left|\left|\mathbf{X}_{i}\right|\right|_{2}} \max \left(\left|\left|\mathbf{X}_{i}\right|\right|_{2}-\gamma \lambda, 0\right)=\max \left(1-\frac{\gamma \lambda}{\left|\left|\mathbf{X}_{i}\right|\right|_{2}}, 0\right) \cdot \mathbf{X}_{i} \tag{13}
$$
将公式（13）中的$\mathbf{X}$替换为$\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}$, 计算$\mathbf{Z}^{k+1}=\operatorname{prox}_{\color{signal}\lambda||\cdot||_{21}\color{energy}\gamma }\left(\mathbf{Y}^{k}-\mathbf{X}_{\text {in }}\right)$ , 然后计算$\mathbf{X}^{k+1} =\mathbf{X}_{\text {in }}+\mathbf{Z}^{k+1}$ 可得：
$$
\mathbf{X}_{i}^{k+1}=\left(\mathbf{X}_{\text {in }}\right)_{i}+\beta_{i}\left(\mathbf{Y}_{i}^{k}-\left(\mathbf{X}_{\text{in }}\right)_{i}\right)=\left(1-\beta_{i}\right)\left(\mathbf{X}_{\text {in }}\right)_{i}+\beta_{i} \mathbf{Y}_{i}^{k}, \quad \forall i \in [n]
$$
其中 $\beta_{i}:=\max \left(1-\frac{\gamma \lambda}{\left|\left|\mathbf{Y}_{i}^{k}-\left(\mathbf{X}_{\text{in }}\right)_{i}\right|\right|_{2}}, 0\right)$。</p>
<p>综上，最终Adaptive Message Passing（AMP）总结如下：
$$
\left\{\begin{aligned}
\mathbf{Y}^{k} &amp;=(1-2 \gamma(1-\lambda)) \mathbf{X}^{k}+2 \gamma(1-\lambda) \tilde{\mathbf{A}} \mathbf{X}^{k} \\
\beta_{i} &amp;=\max \left(1-\frac{\gamma \lambda}{\left|\left|\mathbf{Y}_{i}^{k}-\left(\mathbf{X}_{\text {in }}\right)_{i}\right|\right|_{2}}, 0\right) \quad \forall i \in[n] \\
\mathbf{X}_{i}^{k+1} &amp;=\left(1-\beta_{i}\right)\left(\mathbf{X}_{\text {in }}\right)_{i}+\beta_{i} \mathbf{Y}_{i}^{k} \quad \forall i \in[n]
\end{aligned}\right.
$$</p>
<p><img loading="lazy" src="/posts/2022-04-05-AirGNN/3.png#center" alt="你想输入的替代文字"  />
</p>
<p>文章证明了$\gamma=\frac{1}{4(1-\lambda)}$或$\gamma=\frac{1}{2(1-\lambda)}$可以保证收敛 （具体过程没看了）。这样AMP简化为$\mathbf{Y}^{k}=\frac{1}{2} \mathbf{X}^{k}+\frac{1}{2} \tilde{\mathbf{A}} \mathbf{X}^{k}$和$\mathbf{Y}^{k}=\tilde{\mathbf{A}} \mathbf{X}^{k}$。</p>
<h1 id="reference">Reference</h1>
<p>[1] <a href="https://math.stackexchange.com/questions/2665254/proximal-operator-of-summation-of-l-1-norm-and-l-2-1-norm">https://math.stackexchange.com/questions/2665254/proximal-operator-of-summation-of-l-1-norm-and-l-2-1-norm</a></p>
<p>[2] <a href="https://math.stackexchange.com/questions/2190885/proximal-operator-of-the-euclidean-norm-l-2-norm">https://math.stackexchange.com/questions/2190885/proximal-operator-of-the-euclidean-norm-l-2-norm</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2020 《When Does Self-Supervision Help Graph Convolutional Networks?》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2020-04-03-ssgcns/</link>
      <pubDate>Fri, 08 Apr 2022 14:04:49 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2020-04-03-ssgcns/</guid>
      <description>ICML2020 &amp;#34;When Does Self-Supervision Help Graph Convolutional Networks?&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2006.09136v4">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文是自监督方法在GCNs上首次系统的探索，设计了3种自监督任务来将分析自监督在GCN中起到的作用。自监督旨在充分利用unlabeled数据中的知识来设计前置任务（pretext task），来帮助模型学习更具迁移性和泛化能力的表示。前置任务可以认为是对目标任务有帮助的辅助正则化网络，设计用于帮助原任务学习到更多下游任务相关的语义信息。</p>
<p>GCN任务通常是直推半监督的（transductive semi-supervised）,含有大量unlabeled数据，而self-supervision(SSL)可以充分利用unlabeled data， 那么就产生了一个值得探索的问题：<strong>将自监督学习应用到GCN上是否也可以达到提升泛化能力和鲁棒能力的效果？</strong></p>
<p>先给结论</p>
<p>Q1: 自监督学习可否在分类任务中提升GCN？ 如果可以，如何将其合并到 GCN 中以最大化增益？</p>
<p>A1: 本文证明了通过多任务学习将自监督学习融入 GCN 是有效的，即多任务损失作为 GCN 训练中的正则化项。 这种作为自监督作为正则化项的方法，强于用自监督来预训练或者self-training。</p>
<p>Q2: 前置任务的设计重要吗？ GCN 有哪些有用的自监督前置任务？</p>
<p>A2: 本文研究了三个基于图属性的自监督任务。 分别是节点聚类node clustering, 图划分graph partitioning 和图补全graph completion。 并且进一步说明不同的模型和数据集倾向于不同的自监督任务。</p>
<p>Q3: 自监督也会影响 GCN 的对抗鲁棒性吗？ 如果是，如何设计前置任务？</p>
<p>A3: 本文进一步将上述发现推广到对抗性训练环境中。提供了广泛的结果，以表明自监督还可以提高 GCN 在各种攻击下的鲁棒性，而不需要更大的模型或额外的数据。</p>
<h1 id="method">Method</h1>
<p>GCNs $\boldsymbol{Z}=\hat{\boldsymbol{A}} \operatorname{ReLU}\left(\hat{\boldsymbol{A}} \boldsymbol{X} \boldsymbol{W}_{0}\right) \boldsymbol{W}_{1}$可以分为两块来看 (1) 特征提取模块$f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) = \hat{\boldsymbol{A}} \operatorname{ReLU}\left(\hat{\boldsymbol{A}} \boldsymbol{X} \boldsymbol{W}_{0}\right)$ 参数为$\theta = \{\boldsymbol{W}_{0}\}$和（2）线性变换模块$\boldsymbol{Z}=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta}$ 其中 参数$ \boldsymbol{\Theta} = \boldsymbol{W}_{1}$。 半监督GCN优化任务的目标函数为：
$$
\begin{aligned}
\boldsymbol{Z} &amp;=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta} \\
\theta^{*}, \boldsymbol{\Theta}^{} &amp;=\arg \min_{\theta, \boldsymbol{\Theta}} \mathcal{L}_{\mathrm{sup}}(\theta, \boldsymbol{\Theta}) \\
&amp;=\arg \min_{\theta, \boldsymbol{\Theta}} \frac{1}{\left|\mathcal{V}_{\text {label }}\right|} \sum_{v_{n} \in \mathcal{V}_{\text {label }}} L\left(\boldsymbol{z}_{n}, \boldsymbol{y}_{n}\right)
\end{aligned} \tag{1}
$$
其中$L(\cdot, \cdot)$是每个labeled node的损失函数。</p>
<h2 id="three-schemes-self-supervision-meets-gcns">Three Schemes: Self-Supervision Meets GCNs</h2>
<p>研究三种将SSL配置到GCNs的方式。 其中 给定输入$\boldsymbol{X}_{ss}$, $\hat{\boldsymbol{A}}_{\mathrm{ss}}$, label $\boldsymbol{Y}_{ss}$和节点集$\mathcal{V}_{ss}$。</p>
<h3 id="pretraining--fintuning">Pretraining &amp; Fintuning</h3>
<p>预训练过程：
$$
\begin{aligned}
\boldsymbol{Z}_{\mathrm{ss}} &amp;=f_{\theta}\left(\boldsymbol{X}_{\mathrm{ss}}, \hat{\boldsymbol{A}}_{\mathrm{ss}}\right) \boldsymbol{\Theta}_{\mathrm{Ss}} \\
\theta_{\mathrm{ss}}^{*}, \boldsymbol{\Theta}_{\mathrm{ss}}^{*} &amp;=\arg \min_{\theta, \boldsymbol{\Theta}_{\mathrm{ss}}} \mathcal{L}_{\mathrm{ss}}\left(\theta, \boldsymbol{\Theta}_{\mathrm{ss}}\right) \\
&amp;=\arg \min_{\theta, \boldsymbol{\Theta}} \frac{1}{\left|\mathcal{V}_{\mathrm{ss}}\right|} \sum_{v_{n} \in \mathcal{V}_{\mathrm{ss}}} \underbrace{L_{\mathrm{ss}}\left(\boldsymbol{z}_{\mathrm{ss}, n}, \boldsymbol{y}_{\mathrm{ss}, n}\right)}_{\text{loss of other task}}<br>
\end{aligned} \tag{2}
$$
也就是在另一个任务训练好的模型参数$\theta_{\mathrm{ss}}^{*}, \boldsymbol{\Theta}_{\mathrm{ss}}^{*}$迁移到新任务（如半监督节点分类任务）上作为初始化参数训练新模型。</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/1.png#center" alt=""  />
</p>
<p>上表中，可以看出用graph partitioning作为预训练任务，得到的模型fine-tuning到节点分类任务上之后，效果仅从79.10变成了79.19,是非常微小的。 本文推测可能原因有两个（1）.两个不同的任务的Loss function不一样，从$\mathcal{L}_{\mathrm{ss}}$变为$\mathcal{L}_{\mathrm{sup}}$会影响实验效果。（2）参数迁移前一句是在多层GCN上的训练结果了，迁移后再训练，相当于深层，易oversmoothing。</p>
<h3 id="self-training">Self-Training</h3>
<p>每次迭代为unlabeled samples分配高度可信的为标签，然后将这些分配了伪标签的节点纳入到下一次迭代的监督训练中，随迭代过程不断更新标签。</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/2.png#center" alt=""  />
</p>
<p>表2可以看出Self-training的方式带来的提升有限</p>
<h3 id="multi-task-learning">Multi-task Learning</h3>
<p>考虑一个目标task和一个自监督task. GCN的目标为公式（1）。该多任务的训练过程如下：
$$
\begin{aligned}
\boldsymbol{Z} &amp;=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta}, \quad \boldsymbol{Z}_{\mathrm{ss}}=f_{\theta}\left(\boldsymbol{X}_{\mathrm{ss}}, \hat{\boldsymbol{A}}_{\mathrm{ss}}\right) \boldsymbol{\Theta}_{\mathrm{ss}} \\
\theta^{*}, \boldsymbol{\Theta}^{*}, \boldsymbol{\Theta}_{\mathrm{ss}}^{*} &amp;=\arg \min_{\theta, \boldsymbol{\Theta}, \boldsymbol{\Theta}_{\mathrm{ss}}} \alpha_{1} \mathcal{L}_{\mathrm{sup}}(\theta, \boldsymbol{\Theta})+\alpha_{2} \mathcal{L}_{\mathrm{ss}}\left(\theta, \boldsymbol{\Theta}_{\mathrm{ss}}\right)
\end{aligned} \tag{3}
$$
其中任务的权重参数$\alpha_{1}, \alpha_{2} \in \mathbb{R}_{&gt;0}$, 半监督目标任务的损失$\mathcal{L}_{\mathrm{sup}}$定义为公式（1）， 辅助自监督损失$\mathcal{L}_{\mathrm{ss}}$定义为公式（2）.其中特征提取器$f_{\theta}(\cdot, \cdot)$对于自监督任务和目标任务是参数共享的，而线性变换参数$\boldsymbol{\Theta}, \boldsymbol{\Theta}_{\mathrm{ss}}$是各自任务的。</p>
<p>在公式(3)中，自监督任务的loss作为一个<strong>regularization term</strong> 与目标任务一同训练。正则化项在图信号处理中是广泛应用的， 常见的有Graph Laplaician Regularization（GLR）， 它用于惩罚相邻节点间的不平滑，用于在学习目标任务的同时保持特征在图结构上的smoothing。虽然GLR可以作为一个自监督任务，但是它是给予不涉及具体数据情况下的平滑先验，SSL的regularization term不用的是，SSL是给予unlabeled data,是一种引入数据驱动的先验知识。综上所述， 多任务学习是3种自监督方式中最通用的。</p>
<h2 id="gcn-specific-self-supervised-tasks">GCN-Specific Self-Supervised Tasks</h2>
<p>本文为 GCN 扩展了一个自监督任务的“工具包”。 通过利用图中的丰富节点和边信息，可以定义各种GCN特定的自监督任务（如表 3 所示），并且进一步证明了不同的自监督任务对不同类型的监督/下游任务有益。这些自监督任务会为节点分配伪标签来构造自监督损失$\mathcal{L}_{ss}$, 如公式（3）所示。</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/4.png#center" alt=""  />
</p>
<h3 id="node-clustering">Node clustering</h3>
<p>第一个任务为节点聚类， 给定节点集$\mathcal{V}$以及feature set $\boldsymbol{X}$, 一个预设值的簇数量$K \in\{1, \ldots,|\mathcal{V}|\}$（是一个超参数）一定要小于等于节点数$|\mathcal{V}|$。 聚类算法输出一个节点集合的集合$\left\{\mathcal{V}_{\text {clu }, 1}, \ldots, \mathcal{V}_{\text {clu }, K} \mid \mathcal{V}_{\text {clu }, n} \subseteq \mathcal{V}, n=1, \ldots, K\right\}$， 其中$\mathcal{V}_{\text {clu }, i}$是一个集合表示在簇$i$中的节点集。
$$
\begin{aligned}
&amp;\mathcal{V}_{\text {clu }, n} \neq \emptyset \quad(n=1, \ldots, K), \quad \cup_{n=1}^{K} \mathcal{V}_{\text {clu }, n}=\mathcal{V} \\
&amp;\mathcal{V}_{\text {clu }, i} \cap \mathcal{V}_{\text {clu }, j}=\emptyset \quad(\forall i, j=1, \ldots, K \text { and } i \neq j)
\end{aligned}
$$
这$K$个簇互相之间没有公共的节点，SSL任务将每个节点所在的簇的index作为伪标签来构造自监督损失$\mathcal{L}_{ss}$：
$$
y_{\mathrm{ss}, n}=k \text { if } v_{n} \in \mathcal{V}_{\mathrm{clu}, k}(\forall n=1, \ldots,|\mathcal{V}|, \forall k=1, \ldots, K)
$$</p>
<h3 id="graph-partitioning">Graph partitioning</h3>
<p>上面的节点聚类任务，是基于特征的，与拓扑无关。 而这里的图划分任务，与feature无关，只与拓扑有关。 具体来说，通过“强”边连接的两个节点很可能属于同一标签类别。 因此，本文提出了一种使用图划分的基于拓扑的自监督任务。</p>
<p>图划分是将图的节点划分为大致相等的子集，使得跨子集间的边数最小化（高聚类，低耦合，同时簇中节点数不能差别太大）。先预定义一个簇数量，$K \in\{1, \ldots,|\mathcal{V}|\}$（超参数）。 和节点聚类任务类似，图划分算法也会输出一个节点集合的集合，用来标识每个节点属于哪个partition: $\left\{\mathcal{V}_{\text {par }, 1}, \ldots, \mathcal{V}_{\text {par }, K} \mid \mathcal{V}_{\text {par }, n} \subseteq \mathcal{V}, n=1, \ldots, K\right\}$, 使得：
$$
\begin{aligned}
&amp;\mathcal{V}_{\text {par }, n} \neq \emptyset \quad(\forall n=1, \ldots, K), \quad \cup_{n=1}^{K} \mathcal{V}_{\text {par }, n}=\mathcal{V} \\
&amp;\mathcal{V}_{\text {par }, i} \cap \mathcal{V}_{\text {par }, j}=\emptyset \quad(\forall i, j=1, \ldots, K \text { and } i \neq j)
\end{aligned}
$$
上面的约束其实和node clustering任务差不多，Graph partitioning任务还需要两个约束，一个是平衡约束来保证簇不要太大：
$$
K \frac{\max_{k}\left|\mathcal{V}_{\text {par }, k}\right|}{|\mathcal{V}|} \leqslant 1+\epsilon, \text { where } \epsilon \in(0,1)
$$
其中$\max_{k}\left|\mathcal{V}_{\text {par }, k}\right|$是节点数最多的簇中的节点数。 另一个约束要保证簇间边要尽可能少，即最小化edgecut:
$$
\text { edgecut }=\frac{1}{2} \sum_{k=1}^{K} \sum_{v_{i} \in \mathcal{V}_{\text {par }, k}} \quad\sum_{\left(v_{i}, v_{j}\right) \in \mathcal{E} ,\text {and } v_{j} \notin \mathcal{V}_{\text {par }, k}} \quad a_{i j}
$$
将每个节点所在的partition index作为label。</p>
<h3 id="graph-completion">Graph completion</h3>
<p>图补全任务如下图所示。</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/5.png#center" alt=""  />
</p>
<p>图补全首先通过删除目标节点的特征来mask目标节点。 然后，通过向 GCN 提供未掩蔽的节点特征（目前仅限于 2 层 GCN 的每个目标节点的二阶邻居）来恢复/预测被mask的节点特征。设计该自监督任务的原因如下：1）标签可以自由获取，也就是节点特征本身； 2）图补全可以帮助网络获得更好的特征表示，这可以教会网络从上下文中提取特征。</p>
<p>最终多任务自监督GCN模型的框架如下图所示：</p>
<p><img loading="lazy" src="/posts/2022-04-09-SSGCNs/3.png#center" alt=""  />
</p>
<h2 id="self-supervision-in-graph-adversarial-defense">Self-Supervision in Graph Adversarial Defense</h2>
<p>本文专注于Evasion Attack，在模型训练好后对目标节点$v_n$扰动， 实际上对于Evasion Attack，对扰动图重新训练或许可以纠正扰动的影响，但是本文这里不考虑重新训练。一个attacker $g$生成新的特征和邻接矩阵：
$$
\boldsymbol{X}^{\prime}, \boldsymbol{A}^{\prime}=g\left(\boldsymbol{X}, \boldsymbol{A}, \boldsymbol{Y}, v_{n}, \theta^{*}, \boldsymbol{\Theta}^{*}\right)
$$
其中$ \theta^{*}, \boldsymbol{\Theta}^{*}$是在clean 图上训练好的模型参数。</p>
<p>对抗训练的目标函数定义为:
$$
\begin{aligned}
\boldsymbol{Z} &amp;=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta}, \quad \boldsymbol{Z}^{\prime}=f_{\theta}\left(\boldsymbol{X}^{\prime}, \boldsymbol{A}^{\prime}\right) \boldsymbol{\Theta} \\
\theta^{*}, \boldsymbol{\Theta}^{*} &amp;=\arg \min_{\theta, \boldsymbol{\Theta}}\left(\mathcal{L}_{\text {sup }}(\theta, \boldsymbol{\Theta})+\alpha_{3} \mathcal{L}_{\mathrm{adv}}(\theta, \boldsymbol{\Theta})\right)
\end{aligned}
$$
表示模型要同时在扰动图和训练图上都保持较好的效果。 本文将基于自监督的对抗训练定义为：
$$
\begin{aligned}
\boldsymbol{Z} &amp;=f_{\theta}(\boldsymbol{X}, \hat{\boldsymbol{A}}) \boldsymbol{\Theta}, \quad \boldsymbol{Z}^{\prime}=f_{\theta}\left(\boldsymbol{X}^{\prime}, \boldsymbol{A}^{\prime}\right) \boldsymbol{\Theta} \\
\boldsymbol{Z}_{\mathrm{ss}}=&amp; f_{\theta}\left(\boldsymbol{X}_ \mathrm{ss}, \boldsymbol{A}_{\mathrm{ss}}\right) \\
\theta^{*}, \boldsymbol{\Theta}^{*}, \boldsymbol{\Theta}_{\mathrm{ss}}^{*}=&amp; \arg \min_{\theta, \boldsymbol{\Theta}, \boldsymbol{\Theta}_{\mathrm{ss}}}\left(\alpha_{1} \mathcal{L}_{\mathrm{sup}}(\theta, \boldsymbol{\Theta})\right.\\
&amp;\left.+\alpha_{2} \mathcal{L}_{\mathrm{ss}}\left(\theta, \boldsymbol{\Theta}_{\mathrm{ss}}\right)+\alpha_{3} \mathcal{L}_{\mathrm{adv}}(\theta, \boldsymbol{\Theta})\right)
\end{aligned}
$$
其中自监督损失被引入到以扰动图数据作为输入的训练中（自监督标签矩阵 $\boldsymbol{Y}_{ss}$ 也是从扰动输入生成的）。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2020 《Robust Graph Representation Learning via Neural Sparsification》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/neuralsparse/</link>
      <pubDate>Fri, 01 Apr 2022 10:55:44 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/neuralsparse/</guid>
      <description>ICML2020 &amp;#34;Robust Graph Representation Learning via Neural Sparsification&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://proceedings.mlr.press/v119/zheng20d.html">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>GNN的neighborhood aggregation中会引入邻域中任务无关的邻居，所以要移除图中有大量任务无关的边，顾本文提出NeuralSparse来解决该问题。</p>
<p>在构造数据集时，两个节点连接的动机可能与拿到这张图后要进行的下游任务无关，例如下游任务是节点分类，那么和这个任务相关的连接应是同类节点间产生边。但是构造图数据集是的两个节点连接的动机可能是特征相似的节点（不一定同类）。</p>
<p>下图给出一个例子，Blue 和 Red节点分别采样自两个独立的二维Gaussian distribution, 图1(a)显示两类节点的原始features有大量重合。对于每个节点，随机抽取10个其他节点作为他的邻居，这样生成的图（图1(b) )中的边和node label没有任何关系，即所有边都与节点分类任务无关。在这个图上做GCN得到(图1(b)下方)的node embedding，可以看到GCN学到的node embedding在任务无关的边上，无法区分两类节点。 图1(c)是DropEdge学到的node embedding，图1(d)为本文的NeuralSparse学到的node embedding。</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/1.png#center" alt=""  />
</p>
<p><strong>Present work</strong>：根据监督信号来选择任务相关的边。 由sparsification network 和GNN两个模块组成. sparsification network旨在参数化稀疏过程，即在给定预算下， 为每个节点选择任务相关的一节邻居。在训练阶段，优化稀疏策略，即训练一个可以将图稀疏化的网络。在测试阶段，将测试图输入网络，得到一个稀疏化的图。</p>
<h1 id="neuralsparse">NeuralSparse</h1>
<h2 id="theoretical-justification">Theoretical justification</h2>
<p>首先从统计学习角度建模问题。将预测任务定义为$P(Y|G)$, 其中$Y$是预测目标，$G$是输入图。本文利用稀疏化子图来移除任务无关的边, 问题形式化为:
$$
P(Y \mid G) \approx \sum_{g \in \mathbb{S}_{G}} P(Y \mid g) P(g \mid G)
$$
$g$是一个稀疏化子图，$\mathbb{S}_{G}$为$G$的稀疏化子图集合。 $P(Y \mid g)$ 为给定一个稀疏化子图$g$，用$g$过GNN后预测为$Y$的概率。
$$
\sum_{g \in \mathbb{S}_{G}} P(Y \mid g) P(g \mid G) \approx \sum_{g \in \mathbb{S}_{G}} Q_{\theta}(Y \mid g) Q_{\phi}(g \mid G)
$$
用函数来近似(计算) 分布， 即给定$g$ 预测为$Y$的概率$ P(Y \mid g)$定义为一个参数为$\theta$的函数$Q_{\theta}(Y \mid g)$, 从$G$中获得子图$g$的概率$P(g \mid G)$定义为一个参数为$\phi$的函数$Q_{\phi}(g \mid G)$。</p>
<p>$Q_{\phi}(g \mid G)$表示输入$G$, 生成一个子图分布，从分布中采样得到子图$g$的概率， 为了使得分布中采样这个过程可微，本文采用reparameterization tricks,使得：
$$
\sum_{g \in \mathbb{S}_{G}} Q_{\theta}(Y \mid g) Q_{\phi}(g \mid G) \propto \sum_{g^{\prime} \sim Q_{\phi}(g \mid G)} Q_{\theta}\left(Y \mid g^{\prime}\right)
$$
$g^{\prime} \sim Q_{\phi}(g \mid G)$表示给定图$G$，生成一个子图分布（每种子图的采样概率）。$\sum_{g^{\prime} \sim Q_{\phi}(g \mid G)} Q_{\theta}\left(Y \mid g^{\prime}\right)$: 表示从子图分布中采样的子图来预测label $Y$的概率。</p>
<p><strong>Goal</strong>: 1. 找到合适的$Q_{\phi}(g \mid G)$， 使得它生成的分布可以采样到最佳的稀疏化子图 ， 即通过优化$\phi$使得$Q_{\phi}(g \mid G)$生成的子图分布中采样到最佳子图的概率是最大的。 2. 找到合适的$Q_{\theta}(Y \mid g)$表示优化GNN，使得采样出的$g$可以最好的预测label。</p>
<h2 id="architecture">Architecture</h2>
<p>包含两个模块： sparsification network 和GNNs.</p>
<h3 id="sparsification-network">Sparsification Network</h3>
<p>目的为输入图生成稀疏化子图，即为每个节点的边生成一个分布，表示边被采样的概率，然后为节点采样边，从而实现采样的系数子图。首先定义所有候选子图。</p>
<p><strong>k-neighbor subgraphs</strong>: 给定输入图$G$，一个$k$-neighbor subgraph和图$G$有相同的节点集，每个节点可以从他的邻居中选择不多于$k$条边。</p>
<p>理由： 超参数$k$可以用来调整任务相关的图数据量。如果$k$是低估的，那么GNN处理的任务相关数据不足，如果$k$被高估，那么下游GNN会拟合更多无关数据。</p>
<p><strong>Sampling k-neighbor subgraphs</strong>：给定$k$和一个图$G=(V, E, \mathbf{A})$, 以节点$u$为例，令$\mathbb{N}_u$为$u$的一阶邻居。</p>
<ol>
<li>$v \sim f_{\phi}\left(V(u), V\left(\mathbb{N}_{u}\right), \mathbf{A}(u)\right)$, 其中，$f_{\phi}(\cdot)$是一个函数，输入为节点$u$的节点属性$V(u)$，节点$u$的邻居属性$V\left(\mathbb{N}_{u}\right)$, 和$u$的边属性$\mathbf{A}(u)$。输出为$u$的邻居分布，$v$从该邻居分布中采样。 比如当前$u$有3个节点，$f_\phi$生成这三个节点的采样分布[0.1, 0.3, 0.6], 那么从这个分布中随机采样一个节点$v$作为$u$的重构邻居。</li>
<li>采样出的节点$v$作为$u$的重构邻居，即$E(u,v)$作为边保留下来。</li>
<li>重复上述过程$k$次，得到$u$的$k$个重构邻居。</li>
</ol>
<p>注意，上述采样过程为不放回过程（sampling without replacement），即邻居只能被选择一次， $f_{\phi}(\cdot)$对所有节点共享，即一个$f_{\phi}(\cdot)$，每个节点都输入它来获得邻居采样分布。</p>
<p><strong>Making samples differentiable</strong> 为了使样本的采样过程可微，本文采用基于Gumbel-Softmax的NN来实现采样函数$f_{\phi}(\cdot)$。</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/2.png#center" alt=""  />
</p>
<p>Gumbel-Softmax [1,2] 是一种reparameterization trick，用于以可微的方式生成离散样本。参数$\tau$越小，生成的连续向量越sharp，越接近one-hot。</p>
<p>以节点$u$为例，$f_\phi(\cdot)$如下：</p>
<ol>
<li>
<p>$\forall v \in \mathbb{N}_u$：
$$
z_{u, v}=\operatorname{MLP}_{\phi}(V(u), V(v), \mathbf{A}(u, v))
$$</p>
</li>
<li>
<p>$\forall v \in \mathbb{N}_u$，使用softmax来计算边被采样的概率：
$$
\pi_{u, v}=\frac{\exp \left(z_{u, v}\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(z_{u, w}\right)}
$$</p>
</li>
<li>
<p>使用Gumbel-Softmax来生成可微样本：
$$
x_{u, v}=\frac{\exp \left(\left(\log \left(\pi_{u, v}\right)+\epsilon_{v}\right) / \tau\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(\left(\log \left(\pi_{u, w}\right)+\epsilon_{w}\right) / \tau\right)}
$$</p>
</li>
</ol>
<p>其中， $x_{u, v}$是一个scalar，$\epsilon_{v}=-\log (-\log (s))$，$s$从$\mathrm{Uniform}(0,1)$中采样， $\tau$是一个temperature超参数，$\tau$越小，分布$x_u$越接近one-hot。</p>
<h3 id="algorithm">Algorithm</h3>
<p>算法如下：</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/3.png#center" alt=""  />
</p>
<p>对所有节点$u \in \mathbb{V}$逐个稀疏化： 先遍历$u$的每个邻居$v$, 对于每个$v$ 通过公式$z_{u, v}=\operatorname{MLP}_{\phi}(V(u), V(v), \mathbf{A}(u, v))$ 计算它对于$u$的分数， 然后将$u$的所有邻居$v$的分数用softmax变成概率。</p>
<p>为$u$做$k$次采样， 每次采样过程如下： 每次采样遍历$u$的所有邻居$v$，根据$x_{u, v}=\frac{\exp \left(\left(\log \left(\pi_{u, v}\right)+\epsilon_{v}\right) / \tau\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(\left(\log \left(\pi_{u, w}\right)+\epsilon_{w}\right) / \tau\right)}$计算$u$到每个邻居的$x_{u,v}$, 每次迭代产生一个向量$\left[x_{u, v}\right]$,用来表示采样出来的边，经过$k$次迭代，产生$k$个表示边的向量，$\tau$越小，每个向量越接近one-hot。 每个向量$\left[x_{u, v}\right]_{v \in \mathbb{N}_u}$表示$u$的一个采样邻居，每个$u$有$k$个这样的邻居表示向量，那么网络中的所有边$\mathbb{H}$就有$|\mathbb{V}|k$个这样的向量，每个向量表示要保留的一条边，得到稀疏化子图，反向传播时，先更新GNN参数，然后直接对$f_\phi$的参数求梯度, 如上图所示。</p>
<h1 id="reference">Reference</h1>
<p>[1] Jang, E., Gu, S., and Poole, B. Categorical reparameteriza- tion with gumbel-softmax. In ICLR, 2017.</p>
<p>[2] Maddison, C. J., Mnih, A., and Teh, Y. W. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Representing Long-Range Context for Graph Neural Networks with Global Attention》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/graphtrans/</link>
      <pubDate>Wed, 30 Mar 2022 10:37:41 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/graphtrans/</guid>
      <description>NeurIPS2021 &amp;#34;Representing Long-Range Context for Graph Neural Networks with Global Attention&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/pdf/2201.08821.pdf">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>加深GNN层数来增加感受野会导致优化不稳定性，比如梯度消失和oversmoothing。 因此本文采用Transformer-based self-attention来学习成对节点间的长距离关系，并且提出一种新型的readout池化机制来学习global graph embedding。即在一个GNN模块后接一个置换不变（permutation-invariant）Transformer, GNN模块捕获local信息，Transformer捕获global信息。</p>
<p>GNN作为一种专门的架构医学系节点<strong>直接邻域结构的局部表示</strong>， 而Transformer作为全局推理模块以位置无关的方式计算所有成对节点的交互。作者认为，一个没有positional encoding的Transformer是置换不变的，因此很适合图。</p>
<h1 id="motivation">Motivation</h1>
<p>强关系Inductive bias(我的理解是Homophily假设) 鼓励学习局部短距离的关联性。 而对于长距离相关性，结构化较低的模块（不需要过于考虑图的结构信息）更受欢迎。</p>
<p><strong>GraphTrans leaves learning long-range dependencies to Transformer</strong>, 通过Transformer来学习图中所有节点对的依赖关系而不是只关注局部邻居。</p>
<p>下图中展示了一个子图的attention map。一共有17个节点，横坐标表示目标节点，纵坐标表示源节点，第$i$行第$j$列表示节点$i$在Transformer中聚合$j$的attention权重。第18行为一个特殊的$&lt;CLS&gt;$token 作为图的readout embedding。结合本文的SOTA效果，表面在学习长距离依赖时不考虑图结构先验（spatial priors）对Graph summarization（graph-level representation）是有必要的</p>
<p><img loading="lazy" src="/posts/2022-03-30-GraphTrans/pic2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="model">Model</h1>
<p><img loading="lazy" src="/posts/2022-03-30-GraphTrans/pic1.png#center" alt="你想输入的替代文字"  />
</p>
<h2 id="gnn-module">GNN Module</h2>
<p>一个通用的GNN模块：
$$
\boldsymbol{h}_{v}^{\ell}=f_{\ell}\left(\boldsymbol{h}_{v}^{\ell-1},\left\{\boldsymbol{h}_{u}^{\ell-1} \mid u \in \mathcal{N}(v)\right\}\right), \quad \ell=1, \ldots, L_{\mathrm{GNN}}
$$</p>
<h2 id="transformer-module">Transformer Module</h2>
<p>通过上面的GNN模块，我们可以得到每个节点的embedding $h_{v}^{L_{\mathrm{GNN}}}$, 将所有节点作为Transformer的Input。传统Transformer中输入先计算Self-attention（当前输入的$Q$向量和所有节点的$K$向量做内积得到输入节点和其他节点的att值，再用这个att值来为当前输入节点加权聚合所有节点的$V$向量），聚合后再和自身相加做residual,然后在做Layer Norm, 即对节点$i$的表示做Layer Norm 为$x_i^\prime = \frac{x_i-m}{\sigma}$, 其中$m$为$x_i$的均值， $\sigma$为$x_i$的标准差。</p>
<p>这里的Transformer不同的是， 先对所有节点做一次MLP，然后直接计算Layer Norm:
$$
\overline{\boldsymbol{h}}_{v}^{0}=\operatorname{LayerNorm}\left(\boldsymbol{W}^{\text {Proj }} \boldsymbol{h}_{v}^{L_{\mathrm{GNN}}}\right)
$$
其中$\boldsymbol{W}^{\text {Proj }} \in \mathbb{R}^{d_{\mathrm{TF}} \times d_{L_{\mathrm{GNN}}}}$， 把GNN的输出维度转为TF的输入维度$d_{\mathrm{TF}}$。将所有节点的GNN node embeddings作为Transformer的输入（无positional encoding）。 每个节点的$Q$, $K$和$V$向量分别用$\boldsymbol{W}_{\ell}^{Q}, \boldsymbol{W}_{\ell}^{K}, \boldsymbol{W}_{\ell}^{V} \in \mathbb{R}^{d_{\mathrm{TF}} / n_{\text {head }} \times d_{\mathrm{TF}} / n_{\text {head }}}$计算， 对于第$\ell$层 Transformer,  节点$v$ 的$Q$向量$Q_v = \boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}$和节点$u$的$K$向量$K_u = \boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}$做内积，得到两个节点之间的attention。然后用$\alpha_{v, u}^{\ell}$来为节点$v$聚合其他所有节点的$V$向量$V_u = \boldsymbol{W}_{\ell}^{V} \overline{\boldsymbol{h}}_{u}^{\ell-1}$, 如下所示:
$$
a_{v, u}^{\ell}=\left(\boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}\right)^{\top}\left(\boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}\right) / \sqrt{d_{\mathrm{TF}}} \tag{1}
$$</p>
<p>$$
\alpha_{v, u}^{\ell}=\operatorname{softmax}_{u \in \mathcal{V}}\left(a_{v, u}^{\ell}\right) \tag{2}
$$</p>
<p>$$
\overline{\boldsymbol{h}}_{v}^{\prime \ell}=\sum_{w \in \mathcal{V}} \alpha_{v, u}^{\ell} \boldsymbol{W}_{\ell}^{V} \overline{\boldsymbol{h}}_{u}^{\ell-1} \tag{3}
$$</p>
<h2 id="cls--embedding-as-a-gnn-readout-method">&lt;CLS&gt;  embedding as a GNN “readout” method</h2>
<p>Graph Pooling 部分旨在基于node embedding，得到整个图的一个global embedding. 大多数pooling方法为简单的mean,sum, 或者构造一个virtual node连接到所有节点并参与训练，这个virtual node聚合所有节点的信息作为global embedding。</p>
<p>本文提出special-token readout module。具体来说，对Transformer的输入$[\overline{\boldsymbol{h}}_{v}^{0}]_{v\in V}$, where $\overline{\boldsymbol{h}}_{v}^{0} \in \mathcal{R}^{d_{TF}}$我们添加一个额外的可学习embedding （可以被认为是一个额外virtual node）$\bar{h}_{\langle\mathrm{CLS}\rangle} \in \mathbb{R}^{d_{\mathrm{TF}}}$, 这样 Transformer 的输入就变为$[\overline{\boldsymbol{h}}_{v}^{0}]_{v \in V} \cup \bar{h}_{\langle\mathrm{CLS}\rangle}$, 因为训练过程中$\overline{\boldsymbol{h}}_{v}^{0}$回聚合来自所有节点的信息，所以用它来作为readout embedding。 最终Transformer输出的token embedding $\overline{\boldsymbol{h}}_{&lt;\mathrm{CLS}&gt;}^{L_{\mathrm{TF}}}$ 再过一层MLP后用Softmax输出图的prediction:
$$
y=\operatorname{softmax}\left(\boldsymbol{W}^{\mathrm{out}} \overline{\boldsymbol{h}}_{&lt;\mathrm{CLS}&gt;}^{L_{\mathrm{TF}}}\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Not All Low-Pass Filters are Robust in Graph Convolutional Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gcn-lfr/</link>
      <pubDate>Tue, 29 Mar 2022 21:20:31 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gcn-lfr/</guid>
      <description>NeurIPS2021 &amp;#34;Not All Low-Pass Filters are Robust in Graph Convolutional Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/forum?id=bDdfxLQITtu">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>很多GNN易受图结构攻击的影响，本文首先证明了symmetric normalized laplacian的低频分量作为GCN的filter，在某个特征值区间内对于结构扰动更加robust。基于该理论提出GCN-LFR，通过一个辅助神经网络迁移低频分量的robustness。</p>
<p>Q: 对抗扰动边是否会对graph spectrum产生同等的影响？过去的研究显示来自结构攻击的扰动在图谱上表达了一种隐含的趋势。如下图所示，结构扰动后，小的特征值（低频）变化较小， 高频变化较大，即高频对扰动更加敏感。</p>
<p><img loading="lazy" src="/posts/2022-04-08-GraphLFR/1.png#center" alt=""  />
</p>
<p>本文证明了当normalized symmetric laplacian的特征值落于某个特定区间时，低频分量会更加robust。</p>
<h1 id="methodology">Methodology</h1>
<p>Poisoning Attack是指训练前扰动：</p>
<p><strong>Problem 1 （Poisoning Attack）:</strong> 给定一个扰动图 $\mathcal{G}^\prime$, 要对目标集合$\mathcal{T}$做对抗防御的目的是设计一个更加鲁棒的模型，使得模型在扰动图上训练后对$\mathcal{T}$中节点的预测结果和在原图上训练得到的预测结果相似：
$$
\min_{\boldsymbol{\Theta}^{r *}} \sum_{u \in \mathcal{T}}\left|\left|\mathcal{M}_{u}^{r}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}^{r *}\right)-\mathcal{M}_{u}\left(\boldsymbol{A}, \boldsymbol{X} ; \boldsymbol{\Theta}^{*}\right)\right|\right|
$$
即模型可以尽可能避免扰动对预测带来的影响。 其中$\mathcal{M}_{u}^{r}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}^{r *}\right)=\hat{\boldsymbol{y}}_{u}^{r}$模型在扰动图上训练过后对节点$u$的预测，$\mathcal{M}_{u}^{r}$是在扰动图上训练过后的模型，最佳参数为$\boldsymbol{\Theta}^{r *}$, $\mathcal{M}$是在原图上训练过后的模型。</p>
<p>观察$\hat{\boldsymbol{A}}$的特征值， 因为$\hat{\boldsymbol{A}} = I - L$, 所以$\hat{\boldsymbol{A}}$的大特征值对应于低频分量，小特征值对应于高频分量。 从图1可以看出$\hat{\boldsymbol{A}}$的大特征值对于结构扰动更加鲁棒, 因为扰动之后大特征值的变化较小。所以GCN-SVD只是用最低频的分量来做defense.</p>
<p>接下来本文证明了只有一条边被扰动的情况下，一定存在低频$\lambda_b$，比所有高频都robust, 鲁棒区间为：
$$
\max \left(0, \frac{d_{b}-d_{a}+c_{a} \lambda_{a}}{c_{b}}\right)&lt;\lambda_{b}&lt;\min \left(\frac{d_{b}+d_{a}-c_{a} \lambda_{a}}{c_{b}}, 1\right)
$$
即，当特征值落于这个区间中时，它一定比高频更加鲁棒。</p>
<p>对于Non-targeted Perturbation,  图中有多条边被扰动，那么特征值的鲁棒区间为：
$$
\max_{v \in \mathcal{P}_{u}, u \in \mathcal{T}}\left(0, \frac{d_{b u v}-d_{a u v}+c_{a u v} \lambda_{a}}{c_{b u v}}\right)&lt;\lambda_{b}&lt;\min_{v \in \mathcal{P}_{u}, u \in \mathcal{T}}\left(\frac{d_{b u v}+d_{a u v}-c_{a u v} \lambda_{a}}{c_{b u v}}, 1\right)
$$
在得到不同扰动情况下的鲁棒区间后，我们可以基于鲁棒区间来增强GCN的鲁棒性。</p>
<h2 id="gcn-lfr">GCN-LFR</h2>
<p>基于鲁棒区间，利用区间内频率分量可以设计更加鲁棒的GCN。</p>
<p><img loading="lazy" src="/posts/2022-04-08-GraphLFR/2.png#center" alt=""  />
</p>
<p>给定一个结构扰动图$\mathcal{G}^\prime$,邻接矩阵为$A^\prime$, GCN-LFR使用一个辅助正则化网络$\mathcal{M}_{\mathrm{LFR}}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}\right)$来计算robust 区间， 但是鲁棒区间时基于原图的，在只给定扰动图的情况下无法计算。 为了解决该问题， 本文用可学习的参数$\mathbf{F}$作为filters来学习鲁棒区间。其中，$\mathbf{F} = \left(\begin{array}{cccc}
f_{1} &amp; &amp; &amp; \\
&amp; f_{2} &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; f_{k}
\end{array}\right)$。</p>
<p>解释：假设$U$是$\hat{\boldsymbol{A}}$的特征向量，则 $\hat{\boldsymbol{A}}$可以分解为:</p>
<p>$$\hat{\boldsymbol{A}} = [u_1, \cdots, u_n] F \left[\begin{array}{l}
u_1^\top \\
\cdots \\
u_n^\top
\end{array}\right] = f_1 u_1 u_1^\top + \cdots + f_n u_n u_n^\top$$</p>
<p>其中$u_1$对应拉普拉斯矩阵的最小特征值的特征向量（$\hat{\boldsymbol{A}}$最大特征值的特征向量），所以$[u_1, \cdots, u_k]$表示最低频的$k$个特征向量。$f_i$是第$i$个频率滤波器的权重。 我们选择$k$个低频滤波器，并自适应的学习他们的权重，即 $U^\prime_{low} = [u_1, \cdots, u_k]$, $\mathbf{F}$是$k$个低频filter的参数， 所以图卷积层$\mathcal{M}_{\text {LFR }}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}, \boldsymbol{F}\right)$可以写作:
$$
\boldsymbol{H}^{(l+1)}=\sigma\left(\boldsymbol{U}_{\text {low }}^{\prime} \boldsymbol{F} \boldsymbol{U}_{\text {low }}^{\prime \top} \boldsymbol{H}^{\prime(l)}\Theta\right)
$$
另外，本文提出交替训练策略，对于在原本扰动图上的模型$\mathcal{M}_{\mathrm{GCN}}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}\right)$以及低频自适应学习的模型$\mathcal{M}_{\mathrm{LFR}}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}, \boldsymbol{F}\right)$, 这两个模型交替训练，损失函数分别用$\alpha$和$1-\alpha$加权：
$$
\mathcal{L}_{\text {total }}=(1-\alpha) \mathcal{L}_{\mathrm{GCN}}+\alpha \mathcal{L}_{\mathrm{LFR}}
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2020 《Inductive and Unsupervised Representation Learning on Graph Structured Objects》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/seed/</link>
      <pubDate>Mon, 28 Mar 2022 23:44:01 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/seed/</guid>
      <description>ICLR2020 &amp;#34;Inductive and Unsupervised Representation Learning on Graph Structured Objects&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=rkem91rtDB">Paper</a></p>
<p><a href="https://github.com/wenwen0319/SEED-Reimplementation">Code</a></p>
<h1 id="introduction">Introduction</h1>
<p>无监督图学习算法基于重构损失，不可避免的需要图相似度计算（重构embedding和输入embedding的loss), 计算复杂度较高。本文提出一种通用的归纳式无监督图学习算法<strong>SEED</strong>（Sampling, Encoding, and Embedding Distributions）。通过计算采样子图的重构损失来代替整个图的重构损失。 即 先采样子图，在用GNN编码子图，最后计算子图分布的embedding来作为整个图的representation. 过程如下图所示：</p>
<p><img loading="lazy" src="/posts/2022-03-29-seed/pic1.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="seed-sampling-encoding-and-embedding-distributions">SEED: Sampling, Encoding, and Embedding Distributions</h1>
<h2 id="anonymous-random-walk">Anonymous Random Walk</h2>
<p><strong>Definition 1 (Random Anonymous Walks[1]):</strong>  Given a random walk $\mathbf{w}=(w_1, \cdots, w_l)$ where $\langle w_i, w_{i+1} \rangle \in E$, the anonymous walk for $\mathbf{w}$ is defined as：
$$
\mathrm{aw}(\mathbf{w}) = (\mathrm{DIS}(\mathbf{w}, w_1),\mathrm{DIS}(\mathbf{w}, w_2),\cdots, \mathrm{DIS}(\mathbf{w}, w_l) )
$$
where $\mathrm{DIS}(\mathbf{w}, w_i)$ denotes the number of distinct nodes in $\mathbf{w}$ when $w_i$ first appears in $\mathbf{w}$, i.e.
$$
\mathrm{DIS}(\mathbf{w}, w_i) = |{w_1, \cdots w_p}|, \quad p = \min_j {w_j=w_i}
$$
匿名随机游走和随机游走的不同在于，匿名随机游走描述了随机游走的潜在“patterns”, 不管具体被访问的节点是什么。 距离来说，给定两条随机游走序列 $\mathbf{w_1}=(v_1, v_2, v_3, v_4, v_2)$ 和$w_2=(v_2, v_1, v_3, v_4, v_1)$, 这两个RW相关联的匿名随机游走是一样的，即$\mathrm{aw}(\mathbf{w_1}) = \mathrm{aw}(\mathbf{w_2}) = (1,2,3,4,2)$, 即使$\mathbf{w_1}$和$\mathbf{w_2}$访问不同的节点。即每个节点在RW中首次被访问时的位置就是这个点在ARW中的id,如在$\mathbf{w_2}$中，$v_1$首次访问是在第二个时刻，那么他的id就是2，在ARW中用2表示。</p>
<h2 id="sampling">Sampling</h2>
<p>本文提出WEAVE随机游走来表示子图</p>
<p><img loading="lazy" src="/posts/2022-03-29-seed/pic2.png#center" alt="你想输入的替代文字"  />
</p>
<p>上图中所有的$a$代表属性一样的节点， 所有的$b$也代表属性一样的节点，那么构造如图中两条vanilla random walks将得到两条完全相同的随机游走序列，因为序列中的节点属性排列完全一样（这里不会去构造induced subgraph）。为了可以区分两个图，提出了WEAVE, i.e.,  random Walk with EArliest Visit timE。实际上就是为每个随机游走序列上的节点拼接他在匿名随机游走序列中的index。这样就可以区分两个属性完全一样的随机游走序列。</p>
<p>简单来说这种方法会记录节点首次被访问的时间，这个时间作为节点的index，从而随机游走序列可以反映子图结构。</p>
<p>一个长度为$k$的WEAVE序列可以表示为：$X=\left[\mathbf{x}^{(0)}, \mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(k)}\right]$, 其中$\mathbf{x}^{(p)}$是序列上的第$p$个节点， $\mathbf{x}^{(p)}=\left[\mathbf{x}_{a}^{(p)}, \mathbf{x}_{t}^{(p)}\right]\in \mathbb{R}^{k \times (d+\ell)}$, 是两个向量的拼接，$\mathbf{x}_{a}^{(p)} \in \mathbb{R}^d$代表这个节点的node feature, $ \mathbf{x}_{t}^{(p)} \in \mathbb{R}^\ell$是是节点在匿名随机游走中的idx， 用onehot向量表示（即该节点首次被访问的时间）。</p>
<p>最终，如果要从输入图中sample $s$条随机游走路径，将会生成$s$个子图，用矩阵表示为$\left\{X_{1}, X_{2}, \ldots, X_{s}\right\}$。</p>
<h2 id="encoding">Encoding</h2>
<p>用$s$个随机游走序列表示$\mathcal{G}$的$s$个子图。对每个子图使用auto encoder 计算embedding:
$$
\mathbf{z}=f\left(X ; \theta_{e}\right), \quad \hat{X}=g\left(\mathbf{z} ; \theta_{d}\right)
$$
其中$X$表示一个子图（WEAVE）, 先用$f_{\theta_e}$得到这个子图的pooling embedding, 在用$g_{\theta_d}$将子图的embedding重构为矩阵$\hat{X}$。每个子图的重构损失为：
$$
\mathcal{L}=||X-\hat{X}||_{2}^{2}
$$
通过对每个子图的$\mathcal{L}$做SGD来优化$\theta_e$和$\theta_d$来使得重构误差最小。 最终对于图$\mathcal{G}$我们可以得到它的$s$个子图表示：$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$.</p>
<h2 id="embedding-distribution">Embedding Distribution</h2>
<p>假设我们已经有了输入图$\mathcal{G}$的子图表示向量集$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$, 要将他们融合成一个embedding来表示整个图。可以把这个图的子图集合看做一个distribution，每个子图是这个distribution中的一个样本。 如果两个Graph的子图分布相似，那么这两个Graph的相似度应该更高。 所以目标就变为，给定两个图$\mathcal{G}$和$\mathcal{H}$, 他们的子图表示分别为$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{s}\right\}$和$\left\{\mathbf{h}_{1}, \mathbf{h}_{2}, \cdots, \mathbf{h}_{s}\right\}$。这是两个分布的样本，我们要计算两个分布的距离，本文使用MMD, 目的是求两个分布的distribution embeddings, 然后求两个distribution embeddings间的距离。MMD可以参考<a href="https://jhuow.github.io/posts/mmd/">这里</a>。</p>
<p>用$P_{\mathcal{}G}$和$P_{\mathcal{H}}$分别表示这两个图的子图分布， 两个分布之间的MMD距离可以用下式计算得到。
$$
\begin{aligned}
\widehat{MMD}\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)=&amp; \frac{1}{s(s-1)} \sum_{i=1}^{s} \sum_{j \neq i}^{s} k\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)+\frac{1}{s(s-1)} \sum_{i=1}^{s} \sum_{j \neq i}^{s} k\left(\mathbf{h}_{i}, \mathbf{h}_{j}\right) \\
&amp;-\frac{2}{s^{2}} \sum_{i=1}^{s} \sum_{j=1}^{s} k\left(\mathbf{z}_{i}, \mathbf{h}_{j}\right) \\
=&amp;\left|\left|\hat{\mu}_{\mathcal{G}}-\hat{\mu}_{\mathcal{H}}\right|\right|_{2}^{2} .
\end{aligned}
$$
该式表示的含义为，两个图中的样本$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$和$\left\{\mathbf{h}_{1}, \mathbf{h}_{2}, \cdots, \mathbf{h}_{s}\right\}$分别映射到一个RKHS空间中，<strong>两组样本在这个RKHS空间中的均值来表示这两个分布</strong>。即：
$$
\hat{\mu}_{\mathcal{G}}=\frac{1}{s} \sum_{i=1}^{s} \phi\left(\mathbf{z}_{i}\right), \quad \hat{\mu}_{\mathcal{H}}=\frac{1}{s} \sum_{i=1}^{s} \phi\left(\mathbf{h}_{i}\right)
$$
其中$\phi(\mathbf{z}_{i})$,$\phi(\mathbf{h}_{i})$分别表示 将向量$\mathbf{z}_{i}$和$\mathbf{h}_{i}$ 映射到一个RKHS中，所以$\phi(\cdot)$是一个kernel $k(\cdot, \cdot)$的feature map函数, i.e., $k(u,v) = \langle \phi(u), \phi(v) \rangle$。$\phi(u) = k(\cdot, u)$是kernel $k$对应RKHS中的一个函数（向量）。 所以只要确定一个kernel $k(\cdot, \cdot)$，上面的$\widehat{MMD}(P_{\mathcal{G}}, P_{\mathcal{H}})$就可以求出确定值，表示两个distribution间的距离。 但是知道两个分布在RKHS中的距离还不够，需要知道这两个分布的在RKHS间的均值距离还不够， 我们需要知道这两个分布在RKHS中被映射成了什么向量，即我们要求$\phi(\cdot)$。</p>
<p>假设我们已经有了一个kernel， 这个kernel对应的映射函数是一个恒等映射，那么$\phi(u)=u$, 分布样本在RKHS中的表示就是他们本身，即 $\phi(\mathbf{z}_{i})=\mathbf{z}_{i}$, $\phi(\mathbf{h}_{i})=\mathbf{h}_{i}$。那么这分布的表示向量就是他们的样本在RKHS上的平均（均值平均误差）：
$$
\hat{\mu}_{\mathcal{G}}=\frac{1}{s} \sum_{i=1}^{s} \mathbf{z}_{i}, \quad \hat{\mu}_{\mathcal{H}}=\frac{1}{s} \sum_{i=1}^{s} \mathbf{h}_{i}
$$
如果$k$是一个其他通用kernel, 比如RBF kernel, 那么$k(u,v) = \langle \phi(u), \phi(v) \rangle$这里的$\phi(\cdot)$是不知道的，也就是仅能知道映射后的内积值，不能知道具体的映射是什么，为了求这个映射，本文用神经网络来近似这个映射。</p>
<p>具体来说，定义$\hat{\phi}\left(\cdot ; \theta_{m}\right)$是一个参数为$\theta_{m}$的MLP， 输入为分布的样本，那么用这个函数来对两个分布的样本$\{\mathbf{z_i}\}$和$\{\mathbf{h_i}\}$做映射, 然后用$\hat{\phi}\left(\cdot ; \theta_{m}\right)$来近似kernel真实的映射函数$\phi(\cdot)$。即：
$$
\hat{\mu}_{\mathcal{G}}^{\prime}=\frac{1}{s} \sum_{i=1}^{s} \hat{\phi}\left(\mathbf{z}_{i} ; \theta_{m}\right), \quad \hat{\mu}_{\mathcal{H}}^{\prime}=\frac{1}{s} \sum_{i=1}^{s} \hat{\phi}\left(\mathbf{h}_{i} ; \theta_{m}\right), \quad D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)=\left|\left|\hat{\mu}_{\mathcal{G}}^{\prime}-\hat{\mu}_{\mathcal{H}}^{\prime}\right|\right|_{2}^{2}
$$
上式中的$D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)$表示两个分布中的样本在被$\hat{\phi}\left(\cdot; \theta_{m}\right)$映射后的均值误差。用这个均值误差来近似$\widehat{MMD}(P_{\mathcal{G}}, P_{\mathcal{H}})$中由kernel $k$的映射$\phi(\cdot)$算出的Ground truth均值误差：</p>
<p>$$J\left(\theta_{m}\right)=\left|\left|D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)-\widehat{M M D}\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)\right|\right|_{2}^{2}$$</p>
<p>通过最小化$J\left(\theta_{m}\right)$,来优化$\hat{\phi}\left(\cdot; \theta_{m}\right)$,使其近似称为一个kernel的feature map函数， 即可以将样本映射到一个RKHS空间中的函数。</p>
<p>训练结束后，用$\hat{\mu}_{\mathcal{G}}^{\prime}$来表示输入图$\mathcal{G}$的最终embedding （子图分布embedding）。</p>
<h1 id="reference">Reference</h1>
<p>[1] Micali, S., and Zhu, Z. A. 2016. Reconstructing markov processes from independent and anonymous experiments. Discrete Applied Mathematics 200:108–122.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2018 《Graph Attention Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gat/</link>
      <pubDate>Fri, 14 Sep 2018 23:01:31 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gat/</guid>
      <description>ICLR2018 &amp;#34;Graph Attention Networks&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1710.10903">GAT</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文介绍了一种新型的神经网络架构用来处理图结构。即__Graph Attention Networks__(<strong>GATs</strong>)。该方法利用masked self-attentional layer，即通过网络层的堆叠，可以获取网络中每个节点的领域特征，同时为领域中的不同节点指定不同的权重。这样做的好处是可以不需要各种高成本的矩阵运算也不依赖于的图结构信息。通过这种方式，GAT可以解决基于谱的图神经网络存在的问题，同时，GAT可以使用用归纳（inductive）和直推（transductive）问题。</p>
<p><strong>归纳学习</strong>:先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。</p>
<p><strong>直推学习</strong>:先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。</p>
<h1 id="architecture">Architecture</h1>
<p>图$G$中有$N$个节点，他们的特征向量为：$\textbf{h}={\vec{h_1},\vec{h_2},&hellip;,\vec{h_N}}$，其中，$\vec{h_i} \in \mathbb{R}^F$，$F$是每个节点特征数。我们的目的是输出一个新的节点特征向量集$\textbf{h&rsquo;}={\vec{h_1&rsquo;},\vec{h_2&rsquo;},&hellip;,\vec{h_N&rsquo;}}$，其中$\vec{h_i&rsquo;} \in \mathbb{R}^{F&rsquo;}$。 本质就是修改特征向量的维度（Network embedding）</p>
<p>为了获得足够的表达能力以将输入特征变换为更高级别的特征，需要至少一个可学习的线性变换。因此，以任意节点$i$和$j$为例，分别对节点$i$和节点$j$的特征向量做线性变换$W \in \mathbb{R}^{F \times F&rsquo;}$，这样 就将$\vec{h_i}$和$\vec{h_j}$从$F$维的向量转化为$F&rsquo;$维的向量：
$$
e_{ij} = a(W\vec{h_i},W\vec{h_j})
$$
上式中，分别对$\vec{h_i}$和$\vec{h_j}$做线性变换，然后使用self-attention为图中的每一个节点分配注意力（权重）。上式中，注意力机制$a$是一个$\mathbb{R}^{F&rsquo;} \times \mathbb{R}^{F&rsquo;} \to \mathbb{R}$的映射。最终得到的$e_{ij}$是节点$j$对节点$i$的影响力系数（一个实数）。</p>
<p>但是，上面的方法考虑其他节点对$i$的影响时，将图中的所有节点都纳入了考虑范围，这样就丢失了图的结构信息。因此，本文引入<strong>masked attention</strong>机制，即计算影响力系数$e_{ij}$时， 仅考虑节点$i$的<strong>一部分邻居节点</strong> $j \in \mathcal{N}_i$（$i$也属于$\mathcal{N}_i$）。使用softmax将节点$i$部分邻居的注意力系数分配到(0,1)上：
$$
\alpha_{ij} = \mathrm{softmax}_j(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i}\exp(e_{ik})}
$$
在本文中，$a$是一个单层前馈神经网络，参数是一个权重向量$\vec{\text{a}} \in \mathbb{R}^{2F&rsquo;}$，然后使用负半轴斜率为0.2的<a href="https://blog.csdn.net/sinat_33027857/article/details/80192789">LeakyReLU</a>作为非线性激活函数：
$$
\alpha_{ij} = \frac{\exp(\mathrm{LeakyReLU}(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_j}]))}{\sum_{k\in \mathcal{N}_i} \exp(\mathrm{LeakyReLU}(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_k}]))}
$$
其中$||$表示向量的连接操作。上述过程可以用下图表示：</p>
<p><img loading="lazy" src="/posts/2019-04-14-GAT/1.png#center" alt=""  />
</p>
<p>这样，我们就可以获得节点$j$对节点$i$的注意力系数$\alpha_{ij}$，那么，节点$i$最终的输出特征$\vec{h_i&rsquo;}$就是对$\mathcal{N}_i$中所有节点的加权（加注意力）求和：
$$
\vec{h_i&rsquo;} = \sigma (\sum_{j \in \mathcal{N}_i}\alpha_{ij} W\vec{h_j})
$$</p>
<p>另外，本文使用<strong>multi-head attention</strong>来稳定self-attention的学习过程，如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-04-14-GAT/2.png#center" alt=""  />
</p>
<p>图中是$K=3$ heads的multi-head attention，不同颜色的箭头表示一个独立的attention计算，每个邻居节点做三次attention计算。每次attention计算就是一个普通的self-attention，输出的结果是一个$\vec{h_i&rsquo;}$。multi-head attention为每个节点$i$输出3个不同的$\vec{h_i&rsquo;}$,，然后将这三个向量做连接或者取平均，得到最终的$\vec{h_i&rsquo;}$：
$$
\vec{h_i&rsquo;} = ||^K_{k=1} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)
$$
上式为把不同$k$的向量做连接操作，其中$\alpha_{ij}^k$和$\mathbf{W}^{k}$表示第$k$个head的结果，我们可以注意到，最终输出的结果是$KF&rsquo;$维的。除了concat之外，我们还可以通过求平均的方式来获得$\vec{h_i&rsquo;}$:
$$
\vec{h^\prime_i}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)
$$</p>
<h1 id="comparisions">Comparisions</h1>
<ul>
<li>
<p>GAT是计算高效的。self-attention是在所有边上并行计算，并且输出的特征在所有边上并行计算，从而不需要昂贵的矩阵计算和特征分解。单个head的GAT的时间复杂度为$O\left(|V| F F^{\prime}+|E| F^{\prime}\right)$，其中$F$是输入的特征数，$|V|$和$|E|$分别是节点数和边数。复杂度与GCN相同。</p>
</li>
<li>
<p>与GCN不同的是，GAT为同一邻域中的节点分配不同的重要性（different importance），提升了模型容量。</p>
</li>
<li>
<p>注意机制以共享的方式应用于图中的所有边（共享$\mathbf{W}$），因此它不依赖于对全局图结构的预先访问或其所有节点的（特征）。这样有以下提升：</p>
<ul>
<li>不必是无向图。如果$i \to j$不存在,可以直接不用计算$\alpha_{ij}$。</li>
<li>可直接应用于归纳学习。</li>
</ul>
</li>
<li>
<p>GAT可以被描述为一种特殊的<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf">MoNet(Geometric deep learning on graphs and manifolds using mixture model cnns)</a>。</p>
</li>
</ul>
<h1 id="reference">Reference</h1>
<p>参考：</p>
<p>GCN：https://arxiv.org/abs/1609.02907</p>
<p><a href="https://zhuanlan.zhihu.com/p/34232818">https://zhuanlan.zhihu.com/p/34232818</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/59176692">https://zhuanlan.zhihu.com/p/59176692</a></p>
<p><a href="https://arxiv.org/abs/1710.10903">https://arxiv.org/abs/1710.10903</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
