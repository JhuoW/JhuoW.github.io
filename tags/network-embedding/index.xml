<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Network Embedding on JhuoW‘s Notes</title>
    <link>https://JhuoW.github.io/tags/network-embedding/</link>
    <description>Recent content in Network Embedding on JhuoW‘s Notes</description>
    <image>
      <url>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 May 2019 22:09:38 +0000</lastBuildDate><atom:link href="https://JhuoW.github.io/tags/network-embedding/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>WSDM2016 《Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2019-05-12-embedding-ic/</link>
      <pubDate>Sun, 12 May 2019 22:09:38 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-05-12-embedding-ic/</guid>
      <description>WSDM2016 &amp;#34;Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>文章链接：<a href="https://dl.acm.org/citation.cfm?id=2835817">Embedding_IC</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。</p>
<p>对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：</p>
<ul>
<li>用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。</li>
<li>用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。</li>
<li>不同应用中的级联长度变化很大，难以学习和预测。</li>
</ul>
<p>本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：</p>
<ul>
<li>影响传播是二元的（被感染或不被感染），</li>
<li>扩散网络未知，</li>
<li>影响关系不依赖于传播的内容，</li>
<li>用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。</li>
</ul>
<p>本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：</p>
<p><img loading="lazy" src="1.png" alt=""  />
</p>
<h1 id="model">Model</h1>
<h2 id="notations">Notations</h2>
<p>传播事件集$\mathcal{D}={D_1,D_2,&hellip;,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。</p>
<p>给定一个社交网络，有$N$个用户：$\mathcal{U}={u_1,u_2,&hellip;,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \in \mathcal{U} \wedge t^D(u)&lt; \infty}$， 其中，$t^D: \mathcal{U} \to \mathbb{R}^+$ 表示用户被传染的时间戳， $\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \in \mathcal{U} | t^D(u)&lt;t}$。对称地，用$\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\infty)$表示最终所有被感染的用户，$\bar{D}(infty)$表示最终所有没被感染的用户。</p>
<h2 id="diffusion-model">Diffusion Model</h2>
<p>本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。</p>
<p>在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \in \mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \in \mathcal{U} \backslash I$:
$$
P(v|I) = 1-\prod_{u \in I}(1-P_{u,v})
$$
上式中，$\prod_{u \in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。</p>
<p>接下来就需要给出$P_{u,v}$的定义了，即$v$被$u$传染的概率。$z_u \in \mathbb{R}^d$是传染源用户$u$的表示向量，$\omega_{v} \in \mathbb{R}^d$是传染目标用户$v$的表示向量。那么$P_{u,v}$可以定义如下：
$$
P_{u,v} = f(z_u,\omega_{v})
$$
其中，$f: \mathbb{R}^d \times \mathbb{R}^d \to [0,1]$，是一个映射函数，把两个表示向量映射到概率空间：
$$
f\left(z_{u}, \omega_{v}\right)=\frac{1}{1+\exp \left(z_{u}^{(0)}+\omega_{v}^{(0)}+\sum_{i=1}^{d-1}\left(z_{u}^{(i)}-\omega_{v}^{(i)}\right)^{2}\right)}
$$
其中，$z_{u}^{(i)}$和$\omega_{v}^{(i)}$分别表示$z_u$和$\omega_v$的第$i$个分量。表示随距离增加而递减的传输概率，即$\left(z_{u}^{(i)}-\omega_{v}^{(i)}\right)$越大$f$越小。上式使用了sigmoid函数:$\frac{1}{1+e^{-x}}$返回一个$[0,1]$的概率。</p>
<p>值得注意的是，偏置项$z_{u}^{(0)}$和$\omega_{v}^{(0)}$的作用是反映$u$传入$v$的一般趋势，这样做的目的是避免不同的$u$和$v$产生相同的概率。</p>
<h2 id="learning-algorithm">Learning Algorithm</h2>
<p>考虑所有节点对的传播概率$\mathcal{P}={P_{u,v} | (u,v) \in \mathcal{U}^2}$ (涉及所有节点对)。那么对于特定级联$D$的概率为：
$$
P(D)=\prod_{v \in D(\infty)} P_{v}^{D} \prod_{v \in \overline{D}(\infty)}\left(1-P_{v}^{D}\right)
$$
上式中，$\prod_{v \in D(\infty)} P_{v}^{D}$表示$D$中所有被影响的用户存在的概率，$\prod_{v \in \overline{D}(\infty)}\left(1-P_{v}^{D}\right)$表示$D$中所有未被影响的用户存在的概率。所以$P(D)$就是级联$D$存在的概率。同时，可以用对数似然来表示训练级联集$\mathcal{D}$:
$$
\mathcal{L}(\mathcal{P} ; \mathcal{D})=\sum_{D \in \mathcal{D}}\left(\sum_{v \in D(\infty)} \log \left(P_{v}^{D}\right)+\sum_{v \in \overline{D}(\infty)} \log \left(1-P_{v}^{D}\right)\right)
$$
上式就是模型的目标函数。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2016 SDNE:《Structral Deep Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/sdne/</link>
      <pubDate>Mon, 21 Jan 2019 15:34:36 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/sdne/</guid>
      <description>KDD2016 &amp;#34;Structral Deep Network Embedding&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">SDNE</a></p>
<h3 id="introduction">Introduction</h3>
<p>这是一篇比较早的Network Embedding论文， 较早将深度模型应用到NE任务。 首先，本文提出了当前网络表示学习中遇到的三个问题：<br>
<strong>（1）. 高度非线性</strong><br>
<strong>（2）. 尽可能保持网络结构</strong><br>
<strong>（3）. 现实网络的高度稀疏性</strong><br>
SDNE的主要目标就是保持网络结构的一阶相似性和二阶相似性。<br>
一阶相似性就是网络中边相连的节点对之间具有的相似性。<br>
二阶相似性就是在一个Graph中，拥有共同邻居但是不直接向相连的两个节点具有的相似性。</p>
<p>其中，一阶相似性主要反映了网络的局部特征。 二阶相似性反映了网络的全局特征。</p>
<h3 id="model">Model</h3>
<p>本文的模型主要如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-01-21-SDNE/SDNE.png#center" alt="你想输入的替代文字"  />
</p>
<p>这张图看上去有点复杂，实则原理非常简单。</p>
<p>模型分为无监督部分和有监督部分，无监督部分是一个<strong>深度自编码器</strong> 用来捕获二阶相似度（保留全局结构），监督部分是一个拉普拉斯特征映射捕获一阶相似度（局部结构）。呃呃呃，Emmmmm ,不知道是我理解有问题还是其他原因，文章里说的和我理解的不太一样 /(ㄒoㄒ)/~~。 然后介绍一下具体的模型结构：</p>
<p>深度自编码器的编码部分：</p>
<p>$$y_i^{(k)}=\sigma{(W^{(k)}y_i^{(k-1)}+b^{(k)})}, k=2,&hellip;,K$$</p>
<p>假设第$k$层是 节点$v$的表示向量（仅考虑全局信息），那么从第$k$层开始解码，最终得到$\hat{x_i}$, 所以自编码器的误差就是输入节点$v$的邻接向量的重构误差。所以，二阶相似度损失函数定义为:<br>
$$\mathcal{L}=\sum_{i=1}^n{||\hat{x_i}-x_i||^2_2}$$</p>
<p>值得注意的是，由于网络的稀疏性，邻接矩阵中的0元素远多于非0元素，使用邻接矩阵作为输入的话要处理很多0，这样就做了太多无用功了。为了解决这个问题，对损失函数做了改进如下：<br>
$$\mathcal{L_{2nd}}=\sum_{i=1}^n||(\hat{x_i}-x_i)\odot{b_i}||^2_2=||\hat{X}-X\odot{B}||^2_F$$</p>
<p>其中$\odot$是哈马达乘积，表示对应元素相乘。$b_i={b_{i,j}}^n_{j=1}$， 邻接矩阵中的0对应$b=1$, 非0元素的$b&gt;1$,这样的目的是对于有边连接的节点增加惩罚。可以理解为对有边连接的节点赋予更高权重。</p>
<p>以上我们获得了二阶相似度的损失函数。在介绍一阶相似度之前，我们先来看看<strong>拉普拉斯映射（Laplacian Eigenmap）</strong>  其实LE也是一种经典的NRL方法，主要目的也是降维。其目标函数如下所示:<br>
$$\sum_{i,j} W_{ij}||y_i-y_j||^2$$<br>
LE是通过构建相似关系图来重构局部特征结构,如果放在网络结构中来说,如果节点$v_i$和$v_j$很接近（有边），那么他们在embedding space中的距离也应该相应接近。$y_i$和$y_j$就表示他们在特征空间中的表示。因此，本文定义了保持一阶相似度的目标函数：<br>
$$\mathcal{L_{1st}}=\sum_{i,j=1}^n{s_{i,j}||y_i^{(K)}-y_j^{(K)}||^2_2}=\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}$$<br>
具体来说，$K$就是自编码器第$K$层的输出，即编码结果，需要保持一条边的两个节点在嵌入空间中的表示相对接近。</p>
<p>最终 结合一阶相似度和二阶相似度，本文给出了SDNE的目标函数：<br>
$$\mathcal{L_{mix}}=\mathcal{L_{2nd}+\alpha{\mathcal{L_{1st}}}}+\nu{\mathcal{L_{reg}}}
=||(\hat{X}-X)\odot{B}||^2_F+\alpha{\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}}+\nu{\mathcal{L_{reg}}}$$<br>
其中，为了防止过拟合，添加了$\mathcal L2$-norm单元$\mathcal{L_{reg}}$来防止过拟合:<br>
$$\mathcal{L_{reg}}=\frac{1}{2}\sum_{k=1}^k({||W^{(k)}||^2_F+||\hat{W}^{k}||_F^2})$$</p>
<h3 id="optimization">Optimization</h3>
<p>使用SGD来优化$\mathcal{L_{mix}}$。具体算法如下：<br>
<img loading="lazy" src="/posts/2019-01-21-SDNE/al.png#center" alt="你想输入的替代文字"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ACL2017 《CANE:Context-Aware Network Embedding for Relation Modeling》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/cane/</link>
      <pubDate>Fri, 09 Mar 2018 20:41:15 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/cane/</guid>
      <description>ACL2017 &amp;#34;CANE:Context-Aware Network Embedding for Relation Modeling&amp;#34; 阅读笔记</description>
      <content:encoded><![CDATA[<p><a href="https://aclanthology.org/P17-1158.pdf">paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>在现实世界的社交网络中，一个顶点在与不同的邻居顶点交互时可能表现出不同的方面 (aspect)，这是很直观的。例如，研究人员通常与各种合作伙伴就不同的研究主题进行合作（如下图所示），社交媒体用户与分享不同兴趣的各种朋友联系，一个网页出于不同目的链接到多个其它网页。然而，现有的大多数 NE 方法只为每个顶点安排一个 single embedding 向量，并产生以下两个问题：</p>
<ul>
<li>这些方法在与不同邻居交互时，无法灵活转换不同的aspect</li>
<li>在这些模型中，一个顶点倾向于迫使它的所有邻居之间的 embedding彼此靠近，但事实上并非一直如此。例如下图中，左侧用户和右侧用户共享较少的共同兴趣，但是由于他们都链接到中间用户，因此被认为彼此接近。因此，这使得顶点 embedding 没有区分性。</li>
</ul>
<p><img loading="lazy" src="/posts/CANE/1.png#center" alt=""  />
</p>
<p>为了解决上述问题，本文提出了一个 CANE框架，用于精确建模顶点之间的关系。更具体而言，论文在信息网络上应用 CANE。信息网络的每个顶点还包含丰富的外部信息，例如文本、标签 、或者其它元数据。在这种场景下，上下文的重要性对 network embedding 更为关键。在不失一般性的情况下，论文在基于文本的信息网络中实现了 CANE，但是 CANE可以很容易地扩展到其它类型的信息网络。</p>
<p>在传统的 network embedding模型中，每个顶点都表达为一个静态的 embedding 向量，即 context-free embedding 。相反，CANE 根据与当前顶点交互的不同邻居，从而将动态的 embedding分配给当前顶点，称为 context-aware embedding。以一个顶点$u$为例：当与不同的邻居交互时， 的 context-free embedding保持不变；而当面对不同的邻居时， $u$的 context-aware embedding是动态的。</p>
<p>当顶点$u$与它的邻居顶点$v$交互时，它们彼此相关的 context embedding 分别来自它们的文本信息。对于每个顶点，可以轻松地使用神经模型neural model ，例如卷积神经网络和循环神经网络来构建 context-free embedding 和 text-based embedding 。为了实现 context-aware text-based embedding，论文引入了 selective attention 方案，并在这些神经模型中建立了  和  之间的互注意力 mutual attention 。mutual attention 预期引导神经模型强调那些被相邻顶点 focus 的单词，并最终获得 context-aware embedding。每个顶点的 context-free embedding 和 context-aware embedding 都可以通过使用现有的 network embedding 方法学到（如 DeepWalk，LINE，node2vec）并拼接起来。</p>
<p>论文对不同领域的三个真实数据集进行了实验。与其它 state-of-the-art 方法相比，链接预测的实验结果展示了论文框架的有效性。结果表明，context-aware embedding 对于网络分析至关重要，特别是对于那些涉及顶点之间复杂交互的任务，如链接预测。论文还通过顶点分类和案例研究来探索论文框架的性能，这再次证实了 CANE 模型的灵活性和优越性。</p>
<h1 id="model">Model</h1>
<p>给定信息网络$G = (V,E,T)$, ，其中$V$为顶点集合， $E \subseteq V \times V$为边集合， $T$为顶点的文本信息。每条边表示顶点之间的关系，并且关联一个权重$w_{u,v}$ 。这里顶点$v \in V$的文本信息表达为单词序列$\mathcal{S}_{v}=\left(t_{1}, t_{2}, \cdots, t_{n_{v}}\right)$，其中$n_{v}=\left|\mathcal{S}_{v}\right|$为序列$\mathcal{S}_{v}$的长度。</p>
<p>NE旨在根据网络结构和关联信息（如文本和 label）为每个顶点$v \in V$ 学习低维embedding$\overrightarrow{\mathbf{v}} \in \mathbb{R}^{d}$, 其中 $d \ll |V|$为embedding的维度。</p>
<p>Context-free Embedding 的定义：传统的 network representation learning 模型为每个顶点学习 context-free embedding 。这意味着顶点的 embedding 是固定的，并且不会根据顶点的上下文信息（即与之交互的另一个顶点）而改变。</p>
<p>Context-aware Embedding 的定义：与现有的、学习 context-free embedding 的NE 模型不同，CANE 根据顶点不同的上下文来学习该顶点的各种 embedding 。具体而言，对于边$e_{u,v}$， CANE 学习 context-aware embedding $\overrightarrow{\mathbf{v}}_{(u)}$和$\overrightarrow{\mathbf{u}}_{(v)}$</p>
<h2 id="framework">Framework</h2>
<p>为了充分利用网络结构和关联的文本信息，为顶点$v$提供了两种类型的 embedding：structure-based embedding $\overrightarrow{\mathbf{v}}^{s}$, text-based embedding  $\overrightarrow{\mathbf{v}}^{t}$。structure-based embeding 可以捕获网络结构中的信息，而 text-based embedding 可以捕获关联文本信息中的文本含义（textual meaning）。使用这些 embedding，可以简单地拼接它们并获得顶点 embedding 为：
$$
\overrightarrow{\mathbf{V}}=\overrightarrow{\mathbf{V}}^{s} \oplus \overrightarrow{\mathbf{V}}^{t}
$$
其中$\oplus$为拼接算子。注意， text-based embedding $\overrightarrow{\mathbf{v}}^{t}$可以是 context-free 的、也可以是context-aware的，这将在后面详细介绍。当$\overrightarrow{\mathbf{v}}^{t}$是 context-aware时，整个顶点 embedding $\overrightarrow{\mathbf{v}}$也将是 context-aware。</p>
<p>通过上述定义，CANE旨在最大化所有边上的目标函数，如下所示：
$$
\mathcal{L}=\sum_{e \in E} L(e)
$$
这里每条边的目标函数$L(e)$由两部分组成：
$$
L(e) = L_s(e) + L_t(e)
$$
$L_s(e)$表示基于结构的目标函数, $L_t(e)$表示基于文本的目标函数。 接下来分别对这两个目标函数进行详细介绍。</p>
<h2 id="基于结构的目标函数">基于结构的目标函数</h2>
<p>为了不失一般性，假设网络是有向的，因为无向边可以被认为是两个方向相反、且权重相等的有向边。因此，基于结构的目标函数旨在使用 structure-based embedding来衡量观察到一条有向边的对数似然（log-likelihood），即：
$$
L_{s}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)
$$
遵从 LINE，将上式中的条件概率定义为：
$$
p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)=\frac{\exp \left(\overrightarrow{\mathbf{u}}^{s} \cdot \overrightarrow{\mathbf{v}}^{s}\right)}{\sum_{z \in V} \overrightarrow{\mathbf{u}}^{s} \cdot \overrightarrow{\mathbf{z}}^{s}}
$$</p>
<h2 id="基于文本的目标函数">基于文本的目标函数</h2>
<p>现实世界社交网络中的顶点通常伴随着关联的文本信息。因此，本文提出了基于文本的目标函数来利用这些文本信息，并学习 text-based embedding 。</p>
<p>基于文本的目标函数$L_{t}(e)$可以通过各种度量指标来定义。为了与$L_{s}(e)$ 兼容，将$L_{t}(e)$定义如下：
$$
L_{t}(e)=\alpha L_{t, t}(e)+\beta L_{t, s}(e)+\gamma L_{s, t}(e)
$$
其中：$\alpha$，$\beta$，$\gamma$为对应部分的重要性，为超参数。</p>
<p>$L_{t,t}(e)$，$L_{t, s}(e)$，$L_{s, t}(e)$定义为：
$$
\begin{aligned}
&amp;L_{t, t}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{t} \mid \overrightarrow{\mathbf{u}}^{t}\right) \\
&amp;L_{t, s}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{t} \mid \overrightarrow{\mathbf{u}}^{s}\right) \\
&amp;L_{s, t}(e)=w_{u, v} \log p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{t}\right)
\end{aligned}
$$
这里构建了 structure-based embedding 、 text-based embedding 之间的桥梁，使得信息在结构和文本之间流动。上式中的条件概率将两种类型的顶点 embedding 映射到相同的 representation 空间中，但是考虑到它们自身的特点，这里并未强制它们相同。类似地，使用 softmax 函数来计算概率，如公式$p\left(\overrightarrow{\mathbf{v}}^{s} \mid \overrightarrow{\mathbf{u}}^{s}\right)$所示。</p>
<p>structure-based embedding 被视为 parameter，与传统的 network embedding 模型相同。但是对于 text-based embedding，本文从顶点的关联文本信息中获取它们。此外， text-based embedding 可以通过 context-free 的方式、或者 context-aware 的方式获取。</p>
<h2 id="context-free-text-embedding">Context-Free Text Embedding</h2>
<p>有多种神经网络模型可以从单词序列 word sequence 中获取 text embedding，如卷积神经网络 CNN、循环神经网络RNN。在这项工作中，研究了用于文本建模的不同神经网络，包括 CNN、Bidirectional RNN、GRU，并采用了性能最好的 CNN。CNN 可以在单词之间捕获局部语义依赖性。</p>
<p>CNN 以一个顶点的单词序列作为输入，通过 lookup、卷积、池化这三层得到基于文本的 embedding 。</p>
<ul>
<li>
<p>lookup：给定一个单词序列 $\mathcal{S}=\left(t_{1}, t_{2}, \cdots, t_{n}\right)$, lookup layer 将每个单词$t_i$转换为对应的 word embedding $\overrightarrow{\mathbf{t}}_{i} \in \mathbb{R}^{d^{\prime}}$, 并且获得 embedding序列$\mathbf{S}=\left(\overrightarrow{\mathbf{t}}_{1}, \cdots, \overrightarrow{\mathbf{t}}_{n}\right)$, 其中$d^{\prime}$为word embedding的维度。</p>
</li>
<li>
<p>卷积：lookup之后，卷积层提取输入的embedding序列$\mathbf{S}$局部特征 local feature。具体而言，它使用卷积矩阵$\mathbf{C} \in \mathbb{R}^{d \times\left(l \times d^{\prime}\right)}$在长度为$l$的滑动窗口上进行卷积操作，如下所示：</p>
</li>
</ul>
<p>$$
\overrightarrow{\mathbf{x}}_{i}=\mathbf{C} * \mathbf{S}_{i: i+l-1}+\overrightarrow{\mathbf{b}}
$$</p>
<p>$\mathbf{C}$表示$d$个卷积核，每个卷积核的尺寸为$l \times d^\prime$。 $d$为卷积核数量，也称作输出通道数。</p>
<p>其中：$\mathbf{S}_{i: i+l-1} \in \mathbb{R}^{l \times d}$为第$i$个滑动窗口内所有word embedding拼接得到的矩阵。$\overrightarrow{\mathbf{b}} \in \mathbb{R}^{d}$为bias。注意，在单词序列的边缘添加了零填充向量。</p>
<ul>
<li>最大池化：为了获得 text embedding $\overrightarrow{\mathbf{v}}^{t}$, 对 $\left\{\overrightarrow{\mathbf{x}}_{1}, \cdots, \overrightarrow{\mathbf{x}}_{n}\right\}$进行最大池化和非线性变换，如下所示：</li>
</ul>
<p>$$
\begin{gathered}
r_{i}=\tanh \left(\max \left(x_{1, i}, x_{2, i}, \cdots, x_{n, i}\right)\right), \quad i=1, \cdots, d \\
\overrightarrow{\mathbf{v}}^{t}=\left(r_{1}, \cdots, r_{d}\right)^{\top} \in \mathbb{R}^{d}
\end{gathered}
$$</p>
<p>其中max是在 embedding维度上进行的，tanh为逐元素的函数。</p>
<h2 id="context-aware-text-embedding">Context-Aware Text Embedding</h2>
<p>假设顶点在与其它顶点交互时扮演不同的角色。换句话讲，对于给定的顶点，其它不同的顶点与它有不同的焦点，这将导致 context-aware text embedding。</p>
<p>为了实现这一点，采用 mutual attention来获得 context-aware text embedding。mutual attention使 CNN 中的池化层能够感知 edge中的顶点 pair，从而使得来自一个顶点的文本信息可以直接影响另一个顶点的 text embedding，反之亦然。</p>
<p><img loading="lazy" src="/posts/CANE/2.png#center" alt=""  />
</p>
<p>如上图所示，对于context-aware text embedding的生成过程， 给定一条边$e_{u,v}$和两个对应的文本序列$\mathcal{S}_u$和$\mathcal{S}_v$, 可以通过卷积层得到矩阵$\mathbf{P} \in \mathbb{R}^{d \times m}$和$\mathbf{Q} \in \mathbb{R}^{d \times n}$ 。这里$m$表示$\mathcal{S}_u$的长度，$n$表示$\mathcal{S}_v$的长度。通过引入一个注意力矩阵$\mathbf{A} \in \mathbb{R}^{d \times d}$，计算相关性矩阵$\mathbf{F} \in \mathbb{R}^{m \times n}$:
$$
\mathbf{F}=\tanh \left(\mathbf{P}^{\top} \mathbf{A} \mathbf{Q}\right)
$$
其中$\mathbf{F}$中的每个元素$\mathbf{F}_{i,j}$表示两个隐向量（即$\overrightarrow{\mathbf{p}}_{i}$， $\overrightarrow{\mathbf{q}}_{j}$）之间的 pair-wise相关性得分。 然后，沿 $\mathbf{F}$的行和列进行池化操作从而生成重要性向量，分别称作行池化row-pooling和列池化column-pooling。根据实验，均值池化的性能优于最大池化。因此，采用如下的均值池化操作：
$$
\begin{aligned}
g_{i}^{p} &amp;=\operatorname{mean}\left(F_{i, 1}, F_{i, 2}, \cdots, F_{i, n}\right) \\
g_{i}^{q} &amp;=\operatorname{mean}\left(F_{1, i}, F_{2, i}, \cdots, F_{m, i}\right)
\end{aligned}
$$
其中：$\overrightarrow{\mathbf{g}}^{p}=\left(g_{1}^{p}, \cdots, g_{m}^{p}\right)^{\top} \in \mathbb{R}^{m}$表示行池化向量，为$\mathbf{P}$的重要性向量。$\overrightarrow{\mathbf{g}}^{q}=\left(g_{1}^{q}, \cdots, g_{m}^{q}\right)^{\top} \in \mathbb{R}^{m}$表示列池化向量，为$\mathbf{Q}$的重要性向量。</p>
<p>接下来，使用softmax函数将重要性向量$\overrightarrow{\mathbf{g}}^{p}$和$\overrightarrow{\mathbf{g}}^{q}$转换为注意力向量 $\overrightarrow{\mathbf{a}}^{p}$和$\overrightarrow{\mathbf{a}}^{q}$：
$$
\begin{aligned}
a_{i}^{p}=\frac{\exp \left(g_{i}^{p}\right)}{\sum_{j=1}^{m} \exp \left(g_{j}^{p}\right)}, \quad a_{i}^{q}=\frac{\exp \left(g_{i}^{q}\right)}{\sum_{j=1}^{n} \exp \left(g_{j}^{q}\right)} \
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\overrightarrow{\mathbf{a}}^{p}=\left(a_{1}^{p}, \cdots, a_{m}^{p}\right)^{\top}  \in \mathbb{R}^{m} \\
\overrightarrow{\mathbf{a}}^{q}=\left(a_{1}^{q}, \cdots, a_{n}^{q}\right)^{\top} \in \mathbb{R}^{n}
\end{aligned}
$$
最后，顶点$u$和$v$ 的 context-aware text embedding计算为：
$$
\overrightarrow{\mathbf{u}}_{(v)}^{t}=\mathbf{P} \overrightarrow{\mathbf{a}}^{p}, \quad \overrightarrow{\mathbf{v}}_{(u)}^{t}=\mathbf{Q} \overrightarrow{\mathbf{a}}^{q}
$$
现在，给定一条边$e_{u,v}$，可以获得 context-aware embedding，它是 structure embedding 和 context-aware text embedding的拼接：
$$
\overrightarrow{\mathbf{u}}_{(v)}=\overrightarrow{\mathbf{u}}^{s} \oplus \overrightarrow{\mathbf{u}}_{(v)}^{t}, \quad \overrightarrow{\mathbf{v}}_{(u)}=\overrightarrow{\mathbf{v}}^{s} \oplus \overrightarrow{\mathbf{v}}_{(u)}^{t}
$$</p>
<h2 id="cane优化过程">CANE优化过程</h2>
<p>CANE 旨在最大化若干个条件概率。直观而言，使用 softmax函数优化条件概率在计算上代价太大。因此，使用负采样，并将目标函数转换为以下形式：
$$
\log \sigma(\overrightarrow{\mathbf{u}} \cdot \overrightarrow{\mathbf{v}})+k \times \mathbb{E}_{z \sim P(v)}[\log \sigma(-\overrightarrow{\mathbf{u}} \cdot \overrightarrow{\mathbf{z}})]
$$
其中$k$为负采样比例， $\sigma$为sigmoid, $P(v) \propto d_{v}^{3 / 4}$为负顶点的采样分布，$d_{v}$为顶点$v$的out-degree。</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
