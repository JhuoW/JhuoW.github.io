<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on JhuoW‘s Notes</title>
    <link>https://JhuoW.github.io/posts/</link>
    <description>Recent content in Posts on JhuoW‘s Notes</description>
    <image>
      <url>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Apr 2022 15:00:20 +0800</lastBuildDate><atom:link href="https://JhuoW.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Everything about Graph Laplacian</title>
      <link>https://JhuoW.github.io/posts/laplacian/</link>
      <pubDate>Sat, 02 Apr 2022 15:00:20 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/laplacian/</guid>
      <description>Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem.</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds.  The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem. Another application is spectral matching that solves for graph matching.</p>
<h1 id="basic-notations">Basic notations</h1>
<p>We consider simple graphs (no multiple edges or loops), $\mathcal{G}={\mathcal{V}, \mathcal{E}}$ :</p>
<ul>
<li>
<p>$\mathcal{V}(\mathcal{G})=\left\{v_{1}, \ldots, v_{n}\right\}$ is called the vertex set with $n=|\mathcal{V}|$;</p>
</li>
<li>
<p>$\mathcal{E}(\mathcal{G})=\left\{e_{i j}\right\}$ is called the edge set with $m=|\mathcal{E}|$;</p>
</li>
<li>
<p>An edge $e_{i j}$ connects vertices $v_{i}$ and $v_{j}$ if they are adjacent or neighbors. One possible notation for adjacency is $v_{i} \sim v_{j}$;</p>
</li>
<li>
<p>The number of neighbors of a node $v$ is called the degree of $v$ and is denoted by $d(v), d\left(v_{i}\right)=\sum_{v_{i} \sim v_{j}} e_{i j}$. If all the nodes of a graph have the same degree, the graph is regular; The nodes of an Eulerian graph have even degree.</p>
</li>
<li>
<p>A graph is complete if there is an edge between every pair of vertices.</p>
</li>
</ul>
<h1 id="subgraph-of-a-graph">Subgraph of a graph</h1>
<ul>
<li>
<p>$\mathcal{H}$ is a subgraph of $\mathcal{G}$ if $\mathcal{V}(\mathcal{H}) \subseteq \mathcal{V}(\mathcal{G})$ and $\mathcal{E}(\mathcal{H}) \subseteq \mathcal{E}(\mathcal{G})$;</p>
</li>
<li>
<p>a subgraph $\mathcal{H}$ is an induced subgraph of $\mathcal{G}$ if two vertices of $\mathcal{V}(\mathcal{H})$ are adjacent if and only if they are adjacent in $\mathcal{G}$.</p>
</li>
<li>
<p>A clique is a complete subgraph of a graph.</p>
</li>
<li>
<p>A path of $k$ vertices is a sequence of $k$ distinct vertices such that consecutive vertices are adjacent.</p>
</li>
<li>
<p>A cycle is a connected subgraph where every vertex has exactly two neighbors.</p>
</li>
<li>
<p>A graph containing no cycles is a forest. A connected forest is a tree.</p>
</li>
</ul>
<h1 id="a-k-partite-graph">A k-partite graph</h1>
<ul>
<li>A graph is called k-partite if its set of vertices admits a partition into $k$ classes such that the vertices of the same class are not adjacent.</li>
<li>An example of a bipartite graph.</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/1.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-adjacency-matrix-of-a-graph">The adjacency matrix of a graph</h1>
<ul>
<li>For a graph with $n$ vertices, the entries of the $n \times n$ adjacency matrix are defined by:</li>
</ul>
<p>$$
\mathbf{A}:= \begin{cases}A_{i j}=1 &amp; \text { if there is an edge } e_{i j} \\ A_{i j}=0 &amp; \text { if there is no edge } \\ A_{i i}=0 &amp; \end{cases}
$$</p>
<p>$$
\begin{aligned}
&amp; \mathbf{A}=\left[\begin{array}{llll}0 &amp; 1 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 1 &amp; 1 \\1 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 &amp; 0\end{array}\right]
\end{aligned}
$$</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="eigenvalues-and-eigenvectors">Eigenvalues and eigenvectors</h1>
<ul>
<li>
<p>A is a real-symmetric matrix: it has $n$ real eigenvalues and its $n$ real eigenvectors form an orthonormal basis.</p>
</li>
<li>
<p>Let $\left\{\lambda_{1}, \ldots, \lambda_{i}, \ldots, \lambda_{r}\right\}$ be the set of distinct eigenvalues.</p>
</li>
<li>
<p>The eigenspace $S_{i}$ contains the eigenvectors associated with $\lambda_{i}$ :</p>
</li>
</ul>
<p>$$
S_{i}=\left\{\boldsymbol{x} \in \mathbb{R}^{n} \mid \mathbf{A} \boldsymbol{x}=\lambda_{i} \boldsymbol{x}\right\}
$$</p>
<ul>
<li>
<p>For real-symmetric matrices, the algebraic multiplicity is equal to the geometric multiplicity, for all the eigenvalues.</p>
</li>
<li>
<p>The dimension of $S_{i}$ (geometric multiplicity) is equal to the multiplicity of $\lambda_{i}$.</p>
</li>
<li>
<p>If $\lambda_{i} \neq \lambda_{j}$ then $S_{i}$ and $S_{j}$ are mutually orthogonal.</p>
</li>
</ul>
<h1 id="real-valued-functions-on-graphs">Real-valued functions on graphs</h1>
<ul>
<li>
<p>We consider real-valued functions on the set of the graph&rsquo;s vertices, $\boldsymbol{f}: \mathcal{V} \longrightarrow \mathbb{R}$. Such a function assigns a real number to each graph node.</p>
</li>
<li>
<p>$\boldsymbol{f}$ is a vector indexed by the graph&rsquo;s vertices, hence $\boldsymbol{f} \in \mathbb{R}^{n}$.</p>
</li>
<li>
<p>Notation: $\boldsymbol{f}=\left(f\left(v_{1}\right), \ldots, f\left(v_{n}\right)\right)=(f(1), \ldots, f(n))$.</p>
</li>
<li>
<p>The eigenvectors of the adjacency matrix, $\mathbf{A} \boldsymbol{x}=\lambda \boldsymbol{x}$, can be viewed as eigenfunctions.</p>
</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/3.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="matrix-a-as-an-operator-and-quadratic-form">Matrix A as an operator and quadratic form</h1>
<ul>
<li>The adjacency matrix can be viewed as an operator</li>
</ul>
<p>$$
\boldsymbol{g}=\mathbf{A} \boldsymbol{f} ; g(i)=\sum_{i \sim j} f(j)
$$</p>
<ul>
<li>It can also be viewed as a quadratic form:</li>
</ul>
<p>$$
\boldsymbol{f}^{\top} \mathbf{A} \boldsymbol{f}=\sum_{e_{i j}} f(i) f(j)
$$</p>
<h1 id="the-incidence-matrix-of-a-graph">The incidence matrix of a graph</h1>
<ul>
<li>
<p>Let each edge in the graph have an arbitrary but fixed orientation;</p>
</li>
<li>
<p>The incidence matrix of a graph is a $|\mathcal{E}| \times|\mathcal{V}|(m \times n)$ matrix defined as follows:</p>
</li>
</ul>
<p>$$
\nabla:= \begin{cases}\nabla_{e v}=-1 &amp; \text { if } v \text { is the initial vertex of edge } e \\ \nabla_{e v}=1 &amp; \text { if } v \text { is the terminal vertex of edge } e \\ \nabla_{e v}=0 &amp; \text { if } v \text { is not in } e\end{cases}
$$</p>
<p>$$
\begin{aligned}
&amp; \nabla=\left[\begin{array}{cccc}-1 &amp; 1 &amp; 0 &amp; 0 \\1 &amp; 0 &amp; -1 &amp; 0 \\0 &amp; -1 &amp; 1 &amp; 0 \\0 &amp; -1 &amp; 0 &amp; +1\end{array}\right]
\end{aligned}
$$</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-incidence-matrix-a-discrete-differential-operator">The incidence matrix: A discrete differential operator</h1>
<ul>
<li>
<p>The mapping $\boldsymbol{f} \longrightarrow \nabla \boldsymbol{f}$ is known as the co-boundary mapping of the graph.</p>
</li>
<li>
<p>$(\nabla \boldsymbol{f})\left(e_{i j}\right)=f\left(v_{j}\right)-f\left(v_{i}\right)$</p>
</li>
</ul>
<p>$$
\left(\begin{array}{c}
f(2)-f(1) \\
f(1)-f(3) \\
f(3)-f(2) \\
f(4)-f(2)
\end{array}\right)=\left[\begin{array}{cccc}
-1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; -1 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; +1
\end{array}\right]\left(\begin{array}{c}
f(1) \\
f(2) \\
f(3) \\
f(4)
\end{array}\right)
$$</p>
<h1 id="the-laplacian-matrix-of-a-graph">The Laplacian matrix of a graph</h1>
<ul>
<li>
<p>$\mathbf{L}=\nabla^{\top} \nabla$</p>
</li>
<li>
<p>$(\mathbf{L} \boldsymbol{f})\left(v_{i}\right)=\sum_{v_{j} \sim v_{i}}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)$</p>
</li>
<li>
<p>Connection between the Laplacian and the adjacency matrices:</p>
</li>
</ul>
<p>$$
\mathbf{L}=\mathbf{D}-\mathbf{A}
$$</p>
<ul>
<li>The degree matrix: $\mathbf{D}:=D_{i i}=d\left(v_{i}\right)$.</li>
</ul>
<p>$$
\mathbf{L}=\left[\begin{array}{cccc}
2 &amp; -1 &amp; -1 &amp; 0 \\
-1 &amp; 3 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 2 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 1
\end{array}\right]
$$</p>
<h1 id="the-laplacian-matrix-of-an-undirected-weighted-graph">The Laplacian matrix of an undirected weighted graph</h1>
<ul>
<li>
<p>We consider undirected weighted graphs: Each edge $e_{i j}$ is weighted by $w_{i j}&gt;0$.</p>
</li>
<li>
<p>The Laplacian as an operator:</p>
</li>
</ul>
<p>$$
(\mathbf{L} \boldsymbol{f})\left(v_{i}\right)=\sum_{v_{j} \sim v_{i}} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)
$$</p>
<ul>
<li>As a quadratic form:</li>
</ul>
<p>$$
\boldsymbol{f}^{\top} \mathbf{L} \boldsymbol{f}=\frac{1}{2} \sum_{e_{i j}} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)^{2}
$$</p>
<ul>
<li>
<p>L is symmetric and positive semi-definite.</p>
</li>
<li>
<p>L has $n$ non-negative, real-valued eigenvalues: $0=\lambda_{1} \leq \lambda_{2} \leq \ldots \leq \lambda_{n} .$</p>
</li>
</ul>
<h1 id="the-laplacian-of-a-3d-discrete-surface-mesh">The Laplacian of a 3D discrete surface (mesh)</h1>
<ul>
<li>
<p>A graph vertex $v_{i}$ is associated with a 3D point $\boldsymbol{v}_{i}$.</p>
</li>
<li>
<p>The weight of an edge $e_{i j}$ is defined by the Gaussian kernel:</p>
</li>
</ul>
<p>$$
w_{i j}=\exp \left(-\left|\boldsymbol{v}_{i}-\boldsymbol{v}_{j}\right|^{2} / \sigma^{2}\right)
$$</p>
<ul>
<li>
<p>$0 \leq w_{\min } \leq w_{i j} \leq w_{\max } \leq 1$</p>
</li>
<li>
<p>Hence, the geometric structure of the mesh is encoded in the weights.</p>
</li>
<li>
<p>Other weighting functions were proposed in the literature.</p>
</li>
</ul>
<h1 id="the-laplacian-of-a-cloud-of-points">The Laplacian of a cloud of points</h1>
<ul>
<li>
<p>3-nearest neighbor graph</p>
</li>
<li>
<p>$\varepsilon$-radius graph</p>
</li>
<li>
<p>KNN may guarantee that the graph is connected (depends on the implementation)</p>
</li>
<li>
<p>$\varepsilon$-radius does not guarantee that the graph has one connected component</p>
</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/4.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-laplacian-of-a-graph-with-one-connected-component">The Laplacian of a graph with one connected component</h1>
<ul>
<li>
<p>Lu $=\lambda \boldsymbol{u}$.</p>
</li>
<li>
<p>$\mathbf{L} \mathbf{1}_{n}=\mathbf{0}, \lambda_{1}=0$ is the smallest eigenvalue.</p>
</li>
<li>
<p>The one vector: $\mathbf{1}_{n}=(1 \ldots 1)^{\top}$.</p>
</li>
<li>
<p>$0=\boldsymbol{u}^{\top} \mathbf{L} \boldsymbol{u}=\sum_{i, j=1}^{n} w_{i j}(u(i)-u(j))^{2}$.</p>
</li>
<li>
<p>If any two vertices are connected by a path, then $\boldsymbol{u}=(u(1), \ldots, u(n))$ needs to be constant at all vertices such that the quadratic form vanishes. Therefore, a graph with one connected component has the constant vector $\boldsymbol{u}_{1}=\mathbf{1}_{n}$ as the only eigenvector with eigenvalue 0 .</p>
</li>
</ul>
<h1 id="a-graph-with-k1-connected-components">A graph with $k&gt;1$ connected components</h1>
<ul>
<li>Each connected component has an associated Laplacian. Therefore, we can write matrix $\mathbf{L}$ as a block diagonal matrix:</li>
</ul>
<p>$$
\mathbf{L}=\left[\begin{array}{lll}
\mathbf{L}_{1} &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \mathbf{L}_{k}
\end{array}\right]
$$</p>
<ul>
<li>
<p>The spectrum of $\mathbf{L}$ is given by the union of the spectra of $\mathbf{L}_{i}$.</p>
</li>
<li>
<p>Each block corresponds to a connected component, hence each matrix $\mathbf{L}_{i}$ has an eigenvalue 0 with multiplicity 1 .</p>
</li>
<li>
<p>The spectrum of $\mathbf{L}$ is given by the union of the spectra of $\mathbf{L}_{i}$.</p>
</li>
<li>
<p>The eigenvalue $\lambda_{1}=0$ has multiplicity $k$.</p>
</li>
</ul>
<h1 id="the-eigenspace-of-lambda_10-with-multiplicity-k">The eigenspace of $\lambda_{1}=0$ with multiplicity $k$</h1>
<ul>
<li>The eigenspace corresponding to $\lambda_{1}=\ldots=\lambda_{k}=0$ is spanned by the $k$ mutually orthogonal vectors:</li>
</ul>
<p>$$
\begin{aligned}
\boldsymbol{u}_{1} &amp;=\mathbf{1}_{L_{1}} \\
&amp; \cdots \\
\boldsymbol{u}_{k} &amp;=\mathbf{1}_{L_{k}}
\end{aligned}
$$</p>
<ul>
<li>
<p>with $\mathbf{1}_{L_{i}}=(0000111110000)^{\top} \in \mathbb{R}^{n}$</p>
</li>
<li>
<p>These vectors are the indicator vectors of the graph&rsquo;s connected components.</p>
</li>
<li>
<p>Notice that $\mathbf{1}_{L_{1}}+\ldots+\mathbf{1}_{L_{k}}=\mathbf{1}_{n}$</p>
</li>
</ul>
<h1 id="the-fiedler-vector-of-the-graph-laplacian">The Fiedler vector of the graph Laplacian</h1>
<ul>
<li>
<p>The first non-null eigenvalue $\lambda_{k+1}$ is called the Fiedler value.</p>
</li>
<li>
<p>The corresponding eigenvector $\boldsymbol{u}_{k+1}$ is called the Fiedler vector.</p>
</li>
<li>
<p>The multiplicity of the Fiedler eigenvalue is always equal to $1 .$</p>
</li>
<li>
<p>The Fiedler value is the algebraic connectivity of a graph, the further from 0 , the more connected.</p>
</li>
<li>
<p>The Fidler vector has been extensively used for spectral bi-partioning</p>
</li>
<li>
<p>Theoretical results are summarized in Spielman &amp; Teng 2007: <a href="http://cs-www.cs.yale.edu/homes/spielman/">http://cs-www.cs.yale.edu/homes/spielman/</a></p>
</li>
</ul>
<h1 id="eigenvectors-of-the-laplacian-of-connected-graphs">Eigenvectors of the Laplacian of connected graphs</h1>
<ul>
<li>
<p>$\boldsymbol{u}_{1}=\mathbf{1}_{n}, \mathbf{L} \mathbf{1}_{n}=\mathbf{0}$.</p>
</li>
<li>
<p>$\boldsymbol{u}_{2}$ is the the Fiedler vector with multiplicity 1 .</p>
</li>
<li>
<p>The eigenvectors form an orthonormal basis: $\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{j}=\delta_{i j}$.</p>
</li>
<li>
<p>For any eigenvector $\boldsymbol{u}_{i}=\left(\boldsymbol{u}_{i}\left(v_{1}\right) \ldots \boldsymbol{u}_{i}\left(v_{n}\right)\right)^{\top}, 2 \leq i \leq n$ :</p>
</li>
</ul>
<p>$$
\boldsymbol{u}_{i}^{\top} \mathbf{1}_{n}=0
$$</p>
<ul>
<li>Hence the components of $\boldsymbol{u}_{i}, 2 \leq i \leq n$ satisfy:</li>
</ul>
<p>$$
\sum_{j=1}^{n} \boldsymbol{u}_{i}\left(v_{j}\right)=0
$$</p>
<ul>
<li>Each component is bounded by:</li>
</ul>
<p>$$
-1&lt;\boldsymbol{u}_{i}\left(v_{j}\right)&lt;1
$$</p>
<h1 id="laplacian-embedding-mapping-a-graph-on-a-line">Laplacian embedding: Mapping a graph on a line</h1>
<ul>
<li>Map a weighted graph onto a line such that connected nodes stay as close as possible, i.e., minimize $\sum_{i, j=1}^{n} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)^{2}$, or:</li>
</ul>
<p>$$
\arg \min _{\boldsymbol{f}} \boldsymbol{f}^{\top} \mathbf{L} \boldsymbol{f} \text { with: } \boldsymbol{f}^{\top} \boldsymbol{f}=1 \text { and } \boldsymbol{f}^{\top} \mathbf{1}=0
$$</p>
<ul>
<li>
<p>The solution is the eigenvector associated with the smallest nonzero eigenvalue of the eigenvalue problem: $\mathbf{L} \boldsymbol{f}=\lambda \boldsymbol{f}$, namely the Fiedler vector $\boldsymbol{u}_{2}$.</p>
</li>
<li>
<p>For more details on this minimization see Golub &amp; Van Loan Matrix Computations, chapter 8 (The symmetric eigenvalue problem).</p>
</li>
</ul>
<p><em><strong>Example of mapping a graph on the Fiedler vector</strong></em>:</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/5.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="laplacian-embedding">Laplacian embedding</h1>
<ul>
<li>
<p>Embed the graph in a $k$-dimensional Euclidean space. The embedding is given by the $n \times k$ matrix $\mathbf{F}=\left[\boldsymbol{f}_{1} \boldsymbol{f}_{2} \ldots \boldsymbol{f}_{k}\right]$ where the $i$-th row of this matrix $-\boldsymbol{f}^{(i)}-$ corresponds to the Euclidean coordinates of the $i$-th graph node $v_{i}$.</p>
</li>
<li>
<p>We need to minimize:</p>
</li>
</ul>
<p>$$
\arg \min_{\boldsymbol{f}_{1} \ldots} \sum_{k}^{n} \sum_{i, j=1}^{n} w_{i j}\left|\left|\boldsymbol{f}^{(i)}-\boldsymbol{f}^{(j)}\right|\right|^{2} \text { with: } \mathbf{F}^{\top} \mathbf{F}=\mathbf{I}
$$</p>
<ul>
<li>The solution is provided by the matrix of eigenvectors corresponding to the $k$ lowest nonzero eigenvalues of the eigenvalue problem $\mathbf{L} \boldsymbol{f}=\lambda \boldsymbol{f}$.</li>
</ul>
<h1 id="spectral-embedding-using-the-unnormalized-laplacian">Spectral embedding using the unnormalized Laplacian</h1>
<ul>
<li>
<p>Compute the eigendecomposition $\mathbf{L}=\mathbf{D}-\mathbf{A}$.</p>
</li>
<li>
<p>Select the $k$ smallest non-null eigenvalues $\lambda_{2} \leq \ldots \leq \lambda_{k+1}$</p>
</li>
<li>
<p>$\lambda_{k+2}-\lambda_{k+1}=$ eigengap.</p>
</li>
<li>
<p>We obtain the $n \times k$ matrix $\mathbf{U}=\left[\boldsymbol{u}_{2} \ldots \boldsymbol{u}_{k+1}\right]$ :</p>
</li>
</ul>
<p>$$
\mathbf{U}=\left[\begin{array}{ccc}
\boldsymbol{u}_{2}\left(v_{1}\right) &amp; \ldots &amp; \boldsymbol{u}_{k+1}\left(v_{1}\right) \\
\vdots &amp; &amp; \vdots \\
\boldsymbol{u}_{2}\left(v_{n}\right) &amp; \ldots &amp; \boldsymbol{u}_{k+1}\left(v_{n}\right)
\end{array}\right]
$$</p>
<ul>
<li>
<p>$\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{j}=\delta_{i j}$ (orthonormal vectors), hence $\mathbf{U}^{\top} \mathbf{U}=\mathbf{I}_{k}$.</p>
</li>
<li>
<p>Column $i(2 \leq i \leq k+1)$ of this matrix is a mapping on the eigenvector $\boldsymbol{u}_{i}$.</p>
</li>
</ul>
<h1 id="euclidean-l-embedding-of-the-graphs-vertices">Euclidean L-embedding of the graph&rsquo;s vertices</h1>
<ul>
<li>(Euclidean) L-embedding of a graph:</li>
</ul>
<p>$$
\mathbf{X}=\boldsymbol{\Lambda}_{k}^{-\frac{1}{2}} \mathbf{U}^{\top}=\left[\begin{array}{llll}
\boldsymbol{x}_{1} &amp; \ldots &amp; \boldsymbol{x}_{j} \ldots &amp; \boldsymbol{x}_{n}
\end{array}\right]
$$</p>
<p>The coordinates of a vertex $v_{j}$ are:</p>
<p>$$
\boldsymbol{x}_{j}=\left(\begin{array}{c}
\frac{\boldsymbol{u}_{2}\left(v_{j}\right)}{\sqrt{\lambda_{2}}} \\
\vdots \\
\frac{\boldsymbol{u}_{k+1}\left(v_{j}\right)}{\sqrt{\lambda_{k+1}}}
\end{array}\right)
$$</p>
<h1 id="justification-for-choosing-the-l-embedding">Justification for choosing the L-embedding</h1>
<p>Both</p>
<ul>
<li>
<p>the commute-time distance (CTD) and</p>
</li>
<li>
<p>the principal-component analysis of a graph (graph PCA)</p>
</li>
</ul>
<p>are two important concepts; They allow to reason &ldquo;statistically&rdquo; on a graph. They are both associated with the unnormalized Laplacian matrix.</p>
<h1 id="the-commute-time-distance">The commute-time distance</h1>
<ul>
<li>
<p>The CTD is a well known quantity in Markov chains;</p>
</li>
<li>
<p>It is the average number of (weighted) edges that it takes, starting at vertex $v_{i}$, to randomly reach vertex $v_{j}$ for the first time and go back;</p>
</li>
<li>
<p>The CTD decreases as the number of connections between the two nodes increases;</p>
</li>
<li>
<p>It captures the connectivity structure of a small graph volume rather than a single path between the two vertices - such as the shortest-path geodesic distance.</p>
</li>
<li>
<p>The CTD can be computed in closed form:</p>
</li>
</ul>
<p>$$
\operatorname{CTD}^{2}\left(v_{i}, v_{j}\right)=\operatorname{vol}(\mathcal{G})\left|\left|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right|\right|^{2}
$$</p>
<h1 id="the-graph-pca">The graph PCA</h1>
<ul>
<li>The mean (remember that $\sum_{j=1}^{n} \boldsymbol{u}<em>{i}\left(v</em>{j}\right)=0$ ):</li>
</ul>
<p>$$
\overline{\boldsymbol{x}}=\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{x}_{j}=\boldsymbol{\Lambda}_{k}^{-\frac{1}{2}}\left(\begin{array}{c}
\sum_{j=1}^{n} \boldsymbol{u}_{2}\left(v_{j}\right) \\
\vdots \\
\sum_{j=1}^{n} \boldsymbol{u}_{k+1}\left(v_{j}\right)
\end{array}\right)=\left(\begin{array}{c}
0 \\
\vdots \\
0
\end{array}\right)
$$</p>
<ul>
<li>The covariance matrix:</li>
</ul>
<p>$$
\mathbf{S}=\frac{1}{n} \sum_{j=1}^{n} \boldsymbol{x}_{j} \boldsymbol{x}_{j}^{\top}=\frac{1}{n} \mathbf{X} \mathbf{X}^{\top}=\frac{1}{n} \boldsymbol{\Lambda}_{k}^{-\frac{1}{2}} \mathbf{U}^{\top} \mathbf{U} \boldsymbol{\Lambda}_{k}^{-\frac{1}{2}}=\frac{1}{n} \boldsymbol{\Lambda}_{k}^{-1}
$$</p>
<ul>
<li>The vectors $\boldsymbol{u}_{2}, \ldots, \boldsymbol{u}_{k+1}$ are the directions of maximum variance of the graph embedding, with $\lambda_{2}^{-1} \geq \ldots \geq \lambda_{k+1}^{-1}$.</li>
</ul>
<h1 id="other-laplacian-matrices">Other Laplacian matrices</h1>
<ul>
<li>The normalized graph Laplacian (symmetric and semi-definite positive):</li>
</ul>
<p>$$
\mathbf{L}_{n}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}}=\mathbf{I}-\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}
$$</p>
<ul>
<li>The transition matrix (allows an analogy with Markov chains):</li>
</ul>
<p>$$
\mathbf{L}_{t}=\mathbf{D}^{-1} \mathbf{A}
$$</p>
<ul>
<li>The random-walk graph Laplacian:</li>
</ul>
<p>$$
\mathbf{L}_{r}=\mathbf{D}^{-1} \mathbf{L}=\mathbf{I}-\mathbf{L}_{t}
$$</p>
<ul>
<li>These matrices are similar:</li>
</ul>
<p>$$
\mathbf{L}_{r}=\mathbf{D}^{-\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} \mathbf{D}^{\frac{1}{2}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L}_{n} \mathbf{D}^{\frac{1}{2}}
$$</p>
<h1 id="eigenvalues-and-eigenvectors-of-mathrml_n-and-mathrml_r">Eigenvalues and eigenvectors of $\mathrm{L}_{n}$ and $\mathrm{L}_{r}$</h1>
<ul>
<li>$\mathbf{L}_{r} \boldsymbol{w}=\lambda \boldsymbol{w} \Longleftrightarrow \mathbf{L} \boldsymbol{w}=\lambda \mathbf{D} \boldsymbol{w}$, hence:</li>
</ul>
<p>$$
\mathbf{L}_{r}: \quad \lambda_{1}=0 ; \quad \boldsymbol{w}_{1}=\mathbf{1}
$$</p>
<ul>
<li>$\mathbf{L}_{n} \boldsymbol{v}=\lambda \boldsymbol{v}$. By virtue of the similarity transformation between the two matrices:</li>
</ul>
<p>$$
\mathbf{L}_{n}: \quad \lambda_{1}=0 \quad \boldsymbol{v}_{1}=\mathbf{D}^{\frac{1}{2}} \mathbf{1}
$$</p>
<ul>
<li>More generally, the two matrices have the same eigenvalues:</li>
</ul>
<p>$$
0=\lambda_{1} \leq \ldots \leq \lambda_{i} \ldots \leq \lambda_{n}
$$</p>
<ul>
<li>Their eigenvectors are related by:</li>
</ul>
<p>$$
\boldsymbol{v}_{i}=\mathbf{D}^{\frac{1}{2}} \boldsymbol{w}_{i}, \forall i=1 \ldots n
$$</p>
<h1 id="spectral-embedding-using-the-random-walk-laplacian-mathbfl_r">Spectral embedding using the random-walk Laplacian $\mathbf{L}_{r}$</h1>
<ul>
<li>The $n \times k$ matrix contains the first $k$ eigenvectors of $\mathbf{L}_{r}$ :</li>
</ul>
<p>$$
\mathbf{W}=\left[\begin{array}{lll}
\boldsymbol{w}_{2} &amp; \ldots &amp; \boldsymbol{w}_{k+1}
\end{array}\right]
$$</p>
<ul>
<li>It is straightforward to obtain the following expressions, where $\boldsymbol{d}$ and $\mathbf{D}$ are the degree-vector and the degree-matrix:</li>
</ul>
<p>$$
\begin{gathered}
\boldsymbol{w}_{i}^{\top} \boldsymbol{d}=0, \forall i, 2 \leq i \leq n \\
\mathbf{W}^{\top} \mathbf{D W}=\mathbf{I}_{k}
\end{gathered}
$$</p>
<ul>
<li>The isometric embedding using the random-walk Laplacian:</li>
</ul>
<p>$$
\mathbf{Y}=\mathbf{W}^{\top}=\left[\begin{array}{lll}
\boldsymbol{y}_{1} &amp; \ldots &amp; \boldsymbol{y}_{n}
\end{array}\right]
$$</p>
<h1 id="the-normalized-additive-laplacian">The normalized additive Laplacian</h1>
<ul>
<li>Some authors use the following matrix:</li>
</ul>
<p>$$
\mathbf{L}_{a}=\frac{1}{d_{\max }}\left(\mathbf{A}+d_{\max } \mathbf{I}-\mathbf{D}\right)
$$</p>
<ul>
<li>This matrix is closely related to L:</li>
</ul>
<p>$$
\mathbf{L}_{a}=\frac{1}{d_{\max }}\left(d_{\max } \mathbf{I}-\mathbf{L}\right)
$$</p>
<ul>
<li>and we have:</li>
</ul>
<p>$$
\mathbf{L}_{a} \boldsymbol{u}=\mu \boldsymbol{u} \Longleftrightarrow \mathbf{L} \boldsymbol{u}=\lambda \boldsymbol{u}, \mu=1-\frac{\lambda}{d_{\max }}
$$</p>
<h1 id="the-graph-partitioning-problem">The graph partitioning problem</h1>
<ul>
<li>The graph-cut problem: Partition the graph such that:</li>
</ul>
<p>(1) Edges between groups have very low weight, and</p>
<p>(2) Edges within a group have high weight.</p>
<p>$\operatorname{cut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} W\left(A_{i}, \bar{A}_{i}\right)$ with $W(A, B)=\sum_{i \in A, j \in B} w_{i j}$</p>
<ul>
<li>Ratio cut:</li>
</ul>
<p>$$
\operatorname{RatioCut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\left|A_{i}\right|}
$$</p>
<ul>
<li>Normalized cut:</li>
</ul>
<p>$$
\operatorname{NCut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\operatorname{vol}\left(A_{i}\right)}
$$</p>
<h1 id="what-is-spectral-clustering">What is spectral clustering?</h1>
<p>See my <a href="https://jhuow.fun/posts/2019-09-07-spectral-clustering/">Blog</a> of Spectral Clustering (in Chinese).</p>
<ul>
<li>
<p>Both ratio-cut and normalized-cut minimizations are NP-hard problems</p>
</li>
<li>
<p>Spectral clustering is a way to solve relaxed versions of these problems:</p>
</li>
</ul>
<p>(1) The smallest non-null eigenvectors of the unnormalized Laplacian approximate the RatioCut minimization criterion, and</p>
<p>(2) The smallest non-null eigenvectors of the random-walk Laplacian approximate the NCut criterion.</p>
<h1 id="spectral-clustering-using-the-random-walk-laplacian">Spectral clustering using the random-walk Laplacian</h1>
<ul>
<li>
<p>For details see (von Luxburg &lsquo;07)</p>
</li>
<li>
<p>Input: Laplacian $\mathbf{L}_{r}$ and the number $k$ of clusters to compute.</p>
</li>
<li>
<p>Output: Cluster $C_{1}, \ldots, C_{k}$.</p>
</li>
</ul>
<p>(3) Compute W formed with the first $k$ eigenvectors of the random-walk Laplacian.</p>
<p>(2) Determine the spectral embedding $\mathbf{Y}=\mathbf{W}^{\top}$</p>
<p>(3) Cluster the columns $\boldsymbol{y}_{j}, j=1, \ldots, n$ into $k$ clusters using the K-means algorithm.</p>
<h1 id="k-means-clustering">K-means clustering</h1>
<p>See Bishop'2006 (pages 424-428) for more details.</p>
<ul>
<li>
<p>What is a cluster: a group of points whose inter-point distance are small compared to distances to points outside the cluster.</p>
</li>
<li>
<p>Cluster centers: $\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k}$.</p>
</li>
<li>
<p>Goal: find an assignment of points to clusters as well as a set of vectors $\mu_{i}$.</p>
</li>
<li>
<p>Notations: For each point $\boldsymbol{y}_{j}$ there is a binary indicator variable $r_{j i} \in{0,1}$.</p>
</li>
<li>
<p>Objective: minimize the following distorsion measure:</p>
</li>
</ul>
<p>$$
J=\sum_{j=1}^{n} \sum_{i=1}^{k} r_{j i}\left|\left|\boldsymbol{y}_{j}-\boldsymbol{\mu}_{i}\right|\right|^{2}
$$</p>
<h1 id="the-k-means-algorithm">The K-means algorithm</h1>
<p>(1) Initialization: Choose initial values for $\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k}$.</p>
<p>(2) First step: Assign the $j$-th point to the closest cluster center:</p>
<p>$$
r_{j i}= \begin{cases}1 &amp; \text { if } i=\arg \min_{l}\left|\left|\boldsymbol{y}_{j}-\mu_{l}\right|\right|^{2} \\ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>(3) Second Step: Minimize $J$ to estimate the cluster centers:</p>
<p>$$
\boldsymbol{\mu}_{i}=\frac{\sum_{j=1}^{n} r_{j i} \boldsymbol{y}_{j}}{\sum_{j=1}^{n} r_{j i}}
$$</p>
<p>(4) Convergence: Repeat until no more change in the assignments.</p>
<h1 id="reference">Reference</h1>
<p><a href="https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf">https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf</a></p>
<p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Topology-Imbalance Learning for Semi-Supervised Node Classification》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2022-04-02-tinl/</link>
      <pubDate>Sat, 02 Apr 2022 10:34:01 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2022-04-02-tinl/</guid>
      <description>Paper
Introduction 类别不均衡（Class Imbalance）是真实场景中非常常见的问题。一般在我们提及类别不均衡时，默认指的是数量不均衡：即不同类中训练样本数量的不一致带来的模型于不同类别学习能力的差异，由此引起的一个严重问题是模型的决策边界会主要由数量多的类来决定 。
但是在图结构中，不同类别的训练样本不仅有在数量上的差异，也有在位置结构上的差异.这就使得图上的类别不均衡问题有了一个独特的来源：拓扑不均衡。这个工作最主要的动机就是研究拓扑不均衡的特点，危害以及解决方法，希望能够引起社区对拓扑不均衡问题的重视。
本文提出Topology-Imbalance Node Representation Learning （TINL）, 主要关注拓扑不平衡导致的决策边界漂移。所谓拓扑不平衡值得是， labeled nodes的位置如果位于拓扑中的决策边界，那么会传播错误的影响。 如上图所示，颜色和色调分别表示节点从labeled node接收到的influence类型和强度，节点R1位于两类节点的拓扑边界，第一张图可以看出，两个$\mathbf{x}$节点面临influence conflict问题，两个$\mathbf{Y}$节点由于远离R2，面临影响力不足的问题。也就是，如果决策便捷有labeled node（如R1）, 那么他的影响力很容易传播给另一个类的边界unlabeled节点，导致影响力冲突，从而分类错误。 而冲突较小的labeled node更可能位于类的拓扑中心（如R2）,顾增加其权重，是的它在训练过程中发挥更大作用。
Understanding Topology Imbalance via Label Propagation Label Propagation中，labels从labeled node延边传播， 看做label从labeled node开始的随机游走过程。LP最终收敛状态可以认为每个节点的soft-labels: $$ \boldsymbol{Y}=\alpha\left(\boldsymbol{I}-(1-\alpha) \boldsymbol{A}^{\prime}\right)^{-1} \boldsymbol{Y}^{0} $$ 其中$\boldsymbol{A}^{\prime}=\boldsymbol{D}^{-\frac{1}{2}} A D^{-\frac{1}{2}}$，其实就是PageRank的极限分布， $\boldsymbol{Y}^{0}$为每个节点的初始one-hot label。 第$i$个节点的预测结果为$\boldsymbol{q}_{i}=\arg \max _{j} \boldsymbol{Y}_{i j}$，每个节点的预测向量反映了每个节点主要受哪个类的影响。图(a)反映了GCN与LP的预测一致性，所以LP的节点影响力边界可以作为GNN的决策边界。理想状态下，labeled node的影响力边界应与真实类边界一致，例如红色的labeled node 在LP下所传播的影响力范围，应与所有红色node的范围一致。但是如图(b)所示，蓝色的labeled node如果较多位于真实类边界，这些位于边界的节点也会传播影响力，从而导致位于边界的真是红色节点被预测为蓝色，预测边界向红色类偏移。
Measuring Topology Imbalance by Influence Conflict 可以看出，位于决策边界的labeled node 会不可避免的将影响力传播到其他类节点，因此需要衡量labeled node与其所属类的相对拓扑位置（位于类边缘还是中心）。由于Homophily， 位于类边缘的节点也具有和其邻居相似的性质，因此利用邻域特征差别来判断labeled node是否位于边缘是不可靠的。因此本文利用整个图中的节点影响力冲突，提出基于冲突检测的拓扑相对位置Conflict Detection-based Topology Relative Location metric (Totoro).
Personalized PageRank矩阵定义为： $$ \boldsymbol{P} = \alpha\left(\boldsymbol{I}-(1-\alpha) \boldsymbol{A}^{\prime}\right)^{-1} $$ $\boldsymbol{P}_{ij} = \boldsymbol{P}(j \to^\infty i)$， 可以用来反映拓扑中节点$i$对节点$j$的影响力（随机游走越有可能到达的两个节点，在拓扑中的越能相互影响）。</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/abs/2110.04099v1">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>类别不均衡（Class Imbalance）是真实场景中非常常见的问题。一般在我们提及类别不均衡时，默认指的是<strong>数量不均衡</strong>：即不同类中训练样本数量的不一致带来的模型于不同类别学习能力的差异，由此引起的一个严重问题是模型的决策边界会主要由数量多的类来决定 。</p>
<p>但是在图结构中，<strong>不同类别的训练样本不仅有在数量上的差异，也有在位置结构上的差异</strong>.这就使得图上的类别不均衡问题有了一个独特的来源：<strong>拓扑不均衡</strong>。这个工作最主要的动机就是研究拓扑不均衡的特点，危害以及解决方法，希望能够引起社区对拓扑不均衡问题的重视。</p>
<p><img loading="lazy" src="/posts/2022-04-02-tnil/1.png" alt="1"  />
</p>
<p>本文提出Topology-Imbalance Node Representation Learning （TINL）, 主要关注拓扑不平衡导致的决策边界漂移。所谓拓扑不平衡值得是， labeled nodes的位置如果位于拓扑中的决策边界，那么会传播错误的影响。 如上图所示，颜色和色调分别表示节点从labeled node接收到的influence类型和强度，节点R1位于两类节点的拓扑边界，第一张图可以看出，两个$\mathbf{x}$节点面临influence conflict问题，两个$\mathbf{Y}$节点由于远离R2，面临影响力不足的问题。也就是，如果决策便捷有labeled node（如R1）, 那么他的影响力很容易传播给另一个类的边界unlabeled节点，导致影响力冲突，从而分类错误。 而冲突较小的labeled node更可能位于类的拓扑中心（如R2）,顾增加其权重，是的它在训练过程中发挥更大作用。</p>
<h1 id="understanding-topology-imbalance-via-label-propagation">Understanding Topology Imbalance via Label Propagation</h1>
<p><img loading="lazy" src="/posts/2022-04-02-tnil/2.png" alt="1"  />
</p>
<p>Label Propagation中，labels从labeled node延边传播， 看做label从labeled node开始的随机游走过程。LP最终收敛状态可以认为每个节点的soft-labels:
$$
\boldsymbol{Y}=\alpha\left(\boldsymbol{I}-(1-\alpha) \boldsymbol{A}^{\prime}\right)^{-1} \boldsymbol{Y}^{0}
$$
其中$\boldsymbol{A}^{\prime}=\boldsymbol{D}^{-\frac{1}{2}} A D^{-\frac{1}{2}}$，其实就是PageRank的极限分布， $\boldsymbol{Y}^{0}$为每个节点的初始one-hot label。 第$i$个节点的预测结果为$\boldsymbol{q}_{i}=\arg \max _{j} \boldsymbol{Y}_{i j}$，每个节点的预测向量反映了每个节点主要受哪个类的影响。图(a)反映了GCN与LP的预测一致性，所以LP的节点影响力边界可以作为GNN的决策边界。理想状态下，labeled node的影响力边界应与真实类边界一致，例如红色的labeled node 在LP下所传播的影响力范围，应与所有红色node的范围一致。但是如图(b)所示，蓝色的labeled node如果较多位于真实类边界，这些位于边界的节点也会传播影响力，从而导致位于边界的真是红色节点被预测为蓝色，预测边界向红色类偏移。</p>
<h1 id="measuring-topology-imbalance-by-influence-conflict">Measuring Topology Imbalance by Influence Conflict</h1>
<p>可以看出，位于决策边界的labeled node 会不可避免的将影响力传播到其他类节点，因此需要衡量labeled node与其所属类的相对拓扑位置（位于类边缘还是中心）。由于Homophily， 位于类边缘的节点也具有和其邻居相似的性质，因此利用邻域特征差别来判断labeled node是否位于边缘是不可靠的。因此本文利用整个图中的节点影响力冲突，提出基于冲突检测的拓扑相对位置Conflict Detection-based Topology Relative Location metric (Totoro).</p>
<p>Personalized PageRank矩阵定义为：
$$
\boldsymbol{P} = \alpha\left(\boldsymbol{I}-(1-\alpha) \boldsymbol{A}^{\prime}\right)^{-1}
$$
$\boldsymbol{P}_{ij} = \boldsymbol{P}(j \to^\infty i)$， 可以用来反映拓扑中节点$i$对节点$j$的影响力（随机游走越有可能到达的两个节点，在拓扑中的越能相互影响）。</p>
<p><strong>Node influence conflict denotes topological position.</strong> $\boldsymbol{P}$可以看做每个节点向外施加影响力的分布。 如果一个labeled node $v$ 在周围子图中受到了来自其他类中的labeled node的异质影响，而$v$本身也具有较大的影响力，那么可以认为$v$具有较大影响力冲突，他更可能位于所在类的拓扑边界。</p>
<p>基于上述假设，本文将 从节点$v$开始在图上随机游走时， 节点$v$与其他类的labeled nodes之间的影响力冲突的期望作为节点$v$与其所在类的类中心的接近程度的度量。labeled node $v$ 的Totoro值定义如下：
$$
\boldsymbol{T}_{v}=\mathbb{E}_{x \sim \boldsymbol{P}_{v, :}}\left[\sum_{j \in[1, k], j \neq \boldsymbol{y}_{v}} \frac{1}{\left|\mathcal{C}_{j}\right|} \sum_{i \in \mathcal{C}_{j}} \boldsymbol{P}_{i, x}\right]
$$
其中， $\mathbb{E}_{x \sim \boldsymbol{P}_{v, :}}$： $x$节点受$v$的影响程度，$\sum_{j \in[1, k], j \neq \boldsymbol{y}_{v}}$表示其他所有类（不包括$v$所在的类）。 $\frac{1}{\left|\mathcal{C}_{j}\right|} \sum_{i \in \mathcal{C}_{j}} \boldsymbol{P}_{i, x}$表示类$\mathcal{C}_{j}$中的labeled node对$x$的平均影响。 $\boldsymbol{T}_{v}$越大，表示labeled node $v$对$x$的影响力很大，而且其他类的labeled node 对$x$的影响也很大，那么可以认为$v$越接近类边界。</p>
<p>整个数据集的conflict可以表示为所有labeled node 的Totoro value之和：$\sum_{v \in \mathcal{L}} \boldsymbol{T}_{v}$</p>
<h1 id="node-re-weighting">Node Re-weighting</h1>
<h2 id="preliminary">Preliminary</h2>
<p>余弦退火：
$$
\eta_{t}=\eta_{\min }^{i}+\frac{1}{2}\left(\eta_{\max }^{i}-\eta_{\min }^{i}\right)\left(1+\cos \left(\frac{T_{\text {cur }}}{T_{i}} \pi\right)\right)
$$
$\eta_{\min }$: 最小学习率</p>
<p>$\eta_{\max }$: 最大学习率</p>
<p>$T_{\text {cur }}$: 当前执行多少个epoch</p>
<p>$i$: 第$i$次迭代</p>
<p><img loading="lazy" src="/posts/2022-04-02-tnil/3.png#center" alt="1"  />
</p>
<h2 id="renode">ReNode</h2>
<p>本文提出模型无关的训练权重re-weight 机制：<strong>ReNode</strong>.</p>
<p>本文基于余弦退货算法来为训练节点（labeled nodes）加权：
$$
\boldsymbol{w}_{v}=w_{\min }+\frac{1}{2}\left(w_{\max }-w_{\min }\right)\left(1+\cos \left(\frac{\operatorname{Rank}\left(\boldsymbol{T}_{v}\right)}{|\mathcal{L}|} \pi\right)\right), \quad v \in \mathcal{L}
$$
上式中$\boldsymbol{T}_{v}$越大(越接近决策边界)，在所有labeled node $v \in \mathcal{L}$的排名越高，$\operatorname{Rank}\left(\boldsymbol{T}_{v}\right)$越大，$\boldsymbol{w}_{v}$越小，越接近$w_{\min }$。</p>
<p>最终，对于一个quantity-balanced，topology-imbalanced (class labeled node 数量是平衡的，但拓扑不平衡) node classification task, the training loss $L_T$ is computed by:
$$
L_{T}=-\frac{1}{|\mathcal{L}|} \sum_{v \in \mathcal{L}} \boldsymbol{w}_{v} \sum_{c=1}^{k} \boldsymbol{y}_{v}^{* c} \log \boldsymbol{g}_{v}^{c}, \quad \boldsymbol{g}=\operatorname{softmax}(\mathcal{F}(\boldsymbol{X}, \boldsymbol{A}, \boldsymbol{\theta}))
$$
其中$\mathcal{F}$是任意GNN encoder,$g_i$为GNN对第$i$个节点的output。$\boldsymbol{y}_{v}^{* c} \log \boldsymbol{g}_{v}^{c}$为cross-entropy。 对于每个training labeled node，计算它的CE loss时，用这个节点的权重为loss加权，说明越靠近决策边界的节点，他的损失权重尽可能小，意味着model倾向于把它当做一个unlabeled node，它的损失对于总损失贡献较小。</p>
<h2 id="renode-to-jointly-handle-tinl-and-qinl">ReNode to Jointly Handle TINL and QINL</h2>
<p>若要同时处理数量不平衡且拓扑不平衡问题， loss定义如下：
$$
L_{Q}=-\frac{1}{|\mathcal{L}|} \sum_{v \in \mathcal{L}} \boldsymbol{w}_{v} \frac{|\overline{\mathcal{C}}|}{\left|\mathcal{C}_{j}\right|} \sum_{c=1}^{k} \boldsymbol{y}_{v}^{* c} \log \boldsymbol{g}_{v}^{c}
$$
与$L_T$的不同就是多了对类的加权，若labeled node所在的类 training node较少，那么增加权重。同时，接近拓扑边界的节点权重降低。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICML2020 《Robust Graph Representation Learning via Neural Sparsification》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/neuralsparse/</link>
      <pubDate>Fri, 01 Apr 2022 10:55:44 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/neuralsparse/</guid>
      <description>Paper
Introduction GNN的neighborhood aggregation中会引入邻域中任务无关的邻居，所以要移除图中有大量任务无关的边，顾本文提出NeuralSparse来解决该问题。
在构造数据集时，两个节点连接的动机可能与拿到这张图后要进行的下游任务无关，例如下游任务是节点分类，那么和这个任务相关的连接应是同类节点间产生边。但是构造图数据集是的两个节点连接的动机可能是特征相似的节点（不一定同类）。
下图给出一个例子，Blue 和 Red节点分别采样自两个独立的二维Gaussian distribution, 图1(a)显示两类节点的原始features有大量重合。对于每个节点，随机抽取10个其他节点作为他的邻居，这样生成的图（图1(b) )中的边和node label没有任何关系，即所有边都与节点分类任务无关。在这个图上做GCN得到(图1(b)下方)的node embedding，可以看到GCN学到的node embedding在任务无关的边上，无法区分两类节点。 图1(c)是DropEdge学到的node embedding，图1(d)为本文的NeuralSparse学到的node embedding。
Present work：根据监督信号来选择任务相关的边。 由sparsification network 和GNN两个模块组成. sparsification network旨在参数化稀疏过程，即在给定预算下， 为每个节点选择任务相关的一节邻居。在训练阶段，优化稀疏策略，即训练一个可以将图稀疏化的网络。在测试阶段，将测试图输入网络，得到一个稀疏化的图。
NeuralSparse Theoretical justification 首先从统计学习角度建模问题。将预测任务定义为$P(Y|G)$, 其中$Y$是预测目标，$G$是输入图。本文利用稀疏化子图来移除任务无关的边, 问题形式化为: $$ P(Y \mid G) \approx \sum_{g \in \mathbb{S}_{G}} P(Y \mid g) P(g \mid G) $$ $g$是一个稀疏化子图，$\mathbb{S}_{G}$为$G$的稀疏化子图集合。 $P(Y \mid g)$ 为给定一个稀疏化子图$g$，用$g$过GNN后预测为$Y$的概率。 $$ \sum_{g \in \mathbb{S}_{G}} P(Y \mid g) P(g \mid G) \approx \sum_{g \in \mathbb{S}_{G}} Q_{\theta}(Y \mid g) Q_{\phi}(g \mid G) $$ 用函数来近似(计算) 分布， 即给定$g$ 预测为$Y$的概率$ P(Y \mid g)$定义为一个参数为$\theta$的函数$Q_{\theta}(Y \mid g)$, 从$G$中获得子图$g$的概率$P(g \mid G)$定义为一个参数为$\phi$的函数$Q_{\phi}(g \mid G)$。</description>
      <content:encoded><![CDATA[<p><a href="https://proceedings.mlr.press/v119/zheng20d.html">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>GNN的neighborhood aggregation中会引入邻域中任务无关的邻居，所以要移除图中有大量任务无关的边，顾本文提出NeuralSparse来解决该问题。</p>
<p>在构造数据集时，两个节点连接的动机可能与拿到这张图后要进行的下游任务无关，例如下游任务是节点分类，那么和这个任务相关的连接应是同类节点间产生边。但是构造图数据集是的两个节点连接的动机可能是特征相似的节点（不一定同类）。</p>
<p>下图给出一个例子，Blue 和 Red节点分别采样自两个独立的二维Gaussian distribution, 图1(a)显示两类节点的原始features有大量重合。对于每个节点，随机抽取10个其他节点作为他的邻居，这样生成的图（图1(b) )中的边和node label没有任何关系，即所有边都与节点分类任务无关。在这个图上做GCN得到(图1(b)下方)的node embedding，可以看到GCN学到的node embedding在任务无关的边上，无法区分两类节点。 图1(c)是DropEdge学到的node embedding，图1(d)为本文的NeuralSparse学到的node embedding。</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/1.png#center" alt=""  />
</p>
<p><strong>Present work</strong>：根据监督信号来选择任务相关的边。 由sparsification network 和GNN两个模块组成. sparsification network旨在参数化稀疏过程，即在给定预算下， 为每个节点选择任务相关的一节邻居。在训练阶段，优化稀疏策略，即训练一个可以将图稀疏化的网络。在测试阶段，将测试图输入网络，得到一个稀疏化的图。</p>
<h1 id="neuralsparse">NeuralSparse</h1>
<h2 id="theoretical-justification">Theoretical justification</h2>
<p>首先从统计学习角度建模问题。将预测任务定义为$P(Y|G)$, 其中$Y$是预测目标，$G$是输入图。本文利用稀疏化子图来移除任务无关的边, 问题形式化为:
$$
P(Y \mid G) \approx \sum_{g \in \mathbb{S}_{G}} P(Y \mid g) P(g \mid G)
$$
$g$是一个稀疏化子图，$\mathbb{S}_{G}$为$G$的稀疏化子图集合。 $P(Y \mid g)$ 为给定一个稀疏化子图$g$，用$g$过GNN后预测为$Y$的概率。
$$
\sum_{g \in \mathbb{S}_{G}} P(Y \mid g) P(g \mid G) \approx \sum_{g \in \mathbb{S}_{G}} Q_{\theta}(Y \mid g) Q_{\phi}(g \mid G)
$$
用函数来近似(计算) 分布， 即给定$g$ 预测为$Y$的概率$ P(Y \mid g)$定义为一个参数为$\theta$的函数$Q_{\theta}(Y \mid g)$, 从$G$中获得子图$g$的概率$P(g \mid G)$定义为一个参数为$\phi$的函数$Q_{\phi}(g \mid G)$。</p>
<p>$Q_{\phi}(g \mid G)$表示输入$G$, 生成一个子图分布，从分布中采样得到子图$g$的概率， 为了使得分布中采样这个过程可微，本文采用reparameterization tricks,使得：
$$
\sum_{g \in \mathbb{S}_{G}} Q_{\theta}(Y \mid g) Q_{\phi}(g \mid G) \propto \sum_{g^{\prime} \sim Q_{\phi}(g \mid G)} Q_{\theta}\left(Y \mid g^{\prime}\right)
$$
$g^{\prime} \sim Q_{\phi}(g \mid G)$表示给定图$G$，生成一个子图分布（每种子图的采样概率）。$\sum_{g^{\prime} \sim Q_{\phi}(g \mid G)} Q_{\theta}\left(Y \mid g^{\prime}\right)$: 表示从子图分布中采样的子图来预测label $Y$的概率。</p>
<p><strong>Goal</strong>: 1. 找到合适的$Q_{\phi}(g \mid G)$， 使得它生成的分布可以采样到最佳的稀疏化子图 ， 即通过优化$\phi$使得$Q_{\phi}(g \mid G)$生成的子图分布中采样到最佳子图的概率是最大的。 2. 找到合适的$Q_{\theta}(Y \mid g)$表示优化GNN，使得采样出的$g$可以最好的预测label。</p>
<h2 id="architecture">Architecture</h2>
<p>包含两个模块： sparsification network 和GNNs.</p>
<h3 id="sparsification-network">Sparsification Network</h3>
<p>目的为输入图生成稀疏化子图，即为每个节点的边生成一个分布，表示边被采样的概率，然后为节点采样边，从而实现采样的系数子图。首先定义所有候选子图。</p>
<p><strong>k-neighbor subgraphs</strong>: 给定输入图$G$，一个$k$-neighbor subgraph和图$G$有相同的节点集，每个节点可以从他的邻居中选择不多于$k$条边。</p>
<p>理由： 超参数$k$可以用来调整任务相关的图数据量。如果$k$是低估的，那么GNN处理的任务相关数据不足，如果$k$被高估，那么下游GNN会拟合更多无关数据。</p>
<p><strong>Sampling k-neighbor subgraphs</strong>：给定$k$和一个图$G=(V, E, \mathbf{A})$, 以节点$u$为例，令$\mathbb{N}_u$为$u$的一阶邻居。</p>
<ol>
<li>$v \sim f_{\phi}\left(V(u), V\left(\mathbb{N}_{u}\right), \mathbf{A}(u)\right)$, 其中，$f_{\phi}(\cdot)$是一个函数，输入为节点$u$的节点属性$V(u)$，节点$u$的邻居属性$V\left(\mathbb{N}_{u}\right)$, 和$u$的边属性$\mathbf{A}(u)$。输出为$u$的邻居分布，$v$从该邻居分布中采样。 比如当前$u$有3个节点，$f_\phi$生成这三个节点的采样分布[0.1, 0.3, 0.6], 那么从这个分布中随机采样一个节点$v$作为$u$的重构邻居。</li>
<li>采样出的节点$v$作为$u$的重构邻居，即$E(u,v)$作为边保留下来。</li>
<li>重复上述过程$k$次，得到$u$的$k$个重构邻居。</li>
</ol>
<p>注意，上述采样过程为不放回过程（sampling without replacement），即邻居只能被选择一次， $f_{\phi}(\cdot)$对所有节点共享，即一个$f_{\phi}(\cdot)$，每个节点都输入它来获得邻居采样分布。</p>
<p><strong>Making samples differentiable</strong> 为了使样本的采样过程可微，本文采用基于Gumbel-Softmax的NN来实现采样函数$f_{\phi}(\cdot)$。</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/2.png#center" alt=""  />
</p>
<p>Gumbel-Softmax [1,2] 是一种reparameterization trick，用于以可微的方式生成离散样本。参数$\tau$越小，生成的连续向量越sharp，越接近one-hot。</p>
<p>以节点$u$为例，$f_\phi(\cdot)$如下：</p>
<ol>
<li>
<p>$\forall v \in \mathbb{N}_u$：
$$
z_{u, v}=\operatorname{MLP}_{\phi}(V(u), V(v), \mathbf{A}(u, v))
$$</p>
</li>
<li>
<p>$\forall v \in \mathbb{N}_u$，使用softmax来计算边被采样的概率：
$$
\pi_{u, v}=\frac{\exp \left(z_{u, v}\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(z_{u, w}\right)}
$$</p>
</li>
<li>
<p>使用Gumbel-Softmax来生成可微样本：
$$
x_{u, v}=\frac{\exp \left(\left(\log \left(\pi_{u, v}\right)+\epsilon_{v}\right) / \tau\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(\left(\log \left(\pi_{u, w}\right)+\epsilon_{w}\right) / \tau\right)}
$$</p>
</li>
</ol>
<p>其中， $x_{u, v}$是一个scalar，$\epsilon_{v}=-\log (-\log (s))$，$s$从$\mathrm{Uniform}(0,1)$中采样， $\tau$是一个temperature超参数，$\tau$越小，分布$x_u$越接近one-hot。</p>
<h3 id="algorithm">Algorithm</h3>
<p>算法如下：</p>
<p><img loading="lazy" src="/posts/2022-04-01-Neuralsparse/3.png#center" alt=""  />
</p>
<p>对所有节点$u \in \mathbb{V}$逐个稀疏化： 先遍历$u$的每个邻居$v$, 对于每个$v$ 通过公式$z_{u, v}=\operatorname{MLP}_{\phi}(V(u), V(v), \mathbf{A}(u, v))$ 计算它对于$u$的分数， 然后将$u$的所有邻居$v$的分数用softmax变成概率。</p>
<p>为$u$做$k$次采样， 每次采样过程如下： 每次采样遍历$u$的所有邻居$v$，根据$x_{u, v}=\frac{\exp \left(\left(\log \left(\pi_{u, v}\right)+\epsilon_{v}\right) / \tau\right)}{\sum_{w \in \mathbb{N}_{u}} \exp \left(\left(\log \left(\pi_{u, w}\right)+\epsilon_{w}\right) / \tau\right)}$计算$u$到每个邻居的$x_{u,v}$, 每次迭代产生一个向量$\left[x_{u, v}\right]$,用来表示采样出来的边，经过$k$次迭代，产生$k$个表示边的向量，$\tau$越小，每个向量越接近one-hot。 每个向量$\left[x_{u, v}\right]_{v \in \mathbb{N}_u}$表示$u$的一个采样邻居，每个$u$有$k$个这样的邻居表示向量，那么网络中的所有边$\mathbb{H}$就有$|\mathbb{V}|k$个这样的向量，每个向量表示要保留的一条边，得到稀疏化子图，反向传播时，先更新GNN参数，然后直接对$f_\phi$的参数求梯度, 如上图所示。</p>
<h1 id="reference">Reference</h1>
<p>[1] Jang, E., Gu, S., and Poole, B. Categorical reparameteriza- tion with gumbel-softmax. In ICLR, 2017.</p>
<p>[2] Maddison, C. J., Mnih, A., and Teh, Y. W. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Monte Carlo Tree Search</title>
      <link>https://JhuoW.github.io/posts/monte-carlo-tree-search/</link>
      <pubDate>Wed, 30 Mar 2022 18:09:58 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/monte-carlo-tree-search/</guid>
      <description>单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。
在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?
多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。
  利用（Exploitation）： 保证在过去决策中得到最佳回报
  探索（Exploration）：寄希望在未来能够得到更大的汇报
  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。
但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。
悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机
$I_t$: $t$时刻选择的赌博机
$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励
$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward
$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。
$R_n$越大，就代表$n$次决策的结果越差。
上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡
在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}.</description>
      <content:encoded><![CDATA[<h1 id="单一状态monte-carlo规划多臂赌博机multi-armed-bandits">单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits）</h1>
<p>单一状态$s$, $k$种action（$k$个摇臂）。</p>
<p>在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?</p>
<p>多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。</p>
<ul>
<li>
<p>利用（Exploitation）： 保证在过去决策中得到最佳回报</p>
</li>
<li>
<p>探索（Exploration）：寄希望在未来能够得到更大的汇报</p>
</li>
</ul>
<p>例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。</p>
<p>但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。</p>
<h2 id="悔值函数">悔值函数</h2>
<p>如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数：
$$
R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t}
$$
$i$: 第$i$个赌博机</p>
<p>$I_t$: $t$时刻选择的赌博机</p>
<p>$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励</p>
<p>$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward</p>
<p>$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。</p>
<p>$R_n$越大，就代表$n$次决策的结果越差。</p>
<h2 id="上置信区间upper-confidence-bound-ucb">上置信区间（Upper Confidence Bound, UCB）</h2>
<p>UCB旨在探索和利用间去的平衡</p>
<p>在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机：
$$
I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}.
$$
其中$I_{t}$为$t$时刻要摇的赌博机，</p>
<p>$\overline{X_{i, T_{i}(t-1)}}$要尽可能大：为$t$时刻之前赌博机$i$的平均reward 要尽可能大，</p>
<p>$C_{t-1, T_{i}(t-1)}$要尽可能小：$t$时刻之前$i$出现的次数要尽可能少</p>
<p>其中$C_{t,T_i(t)}$的取值定义如下：
$$
C_{t,T_i(t)}=\sqrt{\frac{2 \operatorname{In} t}{T_i(t)}}
$$
其中$T_i(t)$表示 $t$时刻以前（包括$t$），选到赌博机$i$的次数总和。</p>
<p>若赌博机在$t$时刻之前（包括$t$）时刻摇动的次数越少，$C_{t,T_i(t)}$越大</p>
<p>选出的$I_t$要满足在$t$之前的平均reward尽可能大（利用）， 且在$t$之前出现次数尽可能少（探索）。</p>
<p>也就是说，在第时刻，UCB算法一般会选择具有如下最大值的第$j$个赌博机：
$$
\begin{aligned}
U C B&amp;=\bar{X}_{j}+\sqrt{\frac{2 \operatorname{Inn}}{n_{j}}} \text { 或者 } U C B=\bar{X}_{j}+C \times \sqrt{\frac{2 \operatorname{In} n}{n_{j}}} \\
I_t &amp;= \mathrm{argmax}_j UCB(j)
\end{aligned}
$$
其中$\bar{X}_{j}$是第$j$个赌博机在过去时间内所获得的平均奖赏值，$n_j$是在过去时间内拉动第$j$个赌博机臂膀的总次数，$n$是过去时间内拉动所有赌博机臂膀的总次数。$C$是一个平衡因子，其决定着在选择时偏重探索还是利用。</p>
<p>从这里可看出UCB算法如何在探索-利用(exploration-exploitation)之间寻找平衡：既需要拉动在过去时间内获得最大平均奖赏的赌博机，又希望去选择拉动臂膀次数最少的赌博机。</p>
<h1 id="monte-carlo-tree-search">Monte Carlo Tree Search</h1>
<p>MCTS has four step:</p>
<ul>
<li>Selection 选择</li>
<li>Expansion 拓展</li>
<li>Simulation（rollout) 模拟</li>
<li>Backpropagation 回溯</li>
</ul>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/1.png#center" alt=""  />
</p>
<h2 id="选择">选择</h2>
<p>从根节点R开始，向下递归选择子节点，直至选择一个叶子节点L。
具体来说，通常用UCBl(Upper Confidence Bound,上限置信区间)选择最具“潜力”的后续节点：
$$
U C B=\bar{X}_{j}+\sqrt{\frac{2 \operatorname{In} n}{n_{j}}}
$$</p>
<h2 id="拓展">拓展</h2>
<p>如果L不是一个终止节点（即博弈游戏不)，则随机创建其后的一个未被访问节点，选择该节点作为后续子节点C。</p>
<h2 id="模拟">模拟</h2>
<p>从节点C出发，对游戏进行模拟，直到博弈游戏结束。</p>
<h2 id="反向传播">反向传播</h2>
<p>用模拟所得结果来回溯更新导致这个结果的每个节点中获胜次数和访问次数。</p>
<p><strong>包含两种策略学习机制：</strong></p>
<p><strong>搜索树策略</strong>：从已有的搜索树中选择或创建一个叶子结点（即蒙特卡洛中选择和拓展两个步骤).搜索树策略需要在利用和探索之间保持平衡。</p>
<p><strong>模拟策略</strong>：从非叶子结点出发模拟游戏，得到游戏仿真结果。</p>
<h1 id="例子-围棋">例子： 围棋</h1>
<ul>
<li>
<p>以围棋为例，假设根节点是执黑棋方。</p>
</li>
<li>
<p>图中每一个节点都代表一个局面，每一个局面记录两个值A/B</p>
</li>
</ul>
<p>A: 该局面被访问中黑棋胜利次数。对于黑棋表示己方胜利次数，对于白棋表示己方失败次数（对方胜利次数)；</p>
<p>B: 该局面被访问的总次数</p>
<p>初始状态：<img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>根节点（12/21）是当前局面，该局面被访问过21次，其中黑棋访问该局面后最终获胜12次。 黑执棋方遇到该局面（根节点局面）开始选择下一步action。</p>
<h3 id="选择-1">选择</h3>
<p>黑执棋方遇到（12/21）这个局面时有3个有效action, 黑执行这三个action会得到3种局面（7/10）,（5/8）, (0/3). 黑执棋方要选择其中一个action，是的棋局变为这三种中的一种，那么要如何选择action呢？我们先分别计算UCB1：</p>
<p>左一： 7/10对应的局面Reward为：
$$
\frac{7}{10} + \sqrt{\frac{\log (21)}{10}} = 1.252
$$
3中局面共21次被模拟到，其中，该局面被访问过10次（探索， 第二项）， 黑棋遇到该局面后最终获胜7次（利用， 第一项）。</p>
<p>左二：（5/8）对应局面Reward:
$$
\frac{5}{8} + \sqrt{\frac{\log(21)}{8}} = 1.243
$$
左三： （0/3）对应局面Reward:
$$
\frac{0}{3} + \sqrt{\frac{\log(21)}{3}} = 1.007
$$
由此可见，黑棋选择会导致局面（7/10）的action进行走琪。</p>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白棋遇到局面（7/10）时，有两个有效的action, 分别会导致局面A/B = 2/4和5/6, A为白棋访问该局面后失败的次数，所以白棋访问该局面最终胜利的次数应该是$B-A$：</p>
<p>左一： (2/4)对应的局面Reward (白棋尽可能获胜)为：
$$
(1-\frac{2}{4}) + \sqrt{\frac{\log(21)}{4}}=1.372
$$
左二：  (5/6)对应的局面Reward为：
$$
(1-\frac{5}{6}) + \sqrt{\frac{\log(21)}{4}}=0.879
$$
因此白棋会选择（2/4）局面</p>
<p>即<strong>每一步都寻找最佳应对方式，来最终评估更节点局面的好坏</strong></p>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白色执棋方选择action后到达局面（2/4）, 黑棋执棋方面对该局面，有两种action可选，分别会到达（1/3）和（1/1）， 分别计算UCB1 得：</p>
<p>左一： (1/3)对应reward 为：
$$
\frac{1}{3} + \sqrt{\frac{\log (21)}{3}} = 1.341
$$
左二：(1/1)对应reward为：
$$
\frac{1}{1} + \sqrt{\frac{\log (21)}{1}} = 2.745
$$
则黑棋选择action 使得局面变为(1/1)。右图中可见(1/1)为叶子节点， 白棋面对该局面暂时无候选局面（action）,则进入下一个步骤，拓展。</p>
<h3 id="拓展-1">拓展</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白棋执棋方面对局面(1/1),因为已经是叶子节点， 所以(1/1)会随机产生一个未被访问过的新节点，初始化为(0/0),接着在该节点下进行模拟。 进入第三部， 模拟。</p>
<h3 id="模拟-1">模拟</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq3.png#center" alt=""  />
</p>
<p>黑棋执棋方面局面（0/0），开始进行游戏仿真， 假设经过一些列仿真后，最终白棋获胜，即从更节点到最终模拟结束的叶子节点黑棋失败了。 进入第四部，回溯</p>
<h3 id="回溯">回溯</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq4.png#center" alt=""  />
</p>
<p>根据仿真结果来更新该仿真路径上的每个节点的A/B值。 因为该次模拟最终白棋获胜，所以向上回溯路径上的所有父节点，父节点的A不变，B+1。 若最终黑棋获胜，则父节点的A+1, B+1。</p>
<p><strong>在有限时间里， MCTS会不断重复4个步骤，所有节点的A,B 值会不断变化</strong></p>
<p><strong>到某个局面（节点）后如何下棋（action）, 会选择所有有效Action中UCB1值最大的节点，作为下棋的下一步。</strong></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Neo-GNNs:Neighborhood Overlap-aware Graph Neural Networks for Link Prediction》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/neo-gnns/</link>
      <pubDate>Wed, 30 Mar 2022 13:51:57 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/neo-gnns/</guid>
      <description>Paper
Introduction 由于GNNs过度强调平滑的节点特征而不是图结构，使得在Link Prediction任务上的表现甚至弱于heuristic方法。平滑邻居难以反映邻域结构的相关性以及其他拓扑特征。 Structural information, (e.g., overlapped neighborhoods, degrees, and shortest path), is crucial for link prediction whereas GNNs heavily rely on smoothed node features rather than graph structure。
 Link prediction heuristics: 基于预定义的假设的链路预测。举几个例子[1]：
  Common Neighbors (CN)： 公共邻居较多的节点存在边（heuristic），则需要计算节点对间的公共邻居。 Preferential Attachment (PA): 一个节点当前的连接越多，那么它越有可能接受到新的连接（heuristic）,这需要统计每个节点的度, i.e., $P A(x, y)=|N(x)| *|N(y)|$ Katz Index heuristic: $\sum^{\infty}_{\ell=1} \beta^{\ell}|walks(x,y)=\ell|$ 表示从$x$到$y$的所有路径数， $0&amp;lt;\beta&amp;lt;1$， 表示越长的路径权重越低。 katz Index作为Link prediction heuristic假设来作为边是否存在的预测。  本文提出了Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs)来从邻接矩阵中学习结构信息，并且估计重叠多跳邻域用于link prediction。</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=Ic9vRN3VpZ">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>由于GNNs过度强调平滑的节点特征而不是图结构，使得在Link Prediction任务上的表现甚至弱于heuristic方法。平滑邻居难以反映邻域结构的相关性以及其他拓扑特征。 <strong>Structural information, (e.g., overlapped neighborhoods, degrees, and shortest path), is crucial for link prediction whereas GNNs heavily rely on smoothed node features rather than graph structure</strong>。</p>
<blockquote>
<p><strong>Link prediction heuristics:</strong>  基于预定义的假设的链路预测。举几个例子[1]：</p>
</blockquote>
<ol>
<li>Common Neighbors (CN)： 公共邻居较多的节点存在边（heuristic），则需要计算节点对间的公共邻居。</li>
<li>Preferential Attachment (PA): 一个节点当前的连接越多，那么它越有可能接受到新的连接（heuristic）,这需要统计每个节点的度, i.e., $P A(x, y)=|N(x)| *|N(y)|$</li>
<li>Katz Index heuristic: $\sum^{\infty}_{\ell=1} \beta^{\ell}|walks(x,y)=\ell|$ 表示从$x$到$y$的所有路径数， $0&lt;\beta&lt;1$， 表示越长的路径权重越低。 katz Index作为Link prediction heuristic假设来作为边是否存在的预测。</li>
</ol>
<p>本文提出了Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs)来从邻接矩阵中学习结构信息，并且估计重叠多跳邻域用于link prediction。</p>
<h1 id="preliminaries">Preliminaries</h1>
<h2 id="gnns-for-link-prediction">GNNs for Link Prediction</h2>
<p>$$
\hat{y}_{i j}=\sigma\left(s\left(h_{i}^{(L)}, h_{j}^{(L)}\right)\right)
$$</p>
<p>其中$s(\cdot, \cdot)$ 是一个相似度计算函数 e.g., inner product or MLP. $h_{i}^{(L)}$为 $v_i$的 node embedding.</p>
<h2 id="neighborhood-overlap-based-heuristic-methods">Neighborhood Overlap-based Heuristic Methods</h2>
<p>就是上面提到的CN heuristic。Common Neighbors 通过count节点的公共邻居来衡量两个节点之间的链路存在分数$\mathrm{link}(u,v)$：
$$
S_{C N}(u, v)=|\mathcal{N}(u) \cap \mathcal{N}(v)|=\sum_{k \in \mathcal{N}(u) \cap \mathcal{N}(v)} 1
$$
CN的缺点在于不能衡量公共节点的权重。</p>
<p>Resource Allocation (RA) 认为度叫小的节点因更加重要， 所以用度的倒数来加权公共节点：
$$
S_{R A}(u, v)=\sum_{k \in \mathcal{N}(u) \cap \mathcal{N}(v)} \frac{1}{d_{k}}
$$</p>
<p>Adamic-Adar：通过使用节点 $u$ 和$v$之间的共同邻居度的倒数对数，与 RA 相比，Adamic-Adar 对更高度的惩罚相对减少：
$$
S_{A A}(u, v)=\sum_{k \in \mathcal{N}(u) \cap \mathcal{N}(v)} \frac{1}{\log d_{k}}
$$
上述基于公共邻居的方法存在两个局限，1. 需要手动设计邻居结构特征，比如CN的公共邻居结构特征为1， RA的结构特征为$\frac{1}{d}$, AA 的邻居结构特征为$\frac{1}{\log d}$。 2. 忽略了node features</p>
<p>本文提出的Neo-GNN从邻接矩阵中学习结构特征，并且结合了node feature信息来做Link prediction。</p>
<h1 id="model-neo-gnns">Model: Neo-GNNs</h1>
<p>定义structural feature generator $\mathcal{F}_{\theta}$:
$$
x_{i}^{\text {struct }}=\mathcal{F}_{\theta}\left(A_{i}\right)=f_{\theta_{n o d e}}\left(\sum_{j \in \mathcal{N}_{i}} f_{\theta_{e d g e}}\left(A_{i j}\right)\right)
$$
输入节点$i$的邻居$A_i$，提取自邻接矩阵$A$, Neo-GNNs 只是用$A$作为输入来获得节点的结构特征。 其中，$f_{\theta_{e d g e}}(A_{ij})$生成节点$i$的局部边特征，然后聚合起来用$f_{\theta_{n o d e}}$生成节点$i$的总体结构特征$x_{i}^{\text {struct }}$， 作为节点$i$的structural feature，表示反映了节点$i$的局部结构。其中$f_{\theta_{n o d e}}$和$f_{\theta_{e d g e}}$是两个MLP。 也可以把上面的$A$替换成$A$的幂的组合，那就是$k$跳以内邻域的结构特征。</p>
<p>得到了节点的邻居结构特征$x_{i}^{\text {struct }}$后， 要用<strong>重叠邻居的结构特征</strong>来计算两个节点的相似度分数。 传统的GNN无法计算重叠邻域的结构特征的原因有两个：1. normalized adjacency matrix: 归一化邻接矩阵阻止了GNN计数邻居数量（我的理解是因为Norm adj上的元素为小数）2. 远低于节点数的hidden representation维度$d \ll N$：低维度的节点表示向量使得在neighborhood aggregration后 节点邻域特征难以区分。</p>
<p><img loading="lazy" src="/posts/2022-03-30-NeoGNN/frameworks.png#center" alt=""  />
</p>
<p>本文提出了邻域重叠感知的聚合模式。 注意，上面的节点邻域特征是一个scale, 即$x_{i}^{\text {struct }} \in \mathbb{R}^1$, 整个图的节点邻域结构特征可以表示为$X^{struct} \in \mathbb{R}^{N \times N}$, 为一个对角阵，对角线元素为每个节点的邻域<strong>结构</strong>特征，如Figure 1所示。也就是$X^{struct}$的每一行为一个节点的局部结构特征表示向量，作为这个节点的结构特征。</p>
<p>那么$Z = AX^{struct}$就可以为节点聚合结构特征。 因为$X^{struct}_i$表示节点$v_i$的structural feature (neighborhood structural), 所以$Z_i$表示节点$i$的1-st neighborhood structural feature, 所以$z_{i}^{T} z_{j}=\sum_{k \in \mathcal{N}(i) \cap \mathcal{N}(j)}\left(x_{k}^{s t r u c t}\right)^{2}$表可以表示节点$i$和节点$j$的重叠邻域。</p>
<p><strong>注意</strong> $X_{i}^{\text {struct }}$表示节点$i$自身的结构特征。 而$Z_i$表示节点$i$的邻居的结构特征聚合, 所以$z_{i}^{T} z_{j}$表示节点$i$邻居的结构特征和节点$j$邻居的结构特征的相似度。 $x_i^T x_j$表示节点$i$自身的结构特征和节点$j$自身的结构特征的相似度。</p>
<p>进一步，考虑多跳邻居：
$$
Z=g_{\Phi}\left(\sum_{l=1}^{L} \beta^{l-1} A^{l} X^{\text {struct }}\right)
$$</p>
<p>$A^lX^{struct}$的第$i$行表示节点$i$ 的$l$跳邻居特征。 $Z_i$表示节点$i$在$L$跳以内的邻居结构特征总和。</p>
<p>除了考虑结构特征来预测链接外，还应考虑node features，直接用GNN：
$$
H=\operatorname{GNN}\left(X, \tilde{A}_{G N N} ; W\right)
$$
最终节点$i$和节点$j$的相似度分数表示为：
$$
\left.\hat{y}_{i j}=\alpha \cdot \sigma\left(z_{i}^{T} z_{j}\right)+(1-\alpha) \cdot \sigma\left(s\left(h_{i}, h_{j}\right)\right)\right)
$$
即为邻域结构相似度 与 特征相似度 的加权平均。</p>
<p>最终损失函数要求 3种相似度衡量标准（基于邻域结构，基于节点feature, 两者加权平均）都可以你和真实的相似度，即：
$$
\mathcal{L}=\sum_{(i, j) \in D}\left(\lambda_{1} B C E\left(\hat{y}_{i j}, y_{i j}\right)+\lambda_{2} B C E\left(\sigma\left(z_{i}^{T} z_{j}\right), y_{i j}\right)+\lambda_{3} B C E\left(\sigma\left(s\left(h_{i}, h_{j}\right)\right), y_{i j}\right)\right)
$$
其中$BCE(\cdot, \cdot)$为 binary cross entropy loss。</p>
<h1 id="reference">Reference</h1>
<p>[1] Link Prediction Based on Graph Neural Networks. NeurIPS 2018.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NeurIPS2021 《Representing Long-Range Context for Graph Neural Networks with Global Attention》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/graphtrans/</link>
      <pubDate>Wed, 30 Mar 2022 10:37:41 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/graphtrans/</guid>
      <description>Paper
Introduction 加深GNN层数来增加感受野会导致优化不稳定性，比如梯度消失和oversmoothing。 因此本文采用Transformer-based self-attention来学习成对节点间的长距离关系，并且提出一种新型的readout池化机制来学习global graph embedding。即在一个GNN模块后接一个置换不变（permutation-invariant）Transformer, GNN模块捕获local信息，Transformer捕获global信息。
GNN作为一种专门的架构医学系节点直接邻域结构的局部表示， 而Transformer作为全局推理模块以位置无关的方式计算所有成对节点的交互。作者认为，一个没有positional encoding的Transformer是置换不变的，因此很适合图。
Motivation 强关系Inductive bias(我的理解是Homophily假设) 鼓励学习局部短距离的关联性。 而对于长距离相关性，结构化较低的模块（不需要过于考虑图的结构信息）更受欢迎。
GraphTrans leaves learning long-range dependencies to Transformer, 通过Transformer来学习图中所有节点对的依赖关系而不是只关注局部邻居。
下图中展示了一个子图的attention map。一共有17个节点，横坐标表示目标节点，纵坐标表示源节点，第$i$行第$j$列表示节点$i$在Transformer中聚合$j$的attention权重。第18行为一个特殊的$&amp;lt;CLS&amp;gt;$token 作为图的readout embedding。结合本文的SOTA效果，表面在学习长距离依赖时不考虑图结构先验（spatial priors）对Graph summarization（graph-level representation）是有必要的
Model GNN Module 一个通用的GNN模块： $$ \boldsymbol{h}_{v}^{\ell}=f_{\ell}\left(\boldsymbol{h}_{v}^{\ell-1},\left\{\boldsymbol{h}_{u}^{\ell-1} \mid u \in \mathcal{N}(v)\right\}\right), \quad \ell=1, \ldots, L_{\mathrm{GNN}} $$
Transformer Module 通过上面的GNN模块，我们可以得到每个节点的embedding $h_{v}^{L_{\mathrm{GNN}}}$, 将所有节点作为Transformer的Input。传统Transformer中输入先计算Self-attention（当前输入的$Q$向量和所有节点的$K$向量做内积得到输入节点和其他节点的att值，再用这个att值来为当前输入节点加权聚合所有节点的$V$向量），聚合后再和自身相加做residual,然后在做Layer Norm, 即对节点$i$的表示做Layer Norm 为$x_i^\prime = \frac{x_i-m}{\sigma}$, 其中$m$为$x_i$的均值， $\sigma$为$x_i$的标准差。
这里的Transformer不同的是， 先对所有节点做一次MLP，然后直接计算Layer Norm: $$ \overline{\boldsymbol{h}}_{v}^{0}=\operatorname{LayerNorm}\left(\boldsymbol{W}^{\text {Proj }} \boldsymbol{h}_{v}^{L_{\mathrm{GNN}}}\right) $$ 其中$\boldsymbol{W}^{\text {Proj }} \in \mathbb{R}^{d_{\mathrm{TF}} \times d_{L_{\mathrm{GNN}}}}$， 把GNN的输出维度转为TF的输入维度$d_{\mathrm{TF}}$。将所有节点的GNN node embeddings作为Transformer的输入（无positional encoding）。 每个节点的$Q$, $K$和$V$向量分别用$\boldsymbol{W}_{\ell}^{Q}, \boldsymbol{W}_{\ell}^{K}, \boldsymbol{W}_{\ell}^{V} \in \mathbb{R}^{d_{\mathrm{TF}} / n_{\text {head }} \times d_{\mathrm{TF}} / n_{\text {head }}}$计算， 对于第$\ell$层 Transformer, 节点$v$ 的$Q$向量$Q_v = \boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}$和节点$u$的$K$向量$K_u = \boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}$做内积，得到两个节点之间的attention。然后用$\alpha_{v, u}^{\ell}$来为节点$v$聚合其他所有节点的$V$向量$V_u = \boldsymbol{W}_{\ell}^{V} \overline{\boldsymbol{h}}_{u}^{\ell-1}$, 如下所示: $$ a_{v, u}^{\ell}=\left(\boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}\right)^{\top}\left(\boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}\right) / \sqrt{d_{\mathrm{TF}}} \tag{1} $$</description>
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/pdf/2201.08821.pdf">Paper</a></p>
<h1 id="introduction">Introduction</h1>
<p>加深GNN层数来增加感受野会导致优化不稳定性，比如梯度消失和oversmoothing。 因此本文采用Transformer-based self-attention来学习成对节点间的长距离关系，并且提出一种新型的readout池化机制来学习global graph embedding。即在一个GNN模块后接一个置换不变（permutation-invariant）Transformer, GNN模块捕获local信息，Transformer捕获global信息。</p>
<p>GNN作为一种专门的架构医学系节点<strong>直接邻域结构的局部表示</strong>， 而Transformer作为全局推理模块以位置无关的方式计算所有成对节点的交互。作者认为，一个没有positional encoding的Transformer是置换不变的，因此很适合图。</p>
<h1 id="motivation">Motivation</h1>
<p>强关系Inductive bias(我的理解是Homophily假设) 鼓励学习局部短距离的关联性。 而对于长距离相关性，结构化较低的模块（不需要过于考虑图的结构信息）更受欢迎。</p>
<p><strong>GraphTrans leaves learning long-range dependencies to Transformer</strong>, 通过Transformer来学习图中所有节点对的依赖关系而不是只关注局部邻居。</p>
<p>下图中展示了一个子图的attention map。一共有17个节点，横坐标表示目标节点，纵坐标表示源节点，第$i$行第$j$列表示节点$i$在Transformer中聚合$j$的attention权重。第18行为一个特殊的$&lt;CLS&gt;$token 作为图的readout embedding。结合本文的SOTA效果，表面在学习长距离依赖时不考虑图结构先验（spatial priors）对Graph summarization（graph-level representation）是有必要的</p>
<p><img loading="lazy" src="/posts/2022-03-30-GraphTrans/pic2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="model">Model</h1>
<p><img loading="lazy" src="/posts/2022-03-30-GraphTrans/pic1.png#center" alt="你想输入的替代文字"  />
</p>
<h2 id="gnn-module">GNN Module</h2>
<p>一个通用的GNN模块：
$$
\boldsymbol{h}_{v}^{\ell}=f_{\ell}\left(\boldsymbol{h}_{v}^{\ell-1},\left\{\boldsymbol{h}_{u}^{\ell-1} \mid u \in \mathcal{N}(v)\right\}\right), \quad \ell=1, \ldots, L_{\mathrm{GNN}}
$$</p>
<h2 id="transformer-module">Transformer Module</h2>
<p>通过上面的GNN模块，我们可以得到每个节点的embedding $h_{v}^{L_{\mathrm{GNN}}}$, 将所有节点作为Transformer的Input。传统Transformer中输入先计算Self-attention（当前输入的$Q$向量和所有节点的$K$向量做内积得到输入节点和其他节点的att值，再用这个att值来为当前输入节点加权聚合所有节点的$V$向量），聚合后再和自身相加做residual,然后在做Layer Norm, 即对节点$i$的表示做Layer Norm 为$x_i^\prime = \frac{x_i-m}{\sigma}$, 其中$m$为$x_i$的均值， $\sigma$为$x_i$的标准差。</p>
<p>这里的Transformer不同的是， 先对所有节点做一次MLP，然后直接计算Layer Norm:
$$
\overline{\boldsymbol{h}}_{v}^{0}=\operatorname{LayerNorm}\left(\boldsymbol{W}^{\text {Proj }} \boldsymbol{h}_{v}^{L_{\mathrm{GNN}}}\right)
$$
其中$\boldsymbol{W}^{\text {Proj }} \in \mathbb{R}^{d_{\mathrm{TF}} \times d_{L_{\mathrm{GNN}}}}$， 把GNN的输出维度转为TF的输入维度$d_{\mathrm{TF}}$。将所有节点的GNN node embeddings作为Transformer的输入（无positional encoding）。 每个节点的$Q$, $K$和$V$向量分别用$\boldsymbol{W}_{\ell}^{Q}, \boldsymbol{W}_{\ell}^{K}, \boldsymbol{W}_{\ell}^{V} \in \mathbb{R}^{d_{\mathrm{TF}} / n_{\text {head }} \times d_{\mathrm{TF}} / n_{\text {head }}}$计算， 对于第$\ell$层 Transformer,  节点$v$ 的$Q$向量$Q_v = \boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}$和节点$u$的$K$向量$K_u = \boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}$做内积，得到两个节点之间的attention。然后用$\alpha_{v, u}^{\ell}$来为节点$v$聚合其他所有节点的$V$向量$V_u = \boldsymbol{W}_{\ell}^{V} \overline{\boldsymbol{h}}_{u}^{\ell-1}$, 如下所示:
$$
a_{v, u}^{\ell}=\left(\boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}\right)^{\top}\left(\boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}\right) / \sqrt{d_{\mathrm{TF}}} \tag{1}
$$</p>
<p>$$
\alpha_{v, u}^{\ell}=\operatorname{softmax}_{u \in \mathcal{V}}\left(a_{v, u}^{\ell}\right) \tag{2}
$$</p>
<p>$$
\overline{\boldsymbol{h}}_{v}^{\prime \ell}=\sum_{w \in \mathcal{V}} \alpha_{v, u}^{\ell} \boldsymbol{W}_{\ell}^{V} \overline{\boldsymbol{h}}_{u}^{\ell-1} \tag{3}
$$</p>
<h2 id="cls--embedding-as-a-gnn-readout-method">&lt;CLS&gt;  embedding as a GNN “readout” method</h2>
<p>Graph Pooling 部分旨在基于node embedding，得到整个图的一个global embedding. 大多数pooling方法为简单的mean,sum, 或者构造一个virtual node连接到所有节点并参与训练，这个virtual node聚合所有节点的信息作为global embedding。</p>
<p>本文提出special-token readout module。具体来说，对Transformer的输入$[\overline{\boldsymbol{h}}_{v}^{0}]_{v\in V}$, where $\overline{\boldsymbol{h}}_{v}^{0} \in \mathcal{R}^{d_{TF}}$我们添加一个额外的可学习embedding （可以被认为是一个额外virtual node）$\bar{h}_{\langle\mathrm{CLS}\rangle} \in \mathbb{R}^{d_{\mathrm{TF}}}$, 这样 Transformer 的输入就变为$[\overline{\boldsymbol{h}}_{v}^{0}]_{v \in V} \cup \bar{h}_{\langle\mathrm{CLS}\rangle}$, 因为训练过程中$\overline{\boldsymbol{h}}_{v}^{0}$回聚合来自所有节点的信息，所以用它来作为readout embedding。 最终Transformer输出的token embedding $\overline{\boldsymbol{h}}_{&lt;\mathrm{CLS}&gt;}^{L_{\mathrm{TF}}}$ 再过一层MLP后用Softmax输出图的prediction:
$$
y=\operatorname{softmax}\left(\boldsymbol{W}^{\mathrm{out}} \overline{\boldsymbol{h}}_{&lt;\mathrm{CLS}&gt;}^{L_{\mathrm{TF}}}\right)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Blog, Tools and Survey</title>
      <link>https://JhuoW.github.io/posts/2019-04-28-paper-unscramble/</link>
      <pubDate>Tue, 29 Mar 2022 11:03:50 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-04-28-paper-unscramble/</guid>
      <description>这篇笔记用于收藏别人的博客
Tech Blog    Blog Author     https://michael-bronstein.medium.com/ Michael Bronstein   https://geometricdeeplearning.com/ Michael Bronstein   https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c Vitaly Kurin (Many Paper Notes)   https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html UvA DL Notebooks   https://graph-neural-networks.github.io/index.html GNN Books   http://prob140.org/sp17/textbook/ Probability for Data Science class at UC Berkeley   https://graphreason.github.io/schedule.html Learning and Reasoning with Graph-Structured Representations ICML 2019 Workshop   https://chuxuzhang.github.io/KDD21_Tutorial.html KDD2021 Tutorial: Data Efficient Learning on Graphs   http://songcy.</description>
      <content:encoded><![CDATA[<p><strong>这篇笔记用于收藏别人的博客</strong></p>
<h1 id="tech-blog">Tech Blog</h1>
<table>
<thead>
<tr>
<th>Blog</th>
<th>Author</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><a href="https://michael-bronstein.medium.com/">https://michael-bronstein.medium.com/</a></strong></td>
<td>Michael Bronstein</td>
</tr>
<tr>
<td><strong><a href="https://geometricdeeplearning.com/">https://geometricdeeplearning.com/</a></strong></td>
<td>Michael Bronstein</td>
</tr>
<tr>
<td><strong><a href="https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c">https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c</a></strong></td>
<td>Vitaly Kurin (Many Paper Notes)</td>
</tr>
<tr>
<td><strong><a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html</a></strong></td>
<td>UvA DL Notebooks</td>
</tr>
<tr>
<td><strong><a href="https://graph-neural-networks.github.io/index.html">https://graph-neural-networks.github.io/index.html</a></strong></td>
<td>GNN Books</td>
</tr>
<tr>
<td><strong><a href="http://prob140.org/sp17/textbook/">http://prob140.org/sp17/textbook/</a></strong></td>
<td><a href="http://prob140.org/">Probability for Data Science</a> class at UC Berkeley</td>
</tr>
<tr>
<td><strong><a href="https://graphreason.github.io/schedule.html">https://graphreason.github.io/schedule.html</a></strong></td>
<td><a href="https://graphreason.github.io/index.html">Learning and Reasoning with Graph-Structured Representations</a> ICML 2019 Workshop</td>
</tr>
<tr>
<td><strong><a href="https://chuxuzhang.github.io/KDD21_Tutorial.html">https://chuxuzhang.github.io/KDD21_Tutorial.html</a></strong></td>
<td>KDD2021 Tutorial: Data Efficient Learning on Graphs</td>
</tr>
<tr>
<td><strong><a href="http://songcy.net/posts/">http://songcy.net/posts/</a></strong></td>
<td>Changyue Song (Kernel)</td>
</tr>
<tr>
<td><strong><a href="https://www.cs.mcgill.ca/~wlh/grl_book/">https://www.cs.mcgill.ca/~wlh/grl_book/</a></strong></td>
<td>William L. Hamilton</td>
</tr>
<tr>
<td><strong><a href="https://kexue.fm/">https://kexue.fm/</a></strong></td>
<td>BoJone</td>
</tr>
<tr>
<td><strong><a href="https://danielegrattarola.github.io/blog/">https://danielegrattarola.github.io/blog/</a></strong></td>
<td>Daniele Grattarola (EPFL)</td>
</tr>
<tr>
<td><strong><a href="https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html">https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html</a></strong></td>
<td>Google AI Blog</td>
</tr>
<tr>
<td><strong><a href="https://zhiyuchen.com/blogs/">https://zhiyuchen.com/blogs/</a></strong></td>
<td>Zhiyu Chen</td>
</tr>
<tr>
<td><strong><a href="https://andreasloukas.blog/">https://andreasloukas.blog/</a></strong></td>
<td>Andreas Loukas (EPFL)</td>
</tr>
<tr>
<td><strong><a href="https://irhum.pubpub.org/pub/gnn/release/4">https://irhum.pubpub.org/pub/gnn/release/4</a></strong></td>
<td>Understanding Graph Neural Networks</td>
</tr>
<tr>
<td><strong><a href="https://lilianweng.github.io/">https://lilianweng.github.io/</a></strong></td>
<td>Lilian Weng</td>
</tr>
<tr>
<td><strong><a href="https://www.zhihu.com/column/marlin">https://www.zhihu.com/column/marlin</a></strong></td>
<td>深度学习与图网络</td>
</tr>
<tr>
<td><strong><a href="https://github.com/roboticcam/machine-learning-notes">https://github.com/roboticcam/machine-learning-notes</a></strong></td>
<td>Yida Xu</td>
</tr>
<tr>
<td><strong><a href="https://www.dgl.ai/pages/index.html">https://www.dgl.ai/pages/index.html</a></strong></td>
<td>DGL</td>
</tr>
<tr>
<td><strong><a href="https://www.kexinhuang.com/tech-blog">https://www.kexinhuang.com/tech-blog</a></strong></td>
<td>Kexin Huang</td>
</tr>
<tr>
<td><strong><a href="https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8">https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8</a></strong></td>
<td>Rishabh Anand</td>
</tr>
<tr>
<td><strong><a href="https://saashanair.com/blog">https://saashanair.com/blog</a></strong></td>
<td>Saasha Nair</td>
</tr>
<tr>
<td><strong><a href="http://www.huaxiaozhuan.com/">http://www.huaxiaozhuan.com/</a></strong></td>
<td>华校专</td>
</tr>
<tr>
<td><strong><a href="https://github.com/dglai/WWW20-Hands-on-Tutorial">https://github.com/dglai/WWW20-Hands-on-Tutorial</a></strong></td>
<td>DGL</td>
</tr>
<tr>
<td><strong><a href="https://blog.csdn.net/CSDNTianJi/article/details/104195306">https://blog.csdn.net/CSDNTianJi/article/details/104195306</a></strong></td>
<td>Meng Liu</td>
</tr>
<tr>
<td><a href="https://github.com/tianyicui/pack">https://github.com/tianyicui/pack</a></td>
<td>背包9講</td>
</tr>
<tr>
<td><a href="https://www.fenghz.xyz/">https://www.fenghz.xyz/</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://sakigami-yang.me/2017/08/13/about-kernel-01/">https://sakigami-yang.me/2017/08/13/about-kernel-01/</a></td>
<td>kernel</td>
</tr>
<tr>
<td><a href="https://davidham3.github.io/blog">https://davidham3.github.io/blog</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://fenghz.github.io/index.html">https://fenghz.github.io/index.html</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://archwalker.github.io/">https://archwalker.github.io/</a></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="survey">Survey</h1>
<table>
<thead>
<tr>
<th>Repo</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><a href="https://github.com/naganandy/graph-based-deep-learning-literature">https://github.com/naganandy/graph-based-deep-learning-literature</a></strong></td>
<td><strong>links to conference publications in graph-based deep learning</strong> (Very, Very, Very Important)</td>
</tr>
<tr>
<td><a href="https://github.com/SherylHYX/pytorch_geometric_signed_directed">https://github.com/SherylHYX/pytorch_geometric_signed_directed</a></td>
<td>PyTorch Geometric Signed Directed is a signed/directed graph neural network extension library for PyTorch Geometric.</td>
</tr>
<tr>
<td><a href="https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning">https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning</a></td>
<td>Paper Lists for Fair Graph Learning</td>
</tr>
<tr>
<td><a href="https://github.com/thunlp/PromptPapers">https://github.com/thunlp/PromptPapers</a></td>
<td>Must-read papers on prompt-based tuning for pre-trained language models.</td>
</tr>
<tr>
<td><a href="https://github.com/zhao-tong/graph-data-augmentation-papers">https://github.com/zhao-tong/graph-data-augmentation-papers</a></td>
<td>A curated list of graph data augmentation papers.</td>
</tr>
<tr>
<td><a href="https://github.com/Thinklab-SJTU/ThinkMatch">https://github.com/Thinklab-SJTU/ThinkMatch</a></td>
<td>Code &amp; pretrained models of novel deep graph matching methods.</td>
</tr>
<tr>
<td><a href="https://github.com/FLHonker/Awesome-Knowledge-Distillation">https://github.com/FLHonker/Awesome-Knowledge-Distillation</a></td>
<td>Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)。</td>
</tr>
<tr>
<td><a href="https://github.com/zlpure/awesome-graph-representation-learning">https://github.com/zlpure/awesome-graph-representation-learning</a></td>
<td>A curated list for awesome graph representation learning resources.</td>
</tr>
<tr>
<td><a href="https://github.com/basiralab/GNNs-in-Network-Neuroscience">https://github.com/basiralab/GNNs-in-Network-Neuroscience</a></td>
<td>A review of papers proposing novel GNN methods with application to brain connectivity published in 2017-2020.</td>
</tr>
<tr>
<td><a href="https://github.com/flyingdoog/awesome-graph-explainability-papers">https://github.com/flyingdoog/awesome-graph-explainability-papers</a></td>
<td>Papers about explainability of GNNs</td>
</tr>
<tr>
<td><a href="https://github.com/yuanqidu/awesome-graph-generation">https://github.com/yuanqidu/awesome-graph-generation</a></td>
<td>A curated list of graph generation papers and resources.</td>
</tr>
<tr>
<td><a href="https://github.com/benedekrozemberczki/awesome-decision-tree-papers">https://github.com/benedekrozemberczki/awesome-decision-tree-papers</a></td>
<td>A collection of research papers on decision, classification and regression trees with implementations.</td>
</tr>
<tr>
<td><a href="https://github.com/AstraZeneca/awesome-explainable-graph-reasoning">https://github.com/AstraZeneca/awesome-explainable-graph-reasoning</a></td>
<td>A collection of research papers and software related to explainability in graph machine learning.</td>
</tr>
<tr>
<td><a href="https://github.com/LirongWu/awesome-graph-self-supervised-learning">https://github.com/LirongWu/awesome-graph-self-supervised-learning</a></td>
<td>Awesome Graph Self-Supervised Learning</td>
</tr>
<tr>
<td><a href="https://github.com/Chen-Cai-OSU/awesome-equivariant-network">https://github.com/Chen-Cai-OSU/awesome-equivariant-network</a></td>
<td>Paper list for equivariant neural network</td>
</tr>
<tr>
<td><a href="https://github.com/mengliu1998/DL4DisassortativeGraphs">https://github.com/mengliu1998/DL4DisassortativeGraphs</a></td>
<td>Papers about developing DL methods on disassortative graphs</td>
</tr>
<tr>
<td><a href="https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers">https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers</a></td>
<td>A curated list of graph reinforcement learning papers.</td>
</tr>
<tr>
<td><a href="https://github.com/ChandlerBang/awesome-self-supervised-gnn">https://github.com/ChandlerBang/awesome-self-supervised-gnn</a></td>
<td>Papers about pretraining and self-supervised learning on Graph Neural Networks (GNN).</td>
</tr>
<tr>
<td><a href="https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks">https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks</a></td>
<td>Paper Lists for Graph Neural Networks</td>
</tr>
<tr>
<td><a href="https://github.com/jwzhanggy/IFMLab_GNN">https://github.com/jwzhanggy/IFMLab_GNN</a></td>
<td>Graph Neural Network Models from IFM Lab</td>
</tr>
<tr>
<td><a href="https://github.com/ChandlerBang/awesome-graph-attack-papers">https://github.com/ChandlerBang/awesome-graph-attack-papers</a></td>
<td>Adversarial attacks and defenses on Graph Neural Networks.</td>
</tr>
<tr>
<td><a href="https://github.com/safe-graph/graph-adversarial-learning-literature">https://github.com/safe-graph/graph-adversarial-learning-literature</a></td>
<td>A curated list of adversarial attacks and defenses papers on graph-structured data.</td>
</tr>
<tr>
<td><a href="https://github.com/benedekrozemberczki/awesome-graph-classification">https://github.com/benedekrozemberczki/awesome-graph-classification</a></td>
<td>A collection of important graph embedding, classification and representation learning papers with implementations.</td>
</tr>
<tr>
<td><a href="https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers">https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers</a></td>
<td>A curated list of gradient boosting research papers with implementations.</td>
</tr>
<tr>
<td><a href="https://github.com/benedekrozemberczki/awesome-community-detection">https://github.com/benedekrozemberczki/awesome-community-detection</a></td>
<td>A curated list of community detection research papers with implementations.</td>
</tr>
<tr>
<td><a href="https://github.com/giannifranchi/awesome-uncertainty-deeplearning">https://github.com/giannifranchi/awesome-uncertainty-deeplearning</a></td>
<td>This repository contains a collection of surveys, datasets, papers, and codes, for predictive uncertainty estimation in deep learning models.</td>
</tr>
</tbody>
</table>
<h1 id="useful-repotools">Useful Repo/Tools</h1>
<table>
<thead>
<tr>
<th>Name</th>
<th>Info</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://acronymify.com/">http://acronymify.com/</a></td>
<td>Model Name</td>
</tr>
<tr>
<td><a href="https://csacademy.com/app/graph_editor/">https://csacademy.com/app/graph_editor/</a></td>
<td>Graph Editor</td>
</tr>
<tr>
<td><strong><a href="https://github.com/guanyingc/python_plot_utils">https://github.com/guanyingc/python_plot_utils</a></strong></td>
<td>A simple code for plotting figure, colorbar, and cropping with python</td>
</tr>
<tr>
<td><strong><a href="https://github.com/guanyingc/latex_paper_writing_tips">https://github.com/guanyingc/latex_paper_writing_tips</a></strong></td>
<td>Tips for Writing a Research Paper using LaTeX</td>
</tr>
<tr>
<td><a href="https://github.com/JhuoW/Pytorch_Program_Templete">https://github.com/JhuoW/Pytorch_Program_Templete</a></td>
<td>Pytorch Program Templete GNN</td>
</tr>
<tr>
<td><a href="https://github.com/graph4ai/graph4nlp">https://github.com/graph4ai/graph4nlp</a></td>
<td>Graph4nlp is the library for the easy use of Graph Neural Networks for NLP. Welcome to visit our DLG4NLP website (<a href="https://dlg4nlp.github.io/index.html">https://dlg4nlp.github.io/index.html</a>) for various learning resources!</td>
</tr>
<tr>
<td><a href="https://github.com/benedekrozemberczki/pytorch_geometric_temporal">https://github.com/benedekrozemberczki/pytorch_geometric_temporal</a></td>
<td>PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models (CIKM 2021)</td>
</tr>
<tr>
<td><a href="https://github.com/ysig/GraKeL">https://github.com/ysig/GraKeL</a></td>
<td>A scikit-learn compatible library for graph kernels</td>
</tr>
<tr>
<td><a href="https://github.com/jajupmochi/graphkit-learn">https://github.com/jajupmochi/graphkit-learn</a></td>
<td>A python package for graph kernels, graph edit distances, and graph pre-image problem.</td>
</tr>
<tr>
<td><a href="https://github.com/pliang279/awesome-phd-advice">https://github.com/pliang279/awesome-phd-advice</a></td>
<td>Collection of advice for prospective and current PhD students</td>
</tr>
<tr>
<td><a href="https://github.com/MLEveryday/100-Days-Of-ML-Code">https://github.com/MLEveryday/100-Days-Of-ML-Code</a></td>
<td>100-Days-Of-ML-Code中文版</td>
</tr>
<tr>
<td><a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></td>
<td>《动手学深度学习》</td>
</tr>
<tr>
<td><a href="https://github.com/lukas-blecher/LaTeX-OCR">https://github.com/lukas-blecher/LaTeX-OCR</a></td>
<td>pix2tex: Using a ViT to convert images of equations into LaTeX code.</td>
</tr>
<tr>
<td><a href="https://github.com/thunlp/OpenPrompt">https://github.com/thunlp/OpenPrompt</a></td>
<td>An Open-Source Framework for Prompt-Learning.</td>
</tr>
<tr>
<td><a href="https://github.com/snap-stanford/GraphGym">https://github.com/snap-stanford/GraphGym</a></td>
<td>Platform for designing and evaluating Graph Neural Networks (GNN)</td>
</tr>
</tbody>
</table>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2020 《Inductive and Unsupervised Representation Learning on Graph Structured Objects》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/seed/</link>
      <pubDate>Mon, 28 Mar 2022 23:44:01 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/seed/</guid>
      <description>Paper
Code
Introduction 无监督图学习算法基于重构损失，不可避免的需要图相似度计算（重构embedding和输入embedding的loss), 计算复杂度较高。本文提出一种通用的归纳式无监督图学习算法SEED（Sampling, Encoding, and Embedding Distributions）。通过计算采样子图的重构损失来代替整个图的重构损失。 即 先采样子图，在用GNN编码子图，最后计算子图分布的embedding来作为整个图的representation. 过程如下图所示：
SEED: Sampling, Encoding, and Embedding Distributions Anonymous Random Walk Definition 1 (Random Anonymous Walks[1]): Given a random walk $\mathbf{w}=(w_1, \cdots, w_l)$ where $\langle w_i, w_{i+1} \rangle \in E$, the anonymous walk for $\mathbf{w}$ is defined as： $$ \mathrm{aw}(\mathbf{w}) = (\mathrm{DIS}(\mathbf{w}, w_1),\mathrm{DIS}(\mathbf{w}, w_2),\cdots, \mathrm{DIS}(\mathbf{w}, w_l) ) $$ where $\mathrm{DIS}(\mathbf{w}, w_i)$ denotes the number of distinct nodes in $\mathbf{w}$ when $w_i$ first appears in $\mathbf{w}$, i.</description>
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=rkem91rtDB">Paper</a></p>
<p><a href="https://github.com/wenwen0319/SEED-Reimplementation">Code</a></p>
<h1 id="introduction">Introduction</h1>
<p>无监督图学习算法基于重构损失，不可避免的需要图相似度计算（重构embedding和输入embedding的loss), 计算复杂度较高。本文提出一种通用的归纳式无监督图学习算法<strong>SEED</strong>（Sampling, Encoding, and Embedding Distributions）。通过计算采样子图的重构损失来代替整个图的重构损失。 即 先采样子图，在用GNN编码子图，最后计算子图分布的embedding来作为整个图的representation. 过程如下图所示：</p>
<p><img loading="lazy" src="/posts/2022-03-29-seed/pic1.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="seed-sampling-encoding-and-embedding-distributions">SEED: Sampling, Encoding, and Embedding Distributions</h1>
<h2 id="anonymous-random-walk">Anonymous Random Walk</h2>
<p><strong>Definition 1 (Random Anonymous Walks[1]):</strong>  Given a random walk $\mathbf{w}=(w_1, \cdots, w_l)$ where $\langle w_i, w_{i+1} \rangle \in E$, the anonymous walk for $\mathbf{w}$ is defined as：
$$
\mathrm{aw}(\mathbf{w}) = (\mathrm{DIS}(\mathbf{w}, w_1),\mathrm{DIS}(\mathbf{w}, w_2),\cdots, \mathrm{DIS}(\mathbf{w}, w_l) )
$$
where $\mathrm{DIS}(\mathbf{w}, w_i)$ denotes the number of distinct nodes in $\mathbf{w}$ when $w_i$ first appears in $\mathbf{w}$, i.e.
$$
\mathrm{DIS}(\mathbf{w}, w_i) = |{w_1, \cdots w_p}|, \quad p = \min_j {w_j=w_i}
$$
匿名随机游走和随机游走的不同在于，匿名随机游走描述了随机游走的潜在“patterns”, 不管具体被访问的节点是什么。 距离来说，给定两条随机游走序列 $\mathbf{w_1}=(v_1, v_2, v_3, v_4, v_2)$ 和$w_2=(v_2, v_1, v_3, v_4, v_1)$, 这两个RW相关联的匿名随机游走是一样的，即$\mathrm{aw}(\mathbf{w_1}) = \mathrm{aw}(\mathbf{w_2}) = (1,2,3,4,2)$, 即使$\mathbf{w_1}$和$\mathbf{w_2}$访问不同的节点。即每个节点在RW中首次被访问时的位置就是这个点在ARW中的id,如在$\mathbf{w_2}$中，$v_1$首次访问是在第二个时刻，那么他的id就是2，在ARW中用2表示。</p>
<h2 id="sampling">Sampling</h2>
<p>本文提出WEAVE随机游走来表示子图</p>
<p><img loading="lazy" src="/posts/2022-03-29-seed/pic2.png#center" alt="你想输入的替代文字"  />
</p>
<p>上图中所有的$a$代表属性一样的节点， 所有的$b$也代表属性一样的节点，那么构造如图中两条vanilla random walks将得到两条完全相同的随机游走序列，因为序列中的节点属性排列完全一样（这里不会去构造induced subgraph）。为了可以区分两个图，提出了WEAVE, i.e.,  random Walk with EArliest Visit timE。实际上就是为每个随机游走序列上的节点拼接他在匿名随机游走序列中的index。这样就可以区分两个属性完全一样的随机游走序列。</p>
<p>简单来说这种方法会记录节点首次被访问的时间，这个时间作为节点的index，从而随机游走序列可以反映子图结构。</p>
<p>一个长度为$k$的WEAVE序列可以表示为：$X=\left[\mathbf{x}^{(0)}, \mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(k)}\right]$, 其中$\mathbf{x}^{(p)}$是序列上的第$p$个节点， $\mathbf{x}^{(p)}=\left[\mathbf{x}_{a}^{(p)}, \mathbf{x}_{t}^{(p)}\right]\in \mathbb{R}^{k \times (d+\ell)}$, 是两个向量的拼接，$\mathbf{x}_{a}^{(p)} \in \mathbb{R}^d$代表这个节点的node feature, $ \mathbf{x}_{t}^{(p)} \in \mathbb{R}^\ell$是是节点在匿名随机游走中的idx， 用onehot向量表示（即该节点首次被访问的时间）。</p>
<p>最终，如果要从输入图中sample $s$条随机游走路径，将会生成$s$个子图，用矩阵表示为$\left\{X_{1}, X_{2}, \ldots, X_{s}\right\}$。</p>
<h2 id="encoding">Encoding</h2>
<p>用$s$个随机游走序列表示$\mathcal{G}$的$s$个子图。对每个子图使用auto encoder 计算embedding:
$$
\mathbf{z}=f\left(X ; \theta_{e}\right), \quad \hat{X}=g\left(\mathbf{z} ; \theta_{d}\right)
$$
其中$X$表示一个子图（WEAVE）, 先用$f_{\theta_e}$得到这个子图的pooling embedding, 在用$g_{\theta_d}$将子图的embedding重构为矩阵$\hat{X}$。每个子图的重构损失为：
$$
\mathcal{L}=||X-\hat{X}||_{2}^{2}
$$
通过对每个子图的$\mathcal{L}$做SGD来优化$\theta_e$和$\theta_d$来使得重构误差最小。 最终对于图$\mathcal{G}$我们可以得到它的$s$个子图表示：$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$.</p>
<h2 id="embedding-distribution">Embedding Distribution</h2>
<p>假设我们已经有了输入图$\mathcal{G}$的子图表示向量集$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$, 要将他们融合成一个embedding来表示整个图。可以把这个图的子图集合看做一个distribution，每个子图是这个distribution中的一个样本。 如果两个Graph的子图分布相似，那么这两个Graph的相似度应该更高。 所以目标就变为，给定两个图$\mathcal{G}$和$\mathcal{H}$, 他们的子图表示分别为$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{s}\right\}$和$\left\{\mathbf{h}_{1}, \mathbf{h}_{2}, \cdots, \mathbf{h}_{s}\right\}$。这是两个分布的样本，我们要计算两个分布的距离，本文使用MMD, 目的是求两个分布的distribution embeddings, 然后求两个distribution embeddings间的距离。MMD可以参考<a href="https://jhuow.github.io/posts/mmd/">这里</a>。</p>
<p>用$P_{\mathcal{}G}$和$P_{\mathcal{H}}$分别表示这两个图的子图分布， 两个分布之间的MMD距离可以用下式计算得到。
$$
\begin{aligned}
\widehat{MMD}\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)=&amp; \frac{1}{s(s-1)} \sum_{i=1}^{s} \sum_{j \neq i}^{s} k\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)+\frac{1}{s(s-1)} \sum_{i=1}^{s} \sum_{j \neq i}^{s} k\left(\mathbf{h}_{i}, \mathbf{h}_{j}\right) \\
&amp;-\frac{2}{s^{2}} \sum_{i=1}^{s} \sum_{j=1}^{s} k\left(\mathbf{z}_{i}, \mathbf{h}_{j}\right) \\
=&amp;\left|\left|\hat{\mu}_{\mathcal{G}}-\hat{\mu}_{\mathcal{H}}\right|\right|_{2}^{2} .
\end{aligned}
$$
该式表示的含义为，两个图中的样本$\left\{\mathbf{z}_{1}, \mathbf{z}_{2}, \cdots, \mathbf{z}_{S}\right\}$和$\left\{\mathbf{h}_{1}, \mathbf{h}_{2}, \cdots, \mathbf{h}_{s}\right\}$分别映射到一个RKHS空间中，<strong>两组样本在这个RKHS空间中的均值来表示这两个分布</strong>。即：
$$
\hat{\mu}_{\mathcal{G}}=\frac{1}{s} \sum_{i=1}^{s} \phi\left(\mathbf{z}_{i}\right), \quad \hat{\mu}_{\mathcal{H}}=\frac{1}{s} \sum_{i=1}^{s} \phi\left(\mathbf{h}_{i}\right)
$$
其中$\phi(\mathbf{z}_{i})$,$\phi(\mathbf{h}_{i})$分别表示 将向量$\mathbf{z}_{i}$和$\mathbf{h}_{i}$ 映射到一个RKHS中，所以$\phi(\cdot)$是一个kernel $k(\cdot, \cdot)$的feature map函数, i.e., $k(u,v) = \langle \phi(u), \phi(v) \rangle$。$\phi(u) = k(\cdot, u)$是kernel $k$对应RKHS中的一个函数（向量）。 所以只要确定一个kernel $k(\cdot, \cdot)$，上面的$\widehat{MMD}(P_{\mathcal{G}}, P_{\mathcal{H}})$就可以求出确定值，表示两个distribution间的距离。 但是知道两个分布在RKHS中的距离还不够，需要知道这两个分布的在RKHS间的均值距离还不够， 我们需要知道这两个分布在RKHS中被映射成了什么向量，即我们要求$\phi(\cdot)$。</p>
<p>假设我们已经有了一个kernel， 这个kernel对应的映射函数是一个恒等映射，那么$\phi(u)=u$, 分布样本在RKHS中的表示就是他们本身，即 $\phi(\mathbf{z}_{i})=\mathbf{z}_{i}$, $\phi(\mathbf{h}_{i})=\mathbf{h}_{i}$。那么这分布的表示向量就是他们的样本在RKHS上的平均（均值平均误差）：
$$
\hat{\mu}_{\mathcal{G}}=\frac{1}{s} \sum_{i=1}^{s} \mathbf{z}_{i}, \quad \hat{\mu}_{\mathcal{H}}=\frac{1}{s} \sum_{i=1}^{s} \mathbf{h}_{i}
$$
如果$k$是一个其他通用kernel, 比如RBF kernel, 那么$k(u,v) = \langle \phi(u), \phi(v) \rangle$这里的$\phi(\cdot)$是不知道的，也就是仅能知道映射后的内积值，不能知道具体的映射是什么，为了求这个映射，本文用神经网络来近似这个映射。</p>
<p>具体来说，定义$\hat{\phi}\left(\cdot ; \theta_{m}\right)$是一个参数为$\theta_{m}$的MLP， 输入为分布的样本，那么用这个函数来对两个分布的样本$\{\mathbf{z_i}\}$和$\{\mathbf{h_i}\}$做映射, 然后用$\hat{\phi}\left(\cdot ; \theta_{m}\right)$来近似kernel真实的映射函数$\phi(\cdot)$。即：
$$
\hat{\mu}_{\mathcal{G}}^{\prime}=\frac{1}{s} \sum_{i=1}^{s} \hat{\phi}\left(\mathbf{z}_{i} ; \theta_{m}\right), \quad \hat{\mu}_{\mathcal{H}}^{\prime}=\frac{1}{s} \sum_{i=1}^{s} \hat{\phi}\left(\mathbf{h}_{i} ; \theta_{m}\right), \quad D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)=\left|\left|\hat{\mu}_{\mathcal{G}}^{\prime}-\hat{\mu}_{\mathcal{H}}^{\prime}\right|\right|_{2}^{2}
$$
上式中的$D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)$表示两个分布中的样本在被$\hat{\phi}\left(\cdot; \theta_{m}\right)$映射后的均值误差。用这个均值误差来近似$\widehat{MMD}(P_{\mathcal{G}}, P_{\mathcal{H}})$中由kernel $k$的映射$\phi(\cdot)$算出的Ground truth均值误差：</p>
<p>$$J\left(\theta_{m}\right)=\left|\left|D\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)-\widehat{M M D}\left(P_{\mathcal{G}}, P_{\mathcal{H}}\right)\right|\right|_{2}^{2}$$</p>
<p>通过最小化$J\left(\theta_{m}\right)$,来优化$\hat{\phi}\left(\cdot; \theta_{m}\right)$,使其近似称为一个kernel的feature map函数， 即可以将样本映射到一个RKHS空间中的函数。</p>
<p>训练结束后，用$\hat{\mu}_{\mathcal{G}}^{\prime}$来表示输入图$\mathcal{G}$的最终embedding （子图分布embedding）。</p>
<h1 id="reference">Reference</h1>
<p>[1] Micali, S., and Zhu, Z. A. 2016. Reconstructing markov processes from independent and anonymous experiments. Discrete Applied Mathematics 200:108–122.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Awesome Barbell Graph with Networkx</title>
      <link>https://JhuoW.github.io/posts/barbell_graph/</link>
      <pubDate>Sun, 27 Mar 2022 15:38:50 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/barbell_graph/</guid>
      <description>晕了 import networkx as nx import matplotlib.pyplot as plt n_clique, n_path = 10, 10 clique1 = nx.complete_graph(n_clique) clique1_pos = nx.circular_layout(clique1) clique2 = nx.complete_graph(n_clique) clique2_mapping = {node: node + n_clique for node in clique2} nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes x_diff, y_diff = 8, -1 clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2} path = nx.path_graph(n_path) path_mapping = {node: node + 2 * n_clique for node in path} nx.</description>
      <content:encoded><![CDATA[<h3 id="晕了">晕了</h3>
<pre tabindex="0"><code>import networkx as nx
import matplotlib.pyplot as plt

n_clique, n_path = 10, 10
clique1 = nx.complete_graph(n_clique)
clique1_pos = nx.circular_layout(clique1)
clique2 = nx.complete_graph(n_clique)
clique2_mapping = {node: node + n_clique for node in clique2}
nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes
x_diff, y_diff = 8, -1
clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2}
path = nx.path_graph(n_path)
path_mapping = {node: node + 2 * n_clique for node in path}
nx.relabel_nodes(path, path_mapping, copy=False) # avoids repeated nodes
path_nodes = list(path.nodes)
path_half1_nodes = path_nodes[:n_path//2]
path_half2_nodes = path_nodes[n_path//2:]
path_dist = 0.9
clique2_entry = n_clique + n_clique // 2
path_half1_pos = {node: clique1_pos[0] + (path_dist + i * path_dist, 0) for i, node in enumerate(path_half1_nodes)}
path_half2_pos = {node: clique2_pos[clique2_entry] - (path_dist + i * path_dist, 0) for i, node in enumerate(path_half2_nodes[::-1])}
path_pos = {**path_half1_pos, **path_half2_pos}
barbell = nx.Graph()
barbell.add_edges_from(clique1.edges)
barbell.add_edges_from(clique2.edges)
barbell.add_edges_from(path.edges)
barbell.add_edges_from([(path_half1_nodes[0], 0), (path_half2_nodes[-1], clique2_entry)])
clique_pos = {**clique1_pos, **clique2_pos}
barbell_pos = {**clique_pos, **path_pos}
plt.figure(figsize=(20, 6))
nx.draw(barbell, pos=barbell_pos, with_labels=True)
</code></pre><p><img loading="lazy" src="/posts/barbell-graph/barbell.png#center" alt="你想输入的替代文字"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Maximum Mean Discrepancy</title>
      <link>https://JhuoW.github.io/posts/mmd/</link>
      <pubDate>Sun, 27 Mar 2022 10:45:08 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/mmd/</guid>
      <description>Mean Discrepancy (MD)均值差异 判断2个分布$p$ 和$q$是否相同。
$p$分布生成一个样本空间$\mathbb{P}$ (从$p$中采样$m$个样本)
$q$分布生成一个样本空间$\mathbb{Q}$（从$q$中采样$n$个样本）
函数$f$的输入为 分布生成的样本空间
如果 $$ \begin{equation} \begin{aligned} \mathrm{mean}(f(\mathbb{P})) == \mathrm{mean}(f(\mathbb{Q})) \\ i.e., \frac{1}{m}\sum^m_{i=1}f(p_i) = \frac{1}{n}\sum^n_{i=1}f(q_i) \end{aligned} \end{equation} $$
则$p$和$q$是同一分布。
MD can be defined as $$ \begin{equation} \begin{aligned} \mathrm{MD}&amp;amp;=|\mathrm{mean}(f(\mathbb{P})) -\mathrm{mean}(f(\mathbb{Q})) | \\ &amp;amp;= |\frac{1}{m}\sum^m_{i=1}f(p_i) - \frac{1}{n}\sum^n_{i=1}f(q_i)| \end{aligned} \end{equation} $$
Maximum Mean Discrepancy (MMD) 最大均值差异 定义 MMD: 在函数集$\mathcal{F}=\{f_1, f_2, \cdots \}$中， 找到一个函数$f^*$， 使得$|\mathrm{mean}(f^*(\mathbb{P})) -\mathrm{mean}(f^*(\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的最大均值差异（MMD）。MMD =0 表示两个分布相同。 $$ \operatorname{MMD}[\mathcal{F}, p, q]:=\sup _{f \in \mathcal{F}}\left(\mathbf{E}_{x \sim p}[f(x)]-\mathbf{E}_{y \sim q}[f(y)]\right) $$ 其中$\mathbf{E}_{x \sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\sup$为上确界直接理解为max就好。</description>
      <content:encoded><![CDATA[<h1 id="mean-discrepancy-md均值差异">Mean Discrepancy (MD)均值差异</h1>
<p>判断2个分布$p$ 和$q$是否相同。</p>
<p>$p$分布生成一个样本空间$\mathbb{P}$ (从$p$中采样$m$个样本)</p>
<p>$q$分布生成一个样本空间$\mathbb{Q}$（从$q$中采样$n$个样本）</p>
<p>函数$f$的输入为 分布生成的样本空间</p>
<p>如果
$$
\begin{equation}
\begin{aligned}
\mathrm{mean}(f(\mathbb{P})) == \mathrm{mean}(f(\mathbb{Q})) \\
i.e., \frac{1}{m}\sum^m_{i=1}f(p_i) = \frac{1}{n}\sum^n_{i=1}f(q_i)
\end{aligned}
\end{equation}
$$</p>
<p>则$p$和$q$是同一分布。</p>
<p><strong>MD</strong> can be defined as
$$
\begin{equation}
\begin{aligned}
\mathrm{MD}&amp;=|\mathrm{mean}(f(\mathbb{P})) -\mathrm{mean}(f(\mathbb{Q})) | \\
&amp;= |\frac{1}{m}\sum^m_{i=1}f(p_i) - \frac{1}{n}\sum^n_{i=1}f(q_i)|
\end{aligned}
\end{equation}
$$</p>
<h1 id="maximum-mean-discrepancy-mmd-最大均值差异">Maximum Mean Discrepancy (MMD) 最大均值差异</h1>
<h2 id="定义">定义</h2>
<p><strong>MMD:</strong>  在函数集$\mathcal{F}=\{f_1, f_2, \cdots \}$中， 找到一个函数$f^*$， 使得$|\mathrm{mean}(f^*(\mathbb{P})) -\mathrm{mean}(f^*(\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的<strong>最大均值差异</strong>（MMD）。MMD =0 表示两个分布相同。
$$
\operatorname{MMD}[\mathcal{F}, p, q]:=\sup _{f \in \mathcal{F}}\left(\mathbf{E}_{x \sim p}[f(x)]-\mathbf{E}_{y \sim q}[f(y)]\right)
$$
其中$\mathbf{E}_{x \sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\sup$为上确界直接理解为max就好。</p>
<h2 id="条件">条件</h2>
<p>为了准确判断分布$p$和$q$之间的距离，需要找到一个合适的函数，使得两个分布在这个函数上的距离尽可能大，但搜索空间不能过于大，所以函数空间$\mathcal{F}$要满足两个条件：</p>
<p><strong>C1:</strong> 函数集$\mathcal{F}$要足够丰富， 使得MMD尽可能准确</p>
<p><strong>C2:</strong> 考虑数据集样本数量，随着数据集的增大，MMD要能迅速收敛，要求$\mathcal{F}$足够restrictive (函数集不能无限大)</p>
<p>所以利用<a href="https://jhuow.github.io/posts/rkhs_kernel/">kernel</a> 方法，即， 将两个分布的样本空间映射到一个高维或者无限维的空间$\mathcal{H}$中，如果两个分布的样本在$\mathcal{H}$中的均值依然相等，那么这两个分布相等，MMD=0。两个分布在$\mathcal{H}$中的最大均值为MMD。</p>
<p><strong>因此，当$\mathcal{F}$是再生核Hilbert Space 上的单位球（unit ball）时，可以满足以上两个条件</strong>。 即，将$\mathcal{F}$定义为某个kernel对应的RKHS中的函数， 例如，</p>
<p>给定一个Gaussian Kernel: $k(u,v) = \{\exp({-\frac{||u-v||^2}{2\sigma}})\}_\sigma$, 这个kernel函数是一个Hilbert Space的再生核，那么这个空间可以表示为</p>
<p>$$
\begin{equation}
\mathcal{H}_k = \operatorname{span}({\Phi(x): x \in \mathcal{X}})=\left\{f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right): m \in \mathbf{N}, x_{i} \in \mathcal{X}, \alpha_{i} \in \mathbf{R}\right\} \tag{1}
\end{equation}
$$
空间$\mathcal{X}$中的每个元素$x_i$都对应于一个函数$k(\cdot,x_i)=k_{x_i}(\cdot)$, 那么$\mathcal{X}$中的所有元素所产生的函数$\{k_{x_i}(\cdot)\}_{x_i \in \mathcal{X}}$ 可以span成一个<strong>Function Space</strong>, 如公式1所示， 这个function space中的每个function可以由&quot;basis functions&quot;$\{k_{x_i}(\cdot)\}_{x_i \in \mathcal{X}}$ 通过线性组合得到。那么</p>
<p>$$f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)$$</p>
<p>可以表示kernel $k(\cdot, \cdot)$的RKHS中的每个function。 每个valid kernel都有一个RKHS $\mathcal{H}_k$与它对应。</p>
<p>我们将MMD的候选函数集$\mathcal{F}$定义为某一个kernel $k(\cdot,\cdot)$所对应的RKHS $\mathcal{H}_k$中的函数，这样就可以满足所有候选函数都在$\mathcal{H}_k$中(足够多)，同时如果kernel是Gaussian Kernel, 相当于把样本空间映射到无限高维来做MD,更加准确。</p>
<p>另外，我们限制范式norm$||f||_{\mathcal{H}_k} \leq 1$来避免上界取到无限大</p>
<h2 id="回到mmd">回到MMD</h2>
<p>已知$\mathcal{F}=\{f_1(\cdot), f_2(\cdot), \cdots \}$中的每个函数都是一个高斯核函数$k(\cdot,\cdot)$的RKHS中的函数，要从$\mathcal{H}_k$中选一个函数$f^*(\cdot)$，使得两个分布的样本间距离在$k(\cdot,\cdot)$的RKHS上最大。</p>
<p>因为$f(\cdot)$是$\mathcal{H}_k$中的一个函数，那么$f(\cdot)$可以表示为$\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)$, 此时，下式一定成立（参考<a href="https://jhuow.github.io/posts/rkhs_kernel/">这里</a>）：</p>
<p>$$
f(x) = \langle f(\cdot), k(\cdot, x) \rangle_{\mathcal{H}_k}
$$
$k(\cdot, x) = \Phi(x)$表示将$x$映射到空间$\mathcal{H}_{k}$上的值，即$x$在$\mathcal{H}_{k}$上的表示。 若$k$是Gaussian Kernel, 那么$k(\cdot, x)$就是$x$在无限维空间上的表示。</p>
<p>连续空间中$\mathbf{E}_{x \sim p}[f(x)]$可以写为：
$$
\begin{equation}
\begin{aligned}
\mathbf{E}_{x \sim p}[f(x)] &amp;= \int_x p(x)f(x) dx\\
&amp; = \int_x p(x) \langle f(\cdot), k(\cdot, x) \rangle_{\mathcal{H}_k} dx \\
&amp;= \langle \int_x p(x)f(\cdot) dx, \int_x p(x)k(\cdot, x) dx \rangle_{\mathcal{H}_k}\\
&amp;= \langle f(\cdot), \mu_p\rangle_{\mathcal{H}_k}
\end{aligned}
\end{equation}
$$
其中$\mu_p = \int_x p(x)k(\cdot, x) dx$.</p>
<p>因此，MMD可以改写为：
$$
\begin{equation}
\begin{aligned}
\operatorname{MMD}(\mathrm{p}, \mathrm{q}, \mathcal{H})&amp;:=\sup_{f \in \mathcal{H},|f|_{\mathcal{H}} \leq 1}(\underset{\mathrm{p}(\boldsymbol{x})}{\mathbb{E}}[f(\boldsymbol{x})]-\underset{\mathrm{q}(\boldsymbol{y})}{\mathbb{E}}[f(\boldsymbol{y})])\\
&amp;=\sup_{f \in \mathcal{H},|f|_{\mathcal{H}_k} \leq 1}\left(\left\langle\mu_{\mathrm{p}}-\mu_{\mathrm{q}}, f\right\rangle_{\mathcal{H}_k}\right)
\end{aligned}
\end{equation}
$$
利用内积性质：$\langle a, b \rangle \leq ||a|| ||b||$， 因为
$$
||f(\cdot)||_{\mathcal{H}_k}\leq 1
$$
,
$$
\left\langle\mu_{\mathrm{p}}-\mu_{\mathrm{q}}, f\right\rangle_{\mathcal{H}_k} \leq ||\mu_{\mathrm{p}}-\mu_{\mathrm{q}}||_{\mathcal{H}_k}||f||_{\mathcal{H}_k}
$$
Then,
$$
\sup_{f \in \mathcal{H},|f|_{\mathcal{H}_k} \leq 1}\left(\left\langle\mu_{\mathrm{p}}-\mu_{\mathrm{q}}, f\right\rangle_{\mathcal{H}_k}\right) =||\mu_{\mathrm{p}}-\mu_{\mathrm{q}}||_{\mathcal{H}_k}
$$
其中$\mu_p = \int_x p(x)k(\cdot, x) dx$, $\mu_q = \int_y q(y)k(\cdot, y) dy$ 分别表示分布的期望(均值)。 然而期望无法直接计算，因此用样本空间的均值代替分布的期望：
$$
\begin{equation}
\begin{aligned}
\mathrm{M M D}(p,q,\mathcal{H}_k) &amp; \approx \mathrm{M M D}(X,Y,\mathcal{F}_{\mathcal{H}_k})\\ &amp;=\left|\left|\frac{1}{n} \sum_{i=1}^{n} f(x_i)-\frac{1}{m} \sum_{j=1}^{m} f(x_j)\right|\right|_{\mathcal{H}_k}
\end{aligned}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\begin{aligned}
\mathrm{M M D}^2(p,q,\mathcal{H}_k) &amp; \approx \mathrm{M M D}^2(X,Y,\mathcal{F}_{\mathcal{H}_k})\\
&amp;=\left|\left|\frac{1}{n} \sum_{i=1}^{n} f(x_i)-\frac{1}{m} \sum_{j=1}^{m} f(x_j)\right|\right|_{\mathcal{H}_k}^{2}\\
&amp;= \left|\left|\frac{1}{n^{2}} \sum_{i}^{n} \sum_{i^{\prime}}^{n} \left\langle f(x_i),f(x_i^{\prime})\right\rangle-\frac{2}{n m} \sum_{i}^{n} \sum_{j}^{m} \left\langle f(x_i), f(y_j)\right\rangle+\frac{1}{m^{2}} \sum_{j}^{m} \sum_{j^{\prime}}^{m} \left\langle f(y_j), f(y_j^{\prime})\right\rangle\right|\right|_{\mathcal{H}_k} \\
&amp; = \frac{1}{n^2} K_{x, x^\prime}-\frac{2}{nm} K_{x, y}+\frac{1}{m^{2}} K_{y, y^{\prime}}
\end{aligned}
\end{equation}
$$</p>
<p>令
$$
K=\begin{bmatrix} K_{x, x^{\prime}} &amp; K_{x, y} \\ K_{x, y}&amp; K_{y, y^{\prime}} \end{bmatrix}
$$</p>
<p>$$
M=\begin{bmatrix}\frac{1}{n^{2}} &amp;-\frac{1}{n m} \\ -\frac{1}{n m}&amp; \frac{1}{m^{3}} \end{bmatrix}
$$</p>
<p>最后：
$$
\mathrm{M M D}^2(X,Y,\mathcal{F}_{\mathcal{H}_k}) = tr(KM)
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>https://JhuoW.github.io/posts/rkhs_kernel/</link>
      <pubDate>Sat, 26 Mar 2022 22:39:20 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/rkhs_kernel/</guid>
      <description>Hilbert Space Definition 1 (Norm) Let $\mathcal{F}$ be a vector space over $\mathbb{R}$ (For example $\mathcal{F}=\mathbb{R}^n$ is a vector space). A function $||\cdot||_{\mathcal{F}}: \mathcal{F} \to [0, \inf)$ is said to be a norm on $\mathcal{F}$ if ($||\cdot||_{\mathcal{F}}$ 是一个有效norm算子要满足以下条件):
 For $f \in \mathcal{F}$, $||f||_{\mathcal{F}}=0$ if and only if $f=0$. (norm separates points) $|\lambda f|_{\mathcal{F}}=|\lambda||f|_{\mathcal{F}}$, $\forall \lambda \in \mathbb{R}, \forall f \in \mathcal{F}$ (positive homogeneity). $|f+g|_{\mathcal{F}} \leq|f|_{\mathcal{F}}+|g|_{\mathcal{F}}, \forall f, g \in \mathcal{F}$ (triangle inequality).</description>
      <content:encoded><![CDATA[<h1 id="hilbert-space">Hilbert Space</h1>
<p><em><strong>Definition 1</strong></em> (<a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf">Norm</a>) Let $\mathcal{F}$ be a vector space over $\mathbb{R}$ (For example $\mathcal{F}=\mathbb{R}^n$ is a vector space). A function $||\cdot||_{\mathcal{F}}: \mathcal{F} \to [0, \inf)$ is said to be a norm on $\mathcal{F}$ if ($||\cdot||_{\mathcal{F}}$ 是一个有效norm算子要满足以下条件):</p>
<ol>
<li>For $f \in \mathcal{F}$, $||f||_{\mathcal{F}}=0$ if and only if $f=0$. (norm separates points)</li>
<li>$|\lambda f|_{\mathcal{F}}=|\lambda||f|_{\mathcal{F}}$, $\forall \lambda \in \mathbb{R}, \forall f \in \mathcal{F}$ (positive homogeneity).</li>
<li>$|f+g|_{\mathcal{F}} \leq|f|_{\mathcal{F}}+|g|_{\mathcal{F}}, \forall f, g \in \mathcal{F}$ (triangle inequality).</li>
</ol>
<p>向$||\cdot||_{\mathcal{F}}$中输入任意一个向量，只要满足以上条件，那么$||\cdot||_{\mathcal{F}}$是一个valid norm operator.</p>
<h2 id="inner-product">Inner Product</h2>
<p>An <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">inner product</a> takes two elements of a vector space $\mathcal{X}$ and outputs a number. An inner product could be a usual dot product: $\langle\mathbf{u}, \mathbf{v}\rangle=\mathbf{u}^{\prime} \mathbf{v}=\sum_{i} u^{(i)} v^{(i)}$ (Inner Product can be Dot Product). Or the inner product could be something fancier（即内积不一定表示为点积的形式）. If an Inner Product $\langle \cdot,\cdot \rangle$ is valid, it <em><strong>MUST</strong></em>  satisfy the following conditions:</p>
<ol>
<li>
<p>Symmetry
$$\langle u, v\rangle=\langle v, u\rangle \quad \forall u, v \in \mathcal{X}$$</p>
</li>
<li>
<p>Bilinearity
$$\langle\alpha u+\beta v, w\rangle=\alpha\langle u, w\rangle+\beta\langle v, w\rangle \quad \forall u, v, w \in \mathcal{X}, \forall \alpha, \beta \in \mathbf{R}$$</p>
</li>
<li>
<p>Strict Positive Definiteness
$$
\begin{gathered}
\langle u, u\rangle \geq 0 \forall x \in \mathcal{X} \\
\langle u, u\rangle=0 \Longleftrightarrow u=0
\end{gathered}$$</p>
</li>
</ol>
<p>An  <em><strong>inner product space</strong></em> (or pre-Hilbert space) is a vector space together with an inner product. （包含内积运算的向量空间称为 内积空间，即可以定义内积运算的向量空间）。</p>
<p>Kernel is a kind of Inner Product. For example, the Gaussian kernel is defined as:
$$
\begin{equation}
\langle u, v \rangle = k(u,v) = \exp({-\frac{||u-v||^2}{2\sigma}}) \tag{1}
\end{equation}
$$</p>
<h2 id="hilbert-space-1">Hilbert Space</h2>
<p><em><strong>Definition 2</strong></em> (<a href="https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf">Hilbert Space</a>)  A Hilbert Space is an Inner Product space that is complete and separable with respect to the norm defined by the inner product.</p>
<p>&lsquo;Complete&rsquo; means sequences converge to elements of the space - there aren&rsquo;t any &ldquo;holes&rdquo; in the space.</p>
<h1 id="finite-states">Finite States</h1>
<p>Given finite input space ${x_1, x_2, \cdots x_m }$. I want to be able to take inner products between any two of them using my function $k$  as the inner product ($k$ is customized and satisfy three conditions. For example, $k$ is a Gaussian inner product as Eq.(1)). Inner products by definition are symmetric, so $k(x_i, x_j)=k(x_j, x_i)$ , which yields a symmetric matrix $\mathbf{K}$.</p>
<p>Since $\mathbf{K}$ is real symmetric, and this means we can diagonalize it （实对称阵可以对角化，即特征分解）, and the eigendecomposition takes this form:
$$
\begin{equation}
\begin{aligned}
\mathbf{K} &amp;=\mathbf{V} \Lambda \mathbf{V}^T \\
&amp;= \mathbf{V} 	\begin{bmatrix}
\lambda_1 &amp;   &amp; &amp;  \\
&amp; \lambda_2 &amp;  &amp; \\
&amp;   &amp; \cdots &amp;\
&amp;   &amp;  &amp;\lambda_m
\end{bmatrix} \mathbf{V}^T \\
&amp;= \begin{bmatrix}
v_1 &amp; v_2 &amp; \cdots v_m
\end{bmatrix} \begin{bmatrix}
\lambda_1 &amp;   &amp; &amp;  \\
&amp; \lambda_2 &amp;  &amp; \\
&amp;   &amp; \cdots &amp;\\
&amp;   &amp;  &amp;\lambda_m
\end{bmatrix}
\begin{bmatrix}
v_1^T\\
v_2^T\\
\cdots \\
v_m^T
\end{bmatrix}\\
&amp;=v_1\lambda_1 v_1^T + \cdots + v_m\lambda_m v_m^T = \sum_{t=1}^m v_t\lambda_tv_t^T
\end{aligned} \tag{2}
\end{equation}
$$
Let the $i$-th element of vector $v$ as $v^{(i)}$, then
$$
\begin{equation}
\begin{aligned}
\mathbf{K}_{ij} = k(x_i, x_j) &amp;= [\sum_{t=1}^m v_t\lambda_tv_t^T]_{ij}\\
&amp;=\sum^m_{t=1} v_t^{(i)} \lambda_t v_t^{(j)}
\end{aligned} \tag{3}
\end{equation}
$$
If $\mathbf{K}$ is a <strong>positive semi-definite</strong> (<strong>PSD</strong>) matrix, then $\lambda_1, \cdots \lambda_m \geq 0$.</p>
<blockquote>
<p><em><strong>Assumption 1</strong></em>. All $\lambda_t$ are nonnegative.</p>
</blockquote>
<p>We consider this feature map:
$$
\begin{equation}
\Phi\left(x_{i}\right)=\left[\sqrt{\lambda_{1}} v_{1}^{(i)}, \ldots, \sqrt{\lambda_{t}} v_{t}^{(i)}, \ldots, \sqrt{\lambda_{m}} v_{m}^{(i)}\right] \in \mathbb{R}^m \tag{4}
\end{equation}
$$
(writing it for $x_j$ too):
$$
\begin{equation}
\boldsymbol{\Phi}\left(x_{j}\right)=\left[\sqrt{\lambda_{1}} v_{1}^{(j)}, \ldots, \sqrt{\lambda_{t}} v_{t}^{(j)}, \ldots, \sqrt{\lambda_{m}} v_{m}^{(j)}\right]  \in \mathbb{R}^m \tag{5}
\end{equation}
$$
即 $\Phi: \mathcal{X} \to \mathbb{R}^m$ 将$x\in \mathcal{X}$映射到$m$维向量空间$\mathbb{R}^m$中的一个点。</p>
<p>With this choice, the inner product $k$ is just defined as a dot product in $\mathbb{R}^m$:
$$
\begin{equation}
\left\langle\Phi\left(x_{i}\right), \Phi\left(x_{j}\right)\right\rangle_{\mathbf{R}^{m}}=\sum_{t=1}^{m} \lambda_{t} v_{t}^{(i)} v_{t}^{(j)}=\left(\mathbf{V} \Lambda \mathbf{V}^{\prime}\right)_{i j}=K_{i j}=k\left(x_{i}, x_{j}\right)  \tag{6}
\end{equation}
$$
If there exists an eigenvalue $\lambda_s &lt;0$ (即$\sqrt{\lambda_s} = \sqrt{|\lambda_s|} i$). $\lambda_s$对应的特征向量$v_s$。用$v_s \in \mathbb{R}^m$的$m$个元素$v_s = [v_s^{(1)},\cdots, v_s^{(m)}]$, 来对$\Phi(x_1),\cdots, \Phi(x_m)$做线性组合：
$$
\begin{equation}
\mathbf{z}=\sum_{i=1}^{m} v_{s}^{(i)} \boldsymbol{\Phi}\left(x_{i}\right) \tag{7}
\end{equation}
$$</p>
<p>It is obvious that $\mathbf{z} \in \mathbb{R}^m$. Then calculate
$$
\begin{equation}
\begin{aligned}
|\mathbf{z}|_{2}^{2} &amp;=\langle\mathbf{z}, \mathbf{z}\rangle_{\mathbf{R}^{m}}=\sum_{i} \sum_{j} v_{s}^{(i)} \boldsymbol{\Phi}\left(x_{i}\right)^{T} \boldsymbol{\Phi}\left(x_{j}\right) v_{s}^{(j)}=\sum_{i} \sum_{j} v_{s}^{(i)} K_{i j} v_{s}^{(j)} \\
&amp;=\mathbf{v}_{s}^{T} \mathbf{K} \mathbf{v}_{s}=\lambda_{s}&lt;0
\end{aligned}  \tag{8}
\end{equation}
$$
which conflicts with the geometry of the feature space.</p>
<p>如果$\mathbf{K}$不是半正定，那么feature space $\mathbb{R}^m$存在小于0的值。所以假设Assumption不成立。即，若$k$表示有限集的内积，那么它的Gram Matrix一定半正定(PSD)，否则无法保证该空间中的norm大于0。</p>
<p>有效的内积对应的Gram Matrix 必定PSD.</p>
<h1 id="kernel">Kernel</h1>
<p><em><strong>Definition 3.</strong></em> (<a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">Kernel</a>) A function $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a kernel if</p>
<ol>
<li>$k$ is symmetric: $k(x,y) = k(y,x)$.</li>
<li>$k$ gives rise to a positive semi-definite &ldquo;Gram matrix,&rdquo; i.e., for any $m\in \mathbb{N}$ and any $x_1,\cdots,x_m$ chosen from $X$, the Gram matrix $\mathbf{K}$ defined by $\mathbf{K}_{ij} = k(x_i,x_j)$ is positive semi-definite.</li>
</ol>
<p>Another way to show that a matrix $\mathbf{K}$ is positive semi-definite is to show that
$$
\begin{equation}
\forall \mathbf{c} \in \mathbf{R}^{m}, \mathbf{c}^{T} \mathbf{K} \mathbf{c} \geq 0 \tag{9}
\end{equation}
$$
Here are some nice properties of $k$:</p>
<ul>
<li>$k(u,u) \geq 0$ (Think about the Gram matrix of $m = 1$.)</li>
<li>$k(u, v) \leq \sqrt{k(u, u) k(v, v)}$ (This is the Cauchy-Schwarz inequality.)</li>
</ul>
<h2 id="reproducing-kernel-hilbert-space-rkhs">Reproducing Kernel Hilbert Space (RKHS)</h2>
<p>给定一个kernel $k(\cdot, \cdot): \mathcal{X} \times \mathcal{X} \to \mathbb{R}$. 定义一个函数空间（space of functions）$\mathbf{R}^{\mathcal{X}}:={f: \mathcal{X} \rightarrow \mathbb{R}}$. $\mathbf{R}^{\mathcal{X}}$ 是一个 Hilbert Space， 该空间中的每个元素是一个$\mathcal{X}$映射到$\mathbb{R}$的函数。</p>
<p>令$k_x(\cdot) = k(x, \cdot)$, 假设$x$是一个定值（Constant），自变量（输入）用$\cdot$表示。那么$k(x, \cdot)$ 也是$\mathbf{R}^{\mathcal{X}}$空间中的一个函数。</p>
<p>每个函数$k_x(\cdot)$ 都与一个特定的$x \in \mathcal{X}$有关，即每个$x$对应于一个函数$k_x(\cdot) = k(\cdot, x)$. 这种对应关系表示为$\Phi(x) = k_x(\cdot) = k(x,\cdot)$, 即：
$$
\begin{equation}
\Phi: x \longmapsto k(\cdot, x) \tag{10}
\end{equation}
$$
即 $\Phi$的输入为$x\in \mathcal{X}$, 输出一个函数, 输出的函数属于$\mathbf{R}^{\mathcal{X}}$空间。</p>
<p>在连续空间$\mathcal{X}$中，$x \in \mathcal{X}$ 有无穷多种情况，那么$\Phi(x)=k_x(\cdot)=k(x, \cdot)$也有无穷多种情况，即无穷多种函数。 这些函数可以span 一个Hilbert Space:
$$
\begin{equation}
\mathcal{H}_k = \operatorname{span}({\Phi(x): x \in \mathcal{X}})=\left\{f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right): m \in \mathbf{N}, x_{i} \in \mathcal{X}, \alpha_{i} \in \mathbf{R}\right\}  \tag{11}
\end{equation}
$$
其中$k(x,\cdot)=\Phi(x)$可以理解为将$x$映射为一个函数（or vector）。上述Hilbert Space是由任意$k(x, \cdot)$线性组合而成的函数空间，该空间中的每个元素可以表示为
$$
\begin{equation}
f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)  \tag{12}
\end{equation}
$$
所以$\mathcal{H}$可以看作是kernel $k$对应的一个Hilbert Space。</p>
<p>给定$\mathcal{H}$中的任意两个函数$f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)$, $g(\cdot)=\sum_{j=1}^{m^{\prime}} \beta_{j} k\left(\cdot, x_{j}^{\prime}\right)$。注意$f(\cdot)$和$g(\cdot)$可以表示$\mathcal{H}$中任意两个元素。我们将$\mathcal{H}$上的内积定义为：
$$
\begin{equation}
\langle f, g\rangle_{\mathcal{H}_{k}}=\sum_{i=1}^{m} \sum_{j=1}^{m^{\prime}} \alpha_{i} \beta_{j} k\left(x_{i}, x_{j}^{\prime}\right) \tag{13}
\end{equation}
$$
由<a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">Proof</a>证明了该内积符合三个条件，顾上式是$\mathcal{H}$空间中一个有效的内积算子。注：$\mathcal{H}_k$表示该Hilbert Space是由函数 $k(x,\cdot)$ span而成的，与Kernel $k$有关.</p>
<p>$k(x,\cdot)$也是$\mathcal{H}_k$中的一个函数，那么它与 $f$的内积为：
$$
\begin{equation}
\langle k(\cdot, x), f\rangle_{\mathcal{H}_{k}}=
\sum_{i=1}^m \alpha_i k(x,x_i)
=f(x) \tag{14}
\end{equation}
$$
<strong>Theorem 1.</strong>  $k(\cdot, \cdot)$ is a reproducing kernel of a Hilbert space $\mathcal{H}_k$ if $f(x)=\langle k(x, \cdot), f(\cdot)\rangle$.</p>
<p>$\mathcal{H}_k$ 为$k(\cdot, \cdot)$的再生核希尔伯特空间。</p>
<p>同理，$k(\cdot, x_i)$, $k(\cdot, x_j)$都为$\mathcal{H}_k$中的函数， 计算他们的内积:
$$
\begin{equation}
\left\langle k(\cdot, x_i), k\left(\cdot, x_j\right)\right\rangle_{\mathcal{H}_{k}}=k\left(x_i, x_j\right)  \tag{15}
\end{equation}
$$
因为$ k(\cdot, x_i) = \Phi(x_i)$, $ k(\cdot, x_j) = \Phi(x_j)$, 所以
$$
\begin{equation}
k\left(x_i, x_j\right) = \left\langle  \Phi(x_i), \Phi(x_j)\right\rangle_{\mathcal{H}_{k}} \tag{16}
\end{equation}
$$
表示将$x_i$和$x_j$ 映射成$\mathcal{H}_k$中的函数（向量）后再做内积。</p>
<h4 id="参考文献">参考文献</h4>
<p>[1] <a href="https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf">https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf</a></p>
<p>[2] <a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf">http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf</a></p>
<p>[3] <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf</a></p>
<p>我把本文整理成了<a href="/posts/rkhs/RKHS.pdf">PDF</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>NIPS2018 《DiffPool： Hierarchical Graph Representation Learning with Differentiable Pooling》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/diffpool/</link>
      <pubDate>Thu, 19 Dec 2019 19:32:36 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/diffpool/</guid>
      <description>论文地址： DiffPool
Introduction 传统的GNN算法在Node-level的任务如节点分类、链路预测上有着较好的效果。但是，现有的GNN方法由于其存在平面化的局限性，因此无法学习图的层级表示（意味着无法预测整个图的标签），顾无法实现图分类任务。举个栗子，一个Graph，可以分成600个subgraph，每个节点都存在于其中的某个subgraph（一个节点只存在于一个subgraph中），每个subgraph拥有一个标签，如何预测subgraph的标签是这篇文章主要想解决的问题。传统的GNN的图分类方法都是为Graph中的所有节点生成Embedding，然后将对这些Embedding做全局聚合（池化），如简单的把属于同一个subgraph的节点求和或者输入到MLP中生成一个标签向量来表示整个subgraph，但是这样可能忽略的图的层级结构信息。
本文提出了一种端到端的可微可微图池化模块DiffPool，原理如下图所示：
在深度GNN中的每层中为节点学习可微的软簇分配，将节点映射到簇中，这些簇作为新的节点作为下一层GNN的输入。上图的Original Network部分是一个Subgraph，传统的方法是直接求出这个Subgraph中每个节点的Embedding，然后相加或输入到一个神经网络中，得到一个预测向量，这种方法可以称为“全局池化”。DiffPool中，假设第$l$层的输入是$1000$个簇（如果是第一层输入就是1000个节点），我们先设置第$l+1$层需要输入的簇的个数（假设为$100$），也就是第$l$层输出的簇个数，然后在$l$层中通过一个分配矩阵将$1000$个簇做合并，合并成100个“节点”，然后将这100个节点输入到$l+1$层中，最后图中的节点数逐渐减少，最后，图中的节点只有一个，这个节点的embedding就是整个图的表示，然后将图输入到一个多层感知机MLP中，得到预测向量，在于真值的one-hot向量做cross-entropy，得到Loss。
Model：DiffPool 一个Graph表示为$\mathcal{G} = (A,F)$，其中$A \in {0,1}^{n \times n}$是Graph的邻接矩阵，$F \in \mathbb{R}^{n \times d}$表示节点特征矩阵，每个节点有$d$维的特征。给定一个带标签的子图集$\mathcal{D}=\left\{\left(G_{1}, y_{1}\right),\left(G_{2}, y_{2}\right), \ldots\right\}$， 其中 $y_{i} \in \mathcal{Y}$表示每个子图$G_i \in \mathcal{G}$的标签，任务目标是寻找映射$f: \mathcal{G} \rightarrow \mathcal{Y}$，将图映射到标签集。我们需要一个过程来将每个子图转化为一个有限维度的向量$\mathbb{R}^D$。
Graph Neural Networks 一般，GNN可以表示成&amp;quot;Message Passing&amp;quot;框架： $$ H^{(k)}=M\left(A, H^{(k-1)} ; \theta^{(k)}\right) $$ 其中$H^{(k)} \in \mathbb{R}^{n \times d}$表示GNN迭代$k$次后的node embedding，$M$是一个Message扩散函数，由邻接矩阵$A$和一个可训练的参数$\theta^{(k)}$决定。$H^{(k-1)}$是由前一个message passing过程生成的node embedding。当$k = 1$时，第一个GNN的输入为$H^{(0)}$是原始的节点特征$H^{(0)} = F$。
GNN的一个主要目标是设计一个Message Passage函数$M$，GCN（kipf.2016）是一种流行的GNN，$M$的实现方式是将线性变换和ReLU非线性激活结合起来: $$ H^{(k)}=M\left(A, H^{(k-1)} ; W^{(k)}\right)=\operatorname{ReLU}\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(k-1)} W^{(k-1)}\right) $$ 其中，$\tilde{A} = A+I$是一个加上自环的邻接矩阵，$\tilde{D}=\sum_{j} \tilde{A}_{i j}$是$\tilde{A}$的度矩阵，$W^{(k)} \in \mathbb{R}^{d \times d}$是一个可训练的权重矩阵，$W$与节点个数以及每个节点的度无关，可以看做一个特征增强矩阵，用来规定GCN的输出维度。</description>
      <content:encoded><![CDATA[<p>论文地址： <a href="https://dl.acm.org/doi/pdf/10.5555/3327345.3327389">DiffPool</a></p>
<h1 id="introduction">Introduction</h1>
<p>传统的GNN算法在Node-level的任务如节点分类、链路预测上有着较好的效果。但是，现有的GNN方法由于其存在平面化的局限性，因此无法学习图的层级表示（意味着无法预测整个图的标签），顾无法实现图分类任务。举个栗子，一个Graph，可以分成600个subgraph，每个节点都存在于其中的某个subgraph（一个节点只存在于一个subgraph中），每个subgraph拥有一个标签，如何预测subgraph的标签是这篇文章主要想解决的问题。传统的GNN的图分类方法都是为Graph中的所有节点生成Embedding，然后将对这些Embedding做全局聚合（池化），如简单的把属于同一个subgraph的节点求和或者输入到MLP中生成一个标签向量来表示整个subgraph，但是这样可能忽略的图的层级结构信息。</p>
<p>本文提出了一种端到端的可微可微图池化模块<strong>DiffPool</strong>，原理如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-12-19-diffpool/1.png" alt=""  />
</p>
<p>在深度GNN中的每层中为节点学习可微的软簇分配，将节点映射到簇中，这些簇作为新的节点作为下一层GNN的输入。上图的Original Network部分是一个Subgraph，传统的方法是直接求出这个Subgraph中每个节点的Embedding，然后相加或输入到一个神经网络中，得到一个预测向量，这种方法可以称为“全局池化”。<strong>DiffPool</strong>中，假设第$l$层的输入是$1000$个簇（如果是第一层输入就是1000个节点），我们先设置第$l+1$层需要输入的簇的个数（假设为$100$），也就是第$l$层输出的簇个数，然后在$l$层中通过一个分配矩阵将$1000$个簇做合并，合并成100个“节点”，然后将这100个节点输入到$l+1$层中，最后图中的节点数逐渐减少，最后，图中的节点只有一个，这个节点的embedding就是整个图的表示，然后将图输入到一个多层感知机MLP中，得到预测向量，在于真值的one-hot向量做cross-entropy，得到Loss。</p>
<h1 id="modeldiffpool">Model：DiffPool</h1>
<p>一个Graph表示为$\mathcal{G} = (A,F)$，其中$A \in {0,1}^{n \times n}$是Graph的邻接矩阵，$F \in \mathbb{R}^{n \times d}$表示节点特征矩阵，每个节点有$d$维的特征。给定一个带标签的子图集$\mathcal{D}=\left\{\left(G_{1}, y_{1}\right),\left(G_{2}, y_{2}\right), \ldots\right\}$， 其中 $y_{i} \in \mathcal{Y}$表示每个子图$G_i \in \mathcal{G}$的标签，任务目标是寻找映射$f: \mathcal{G} \rightarrow \mathcal{Y}$，将图映射到标签集。我们需要一个过程来将每个子图转化为一个有限维度的向量$\mathbb{R}^D$。</p>
<h2 id="graph-neural-networks">Graph Neural Networks</h2>
<p>一般，GNN可以表示成&quot;Message Passing&quot;框架：
$$
H^{(k)}=M\left(A, H^{(k-1)} ; \theta^{(k)}\right)
$$
其中$H^{(k)} \in \mathbb{R}^{n \times d}$表示GNN迭代$k$次后的node embedding，$M$是一个Message扩散函数，由邻接矩阵$A$和一个可训练的参数$\theta^{(k)}$决定。$H^{(k-1)}$是由前一个message passing过程生成的node embedding。当$k = 1$时，第一个GNN的输入为$H^{(0)}$是原始的节点特征$H^{(0)} = F$。</p>
<p>GNN的一个主要目标是设计一个Message Passage函数$M$，GCN（kipf.2016）是一种流行的GNN，$M$的实现方式是将线性变换和ReLU非线性激活结合起来:
$$
H^{(k)}=M\left(A, H^{(k-1)} ; W^{(k)}\right)=\operatorname{ReLU}\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(k-1)} W^{(k-1)}\right)
$$
其中，$\tilde{A} = A+I$是一个加上自环的邻接矩阵，$\tilde{D}=\sum_{j} \tilde{A}_{i j}$是$\tilde{A}$的度矩阵，$W^{(k)} \in \mathbb{R}^{d \times d}$是一个可训练的权重矩阵，$W$与节点个数以及每个节点的度无关，可以看做一个特征增强矩阵，用来规定GCN的输出维度。</p>
<p>一个完整的GNN模型会迭代$K$次来输出最终的node embedding$Z = H^{(K)} \in \mathbb{R}^{n \times d}$。对于GCN，GAT，GraphSage，$K$一般取2-6。文中为了简单表示，忽略了GNN的内部结构，用$Z=GNN(A,X)$来表示一个任意的执行$K$次的GNN模块。</p>
<h2 id="gnn和池化层的堆叠">GNN和池化层的堆叠</h2>
<p>这篇工作的目标是定义一个一般的，端到端的可微策略，允许以层级的方式堆叠多个GNN模块。给定原始的邻接矩阵$A \in \mathbb{R}^{n \times n}$，$Z=GNN(A,X)$十一GNN模块的输出（假设这个GNN模块做了3次迭代）。我们需要定义一个策略来输出一个新的粗化图，这个粗化图包含$m$个节点，$m &lt; n$，它的邻接矩阵一个带权重的邻接矩阵$A&rsquo; \in \mathbb{R}^{m \times m}$，同时，输出node embedding $Z&rsquo; \in \mathbb{R}^{m \times d}$。这个粗化图（$m$个节点的图）作为下一层GNN的输入 （将$A&rsquo;$和$Z&rsquo;$输入下一个GNN层）。最后所有节点粗化为只有一个节点的图，这个节点的embedding就是这个subgraph的表示。因此，目标为：如何使用上一层GNN的输出结果，对节点做合并或池化，是的图中的节点减少，再将粗化的图输入到下一个GNN中。</p>
<h2 id="基于可学习分配的可微分池化">基于可学习分配的可微分池化</h2>
<p><strong>DiffPool</strong>通过对一个GNN模块的输出学习一个聚类分配矩阵来解决这个问题。可微池化层根据$l-1$层的GNN模块（假设是一个3次迭代的GNN模块）产生的node embedding来对节点做合并，从而产生一个粗化图，这个粗化图作为$l$层GNN模块的输入，最终，整个subgraph被粗化为一个cluster，可以看做一个节点。</p>
<h3 id="用分配矩阵进行池化">用分配矩阵进行池化</h3>
<p>$S^{(l)} \in \mathbb{R}^{n_{l} \times n_{l+1}}$表示第$l$层的聚类分配矩阵，$S^{(l)}$的每一行表示第l层的每个节点（cluster）,每一列表示$l+1$层的每个cluster（节点）。$S^{(l)}_{ij}$表示第$l$层的节点$i$属于第$l+1$层cluster $j$的概率，所以$S^{(l)}$是个概率矩阵。</p>
<p>假如已经有了第$l$层的节点分配矩阵$S^{(l)}$，将第$l$层的邻接矩阵表示为$A^{(l)}$，将第$l$层GNN模块的输出节点特征（node embedding）表示为$Z^{(l)}$，通过DiffPool层可以将第$l$层的图粗化为$\left(A^{(l+1)}, X^{(l+1)}\right)=\operatorname{DIFFPOOL}\left(A^{(l)}, Z^{(l)}\right)$，其中，$A^{(l+1)}$是$l+1$层图的邻接矩阵，是一个粗化后的图，$X^{(l+1)}$是下一层的输入特征（node/cluster embedding）：
$$
\begin{aligned}
&amp;X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1} \times d}\
&amp;A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times n_{l+1}}
\end{aligned}
$$
上面第一个公式将第$l$层节点嵌入$Z^{(l)}$转化为下一层的输入特征$X^{(l+1)}$。第二个公式将第$l$层的邻接矩阵转化为$l+1$层的粗化图邻接矩阵$A^{(l+1)}$。$n_{l+1}$是$l+1$层节点（cluster）的数量。最后，将$A^{(l+1)}$和$X^{(l+1)}$作为下一层GNN的输入。这样图中的节点就由$n_l$个下降到$n_{l+1}$个。</p>
<h3 id="学习分配矩阵s">学习分配矩阵S</h3>
<p>第$l$层的输入特征$X^{(l)}$，用一个GNN模块（代码中是一个3层的GCN）得到node embedding：
$$
Z^{(l)}=\mathrm{GNN}_{l, \text { embed }}\left(A^{(l)}, X^{(l)}\right)
$$
用另外一个GNN模块（代码中是一个3层的GCN）在用一个softmax转化为概率矩阵来的到节点分配矩阵：
$$
S^{(l)}=\operatorname{softmax}\left(\mathrm{GNN}_{l, \mathrm{pool}}\left(A^{(l)}, X^{(l)}\right)\right)
$$
$S^{(l)}$是一个$n_l \times n_{l+1}$的全链接矩阵，$S^{(l)}_{ij}$表示第$l$层的节点$i$属于第$l+1$层cluster $j$的概率。</p>
<p>$l=0$时，第一层GNN的输入是subgraph的原始邻接矩阵$A$和特征矩阵$F$，倒数第二层$l=L-1$时的分配矩阵$S^{(L-1)}$是一个全1向量，那么最后将所以节点归为一类，产生一个代表整个图的嵌入向量。</p>
<p>所以，把图节点的合并过程称为分层的图表示学习（Hierarchical Graph Representation Learning）。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Notes for Spectral Clustering</title>
      <link>https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/</link>
      <pubDate>Sat, 07 Sep 2019 09:11:09 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/</guid>
      <description>最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。
本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments
Introduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。
基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} &amp;gt; 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\displaystyle \left(\begin{array}{ccc}{d_{1}} &amp;amp; {\ldots} &amp;amp; {\ldots} \\\ {\ldots} &amp;amp; {d_{2}} &amp;amp; {\ldots} \\\ {\vdots} &amp;amp; {\vdots} &amp;amp; {\ddots} \\\ {\ldots} &amp;amp; {\ldots} &amp;amp; {d_{n}}\end{array}\right) ^{n\times n} $$ 是一个$n \times n$的对角阵，对角元素是每个节点的度和。
定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义： $$|A|=A 中的节点个数 $$
$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$</description>
      <content:encoded><![CDATA[<p>最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。</p>
<p>本文主要参考了：[1] <a href="https://www.cnblogs.com/pinard/p/6221564.html#!comments">https://www.cnblogs.com/pinard/p/6221564.html#!comments</a></p>
<h1 id="introduction">Introduction</h1>
<p>谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。</p>
<h1 id="基础1-无向权重图">基础1： 无向权重图</h1>
<p>对于边$(v_i,v_j)$, 它的权重$w_{ij} &gt; 0$。对于没有边的节点$v_i$和$v_j$,  他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即：
$$
d_i = \sum_{j=1}^n w_{ij}
$$
根据所有节点的度值，我们可以得到一个度矩阵$D$:
$$
D=\displaystyle \left(\begin{array}{ccc}{d_{1}} &amp; {\ldots} &amp; {\ldots} \\\ {\ldots} &amp; {d_{2}} &amp; {\ldots} \\\ {\vdots} &amp; {\vdots} &amp; {\ddots} \\\ {\ldots} &amp; {\ldots} &amp; {d_{n}}\end{array}\right) ^{n\times n}
$$
是一个$n \times n$的对角阵，对角元素是每个节点的度和。</p>
<p>定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义：
$$|A|=A 中的节点个数 $$</p>
<p>$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$</p>
<h1 id="基础2相似矩阵">基础2：相似矩阵</h1>
<p>再提下谱聚类的基本思想： 距离较远的两个点之间权重值较低，距离较近的两个点之间权重值较高</p>
<p>但是在谱聚类中，我们只能获得数据点的定义，无法给出邻接矩阵，因为是离散的分布在空间中，无法得知其中的连接关系。</p>
<p>一般来说，通过样本点距离度量的相似矩阵$S$来获得邻接矩阵$W$.</p>
<p>构建邻接矩阵$W$有两种方法: $\epsilon$-邻近法， K邻近法和全连接法。</p>
<h2 id="epsilon-邻近法">$\epsilon$-邻近法</h2>
<p>$\epsilon$为距离的阈值，欧式距离$S_{ij}$表示两点$v_i$,$v_j$的坐标$x_i$,$x_j$之间的距离。 即 $S_{ij} =||x_i-x_j||^2_2$为相似矩阵$S \in \mathbb{R}^{n \times n}$的第$i$行第$j$个元素，则邻接矩阵$W$可以表示为：</p>
<p>$$
w_{ij}=\left\{\begin{array}{ll}{0} &amp; {s_{i j}&gt;\epsilon} \\  {\epsilon} &amp; {s_{i j} \leq \epsilon}\end{array}\right.
$$</p>
<p>意思是如果两点之间的距离大于$\epsilon$，那么他们之间的权重为0， 如果他们之间的距离小于$\epsilon$，他们之间的权重为$\epsilon$。 这种方法的弊端在于点之间的距离只有两种情况，不够精确，故很少采用。</p>
<h2 id="k邻近法">K邻近法</h2>
<p>利用<strong>KNN</strong>算法遍历所有样本点，取每个样本点最近的$k$个点作为近邻，只有和样本点距离最近的$k$个点的$w_{ij} &gt;0$，但会出现一种情况， $v_i$的k个近邻中有$v_j$，但$v_j$的k个近邻中没有$v_i$，这样会造成邻接矩阵$W$的不对称， 因此提供两种解决办法</p>
<p>第一种： 只要$v_j$在$v_i$的K邻域中，那么不管$v_i$在不在$v_j$的K邻域中，都把$v_i$加入$v_j$的邻域中，即：</p>
<p>$$
w_{i j}=w_{j i}=\left\{\begin{array}{ll}{0} &amp; {x_{i} \notin K N N\left(x_{j}\right) \text { and } x_{j} \notin K N N\left(x_{i}\right)} \\ {\exp \left(-\frac{\left||x_{i}-x_{j}\right||^2_2}{2 \sigma^{2}}\right)} &amp; {x_{i} \in K N N\left(x_{j}\right) \text { or } x_{j} \in K N N\left(x_{i}\right)}\end{array}\right.
$$
第二种，必须$v_i$在$v_j$的K邻域中，且$v_j$在$v_i$的K邻域中，那么才保留两者间的权重，否则都为0，即：</p>
<p>$$
w_{i j}=w_{j i}=\left\{\begin{array}{ll}{0} &amp; {x_{i} \notin K N N\left(x_{j}\right) \text { or } x_{j} \notin K N N\left(x_{i}\right)} \\ {\exp \left(-\frac{||x_{i}-x_{j}||^2_2}{2 \sigma^{2}}\right)} &amp; {x_{i} \in K N N\left(x_{j}\right) \text { and } x_{j} \in K N N\left(x_{i}\right)}\end{array}\right.
$$</p>
<h2 id="全连接法">全连接法</h2>
<p>设所有点之间的权重都大于0，可以选择不同的核函数来定义边的权重，常用的如多项式核函数，高斯核函数， Sigmod核函数。 最常用的为高斯核函数RBF，将两点之间的距离带入高斯核函数RBF中，即：
$$
w_{i j}=w_{ji}=s_{i j}=s_{ji}=\exp \left(-\frac{\left|x_{i}-x_{j}\right|_{2}^{2}}{2 \sigma^{2}}\right)
$$
其中，$sigma$为为函数的宽度参数 , 控制了函数的径向作用范围。</p>
<h1 id="基础3拉普拉斯矩阵">基础3：拉普拉斯矩阵</h1>
<p>拉普拉斯矩阵定义为$L = D-W$ 如上文所示，$D$为度矩阵，$W$为邻接矩阵。</p>
<p>拉普拉斯矩阵具有如下性质：</p>
<ol>
<li>
<p>$L$是对称阵 （因为$D$和$W$都是对称阵）</p>
</li>
<li>
<p>$L$的所有特征值都是实数 （因为$L$是对称阵）</p>
</li>
<li>
<p>对于任意向量$f$， 有$f^TLf = \displaystyle \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n w_{ij} (f_i-f_j)^2$</p>
<p>推导：
$$
\begin{aligned}
f^TLf &amp;= f^TDf-f^TWf\\
&amp;= \sum^n_{i = 1}d_if_i^2 - \sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij}\\
&amp;= \frac{1}{2}\left(\sum^n_{i=1}d_if_i^2 -2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum^n_{i=1}d_if_i^2\right)\\
&amp;由于d_i = \sum_{j = 1}^nw_{ij}, 将d_i带入上式得\\
f^TLf &amp;= \frac{1}{2} \left(\sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2-2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2\right)\\
&amp; = \frac{1}{2} \left(\sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2-2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum_{j = 1}^n\sum_{i =1}^n w_{ji}f_j^2\right)\\
&amp;= \frac{1}{2}\left(\sum_{i=1}^n\sum_{j = 1}^n w_{ij} (f_i-f_j)^2\right)
\end{aligned}
$$</p>
</li>
<li>
<p>拉普帕斯矩阵是半正定的，且$n$个实数特征值都$\geq$，即 $0=\lambda_1 \leq \lambda_2 \cdots \leq \lambda_n$，且最小的特征值为0。</p>
<p>证明，因为$f^TLf \geq 0$ 所以$L$半正定。</p>
</li>
</ol>
<h1 id="基础4无向图切图">基础4：无向图切图</h1>
<p>对于无向图$G$，目的是将图$G = (v,E)$切成相互没有连接的k个子图，每个子图是一个节点集合：$A_1,A_2,\cdots, A_k$，满足$A_i \cap A_j = \phi$ 且$A_1 \cup A_2 \cup \cdots \cup A_k = V$，对于两个节点集合$A ,B \subset V$, $A \cap B = \phi$，定义$A$,$B$之间的切图权重为：
$$
W(A,B) = \sum_{v_i\in A, v_j \in B} w_{ij}  \quad 表示A中节点到B中节点的权重和
$$
对于$k$个子图节点集合$A_1,A_2,\cdots, A_k$，定义切图$Cut$为：
$$
Cut(A_1,A_2, \cdots, A_k) = \frac{1}{2}\sum^k_{i = 1} W(A_i,\overline{A_i})
$$
其中$\overline{A_i}$是$A_i$的补集，也就是除$A_i$外其他所有节点的集合，$Cut$计算的是$A_i$中每个节点到$\overline{A_i}$中每个节点的权重总和，如果最小化$Cut$，就相当于最小化每个子集中节点到自己外节点的权重，但是会存在一个问题，如下图所示：</p>
<p><img loading="lazy" src="1.jpg" alt=""  />
</p>
<p>如果按左边那条线分割，可以保证有边子图到左边子图的权重最小，但不是最佳分割。</p>
<h1 id="谱聚类切图聚类">谱聚类：切图聚类</h1>
<p>为了避免上述效果不佳的情况，提供两种切图方式，1.RatioCut, 2.Ncut.</p>
<h2 id="ratiocut-切图">RatioCut 切图</h2>
<p>最小化$Cut(A_1,A_2, \cdots, A_k)$的同时，最大化每个子图中接待你的个数，即：A_{i}, \overline{A}_{i}</p>
<p>$$RatioCut\left(A_{1}, A_{2}, \ldots A_{k}\right)=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \overline{A_i}\right)}{\left|A_{i}\right|}$$</p>
<p>目标是最小化$RatioCut\left(A_{1}, A_{2}, \ldots A_{k}\right)$。</p>
<p>为此，我们引入一个<strong>指示向量（indicator vector）</strong>$h_j \in {h_1,h_2,\cdots, h_k}$，其中$j = 1,2,\cdots,k$，对于其中任意一个向量$h_j$，它是一个$n$维向量，即：
$$
h_j = (h_{1j},h_{2j}, \cdots, h_{nj})^T \\
h_{i j}=\left\{\begin{array}{ll}{0} &amp; {v_{i} \notin A_{j}} \\ {\frac{1}{\sqrt{\left|A_{j}\right|}}} &amp; {v_{i} \in A_{j}}\end{array}\right.
$$
$h_ij$表示节点$v_i$ 是否属于子图$A_j$, 如果属于，那么$h_{ij} = \frac{1}{\sqrt{\left|A_{j}\right|}}$，如果不属于，那么$h_{ij} = 0$。</p>
<p>那么对于$h_i^TLh_i$有：
$$
\begin{aligned}
h_i^T L h_i &amp;= \frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\
&amp;= \frac{1}{2}\left(\sum_{m\in A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2+\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2 + \sum_{m\notin A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2\right)\\
&amp; 上式中的mn是图中任意选取的两个不同的节点， 针对子图A_i及其对应的指示向量h_i,\\
&amp;任意选取的节点对有四种情况。其中 根据h_{ij}的定义，第一项和第四项为0，所以\\
&amp;=\frac{1}{2} \left(\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2 +
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2\right) \\
&amp;=\frac{1}{2} \left(\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(\frac{1}{\sqrt{\left|A_{i}\right|}})^2 +
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(-\frac{1}{\sqrt{\left|A_{i}\right|}})^2\right) \\
&amp;=\frac{1}{2}\left(\frac{1}{|A_i|}Cut(A_i,\overline{A_i}) + \frac{1}{|A_i|}Cut(A_i,\overline{A_i})\right)\\
&amp;=\frac{Cut(A_i,\overline{A_i})}{|A_i|} = RatioCut(A_i)
\end{aligned}
$$
上式$h_i^TLh_i$可以看做是子图$A_i$的RatioCut，那么：
$$
\begin{aligned}
RatioCut(A_1,A_2,\cdots,A_k) &amp;=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \overline{A_i}\right)}{\left|A_{i}\right|}  = \sum_{i = 1}^k \frac{Cut(A_i,\overline{A_i})}{|A_i|} \\
&amp;= \sum_{i=1}^k h_i^TLh^i = \sum_{i=1}^k (H^TLH)_{ii} = tr(H^TLH)
\end{aligned}
$$
每个$h_i^TLh^j$的值对应于矩阵$H^TLH$在位置$ij$处的值：
$$
H=(h_1,h_2,\cdots,h_k) \in \mathbb{R}^{n\times k}
$$
$$
h_i^TLh_j = (H^TLH)_{ij} \to h^T_iLh_i = (H^TLH)_{ii}
$$</p>
<p>由于$h_i\cdot h_j = 0, h_i \cdot h_i = 1$, 所以$H^TH = I$是一个单位矩阵</p>
<p>所以切图优化函数为：
$$
\underbrace{\arg \min }_{H} RatioCut\left(A_1,A_2,\cdots A_k\right) = \underbrace{\arg \min }_{H} \operatorname{tr}\left(H^{T} L H\right) \quad \text { s.t. } \quad H^{T} H=I
$$
$H$中的每个指示向量$h$是$n$维的，每个向量中的元素有两种取值，分别是0和$\frac{1}{\sqrt{\left|A_{j}\right|}}$， 所以每个$h$有$2^n$种可能性，所以整个$H$有$k2^n$中，因此上述目标函数是个NP-hard问题。</p>
<p>注意到$tr(H^TLH)$中每个优化子目标$h_i^TLh_i$，其中，$h$是单位正交基，基每个元素的平方和等于1，$L$为对称矩阵，<strong>此时$h^T_iLh_i$的最大值为$L$的最大特征值， $h^T_iLh_i$的最小值是$L$的最小特征值</strong>。在谱聚类中，我们的目标是找到目标函数的最小特征值，从而使目标函数最小，得到对应的特征向量。</p>
<p>对于$h^T_iLh_i$，目标是找到$L$最小的特征值，这个值就是$h^T_iLh_i$的最小值，对于$tr(H^TLH) = \sum^k_{i=1} h^T_iLh_i$，目标是找到$k$个最小的特征值，从而使$tr(H^TLH)$最小。</p>
<p>通过找到$L$最小的$k$个特征值，可以对应得到$k$个特征向量，这$k$个特征向量可以组成一个$n \times k$的矩阵，这个矩阵就是我们需要的指示向量矩阵$H$，里面包含了每个节点所属子图的信息。一般来说 我们需要对$H$按行做标准化：
$$
h_{ij}^* = \frac{h_{ij}}{\sqrt{\sum_{t=1}^kh^2_{it}}}
$$
注意到，$H$的每行是一个$k$维行向量，即$H_i = (H_{i1},H_{i2},\cdots, H_{ik})$， 表示节点$i$属于每个子图的指标值， 我们可以把$H_i$当做节点$v_i$的表示向量， 由于归一化后的$H$还不能明确指示各个样本的归属， 我们还需要对$H$代表的所有节点做一次传统聚类，如K-Means。</p>
<h2 id="ncut切图">NCut切图</h2>
<p>把$RatioCut$的分母从$|A_i|$换成$vol(A_i) = \sum_{j \in A_i}d_j$， 为$A_i$中所有节点的权重之和，一般来说$NCut$效果好于$RatioCut$:
$$
NCut(A_1,A_2,\cdots,A_k) = \frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\overline{A_i})}{vol(A_i)} = \sum^k_{i = 1}\frac{Cut(A_i)}{vol(A_i)}
$$
$NCut$指示向量$h$做了改进，$RatioCut$切图在指示向量中使用$\frac{1}{\sqrt{|A_i|}}$表示某节点归属于子图$A_i$，而$NCut$切图使用子图权重$\frac{1}{\sqrt{vol{A_i}}}$来表示某节点归属子图$A_i$如下：
$$
h_{i j}=\left\{\begin{array}{ll}{0} &amp; {v_{i} \notin A_{j}} \\ {\frac{1}{\sqrt{v o l\left(A_{j}\right)}}} &amp; {v_{i} \in A_{j}}\end{array}\right.
$$
上式表示如果节点$v_i$在子图$A_j$中，那么指示向量$h_j$的第$i$个元素为$\frac{1}{\sqrt{vol{A_j}}}$。</p>
<p>那么对于$h_i^TLh_i$有：
$$
h^T_iLh_i = \frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2 = \frac{Cut(A_i)}{vol(A_i)} =NCut(A_i)
$$
目标函数：
$$
NCut(A_i,A_2,\cdots,A_k) = \sum^k_{i = 1} NCut(A_i) = \sum^k_{i=1}h^T_iLh_i =\sum^k_{i=1}(H^TLH)_{ii} = tr(H^TLH)
$$
此时，$h_i \cdot h_j = 0$，$h_i\cdot h_i = \frac{|A_i|}{vol(A_i)} \neq 1$， 所以$H^TH \neq I$。</p>
<p>但是， 由于：$h^T_iDh_i = \sum^n_{j = 1} h_{ij}^2d_j$, $d_j$为节点$v_j$的权重和，$h_{ij}$的值表示节点j是否在子图$A_i$中，如果在子图$A_i$中，那么$h_{ij}^2 = \frac{1}{vol(A_i)}$，否则为0。</p>
<p>$$h^T_iDh_i = \frac{1}{vol(A_i)} \sum_{v_j \in A_i} d_j = \frac{1}{vol(A_i)} \cdot vol(A_i) = 1$$</p>
<p>最终目标函数为：
$$
\underbrace{\arg \min } _{H}\operatorname{tr}\left(H^{T} L H\right) \quad \text { s.t. }\quad H^{T} D H=I
$$
由于$H$中的指示向量$h$不是标准正交基，所以RatioCut中加粗的定理不能直接使用，所以，令$H = D^{-\frac{1}{2}}F$, $D^{-\frac{1}{2}}$表示对$D$对角线上元素开方后求逆，那么：
$$
H^TLH = F^TD^{-\frac{1}{2}}LD^{-\frac{1}{2}}F
$$</p>
<p>$$
H^TDH = F^TD^{-\frac{1}{2}}DD^{-\frac{1}{2}}F = F^TF=I
$$
所以目标函数转化为：
$$
\underbrace{\arg \min }_{F} \operatorname{tr}\left(F^{T} D^{-1 / 2} L D^{-1 / 2} F\right) \quad \text { s.t. } \quad F^{T} F=I
$$
同$RatioCut$，$D^{-1 / 2} L D^{-1 / 2}$是对称矩阵，$F$中的每个向量为标准正交基，只需要求出$D^{-1 / 2} L D^{-1 / 2}$的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，最后得到特征矩阵$F$，然后对$F$的行向量做一次传统聚类，如K-means.</p>
<p>一般来说， $D^{-1 / 2} L D^{-1 / 2}$相当于对拉普拉斯矩阵$L$做了一次标准化，即$\frac{L_{i j}}{\sqrt{d_{i} * d_{j}}}$.</p>
<p>我把本文整理成了<a href="/posts/2019-09-07-spectral-clustering/Spectral-Cluster.pdf">PDF</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>AAAI2017 M-NMF:《Community Preserving Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/m-nmf/</link>
      <pubDate>Wed, 29 May 2019 10:46:44 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/m-nmf/</guid>
      <description>论文地址：M-NMF
Introduction Network Embedding的基本需求是保存网络机构和固有属性。而先前的方法主要保存了网络的围观结构（microscopic structure）如一阶相似度和二阶相似度（LINE，SDNE等）， 忽略了网络结构的介观社区结构（mesoscopic community structure）。针对这个问题，本文提出了一种模块化非负矩阵分解模型M-NMF，该模型将网络的社区结构融入network embedding中。利用节点表示和社区结构之间的一致关系（consensus relationship）， 联合优化基于非负矩阵分解的表示学习模型和基于模块化的社区发现模型， 这样就可以在节点表示中同时保存了微观网络结构和介观社区结构。
具体来说，对于微观结构（节点对相似性），本文使用矩阵分解来融合节点的一节相似性和二阶相似性；对于介观社区结构，通过模块度约束项来检测社区。因此，上面的两项可以通过节点的表示和基于辅助社区表示矩阵社区结构之间的一致关系来联合优化。同时，本文给出了乘法更新策略，保证了再推导参数的时候的正确性和收敛性。
M-NMF Model 对于一个无向图$G=(V,E)$, 图中有$n$个节点和$e$条边，用一个0,1二值邻接矩阵表示为$\mathbf{A}=[A_{i,j}] \in \mathbb{R}^{n \times n}$。 $\mathbf{U} \in \mathbb{R}^{n \times m}$ 为节点的表示矩阵，其中$m \leq n$，$m$是节点的嵌入维度。
建模社区结构 本文采用基于模块最大化的社区发现方法 Modularity， 给定一个邻接矩阵表示的网络$\mathbf{A}$，$\mathbf{A}$包含两个社区，根据Newman 2006b，模块度可以定义如下： $$ Q=\frac{1}{4 e} \sum_{i j}\left(A_{i j}-\frac{k_{i} k_{j}}{2 e}\right) h_{i} h_{j} $$ 其中，$k_i$表示节点$i$的度，如果$i$在第一个社区中，那么$h_i=1$,否则$h_i = -1$。
$k_ik_j$表示将所有边一分为二 参考模块度Q，那么节点$i$,$j$之间可能产生的边数。$\frac{k_{i} k_{j}}{2 e}$如果所有边随机放置，节点$i$,$j$之间的期望边数。定义一个模块度矩阵$\mathbf{B} \in \mathbb{R}^{n \times n}$，其中$B_{i,j}=A_{i,j}-\frac{k_{i} k_{j}}{2 e}$，那么$Q=\frac{1}{4 e} \mathbf{h}^{T} \mathbf{B h}$，其中$\mathbf{h}=[h_i] \in \mathbb{R}^n$，表示社区成员指标器。
如果将$Q$拓展到$k &amp;gt; 2$个社区，那么： $$ Q=\operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B H}\right), \quad \text { s.</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14589">M-NMF</a></p>
<h1 id="introduction">Introduction</h1>
<p>Network Embedding的基本需求是保存网络机构和固有属性。而先前的方法主要保存了网络的围观结构（microscopic structure）如一阶相似度和二阶相似度（LINE，SDNE等）， 忽略了网络结构的介观社区结构（mesoscopic community structure）。针对这个问题，本文提出了一种模块化非负矩阵分解模型M-NMF，该模型将网络的社区结构融入network embedding中。利用节点表示和社区结构之间的一致关系（consensus relationship）， 联合优化基于非负矩阵分解的表示学习模型和基于模块化的社区发现模型， 这样就可以在节点表示中同时保存了<strong>微观网络结构</strong>和<strong>介观社区结构</strong>。</p>
<p>具体来说，对于微观结构（节点对相似性），本文使用矩阵分解来融合节点的一节相似性和二阶相似性；对于介观社区结构，通过模块度约束项来检测社区。因此，上面的两项可以通过节点的表示和基于辅助社区表示矩阵社区结构之间的一致关系来联合优化。同时，本文给出了乘法更新策略，保证了再推导参数的时候的正确性和收敛性。</p>
<h1 id="m-nmf-model">M-NMF Model</h1>
<p>对于一个无向图$G=(V,E)$, 图中有$n$个节点和$e$条边，用一个0,1二值邻接矩阵表示为$\mathbf{A}=[A_{i,j}] \in \mathbb{R}^{n \times n}$。 $\mathbf{U} \in \mathbb{R}^{n \times m}$ 为节点的表示矩阵，其中$m \leq n$，$m$是节点的嵌入维度。</p>
<h2 id="建模社区结构">建模社区结构</h2>
<p>本文采用基于模块最大化的社区发现方法 Modularity， 给定一个邻接矩阵表示的网络$\mathbf{A}$，$\mathbf{A}$包含两个社区，根据<a href="http://engr.case.edu/ray_soumya/mlrg/2006%20Modularity%20and%20community%20structure%20in%20networks.pdf">Newman 2006b</a>，模块度可以定义如下：
$$
Q=\frac{1}{4 e} \sum_{i j}\left(A_{i j}-\frac{k_{i} k_{j}}{2 e}\right) h_{i} h_{j}
$$
其中，$k_i$表示节点$i$的度，如果$i$在第一个社区中，那么$h_i=1$,否则$h_i = -1$。</p>
<p>$k_ik_j$表示将所有边一分为二 参考<a href="https://blog.csdn.net/wangyibo0201/article/details/52048248">模块度Q</a>，那么节点$i$,$j$之间可能产生的边数。$\frac{k_{i} k_{j}}{2 e}$如果所有边随机放置，节点$i$,$j$之间的期望边数。定义一个模块度矩阵$\mathbf{B} \in \mathbb{R}^{n \times n}$，其中$B_{i,j}=A_{i,j}-\frac{k_{i} k_{j}}{2 e}$，那么$Q=\frac{1}{4 e} \mathbf{h}^{T} \mathbf{B h}$，其中$\mathbf{h}=[h_i] \in \mathbb{R}^n$，表示社区成员指标器。</p>
<p>如果将$Q$拓展到$k &gt; 2$个社区，那么：
$$
Q=\operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B H}\right), \quad \text { s.t. } \quad \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{H}\right)=n
$$
其中$tr()$表示矩阵的迹（主对角线元素和），$\mathbf{H}$是社区成员指标器，$\mathbf{H} \in \mathbb{R}^{n \times k}$，每行表示一个节点所属社区的one-hot编码。</p>
<h2 id="建模微观结构">建模微观结构</h2>
<h3 id="一阶相似度">一阶相似度</h3>
<p>$$
S^{(1)} = \mathbf{A}
$$</p>
<h3 id="二阶相似度">二阶相似度</h3>
<p>表示为$S^{(2)}$，表示节点的邻域相似度， 用邻接向量的余弦相似度表示：
$$
S_{i j}^{(2)}=\frac{\mathcal{N}_{i} \mathcal{N}_{j}}{\left|\left|\mathcal{N}_{i}\right|\right|\left|\left|\mathcal{N}_{j}\right|\right|}
$$</p>
<p>结合网络的一阶结构的一阶二阶相似度，最终的网络相似度矩阵可以表示为：
$$
\mathbf{S}^{(1)}+\eta \mathbf{S}^{(2)}
$$
然后，文中引入了一个偏置矩阵$\mathbf{M} \in \mathbb{R}^{n \times m}$ 和一个非负表示矩阵$\mathbf{U} \in \mathbb{R}^{n \times m}$。所以微观结构的目标函数就是节点相似度和节点表示之间的误差：
$$
\min \left|\mathbf{S}-\mathbf{M} \mathbf{U}^{T}\right|_{F}^{2}
$$
因为$\mathbf{S} \in \mathbb{R}^{n \times n}$， 矩阵$\mathbf{U} \in \mathbb{R}^{n \times m}$，矩阵$\mathbf{M}$的作用是把$\mathbf{U}$转成$n \times n$，这样就可以计算损失函数了。</p>
<h2 id="统一的ne模型">统一的NE模型</h2>
<p>引入一个非负辅助矩阵$\mathbf{C} \in \mathbb{R}^{k \times m}$, 即为社区表示矩阵，每一行$C_r$表示第$r$个社区的$m$维表示向量。 如果一个节点的表示向量和一个社区的表示向量接近，那么这个节点就很可能在这个社区中。我们把节点$i$和社区$r$之间的从属关系定义为：
$$
\mathbf{U}_{i} \mathbf{C}_{r}
$$
如果两个向量正交，则$\mathbf{U}_{i} \mathbf{C}_{r} = 0$ 那么节点$i$不可能存在于社区$r$中。所以需要使$\mathbf{U}\mathbf{C}^T$更加近似社区指示器$\mathbf{H}$,所以定义如下目标函数：
$$
\begin{aligned}
\min_{\mathbf{M}, \mathbf{U}, \mathbf{H}, \mathbf{C}}\left|\left|\mathbf{S}-\mathbf{M} \mathbf{U}^{T}\right|\right|_{F}^{2}+\alpha\left|\left|\mathbf{H}-\mathbf{U} \mathbf{C}^{T}\right|\right|_{F}^{2}-\beta \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B} \mathbf{H}\right) \\
s.t., \quad \mathrm{M} \geqslant 0, \mathrm{U} \geqslant 0, \mathrm{H} \geqslant 0, \mathrm{C} \geqslant 0, \operatorname{tr}\left(\mathrm{H}^{T} \mathrm{H}\right)=n
\end{aligned}
$$
其中$\alpha$和$\beta$是正参数，最后一项是要最大化模块度。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>WSDM2016 《Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2019-05-12-embedding-ic/</link>
      <pubDate>Sun, 12 May 2019 22:09:38 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-05-12-embedding-ic/</guid>
      <description>文章链接：Embedding_IC
Introduction 本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。
对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：
 用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。 用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。 不同应用中的级联长度变化很大，难以学习和预测。  本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：
 影响传播是二元的（被感染或不被感染）， 扩散网络未知， 影响关系不依赖于传播的内容， 用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。  本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：
Model Notations 传播事件集$\mathcal{D}={D_1,D_2,&amp;hellip;,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。
给定一个社交网络，有$N$个用户：$\mathcal{U}={u_1,u_2,&amp;hellip;,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \in \mathcal{U} \wedge t^D(u)&amp;lt; \infty}$， 其中，$t^D: \mathcal{U} \to \mathbb{R}^+$ 表示用户被传染的时间戳， $\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \in \mathcal{U} | t^D(u)&amp;lt;t}$。对称地，用$\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\infty)$表示最终所有被感染的用户，$\bar{D}(infty)$表示最终所有没被感染的用户。
Diffusion Model 本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。
在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \in \mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \in \mathcal{U} \backslash I$: $$ P(v|I) = 1-\prod_{u \in I}(1-P_{u,v}) $$ 上式中，$\prod_{u \in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。</description>
      <content:encoded><![CDATA[<p>文章链接：<a href="https://dl.acm.org/citation.cfm?id=2835817">Embedding_IC</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。</p>
<p>对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：</p>
<ul>
<li>用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。</li>
<li>用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。</li>
<li>不同应用中的级联长度变化很大，难以学习和预测。</li>
</ul>
<p>本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：</p>
<ul>
<li>影响传播是二元的（被感染或不被感染），</li>
<li>扩散网络未知，</li>
<li>影响关系不依赖于传播的内容，</li>
<li>用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。</li>
</ul>
<p>本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：</p>
<p><img loading="lazy" src="1.png" alt=""  />
</p>
<h1 id="model">Model</h1>
<h2 id="notations">Notations</h2>
<p>传播事件集$\mathcal{D}={D_1,D_2,&hellip;,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。</p>
<p>给定一个社交网络，有$N$个用户：$\mathcal{U}={u_1,u_2,&hellip;,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \in \mathcal{U} \wedge t^D(u)&lt; \infty}$， 其中，$t^D: \mathcal{U} \to \mathbb{R}^+$ 表示用户被传染的时间戳， $\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \in \mathcal{U} | t^D(u)&lt;t}$。对称地，用$\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\infty)$表示最终所有被感染的用户，$\bar{D}(infty)$表示最终所有没被感染的用户。</p>
<h2 id="diffusion-model">Diffusion Model</h2>
<p>本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。</p>
<p>在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \in \mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \in \mathcal{U} \backslash I$:
$$
P(v|I) = 1-\prod_{u \in I}(1-P_{u,v})
$$
上式中，$\prod_{u \in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。</p>
<p>接下来就需要给出$P_{u,v}$的定义了，即$v$被$u$传染的概率。$z_u \in \mathbb{R}^d$是传染源用户$u$的表示向量，$\omega_{v} \in \mathbb{R}^d$是传染目标用户$v$的表示向量。那么$P_{u,v}$可以定义如下：
$$
P_{u,v} = f(z_u,\omega_{v})
$$
其中，$f: \mathbb{R}^d \times \mathbb{R}^d \to [0,1]$，是一个映射函数，把两个表示向量映射到概率空间：
$$
f\left(z_{u}, \omega_{v}\right)=\frac{1}{1+\exp \left(z_{u}^{(0)}+\omega_{v}^{(0)}+\sum_{i=1}^{d-1}\left(z_{u}^{(i)}-\omega_{v}^{(i)}\right)^{2}\right)}
$$
其中，$z_{u}^{(i)}$和$\omega_{v}^{(i)}$分别表示$z_u$和$\omega_v$的第$i$个分量。表示随距离增加而递减的传输概率，即$\left(z_{u}^{(i)}-\omega_{v}^{(i)}\right)$越大$f$越小。上式使用了sigmoid函数:$\frac{1}{1+e^{-x}}$返回一个$[0,1]$的概率。</p>
<p>值得注意的是，偏置项$z_{u}^{(0)}$和$\omega_{v}^{(0)}$的作用是反映$u$传入$v$的一般趋势，这样做的目的是避免不同的$u$和$v$产生相同的概率。</p>
<h2 id="learning-algorithm">Learning Algorithm</h2>
<p>考虑所有节点对的传播概率$\mathcal{P}={P_{u,v} | (u,v) \in \mathcal{U}^2}$ (涉及所有节点对)。那么对于特定级联$D$的概率为：
$$
P(D)=\prod_{v \in D(\infty)} P_{v}^{D} \prod_{v \in \overline{D}(\infty)}\left(1-P_{v}^{D}\right)
$$
上式中，$\prod_{v \in D(\infty)} P_{v}^{D}$表示$D$中所有被影响的用户存在的概率，$\prod_{v \in \overline{D}(\infty)}\left(1-P_{v}^{D}\right)$表示$D$中所有未被影响的用户存在的概率。所以$P(D)$就是级联$D$存在的概率。同时，可以用对数似然来表示训练级联集$\mathcal{D}$:
$$
\mathcal{L}(\mathcal{P} ; \mathcal{D})=\sum_{D \in \mathcal{D}}\left(\sum_{v \in D(\infty)} \log \left(P_{v}^{D}\right)+\sum_{v \in \overline{D}(\infty)} \log \left(1-P_{v}^{D}\right)\right)
$$
上式就是模型的目标函数。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Influence Maximization Conclusion</title>
      <link>https://JhuoW.github.io/posts/2019-05-06-im-conclusion/</link>
      <pubDate>Mon, 06 May 2019 20:16:17 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-05-06-im-conclusion/</guid>
      <description>影响力传播模型 社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择
LC IT模型（独立级联模型和线性阈值模型） WC（权重级联模型） HD（热传播模型） SIR（传染病模型） MIA模型（路径相关） 投票模型 巴斯模型 影响力最大化算法 目前有的几个影响力最大化的算法
   基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）
  A note on maximizing a submodular set function subject to a knapsack constraint 这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样
  Cost-effective outbreak detection in networks （CELF算法）
  Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法） 这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路
    基于中心性的启发式算法
  Efficient influence maximization in social networks W.Chen （DegreeDiscount算法） 这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1</description>
      <content:encoded><![CDATA[<h4 id="影响力传播模型">影响力传播模型</h4>
<p>社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择</p>
<pre tabindex="0"><code>LC IT模型（独立级联模型和线性阈值模型）

WC（权重级联模型）

HD（热传播模型）

SIR（传染病模型）

MIA模型（路径相关）

投票模型

巴斯模型
</code></pre><h4 id="影响力最大化算法">影响力最大化算法</h4>
<p>目前有的几个影响力最大化的算法</p>
<blockquote>
<ul>
<li>
<p>基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）</p>
<ul>
<li>
<p>A note on maximizing a submodular set function subject to a knapsack constraint
这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样</p>
</li>
<li>
<p>Cost-effective outbreak detection in networks （CELF算法）</p>
</li>
<li>
<p>Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法）
这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路</p>
</li>
</ul>
</li>
<li>
<p>基于中心性的启发式算法</p>
</li>
<li>
<p>Efficient influence maximization in social networks W.Chen （DegreeDiscount算法）
这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1</p>
<ul>
<li>
<p>A potential-based node selection strategy for influence max- imization in a social network （TM算法）
这个算法也是一种启发式算法，它是以节点地潜在地影响力作为启发的因素去选择初始的节点，同时设置了一个参数c，分为两个阶段，第一个阶段启发式的选择潜在影响力最大的若干个节点，第二阶段用贪心算法对这些第一阶段选择的种子节点进行筛选，当参数c为0，也就是没有第一阶段，那么算法就变成了传统的贪心算法。算法的缺点也很明显，首先参数c的选择依赖于经验判断，其次是启发式，准确率没有保证，再次也需要使用贪心算法，因此时间复杂度很高</p>
</li>
<li>
<p>A new centrality measure for influence maxi- mization in social networks
考虑到了传播模型中的传播度，利用这个进行启发式选择传播度最高的节点，从而得到更加精确的结果。这个传播度包括了两个方面，第一是节点自己影响他的邻居，第二个是他的邻居影响其他节点，结合影响概率一起构建的模型。优点在于启发式的选择的同时考虑了传播模型。</p>
</li>
</ul>
</li>
<li>
<p>基于影响路径的算法</p>
</li>
<li>
<p>Tractable models for information diffusion in social networks（SP1M算法）
基于影响路径的算法考虑某个节点只会尽可能地影响从这个节点开始地最短或者次最短路径上地节点，因此，可以递归地计算influenc spread而不用像贪心算法那样使用蒙特卡洛模拟，从而导致大量地计算时间，因此提高了算法地效率</p>
<ul>
<li>
<p>Scalable influence maximization for prevalent viral marketing in large-scale social networks （MIA算法）
借鉴了基于路径地影响力最大化算法的思路，提出一种利用局部图结构的树状近似算法来近似influence spread从而也是避免了蒙特卡罗模拟。此算法中，每条路径具有一个传播概率，定义为在这条路径上的每条边的传播概率的乘积。只有具有最大传播概率的路径才能够作为影响力路径来扩散影响力。同时给每一个节点计算树状度，定义为从节点出发的各条路径中，路径传播概率大于阈值 $\theta$的路径上的所有点的集合</p>
</li>
<li>
<p>Scalable and parallelizable processing of influence maximization for large-scale social networks （IPA算法）</p>
</li>
</ul>
</li>
</ul>
<p>该算法不同于chen等人提出的算法，认为每条路径是相互独立的，chen等人只选择了具有最大的propagation 概率的那条路径，但是本论文则选择所有大于阈值 $\theta$的路径，并行的计算他们的influence spread。基于路径的算法也具有缺点，比如没有理论上的准确度保证，同时，针对于特别复杂的图，空间复杂度非常大</p>
<ul>
<li>
<p>基于社区的算法</p>
<ul>
<li>Oasnet: an optimal allocation approach to in- fluence maximization in modular social networks（OASNET算法）</li>
</ul>
<p>这个算法假设社交网络划分社区后的每个社区是相互独立的，社区之间不会存在相互的影响力传播，利用CNM算法进行社区发现。种子节点的选择则分为两个阶段，第一个节点在每个社区内部利用贪心算法选择k个节点，第二个阶段则使用动态规划的方法在$C \times k$个节点中选择最终的k个节点</p>
<ul>
<li>Identifying influential nodes in complex networks with community structure</li>
</ul>
<p>这个算法基于利用社区结构发现社交网络中的最具有影响力的几个节点的研究。首先根据加权图构造概率转移矩阵，然后使用$K-Mediods$聚类方法找到最具有影响力的若干个节点。</p>
<ul>
<li>Cim: community-based influence maximization in social networks（CIM算法）</li>
</ul>
<p>chen等人基于HD（热传播）模型提出的基于社区结构的影响力最大化算法。算法分为好三个阶段，首先是社区发现，作者给出了一种$H_{Clustering}$算法用于社区发现，然后是候选节点迭代，作者根据节点的拓扑结构和它的社区特征进行选择，最后是种子节点的选择，同时考虑了诸多因素，个人认为是一个比较合理的影响力最大化算法。</p>
<ul>
<li>Conformity-aware influence maximization in online social networks （CINEMA算法）</li>
</ul>
<p>基于节点的一致性来设计的算法。传播模型中的概率定义为让第一个节点的影响力指标乘以第二个节点的一致性指标作为传播概率。</p>
<p>当然，基于社区发现的算法也有自己的缺点，首先是在社区内部进行初步的节点的选择，也需要进行蒙特卡洛模拟，因此时间复杂度也会比较大，其次，社区发现的思路，是用节点在社区内的influence spread去模拟它在whole network上的influence spread，近似效果依赖于网络结构，如果社区之间的连接边都比较少，那么近似结果是非常接近的，但是如果社区之间的连接边比较多，及即是hub节点比较多，那么近似效果可想而知</p>
</li>
</ul>
</blockquote>
<pre tabindex="0"><code>1.基于的贪心算法KK（kempe等学者提出的算法）
2.基于贪心算法的改进算法，利用启发式规则改进的NewGreedyIC，MixedGreedyIC，NewGreedyWC算法
	相关论文：Efficient influence maximization in social networks 2009
3.基于贪心算法的改进算法，利用子模性质改进的CELF算法，改进的CELF++算法
	相关论文：Cost-effective outbreak detection in networks 2007
			CELF++：optimizing the greedy algorithm for influence maximization in social networks 2011
4.启发式算法：随机算法，度中心算法，MaxDegree算法，Degree Discount算法
	相关论文：Efficient influence maximization in social networks
5.基于社区划分的算法：OASNET算法，CGA算法等（后面加上现阶段阅读的论文）
	相关论文：Community-based greedy algorithm for mining top-k influential nodes in mobile social networks 2010
6.MIA算法
混合式算法
</code></pre><h4 id="社区划分的算法">社区划分的算法</h4>
<blockquote>
<ul>
<li>基于模块优化的算法</li>
<li>光谱聚类的算法</li>
<li>层次分级的算法</li>
<li>基于标签传播的算法
<ul>
<li>LPA算法（目前的最快的社区划分算法，几乎是线性时间复杂度）</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="论文思考的几个点">论文思考的几个点</h4>
<p>基于社区发现的影响力最大化算法的分析，论文研究的目的：</p>
<ol>
<li>算法的效率保证，时间复杂度尽可能低。</li>
<li>算法的性能保证，尽可能接近最优解。（利用到子模拟性质？）</li>
</ol>
<p>毕业论文的大致的框架，总体是基于社区划分的思路，具体需要做的工作如下：</p>
<blockquote>
<ul>
<li>
<p><strong>传播模型的选择</strong>，如何改进传播模型使得切合实际的传播过程？IC，LT改进？结合PageRank算法或者思想？或者考虑改进HDM传播模型？引入时间空间的因素使得模型更加充分？</p>
<p>（1）传播模型的改进，传播模型中，针对于某个节点的从邻居获得的影响力，不应该简单的直接叠加，而是考虑每个邻居并不是等同对待的，应当区分不同的权重，针对节点之间的互动频率，互动频率高的节点，应该具有更加高的信任度，同时，也可能存在负面的影响力，即反而让节点更不可能选择新产品，这点应该在改进的模型中有所反馈。</p>
<p>（2）至于这个信任程度如何计算出来反应在传播模型中，则可以考虑，根据邻居之间的互动信息，每个节点的活跃程度，邻居节点本身是否是具有很高的度的节点，邻居节点和本节点的观点是正相关还是反相关，从而决定邻居对本节点的信任度。</p>
<p>（3）应该考虑影响力的时效性，是否可以考虑结合HDM和LTM模型一起，加上信任度参数这个观点，一起构建一个新的传播模型。</p>
</li>
<li>
<p><strong>社区发现算法的选择</strong>，社区发现的选择是非常重点的，社区发现本质上是社交网络节点的聚类，应该涉及比较有效率的聚类算法或者选择其他的距离算法？</p>
<p>社区发现聚类算法，一般都是先设置每个节点作为单独的一个社区，然后进行合并，在进行社区聚类发现的时候，不应当单独仅仅考虑边，仅仅利用边的关系，比如CGA算法就利用到了传播模型，结合传播模型进行标签传播，然后获得相应的划分的社区。同时，可以加以改进的地方，比如，社交网络的社区发现不应当仅仅考虑到拓扑结构，还有考虑节点之间的互动交流的信息，互动程度越频繁，那么两个节点在一个社区内部的概率就越大，因此要考虑这个改进点。</p>
<p>同时，借鉴了CGA算法的思想，一个节点的社区内部的影响力和整个社交网络的影响力如何区别？如何用社区内部的影响力去近似？或者考虑hub节点，社区之间的这些连接节点也有着非常重要的作用。</p>
</li>
<li>
<p><strong>社区发现是否可以处理重叠社区的情况</strong>，重叠社区会导致影响力的重复传播，如何减少这种情况的出现，如何设计算法实现重叠社区的处理？</p>
</li>
<li>
<p><strong>各个社区的重要性也是不同的，应该有选择的摒弃一些社区，先给出一个社区选择的模型，比如说利用PageRank先计算出哪些社区比较重要</strong>，有的社区人数多，有的社区人数少，但是处在中心位置，并且一些非重要的社区，往往会关注这些重要社区的传递出来的信息。考虑种子节点选择的时候，应当把社区这些因素考虑进去。我们可以忽略那些不重要的小社区，重要的社区给与比较大的加权值，同时注意影响力避免重叠传播。</p>
</li>
<li>
<p><strong>社区发现之后，如何分配每个社区的种子节点数目？</strong>，直接按照比例分配？亦或是选择一种度量社区重要程度的模型？</p>
<p>CGA算法，使用了动态规划进行贪心选择，在各个社区内部选择相应的种子节点。但是时间复杂度仍然是非常大的，是否可以考虑先在社区内部基于启发式规则，或者PageRank，计算重要的节点，然后全局进行贪心的选择？</p>
</li>
<li>
<p><strong>基于社区发现的算法，实际上是利用节点在社区内的传播来近似它在整个网络上传播的效果</strong>，因此这种近似肯定存在误差，如何减少这种误差的产生？而且这种误差还是和网络中的社区结构有关系的。</p>
</li>
<li>
<p><strong>注意充分利用社区结构的特点，划分社区之后，把社区也视为一个点，作为整体去考虑</strong></p>
</li>
<li>
<p><strong>社区内的候选节点选择</strong>，候选节点应该按照什么标准进行选择，是按照启发式的度选择？还是设计另外的模型？结合PageRank模型？</p>
</li>
<li>
<p><strong>种子节点的获取策略</strong>，如何在这些候选节点上选择出最终的种子节点？直接按照贪心策略暴力选择还是参考CELF算法进行选择？或者是涉及其他的方法？</p>
</li>
</ul>
</blockquote>
<h4 id="思路总结">思路总结</h4>
<blockquote>
<p>基于社区发现的影响力最大化算法框架：</p>
<ul>
<li>社区划分。</li>
<li>充分考虑社区作为一个整体性，来体现社区的一个作用，可以利用PageRank模型，来代表社区的重要程度，这个是社区的一个属性，利用这个模型，选择一些重要的社区，同时摒弃一些小的，没那么重要的社区。而且利用PageRank进行迭代，应该比传统的算法会快一些。</li>
<li>社区内部种子节点的选择，考虑的是社区内部的种子节点在社区内部的影响力传播。如何选择社区内部的种子节点。？？？？？这一点目前还需要多家考虑。</li>
<li>社区出现重叠，如何考虑？</li>
<li>种子节点在社区内的影响力只是对种子节点在全局的影响力的近似，那么需要一种方式来弥补这种误差。显然，种子节点的影响力如果想要传播到另外的社区，那么是通过社区之间的边界节点进行传播的，同时和社区本身的重要性有关，那么这部分的误差通过边界节点来弥补。</li>
<li>最终会得到若干的候选节点，这些节点使用贪心算法进行选择出最终的种子节点，考虑贪心算法的时间复杂度，那么可以考虑使用CELF思路或者是其他的CGA这类的动态规划思路去求解，不过这还是基于蒙特卡罗模拟，时间复杂度仍然相对比较大，对算法进行加速。</li>
<li>影响力传播模型，基于线性阈值模型进行改变，加上信任度参数，因为每个影响力的叠加不是平权的，和用户之间的互动，观点信息，兴趣爱好是否一直存在着相关的关系，因此在影响力传播模型中加入这个考虑因素。</li>
</ul>
</blockquote>
<h4 id="算法的实验部分考虑一些经典的baseline-algorithm">算法的实验部分，考虑一些经典的BaseLine Algorithm</h4>
<blockquote>
<ul>
<li>传统贪心爬山算法-KK算法</li>
<li>CELF算法</li>
<li>CGA算法</li>
<li>CIM算法</li>
<li>启发式算法（度启发式，中心性启发式）DegreeDiscount算法</li>
<li>本论文的算法</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>算法中参数的选择的影响</li>
<li>控制变量对比实验</li>
</ul>
</blockquote>
<h4 id="相关工作总结">相关工作总结</h4>
<blockquote>
<ol>
<li>
<p>Richardson和Domingos在2002年的论文，首次把这个问题作为一个研究方向提出。</p>
</li>
<li>
<p>首先应该数说到的式Kempe的2003年的论文，主要提出了</p>
</li>
</ol>
<p>a）LT IC 模型，并说明了这个问题的NP完全性</p>
<p>b）给出了贪心算法</p>
<p>c）说明了影响力递增的边界递减性质，利用子模性质说明了算法的性能保证</p>
</blockquote>
<h4 id="近期论文阅读总结">近期论文阅读总结</h4>
<ol>
<li>
<p>传播模型的改进，基于PageRank的改进，传统PageRank在考虑某个节点的PR值是均匀分配给链出的节点的（链出的概率为出度的倒数）（即权重级联模型），但是实际上，PR高的节点具有更高的影响力，因此考虑链出的概率不用度，而用PR值的占比，从而更加切合实际的情况。</p>
</li>
<li>
<p>还有的改进算法，改进了PageRank计算模型，把节点自身的属性，节点之间互动的属性，加入到了PageRank模型计算中，使得PageRank能够适用于社交网络中节点重要性的计算。</p>
</li>
<li>
<p>我们可以考虑把以上的两者结合起来给出一种新的信息传播模型（给出概率计算的方法）。</p>
</li>
<li>
<p>数据集选择</p>
<blockquote>
<p>参考宫秀云那篇文章</p>
</blockquote>
</li>
<li>
<p>总结：基于PageRank思想的影响力计算，都是在PageRank的基础上进行模型的改进，加入其他的影响因子，给出不同的权重，从而更加符合实际的应用场景。</p>
</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>社交网络影响最大化（Influence Maximization）中的IC，LT模型</title>
      <link>https://JhuoW.github.io/posts/2019-03-20-ic-lt/</link>
      <pubDate>Wed, 20 Mar 2019 21:34:08 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-03-20-ic-lt/</guid>
      <description>The Independent Cascade Model (IC Model) IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。
在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。
值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。
The Linear Threshold Model (LT Model) LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。
形式上， 在图$G$中每条边$e=(u,v) \in E$有一个权重$b_{u,v}$。 我们定义$\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\sum_{u \in \mathcal{N}_I (v)} b_{u,v} \leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\theta_v$。 LT模型首先为每个节点$v$的阈值$\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。</description>
      <content:encoded><![CDATA[<h1 id="the-independent-cascade-model-ic-model">The Independent Cascade Model (IC Model)</h1>
<p>IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。<br>
在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。<br>
值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。</p>
<h1 id="the-linear-threshold-model-lt-model">The Linear Threshold Model (LT Model)</h1>
<p>LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。<br>
形式上， 在图$G$中每条边$e=(u,v) \in E$有一个权重$b_{u,v}$。 我们定义$\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\sum_{u \in \mathcal{N}_I (v)} b_{u,v} \leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\theta_v$。 LT模型首先为每个节点$v$的阈值$\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>SIGIR18 《BiNE:Bipartite Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/bine/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/bine/</guid>
      <description>论文地址：BiNE
Introduction Bipartite Network(二分网络):如下图所示：
二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。
另一个问题，
如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。
另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。
针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过HITS来衡量。
Model 如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\overrightarrow{u_i}]$, $V=[\overrightarrow{v_i}]$，结构如下图所示：
（取自作者的讲解ppt)
Explicit Relations 同LINE一样， 基于直接连接的目标函数表示为：
$$\mathrm{minimize} \quad O_1=-\sum_{e_{ij} \in E}w_{ij}\log \hat{P}(i,j)$$
Implicit Relations 构造随机游走序列 这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：
$$w^U_{ij}=\sum_{k \in V}w_{ik}w_{jk}$$
$$w^V_{ij}=\sum_{k \in U}w_{ki}w_{kj}$$
其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。 其中$l=\max(H(v_i)\times \max T,\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。
$$D_{v_i}=\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$
表示其中一次随机游走的节点集合$p$表示停止概率。
通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。
对间接关系建模 如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。 对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \in C_s(u_i)$的条件概率。所以目标函数如下： $$\mathrm{maximize} \quad O_2 = \prod_{u_i \in S \land S \in D^U} \prod_{u_c \in C_s(u_i)}P(u_c|u_i)$$</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.comp.nus.edu.sg/~xiangnan/papers/sigir18-bipartiteNE.pdf">BiNE</a></p>
<h1 id="introduction">Introduction</h1>
<p><strong>Bipartite Network(二分网络)</strong>:如下图所示：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/stru.png#center" alt="你想输入的替代文字"  />
<br>
二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。</p>
<p>另一个问题，<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/r1.png#center" alt="你想输入的替代文字"  />
 <img loading="lazy" src="/posts/2019-03-13-BiNE/r2.png#center" alt="你想输入的替代文字"  />
<br>
如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。<br>
另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。</p>
<p>针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过<a href="http://www.cs.cornell.edu/home/kleinber/auth.pdf">HITS</a>来衡量。</p>
<h1 id="model">Model</h1>
<p>如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\overrightarrow{u_i}]$, $V=[\overrightarrow{v_i}]$，结构如下图所示：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/3.png" alt="你想输入的替代文字"  />
（取自作者的讲解ppt)</p>
<h2 id="explicit-relations">Explicit Relations</h2>
<p>同LINE一样， 基于直接连接的目标函数表示为：<br>
$$\mathrm{minimize} \quad O_1=-\sum_{e_{ij} \in E}w_{ij}\log \hat{P}(i,j)$$</p>
<h2 id="implicit-relations">Implicit Relations</h2>
<h3 id="构造随机游走序列">构造随机游走序列</h3>
<p>这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：<br>
$$w^U_{ij}=\sum_{k \in V}w_{ik}w_{jk}$$<br>
$$w^V_{ij}=\sum_{k \in U}w_{ki}w_{kj}$$<br>
其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。
<img loading="lazy" src="/posts/2019-03-13-BiNE/Al1.png" alt="你想输入的替代文字"  />
<br>
其中$l=\max(H(v_i)\times \max T,\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。<br>
$$D_{v_i}=\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$<br>
表示其中一次随机游走的节点集合$p$表示停止概率。</p>
<p>通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。</p>
<h3 id="对间接关系建模">对间接关系建模</h3>
<p>如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。
<img loading="lazy" src="/posts/2019-03-13-BiNE/dd.png#center" alt="你想输入的替代文字"  />
<br>
对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \in C_s(u_i)$的条件概率。所以目标函数如下：
$$\mathrm{maximize} \quad O_2 = \prod_{u_i \in S \land S \in D^U} \prod_{u_c \in C_s(u_i)}P(u_c|u_i)$$<br>
对于$D^V$同理。其中,$p(u_c|u_i) = \frac{\exp(\overrightarrow{u}_i^T \overrightarrow{\theta}_c)}{\sum^{|U|}_{k=1} \exp(\overrightarrow{u}_i^T \overrightarrow{\theta}_k))}$。</p>
<h3 id="negative-sampling">Negative Sampling</h3>
<p>本文的负采样方法是基于局部敏感哈希（LSH）来对与中心节点不相似的节点进行采样。
该方法的strategy是，给定一个中心节点，随机选取一个bucket（序列）并且这个序列不包含给定的中心节点，以此来获得和给定节点尽量不相似的负采样节点。<br>
$N^{ns}_S (u_i)$ 表示$ns$个负采样节点，对于中心节点$u_i$, 那么上文提到的条件概率$p(u_c|u_i)$可以被定义为下式：<br>
$$p(u_c,N^{ns}_S (u_i)|u_i) = \prod_{z \in {u_c} \cup N^{ns}_S (u_i)} P(z|u_i)$$<br>
其中条件概率$P(z|u_i)$定义为：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/4.png#center" alt="你想输入的替代文字"  />
<br>
其中$\sigma$表示sigmoid函数，这样就减少了上文softmax函数造成的计算量过大的问题。</p>
<h2 id="联合优化">联合优化</h2>
<p>通过随机梯度上升对3部分损失函数进行加权优化：<br>
$$\mathrm{maximize} \quad L = \alpha \log O_2+\beta \log O_3 - \gamma O_1$$
最终BiNE的整体算法流程如下：<br>
<img loading="lazy" src="/posts/2019-03-13-BiNE/Al2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="conclusion">Conclusion</h1>
<p>这篇文章提出的分布式训练以及负采样策略还是很值得学习的。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>AAAI2018 《GraphGAN:Graph Representation Learning with Generative Adversarial Nets》Reading Notes</title>
      <link>https://JhuoW.github.io/posts/graphgan/</link>
      <pubDate>Tue, 22 Jan 2019 16:57:05 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/graphgan/</guid>
      <description>论文地址:GraphGAN
Introduction GAN是CV上非常火的方法，作者将其引入Network Embedding领域。为了达到这个目的，首先，作者将当前的NE model分成了两类，分别是生成模型(generative model)和判别模型(discriminative model)。简单来说，生成模型。
所谓生成模型，就是对于每个节点$v_c$，都存在一个潜在的真实连接分布$p_{true}(v|v_c)$, 这个分布反映了由条件生成样本的概率，因此，生成式模型的目的就是求网络中边的极大似然。比较经典的方法有DeepWalk，是通过随机游走来获得center node的上下文，然后最大化上下文节点的似然。Node2vec拓展了DeepWalk,提出了biased随机游走，使得模型在为给定节点生成上下文节点时具有更多的灵活性。
所谓判别模型，目的是学习一个分类器来直接预测边是否存在。将两个节点视为输入特征，然后输出预测的两个节点之间有边的概率，即$p(edge|(v_i,v_j))$,常见的方法有SDNE，PPNE。
于是，本文结合生成对抗网络(GAN),提出了GraphGAN来同一生成和判别器。其中生成器$G(v|v_c)$试图拟合真实的连接概率分布$p_{true}(v|v_c)$,生成最可能和$v_c$有链接的点。判别器$D(v,v_c)$尝试区分强连接节点对和若连接节点对，然后计算$v$和$v_c$存在边的可能性。简单来说，就是把生成器生成的可能与$v_c$有边的节点放入判别器计算两者有边的概率。
除此之外GAN框架下，然而生成器的连接分布是通过softmax实现的，但是传统的softmax无法适配生成器，原因如下:
(1).传统的softmax对网络中的所有节点一视同仁，缺乏对网络结构相似度信息的考虑。
(2).计算成本太高。
因此，论文中提出了新的生成器实现方法 Graph Softmax。并且证明GS具有归一化（normalization）、网络结构感知（graph structure awareness）和高计算效率（computational efficiency）的性质。相应的，文中也提出了基于随机游走(random walk)的生成器策略。
Model 这里挑特别的来说。$\mathcal{N}(v_c)$表示和$v_c$直接相连的节点集。我们把给定节点$v_c$与任意节点$v \in \mathcal{V}$的真实连接分布表示为$p_{true}(v|v_c)$。这么看$\mathcal{N}(v_c)$可以看做从$p_{true}(v|v_c)$中采样的集合。文章的目的是学习两个模型:
Generator $G(v|v_c;\theta_G)$ 生成器，尝试拟合真实连接分布$p_{true}(v|v_c)$，并且从节点集$\mathcal{V}$中生成最有可能和$v_c$相连的节点。
Discriminator $D(v,v_c;\theta_G)$ 判别器，目的是判断已对接点$(v,v_c)$的连接性。$D(v,v_c;\theta_G)$输出$v$和$v_c$有边的概率。
生成器$G$尝试完美拟合$p_{true}(v|v_c)$，并且生成和$v_c$真实邻居高度相似的节点来欺骗判别器。相反，判别器尝试发现输入的顶点是真实的节点对或者有生成器生成出的样本。这是一个极大极小游戏，目标函数$V(G,D)$:
$$\min_{\theta_G} \max_{\theta_D} V(G,D)=\sum_{c=1}^V (\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]+\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))])$$
上面这个公式是本文最关键的公式，以我的分析就是：在给定$\theta_D$的情况下，对其最小化。先来分析$\max_{\theta_D}V(G,D)$,即给定$\theta_G$,使原式最大。当给定$\theta_G$时，通过改变$\theta_D$,使$\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]$达到最大（是判别器D拟合真实分布），同时使$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$最大，即尽可能减小选定节点$v_c$与判别器中生成的$v_c$邻居节点与$v_c$连接的可能性。然后在给定$\theta_D$的情况下，通过改变生成器$\theta_G$继续生成节点，使得$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$尽可能小，即上一步的判别器尽可能把当前生成器生成的节点认为是邻居。 之后再更新判别器,以此类推。这样$G$和$D$各自提高性能，最终$G$生成的分布和真实连接分布无法区分，这样达到最好的效果。整体过程如下图所示:
Discriminator Optimization 对于判别器来说是个简单的sigmoid函数，来计算两个输入节点的表示向量:
$$D(v,v_c;\theta_D)=\frac{1}{1+\exp(-d^\top_v d_{v_c})}$$
其中，$d_v$,$d_{v_c}$是两个输入节点关于判别器$D$的表示向量，$\theta_D$是所有节点表示向量的结合。注意到上面的公式只涉及$v$和$v_c$, 我们只需要更新$d_v$,$d_{v_c}$，通过梯度下降的方法可以实现：
Generator Optimization 对于生成器来说，目标是最小化判别器将生成的样本判断为负样本的对数似然（概率）。换句话说，生成器会调整自己的连接分布（通过调整生成的所有节点的向量表示$\theta_G$）来提升对生成样本的判别分数。由于$v$的采样时离散的，所以使用policy gradient来计算$V(G,D)$关于$\theta_G$的梯度：
为了理解上述公式，注意到$\nabla_{\theta_G}V(G,D)$是一个由$\log(1-D(v,v_c;\theta_D))$加权的梯度$\nabla_{\theta_G}\log G(v|v_c;\theta_G)$的求和，直观上说，这意味着有高概率是负样本的节点会将生成器G“拉着”远离自己（因为我们在$\theta_G$上执行梯度下降)。
Graph Softmax graph softmax的核心思想是定义一种新的计算连接性概率的方式，满足以下性质:
 归一化：$\sum_{v \neq v_c;\theta_G}=1$。 图结构感知：生成器充分利用网络中的结构信息，来估计真实连接分布，如果两个节点在图上越远，那么他们间有边的概率越小。 高效的计算：和传统的softmax不同，$G(v|v_c;\theta_G)$的计算应只涉及图中的一小部分点。  因此，本文以图中节点$v_c$为例，以$v_c$为根构建一棵BFS树$T_c$。$\mathcal{N}_c(v)$为节点$v$在$T_c$上的邻居集合（包括他的父节点和所有子节点）。对于一个given vertex $v$和它的一个邻居$v_i \in \mathcal{N}_c(v)$,定义概率为:</description>
      <content:encoded><![CDATA[<p>论文地址:<a href="https://arxiv.org/abs/1711.08267">GraphGAN</a></p>
<h2 id="introduction">Introduction</h2>
<p>GAN是CV上非常火的方法，作者将其引入Network Embedding领域。为了达到这个目的，首先，作者将当前的NE model分成了两类，分别是生成模型(generative model)和判别模型(discriminative model)。简单来说，生成模型。<br>
所谓生成模型，就是对于每个节点$v_c$，都存在一个潜在的真实连接分布$p_{true}(v|v_c)$, 这个分布反映了由条件生成样本的概率，因此，生成式模型的目的就是求网络中边的极大似然。比较经典的方法有DeepWalk，是通过随机游走来获得center node的上下文，然后最大化上下文节点的似然。Node2vec拓展了DeepWalk,提出了biased随机游走，使得模型在为给定节点生成上下文节点时具有更多的灵活性。<br>
所谓判别模型，目的是学习一个分类器来直接预测边是否存在。将两个节点视为输入特征，然后输出预测的两个节点之间有边的概率，即$p(edge|(v_i,v_j))$,常见的方法有SDNE，PPNE。</p>
<p>于是，本文结合生成对抗网络(GAN),提出了GraphGAN来同一生成和判别器。其中生成器$G(v|v_c)$试图拟合真实的连接概率分布$p_{true}(v|v_c)$,生成最可能和$v_c$有链接的点。判别器$D(v,v_c)$尝试区分强连接节点对和若连接节点对，然后计算$v$和$v_c$存在边的可能性。简单来说，就是把生成器生成的可能与$v_c$有边的节点放入判别器计算两者有边的概率。</p>
<p>除此之外GAN框架下，然而生成器的连接分布是通过softmax实现的，但是传统的softmax无法适配生成器，原因如下:<br>
(1).传统的softmax对网络中的所有节点一视同仁，缺乏对网络结构相似度信息的考虑。<br>
(2).计算成本太高。<br>
因此，论文中提出了新的生成器实现方法 Graph Softmax。并且证明GS具有归一化（normalization）、网络结构感知（graph structure awareness）和高计算效率（computational efficiency）的性质。相应的，文中也提出了基于随机游走(random walk)的生成器策略。</p>
<h2 id="model">Model</h2>
<p>这里挑特别的来说。$\mathcal{N}(v_c)$表示和$v_c$直接相连的节点集。我们把给定节点$v_c$与任意节点$v \in \mathcal{V}$的真实连接分布表示为$p_{true}(v|v_c)$。这么看$\mathcal{N}(v_c)$可以看做从$p_{true}(v|v_c)$中采样的集合。文章的目的是学习两个模型:<br>
<strong>Generator</strong> $G(v|v_c;\theta_G)$ 生成器，尝试拟合真实连接分布$p_{true}(v|v_c)$，并且从节点集$\mathcal{V}$中生成最有可能和$v_c$相连的节点。<br>
<strong>Discriminator</strong> $D(v,v_c;\theta_G)$ 判别器，目的是判断已对接点$(v,v_c)$的连接性。$D(v,v_c;\theta_G)$输出$v$和$v_c$有边的概率。</p>
<p>生成器$G$尝试完美拟合$p_{true}(v|v_c)$，并且生成和$v_c$真实邻居高度相似的节点来欺骗判别器。相反，判别器尝试发现输入的顶点是真实的节点对或者有生成器生成出的样本。这是一个极大极小游戏，目标函数$V(G,D)$:<br>
$$\min_{\theta_G} \max_{\theta_D} V(G,D)=\sum_{c=1}^V (\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]+\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))])$$<br>
上面这个公式是本文最关键的公式，以我的分析就是：在给定$\theta_D$的情况下，对其最小化。先来分析$\max_{\theta_D}V(G,D)$,即给定$\theta_G$,使原式最大。当给定$\theta_G$时，通过改变$\theta_D$,使$\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]$达到最大（是判别器D拟合真实分布），同时使$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$最大，即尽可能减小选定节点$v_c$与判别器中生成的$v_c$邻居节点与$v_c$连接的可能性。然后在给定$\theta_D$的情况下，通过改变生成器$\theta_G$继续生成节点，使得$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$尽可能小，即上一步的判别器尽可能把当前生成器生成的节点认为是邻居。 之后再更新判别器,以此类推。这样$G$和$D$各自提高性能，最终$G$生成的分布和真实连接分布无法区分，这样达到最好的效果。整体过程如下图所示:</p>
<p><img loading="lazy" src="/posts/2019-01-22-GraphGAN/1.png" alt="你想输入的替代文字"  />
</p>
<h3 id="discriminator-optimization">Discriminator Optimization</h3>
<p>对于判别器来说是个简单的sigmoid函数，来计算两个输入节点的表示向量:<br>
$$D(v,v_c;\theta_D)=\frac{1}{1+\exp(-d^\top_v d_{v_c})}$$<br>
其中，$d_v$,$d_{v_c}$是两个输入节点关于判别器$D$的表示向量，$\theta_D$是所有节点表示向量的结合。注意到上面的公式只涉及$v$和$v_c$, 我们只需要更新$d_v$,$d_{v_c}$，通过梯度下降的方法可以实现：<br>
<img loading="lazy" src="/posts/2019-01-22-GraphGAN/2.png" alt="你想输入的替代文字"  />
</p>
<h3 id="generator-optimization">Generator Optimization</h3>
<p>对于生成器来说，目标是最小化判别器将生成的样本判断为负样本的对数似然（概率）。换句话说，生成器会调整自己的连接分布（通过调整生成的所有节点的向量表示$\theta_G$）来提升对生成样本的判别分数。由于$v$的采样时离散的，所以使用policy gradient来计算$V(G,D)$关于$\theta_G$的梯度：<br>
<img loading="lazy" src="/posts/2019-01-22-GraphGAN/3.png" alt="你想输入的替代文字"  />
<br>
为了理解上述公式，注意到$\nabla_{\theta_G}V(G,D)$是一个由$\log(1-D(v,v_c;\theta_D))$加权的梯度$\nabla_{\theta_G}\log G(v|v_c;\theta_G)$的求和，直观上说，这意味着有高概率是负样本的节点会将生成器G“拉着”远离自己（因为我们在$\theta_G$上执行梯度下降)。</p>
<h3 id="graph-softmax">Graph Softmax</h3>
<p>graph softmax的核心思想是定义一种新的计算连接性概率的方式，满足以下性质:</p>
<ul>
<li>归一化：$\sum_{v \neq v_c;\theta_G}=1$。</li>
<li>图结构感知：生成器充分利用网络中的结构信息，来估计真实连接分布，如果两个节点在图上越远，那么他们间有边的概率越小。</li>
<li>高效的计算：和传统的softmax不同，$G(v|v_c;\theta_G)$的计算应只涉及图中的一小部分点。</li>
</ul>
<p>因此，本文以图中节点$v_c$为例，以$v_c$为根构建一棵BFS树$T_c$。$\mathcal{N}_c(v)$为节点$v$在$T_c$上的邻居集合（包括他的父节点和所有子节点）。对于一个given vertex $v$和它的一个邻居$v_i \in \mathcal{N}_c(v)$,定义概率为:<br>
$$p_c(v_i|v)=\frac{\exp (g_{v_i}^\top g_v)}{\sum_{v_i \in \mathcal{N}_c(v)} \exp(g_{v_j}^\top g_v)}$$<br>
这是一个在$\mathcal{N}_c(v)$上的softmax函数。</p>
<p>为了计算$G(v|v_c;\theta_G)$,注意到在$T_c$上，根节点$v_c$到每个节点$v$都有一条唯一的路径， 把这条路径记为$P_{v_c \to v}=(v_{r_0},v_{r_1},&hellip;,v_{r_m})$,其中$v_{r_0}=v_c$, $v_{r_m}=v$,那么在graph softmax中，将$G(v|v_c;\theta_G)$定义为:<br>
$$G(v|v_c;\theta_G)\triangleq (\prod^m_{j=1} p_c(v_{r_j}|v_{r_{j-1}})) \cdot p_c(v_{r_{m-1}}|v_{r_m})$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>CIKM2018 《Enhanced Network Embeddings via Exploiting Edge Labels》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/2019-01-22-ne-edge-labels/</link>
      <pubDate>Tue, 22 Jan 2019 11:02:29 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/2019-01-22-ne-edge-labels/</guid>
      <description>论文地址: Enhanced Network Embeddings via Exploiting Edge Labels
Introduction 这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。
具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个半监督NE模型。
Problem Definition 假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,&amp;hellip;,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \subseteq L$。目的还是一样，学习一个映射函数$\Phi \to \mathbb{R}^{|V| \times d}$, 其中$d \ll |V|$。
Method 首先定义损失函数:
$$\mathcal{L}=(1-\lambda)\mathcal{L}_s+\lambda\mathcal{L}_r$$
其中$\mathcal{L}_s$表示预测节点邻域的损失。$\mathcal{L}_r$表示预测边label的损失。$\lambda$是两种损失的权重。
Structural Loss 第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：
$$\mathcal{L}_s=-\sum_{u \in C(v)} \log Pr(u|v)$$
这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：
$$Pr(u|v)=\frac{\exp(\Phi(u) \cdot \Phi&amp;rsquo;(v))}{\sum_{u&amp;rsquo; \in V} \exp(\Phi(u&amp;rsquo;) \cdot \Phi&amp;rsquo;(v))}$$
这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\Phi(v)$和$\Phi&amp;rsquo;(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。
剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：
$$C(v)={v_{i-w},&amp;hellip;,v_{i-1}} \cup {v_{i+1},&amp;hellip;,v_{i+w}}$$
Relational Loss 由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \in E$,可以用一下方法来表示这条边:
$$\Phi(e)=g(\Phi(u),\Phi(v))$$
其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：</description>
      <content:encoded><![CDATA[<p>论文地址: <a href="https://arxiv.org/abs/1809.05124?context=physics.soc-ph">Enhanced Network Embeddings via Exploiting Edge Labels</a></p>
<h2 id="introduction">Introduction</h2>
<p>这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。</p>
<p>具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个<strong>半监督NE模型</strong>。</p>
<h2 id="problem-definition">Problem Definition</h2>
<p>假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,&hellip;,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \subseteq L$。目的还是一样，学习一个映射函数$\Phi \to \mathbb{R}^{|V| \times d}$, 其中$d \ll |V|$。</p>
<h2 id="method">Method</h2>
<p>首先定义损失函数:<br>
$$\mathcal{L}=(1-\lambda)\mathcal{L}_s+\lambda\mathcal{L}_r$$<br>
其中$\mathcal{L}_s$表示预测节点邻域的损失。$\mathcal{L}_r$表示预测边label的损失。$\lambda$是两种损失的权重。</p>
<h3 id="structural-loss">Structural Loss</h3>
<p>第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：<br>
$$\mathcal{L}_s=-\sum_{u \in C(v)} \log Pr(u|v)$$<br>
这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：<br>
$$Pr(u|v)=\frac{\exp(\Phi(u) \cdot \Phi&rsquo;(v))}{\sum_{u&rsquo; \in V} \exp(\Phi(u&rsquo;) \cdot \Phi&rsquo;(v))}$$<br>
这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\Phi(v)$和$\Phi&rsquo;(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。<br>
剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：<br>
$$C(v)={v_{i-w},&hellip;,v_{i-1}} \cup {v_{i+1},&hellip;,v_{i+w}}$$</p>
<h3 id="relational-loss">Relational Loss</h3>
<p>由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \in E$,可以用一下方法来表示这条边:<br>
$$\Phi(e)=g(\Phi(u),\Phi(v))$$<br>
其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：<br>
$$\Phi(e)=\Phi(u) \oplus \Phi(v)$$<br>
这样我们就获得了edge embedding。直接将它输入前馈神经网络，前馈神经网络第$k$层的定义为:<br>
$$h^{(k)}=f(W^{(k)}h^{(k-1)}+b^{(k)})$$<br>
其中 $h^{(0)}=\Phi(e)$，$f$是除最后一层外采用relu激活函数，最后一层采用sigmoid函数激活,最后一层输出为$\hat{y_i}$。最后最小化二元交叉熵损失函数：</p>
<p>$$\mathcal{L}_r=\sum^{|L|}_{i=1} H(y_i,\hat{y_i}) + (1-y_i) \cdot \log (1-\hat{y_i})$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2016 SDNE:《Structral Deep Network Embedding》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/sdne/</link>
      <pubDate>Mon, 21 Jan 2019 15:34:36 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/sdne/</guid>
      <description>论文地址：SDNE
Introduction 这是一篇比较早的Network Embedding论文， 较早将深度模型应用到NE任务。 首先，本文提出了当前网络表示学习中遇到的三个问题：
（1）. 高度非线性
（2）. 尽可能保持网络结构
（3）. 现实网络的高度稀疏性
SDNE的主要目标就是保持网络结构的一阶相似性和二阶相似性。
一阶相似性就是网络中边相连的节点对之间具有的相似性。
二阶相似性就是在一个Graph中，拥有共同邻居但是不直接向相连的两个节点具有的相似性。
其中，一阶相似性主要反映了网络的局部特征。 二阶相似性反映了网络的全局特征。
Model 本文的模型主要如下图所示：
这张图看上去有点复杂，实则原理非常简单。
模型分为无监督部分和有监督部分，无监督部分是一个深度自编码器 用来捕获二阶相似度（保留全局结构），监督部分是一个拉普拉斯特征映射捕获一阶相似度（局部结构）。呃呃呃，Emmmmm ,不知道是我理解有问题还是其他原因，文章里说的和我理解的不太一样 /(ㄒoㄒ)/~~。 然后介绍一下具体的模型结构：
深度自编码器的编码部分：
$$y_i^{(k)}=\sigma{(W^{(k)}y_i^{(k-1)}+b^{(k)})}, k=2,&amp;hellip;,K$$
假设第$k$层是 节点$v$的表示向量（仅考虑全局信息），那么从第$k$层开始解码，最终得到$\hat{x_i}$, 所以自编码器的误差就是输入节点$v$的邻接向量的重构误差。所以，二阶相似度损失函数定义为:
$$\mathcal{L}=\sum_{i=1}^n{||\hat{x_i}-x_i||^2_2}$$
值得注意的是，由于网络的稀疏性，邻接矩阵中的0元素远多于非0元素，使用邻接矩阵作为输入的话要处理很多0，这样就做了太多无用功了。为了解决这个问题，对损失函数做了改进如下：
$$\mathcal{L_{2nd}}=\sum_{i=1}^n||(\hat{x_i}-x_i)\odot{b_i}||^2_2=||\hat{X}-X\odot{B}||^2_F$$
其中$\odot$是哈马达乘积，表示对应元素相乘。$b_i={b_{i,j}}^n_{j=1}$， 邻接矩阵中的0对应$b=1$, 非0元素的$b&amp;gt;1$,这样的目的是对于有边连接的节点增加惩罚。可以理解为对有边连接的节点赋予更高权重。
以上我们获得了二阶相似度的损失函数。在介绍一阶相似度之前，我们先来看看拉普拉斯映射（Laplacian Eigenmap） 其实LE也是一种经典的NRL方法，主要目的也是降维。其目标函数如下所示:
$$\sum_{i,j} W_{ij}||y_i-y_j||^2$$
LE是通过构建相似关系图来重构局部特征结构,如果放在网络结构中来说,如果节点$v_i$和$v_j$很接近（有边），那么他们在embedding space中的距离也应该相应接近。$y_i$和$y_j$就表示他们在特征空间中的表示。因此，本文定义了保持一阶相似度的目标函数：
$$\mathcal{L_{1st}}=\sum_{i,j=1}^n{s_{i,j}||y_i^{(K)}-y_j^{(K)}||^2_2}=\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}$$
具体来说，$K$就是自编码器第$K$层的输出，即编码结果，需要保持一条边的两个节点在嵌入空间中的表示相对接近。
最终 结合一阶相似度和二阶相似度，本文给出了SDNE的目标函数：
$$\mathcal{L_{mix}}=\mathcal{L_{2nd}+\alpha{\mathcal{L_{1st}}}}+\nu{\mathcal{L_{reg}}} =||(\hat{X}-X)\odot{B}||^2_F+\alpha{\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}}+\nu{\mathcal{L_{reg}}}$$
其中，为了防止过拟合，添加了$\mathcal L2$-norm单元$\mathcal{L_{reg}}$来防止过拟合:
$$\mathcal{L_{reg}}=\frac{1}{2}\sum_{k=1}^k({||W^{(k)}||^2_F+||\hat{W}^{k}||_F^2})$$
Optimization 使用SGD来优化$\mathcal{L_{mix}}$。具体算法如下：</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">SDNE</a></p>
<h3 id="introduction">Introduction</h3>
<p>这是一篇比较早的Network Embedding论文， 较早将深度模型应用到NE任务。 首先，本文提出了当前网络表示学习中遇到的三个问题：<br>
<strong>（1）. 高度非线性</strong><br>
<strong>（2）. 尽可能保持网络结构</strong><br>
<strong>（3）. 现实网络的高度稀疏性</strong><br>
SDNE的主要目标就是保持网络结构的一阶相似性和二阶相似性。<br>
一阶相似性就是网络中边相连的节点对之间具有的相似性。<br>
二阶相似性就是在一个Graph中，拥有共同邻居但是不直接向相连的两个节点具有的相似性。</p>
<p>其中，一阶相似性主要反映了网络的局部特征。 二阶相似性反映了网络的全局特征。</p>
<h3 id="model">Model</h3>
<p>本文的模型主要如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-01-21-SDNE/SDNE.png#center" alt="你想输入的替代文字"  />
</p>
<p>这张图看上去有点复杂，实则原理非常简单。</p>
<p>模型分为无监督部分和有监督部分，无监督部分是一个<strong>深度自编码器</strong> 用来捕获二阶相似度（保留全局结构），监督部分是一个拉普拉斯特征映射捕获一阶相似度（局部结构）。呃呃呃，Emmmmm ,不知道是我理解有问题还是其他原因，文章里说的和我理解的不太一样 /(ㄒoㄒ)/~~。 然后介绍一下具体的模型结构：</p>
<p>深度自编码器的编码部分：</p>
<p>$$y_i^{(k)}=\sigma{(W^{(k)}y_i^{(k-1)}+b^{(k)})}, k=2,&hellip;,K$$</p>
<p>假设第$k$层是 节点$v$的表示向量（仅考虑全局信息），那么从第$k$层开始解码，最终得到$\hat{x_i}$, 所以自编码器的误差就是输入节点$v$的邻接向量的重构误差。所以，二阶相似度损失函数定义为:<br>
$$\mathcal{L}=\sum_{i=1}^n{||\hat{x_i}-x_i||^2_2}$$</p>
<p>值得注意的是，由于网络的稀疏性，邻接矩阵中的0元素远多于非0元素，使用邻接矩阵作为输入的话要处理很多0，这样就做了太多无用功了。为了解决这个问题，对损失函数做了改进如下：<br>
$$\mathcal{L_{2nd}}=\sum_{i=1}^n||(\hat{x_i}-x_i)\odot{b_i}||^2_2=||\hat{X}-X\odot{B}||^2_F$$</p>
<p>其中$\odot$是哈马达乘积，表示对应元素相乘。$b_i={b_{i,j}}^n_{j=1}$， 邻接矩阵中的0对应$b=1$, 非0元素的$b&gt;1$,这样的目的是对于有边连接的节点增加惩罚。可以理解为对有边连接的节点赋予更高权重。</p>
<p>以上我们获得了二阶相似度的损失函数。在介绍一阶相似度之前，我们先来看看<strong>拉普拉斯映射（Laplacian Eigenmap）</strong>  其实LE也是一种经典的NRL方法，主要目的也是降维。其目标函数如下所示:<br>
$$\sum_{i,j} W_{ij}||y_i-y_j||^2$$<br>
LE是通过构建相似关系图来重构局部特征结构,如果放在网络结构中来说,如果节点$v_i$和$v_j$很接近（有边），那么他们在embedding space中的距离也应该相应接近。$y_i$和$y_j$就表示他们在特征空间中的表示。因此，本文定义了保持一阶相似度的目标函数：<br>
$$\mathcal{L_{1st}}=\sum_{i,j=1}^n{s_{i,j}||y_i^{(K)}-y_j^{(K)}||^2_2}=\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}$$<br>
具体来说，$K$就是自编码器第$K$层的输出，即编码结果，需要保持一条边的两个节点在嵌入空间中的表示相对接近。</p>
<p>最终 结合一阶相似度和二阶相似度，本文给出了SDNE的目标函数：<br>
$$\mathcal{L_{mix}}=\mathcal{L_{2nd}+\alpha{\mathcal{L_{1st}}}}+\nu{\mathcal{L_{reg}}}
=||(\hat{X}-X)\odot{B}||^2_F+\alpha{\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}}+\nu{\mathcal{L_{reg}}}$$<br>
其中，为了防止过拟合，添加了$\mathcal L2$-norm单元$\mathcal{L_{reg}}$来防止过拟合:<br>
$$\mathcal{L_{reg}}=\frac{1}{2}\sum_{k=1}^k({||W^{(k)}||^2_F+||\hat{W}^{k}||_F^2})$$</p>
<h3 id="optimization">Optimization</h3>
<p>使用SGD来优化$\mathcal{L_{mix}}$。具体算法如下：<br>
<img loading="lazy" src="/posts/2019-01-21-SDNE/al.png#center" alt="你想输入的替代文字"  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ICLR2018 《Graph Attention Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/gat/</link>
      <pubDate>Fri, 14 Sep 2018 23:01:31 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/gat/</guid>
      <description>论文地址：GAT
Introduction 本文介绍了一种新型的神经网络架构用来处理图结构。即__Graph Attention Networks__(GATs)。该方法利用masked self-attentional layer，即通过网络层的堆叠，可以获取网络中每个节点的领域特征，同时为领域中的不同节点指定不同的权重。这样做的好处是可以不需要各种高成本的矩阵运算也不依赖于的图结构信息。通过这种方式，GAT可以解决基于谱的图神经网络存在的问题，同时，GAT可以使用用归纳（inductive）和直推（transductive）问题。
归纳学习:先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。
直推学习:先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。
Architecture 图$G$中有$N$个节点，他们的特征向量为：$\textbf{h}={\vec{h_1},\vec{h_2},&amp;hellip;,\vec{h_N}}$，其中，$\vec{h_i} \in \mathbb{R}^F$，$F$是每个节点特征数。我们的目的是输出一个新的节点特征向量集$\textbf{h&amp;rsquo;}={\vec{h_1&amp;rsquo;},\vec{h_2&amp;rsquo;},&amp;hellip;,\vec{h_N&amp;rsquo;}}$，其中$\vec{h_i&amp;rsquo;} \in \mathbb{R}^{F&amp;rsquo;}$。 本质就是修改特征向量的维度（Network embedding）
为了获得足够的表达能力以将输入特征变换为更高级别的特征，需要至少一个可学习的线性变换。因此，以任意节点$i$和$j$为例，分别对节点$i$和节点$j$的特征向量做线性变换$W \in \mathbb{R}^{F \times F&amp;rsquo;}$，这样 就将$\vec{h_i}$和$\vec{h_j}$从$F$维的向量转化为$F&amp;rsquo;$维的向量： $$ e_{ij} = a(W\vec{h_i},W\vec{h_j}) $$ 上式中，分别对$\vec{h_i}$和$\vec{h_j}$做线性变换，然后使用self-attention为图中的每一个节点分配注意力（权重）。上式中，注意力机制$a$是一个$\mathbb{R}^{F&amp;rsquo;} \times \mathbb{R}^{F&amp;rsquo;} \to \mathbb{R}$的映射。最终得到的$e_{ij}$是节点$j$对节点$i$的影响力系数（一个实数）。
但是，上面的方法考虑其他节点对$i$的影响时，将图中的所有节点都纳入了考虑范围，这样就丢失了图的结构信息。因此，本文引入masked attention机制，即计算影响力系数$e_{ij}$时， 仅考虑节点$i$的一部分邻居节点 $j \in \mathcal{N}_i$（$i$也属于$\mathcal{N}_i$）。使用softmax将节点$i$部分邻居的注意力系数分配到(0,1)上： $$ \alpha_{ij} = \mathrm{softmax}_j(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i}\exp(e_{ik})} $$ 在本文中，$a$是一个单层前馈神经网络，参数是一个权重向量$\vec{\text{a}} \in \mathbb{R}^{2F&amp;rsquo;}$，然后使用负半轴斜率为0.2的LeakyReLU作为非线性激活函数： $$ \alpha_{ij} = \frac{\exp(\mathrm{LeakyReLU}(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_j}]))}{\sum_{k\in \mathcal{N}_i} \exp(\mathrm{LeakyReLU}(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_k}]))} $$ 其中$||$表示向量的连接操作。上述过程可以用下图表示：
这样，我们就可以获得节点$j$对节点$i$的注意力系数$\alpha_{ij}$，那么，节点$i$最终的输出特征$\vec{h_i&amp;rsquo;}$就是对$\mathcal{N}_i$中所有节点的加权（加注意力）求和： $$ \vec{h_i&amp;rsquo;} = \sigma (\sum_{j \in \mathcal{N}_i}\alpha_{ij} W\vec{h_j}) $$
另外，本文使用multi-head attention来稳定self-attention的学习过程，如下图所示：</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1710.10903">GAT</a></p>
<h1 id="introduction">Introduction</h1>
<p>本文介绍了一种新型的神经网络架构用来处理图结构。即__Graph Attention Networks__(<strong>GATs</strong>)。该方法利用masked self-attentional layer，即通过网络层的堆叠，可以获取网络中每个节点的领域特征，同时为领域中的不同节点指定不同的权重。这样做的好处是可以不需要各种高成本的矩阵运算也不依赖于的图结构信息。通过这种方式，GAT可以解决基于谱的图神经网络存在的问题，同时，GAT可以使用用归纳（inductive）和直推（transductive）问题。</p>
<p><strong>归纳学习</strong>:先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。</p>
<p><strong>直推学习</strong>:先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。</p>
<h1 id="architecture">Architecture</h1>
<p>图$G$中有$N$个节点，他们的特征向量为：$\textbf{h}={\vec{h_1},\vec{h_2},&hellip;,\vec{h_N}}$，其中，$\vec{h_i} \in \mathbb{R}^F$，$F$是每个节点特征数。我们的目的是输出一个新的节点特征向量集$\textbf{h&rsquo;}={\vec{h_1&rsquo;},\vec{h_2&rsquo;},&hellip;,\vec{h_N&rsquo;}}$，其中$\vec{h_i&rsquo;} \in \mathbb{R}^{F&rsquo;}$。 本质就是修改特征向量的维度（Network embedding）</p>
<p>为了获得足够的表达能力以将输入特征变换为更高级别的特征，需要至少一个可学习的线性变换。因此，以任意节点$i$和$j$为例，分别对节点$i$和节点$j$的特征向量做线性变换$W \in \mathbb{R}^{F \times F&rsquo;}$，这样 就将$\vec{h_i}$和$\vec{h_j}$从$F$维的向量转化为$F&rsquo;$维的向量：
$$
e_{ij} = a(W\vec{h_i},W\vec{h_j})
$$
上式中，分别对$\vec{h_i}$和$\vec{h_j}$做线性变换，然后使用self-attention为图中的每一个节点分配注意力（权重）。上式中，注意力机制$a$是一个$\mathbb{R}^{F&rsquo;} \times \mathbb{R}^{F&rsquo;} \to \mathbb{R}$的映射。最终得到的$e_{ij}$是节点$j$对节点$i$的影响力系数（一个实数）。</p>
<p>但是，上面的方法考虑其他节点对$i$的影响时，将图中的所有节点都纳入了考虑范围，这样就丢失了图的结构信息。因此，本文引入<strong>masked attention</strong>机制，即计算影响力系数$e_{ij}$时， 仅考虑节点$i$的<strong>一部分邻居节点</strong> $j \in \mathcal{N}_i$（$i$也属于$\mathcal{N}_i$）。使用softmax将节点$i$部分邻居的注意力系数分配到(0,1)上：
$$
\alpha_{ij} = \mathrm{softmax}_j(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i}\exp(e_{ik})}
$$
在本文中，$a$是一个单层前馈神经网络，参数是一个权重向量$\vec{\text{a}} \in \mathbb{R}^{2F&rsquo;}$，然后使用负半轴斜率为0.2的<a href="https://blog.csdn.net/sinat_33027857/article/details/80192789">LeakyReLU</a>作为非线性激活函数：
$$
\alpha_{ij} = \frac{\exp(\mathrm{LeakyReLU}(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_j}]))}{\sum_{k\in \mathcal{N}_i} \exp(\mathrm{LeakyReLU}(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_k}]))}
$$
其中$||$表示向量的连接操作。上述过程可以用下图表示：</p>
<p><img loading="lazy" src="/posts/2019-04-14-GAT/1.png#center" alt=""  />
</p>
<p>这样，我们就可以获得节点$j$对节点$i$的注意力系数$\alpha_{ij}$，那么，节点$i$最终的输出特征$\vec{h_i&rsquo;}$就是对$\mathcal{N}_i$中所有节点的加权（加注意力）求和：
$$
\vec{h_i&rsquo;} = \sigma (\sum_{j \in \mathcal{N}_i}\alpha_{ij} W\vec{h_j})
$$</p>
<p>另外，本文使用<strong>multi-head attention</strong>来稳定self-attention的学习过程，如下图所示：</p>
<p><img loading="lazy" src="/posts/2019-04-14-GAT/2.png#center" alt=""  />
</p>
<p>图中是$K=3$ heads的multi-head attention，不同颜色的箭头表示一个独立的attention计算，每个邻居节点做三次attention计算。每次attention计算就是一个普通的self-attention，输出的结果是一个$\vec{h_i&rsquo;}$。multi-head attention为每个节点$i$输出3个不同的$\vec{h_i&rsquo;}$,，然后将这三个向量做连接或者取平均，得到最终的$\vec{h_i&rsquo;}$：
$$
\vec{h_i&rsquo;} = ||^K_{k=1} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)
$$
上式为把不同$k$的向量做连接操作，其中$\alpha_{ij}^k$和$\mathbf{W}^{k}$表示第$k$个head的结果，我们可以注意到，最终输出的结果是$KF&rsquo;$维的。除了concat之外，我们还可以通过求平均的方式来获得$\vec{h_i&rsquo;}$:
$$
\vec{h^\prime_i}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)
$$</p>
<h1 id="comparisions">Comparisions</h1>
<ul>
<li>
<p>GAT是计算高效的。self-attention是在所有边上并行计算，并且输出的特征在所有边上并行计算，从而不需要昂贵的矩阵计算和特征分解。单个head的GAT的时间复杂度为$O\left(|V| F F^{\prime}+|E| F^{\prime}\right)$，其中$F$是输入的特征数，$|V|$和$|E|$分别是节点数和边数。复杂度与GCN相同。</p>
</li>
<li>
<p>与GCN不同的是，GAT为同一邻域中的节点分配不同的重要性（different importance），提升了模型容量。</p>
</li>
<li>
<p>注意机制以共享的方式应用于图中的所有边（共享$\mathbf{W}$），因此它不依赖于对全局图结构的预先访问或其所有节点的（特征）。这样有以下提升：</p>
<ul>
<li>不必是无向图。如果$i \to j$不存在,可以直接不用计算$\alpha_{ij}$。</li>
<li>可直接应用于归纳学习。</li>
</ul>
</li>
<li>
<p>GAT可以被描述为一种特殊的<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf">MoNet(Geometric deep learning on graphs and manifolds using mixture model cnns)</a>。</p>
</li>
</ul>
<h1 id="reference">Reference</h1>
<p>参考：</p>
<p>GCN：https://arxiv.org/abs/1609.02907</p>
<p><a href="https://zhuanlan.zhihu.com/p/34232818">https://zhuanlan.zhihu.com/p/34232818</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/59176692">https://zhuanlan.zhihu.com/p/59176692</a></p>
<p><a href="https://arxiv.org/abs/1710.10903">https://arxiv.org/abs/1710.10903</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KDD2017 《metapath2vec:Scalable Representation Learning for Heterogeneous Networks》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/metapath2vec/</link>
      <pubDate>Fri, 29 Jun 2018 16:29:18 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/metapath2vec/</guid>
      <description>论文地址： metapath2vec
Introduction 真实网络中通常包含多种类型的节点和关系，这些节点和关系共同组成了异质信息网络（Heterogeneous Information Network），传统的NE方法如DeepWalk, Line , node2vec更多关注与同质网络从而忽略了网络的异质性。针对这个问题，这篇文章提出了metapath2vec基于meta-path的随机游走来构建节点的异质邻域，采用异质Skip-gram模型来学习节点的representation。 另外，在负采样时也考虑异质性，从而进一步提出了metapath2vec++。如下：
Definition Definition 1: 异质网络： $G = (V,E,T)$，其中每个节点和边分别对应一个映射 $\phi(v) : V \rightarrow T_{V}$ 和$\varphi(e) : E \rightarrow T_{E}$。 其中$T_V$和$T_E$分别表示节点的类型集合以及关系的类型集合。且$\left|T_{V}\right|+\left|T_{E}\right|&amp;gt;2$。
Definition 2: 异质网络表示学习 为网络中的节点学习一个$d$维的表示 $\mathrm{X} \in \mathbb{R}^{|V| \times d}$，$d \ll|V|$。节点的向量表示可以捕获节点间结构和语义关系。
Model metapath2vec Skip-Gram模型是给定一个节点，预测向下文节点的概率。metapath2vec的目的是考虑多种类型节点和多种类型关系的情况下，最大化网络概率。为了将异质信息融入skip-gram模型中，首先提出异质skip-gram。
Heterogeneous Skip-Gram 对于节点类型$|T_V| &amp;gt; 1$的网络$G$， 给定一个节点$v$，需要最大化属于类型$t \in T_V$的异质上下文节点的概率： $$ \arg \max_{\theta} \sum_{v \in V} \sum_{t \in T_{V}} \sum_{c_{t} \in N_{t}(v)} \log p\left(c_{t} | v ; \theta\right) $$ 其中，$N_t(v)$表示$v$属于第$t$个类型的邻居节点。 $p\left(c_{t} | v ; \theta\right)$ 表示给定节点$v$，它的$t$类型邻域概率： $p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u \in V} e^{X_{u} \cdot X_{v}}}$，是一个softmax函数，$X_v$是节点$v$的表示。由于softmax函数的分母每一次都要遍历所有节点，所以采用负采样改进$\log p(\cdot)$ 如下： $$ \log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u^{m} \sim P(u)}\left[\log \sigma\left(-X_{u^{m}} \cdot X_{v}\right)\right] $$ 其中 $\sigma(x)=\frac{1}{1+e^{-x}}$。</description>
      <content:encoded><![CDATA[<p>论文地址： <a href="https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf">metapath2vec</a></p>
<h1 id="introduction">Introduction</h1>
<p>真实网络中通常包含多种类型的节点和关系，这些节点和关系共同组成了异质信息网络（Heterogeneous Information Network），传统的NE方法如DeepWalk, Line , node2vec更多关注与同质网络从而忽略了网络的异质性。针对这个问题，这篇文章提出了metapath2vec基于meta-path的随机游走来构建节点的异质邻域，采用异质Skip-gram模型来学习节点的representation。 另外，在负采样时也考虑异质性，从而进一步提出了metapath2vec++。如下：</p>
<p><img loading="lazy" src="/posts/2019-06-29-metapath/1.png" alt=""  />
</p>
<h1 id="definition">Definition</h1>
<p><strong>Definition 1:</strong> 异质网络： $G = (V,E,T)$，其中每个节点和边分别对应一个映射 $\phi(v) : V \rightarrow T_{V}$ 和$\varphi(e) : E \rightarrow T_{E}$。 其中$T_V$和$T_E$分别表示节点的类型集合以及关系的类型集合。且$\left|T_{V}\right|+\left|T_{E}\right|&gt;2$。</p>
<p><strong>Definition 2:</strong> 异质网络表示学习 为网络中的节点学习一个$d$维的表示 $\mathrm{X} \in \mathbb{R}^{|V| \times d}$，$d \ll|V|$。节点的向量表示可以捕获节点间结构和语义关系。</p>
<h1 id="model">Model</h1>
<h2 id="metapath2vec">metapath2vec</h2>
<p>Skip-Gram模型是给定一个节点，预测向下文节点的概率。metapath2vec的目的是考虑多种类型节点和多种类型关系的情况下，最大化网络概率。为了将异质信息融入skip-gram模型中，首先提出异质skip-gram。</p>
<h3 id="heterogeneous-skip-gram">Heterogeneous Skip-Gram</h3>
<p>对于节点类型$|T_V| &gt; 1$的网络$G$， 给定一个节点$v$，需要最大化属于类型$t \in T_V$的异质上下文节点的概率：
$$
\arg \max_{\theta} \sum_{v \in V} \sum_{t \in T_{V}} \sum_{c_{t} \in N_{t}(v)} \log p\left(c_{t} | v ; \theta\right)
$$
其中，$N_t(v)$表示$v$属于第$t$个类型的邻居节点。 $p\left(c_{t} | v ; \theta\right)$ 表示给定节点$v$，它的$t$类型邻域概率： $p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u \in V} e^{X_{u} \cdot X_{v}}}$，是一个softmax函数，$X_v$是节点$v$的表示。由于softmax函数的分母每一次都要遍历所有节点，所以采用负采样改进$\log p(\cdot)$ 如下：
$$
\log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u^{m} \sim P(u)}\left[\log \sigma\left(-X_{u^{m}} \cdot X_{v}\right)\right]
$$
其中 $\sigma(x)=\frac{1}{1+e^{-x}}$。</p>
<h3 id="meta-path-based-random-walks">Meta-Path-Based Random Walks</h3>
<p>在第$i$步时，转移概率$p(v^{i+1}|v^i)$表示为忽略节点类型情况下$v^i$的邻居分布。但是，PathSim提出，异质信息网络中的随机游走偏向于高度可见的节点，即具有主导数量路径的节点，所以 本文设计了基于元路径的随机游走来生成path，从而能够捕获不同类型节点间的结构联系和语义关系，提出了促进异构网络结构转换为metapath2vec的skip-gram。</p>
<p>一个meta-path模式$\mathcal{P}: V_{1} \stackrel{R_{1}}{\longrightarrow} V_{2} \stackrel{R_{2}}{\longrightarrow} \dots V_{t} \stackrel{R_{t}}{\longrightarrow} V_{t+1} \cdots \stackrel{R_{l-1}}{\longrightarrow} V_{l}$， 其中 $R=R_{1} \circ R_{2} \circ \cdots \circ R_{l-1}$ 节点类型$V_{1}$到$V_{l}$之间的组合关系。那么节点间的跳转概率定义为：
$$
p\left(v^{i+1} | v_{t}^{i}, \mathcal{P}\right)=\left\{\begin{array}{cc}{\frac{1}{\left|N_{t+1}\left(v_{t}^{i}\right)\right|}} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \in E, \phi\left(v^{i+1}\right)=t+1} \\
{0} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \in E, \phi\left(v^{i+1}\right) \neq t+1} \\
{0} &amp; {\left(v^{i+1}, v_{t}^{i}\right) \notin E}\end{array}\right.
$$
其中$v^i_t \in V_t$，$N_{t+1}\left(v_{t}^{i}\right)$表示属于$t$类型的节点$v$的属于$t+1$类型的邻居。如果下一个节点$v^{i+1}$和$v^i_t$之间有边，并且$v^{i+1}$是$t+1$类型的节点 那么转移概率服从平均分布。其中，$v^{i+1}$服从meta-path所定义的下移节点类型。如图（a）中，原路径为$OAPVPAO$，那么节点$a_4$的下一个节点必然要是$P$类。 由于meta-path的对称性，所以：
$$
p\left(v^{i+1} | v_{t}^{i}\right)=p\left(v^{i+1} | v_{1}^{i}\right), \text { if } t=l
$$</p>
<h2 id="metapath2vec-1">metapath2vec++</h2>
<p>由于softmax做归一化时没有考虑节点类型，分母是对所有节点求和，所以为了融合节点类型，给出<strong>Heterogeneous negative sampling</strong>:
$$
p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u_{t} \in V_{t}} e^{X_{u_{t}} \cdot X_{v}}}
$$
<strong>如图（c）所示，metapath2vec++对每种类型节点指定不同的一组多项式分布</strong>，相当于在输出层根据节点类型，把异质网络分解成不同的同质网络，同样采用负采用的方法简化计算：
$$
O(\mathrm{X})=\log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u_{t}^{m} \sim P_{t}\left(u_{t}\right)}\left[\log \sigma\left(-X_{u_{t}^{m}} \cdot X_{v}\right)\right]
$$
算法如下：</p>
<p><img loading="lazy" src="/posts/2019-06-29-metapath/2.png#center" alt=""  />
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>OpenCV轮廓提取并计算图片中某一封闭区域的面积</title>
      <link>https://JhuoW.github.io/posts/pic-closed-edge/</link>
      <pubDate>Mon, 02 Apr 2018 15:43:44 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/pic-closed-edge/</guid>
      <description>最近遇到一个问题，如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积
之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：
我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：
{% codeblock %}
import cv2 import numpy as np # Input image img = cv2.imread(&#39;cut.jpeg&#39;, cv2.IMREAD_GRAYSCALE) # Needed due to JPG artifacts _, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY) # Dilate to better detect contours temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) # Find largest contour _, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) largestCnt = [] for cnt in cnts: if len(cnt) &amp;gt; len(largestCnt): largestCnt = cnt # Determine center of area of largest contour M = cv2.</description>
      <content:encoded><![CDATA[<p>最近遇到一个问题，如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积<br>
<img loading="lazy" src="/posts/2019-04-02-Pic-closed-edge/1.png" alt=""  />
<br>
之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：<br>
<img loading="lazy" src="/posts/2019-04-02-Pic-closed-edge/edge.jpeg" alt=""  />
<br>
我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：<br>
{% codeblock %}</p>
<pre><code>import cv2
import numpy as np

# Input image
img = cv2.imread('cut.jpeg', cv2.IMREAD_GRAYSCALE)

# Needed due to JPG artifacts
_, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)

# Dilate to better detect contours
temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))

# Find largest contour
_, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
largestCnt = []
for cnt in cnts:
    if len(cnt) &gt; len(largestCnt):
        largestCnt = cnt

# Determine center of area of largest contour
M = cv2.moments(largestCnt)
x = int(M[&quot;m10&quot;] / M[&quot;m00&quot;])
y = int(M[&quot;m01&quot;] / M[&quot;m00&quot;])

# Initiale mask for flood filling
width, height = temp.shape
mask = img2 = np.ones((width + 2, height + 2), np.uint8) * 255
mask[1:width, 1:height] = 0

# Generate intermediate image, draw largest contour, flood filled
temp = np.zeros(temp.shape, np.uint8)
temp = cv2.drawContours(temp, largestCnt, -1, 255, cv2.FILLED)
_, temp, mask, _ = cv2.floodFill(temp, mask, (x, y), 255)
temp = cv2.morphologyEx(temp, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))

# Count pixels in desired region
area = cv2.countNonZero(temp)

# Put result on original image
img = cv2.putText(img, str(area), (x, y), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, 255)

cv2.imshow('Input', img)
cv2.imshow('Temp image', temp)

cv2.waitKey(0)
</code></pre>
<p>{% endcodeblock %}</p>
<p>最后我们可以得到一个比较准确的轮廓：<br>
<img loading="lazy" src="/posts/2019-04-02-Pic-closed-edge/img_trk.jpg" alt=""  />
<br>
面积如图中所示：<br>
<img loading="lazy" src="/posts/2019-04-02-Pic-closed-edge/img_tr.jpg" alt=""  />
</p>
<p>参考：<br>
<a href="https://stackoverflow.com/questions/55467031/how-to-get-the-area-of-the-contours">https://stackoverflow.com/questions/55467031/how-to-get-the-area-of-the-contours</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>深度学习中的优化算法总结</title>
      <link>https://JhuoW.github.io/posts/optimizer/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/optimizer/</guid>
      <description>最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。
Gradient Desent(梯度下降) 目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。
(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。
(2).目标函数关于参数$x$在epoch $t$时的梯度：
$$g_t = \nabla_x f(x_t)$$
(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：
$$x_{t+1} = x_t-\eta_t g_t$$
其中$x_{t+1}$为$t+1$时刻的参数值。
Stochastic Gradient Desent(随机梯度下降) 梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。
首先给出无偏估计的定义，稍后会用到：
无偏估计：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。
深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \frac{\displaystyle\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：
$$\nabla f_{batch}(x) = \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x)$$
如果使用GD来优化：
$$x_{t+1} = x_{t}- \eta_t \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x) \ = x_t-\eta_t \nabla f_{batch}(x)$$ 上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。
随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \in {1, \cdots,n}$, 每次从中随机选取一个样本来更新参数$x$。
$$x_{t+1} = x_{t}-\eta_t \nabla f_i(x)$$
这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\nabla f_i(x)$是对梯度$\nabla f_{batch}(x)$的无偏估计，因为：</description>
      <content:encoded><![CDATA[<p>最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。</p>
<h1 id="gradient-desent梯度下降">Gradient Desent(梯度下降)</h1>
<p>目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。<br>
(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。<br>
(2).目标函数关于参数$x$在epoch $t$时的梯度：<br>
$$g_t = \nabla_x f(x_t)$$<br>
(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：<br>
$$x_{t+1} = x_t-\eta_t g_t$$<br>
其中$x_{t+1}$为$t+1$时刻的参数值。</p>
<h2 id="stochastic-gradient-desent随机梯度下降">Stochastic Gradient Desent(随机梯度下降)</h2>
<p>梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。<br>
首先给出无偏估计的定义，稍后会用到：<br>
<a href="https://www.cnblogs.com/notwice/p/8538539.html">无偏估计</a>：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。</p>
<p>深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \frac{\displaystyle\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：<br>
$$\nabla f_{batch}(x) = \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x)$$<br>
如果使用GD来优化：<br>
$$x_{t+1} = x_{t}- \eta_t \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x) \ = x_t-\eta_t \nabla f_{batch}(x)$$
上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。</p>
<p>随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \in {1, \cdots,n}$, 每次从中随机选取一个样本来更新参数$x$。<br>
$$x_{t+1} = x_{t}-\eta_t \nabla f_i(x)$$<br>
这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\nabla f_i(x)$是对梯度$\nabla f_{batch}(x)$的无偏估计，因为：<br>
$$E_i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f_{batch}(\boldsymbol{x})$$<br>
符合无偏估计的定义。</p>
<h2 id="momentum动量法">Momentum(动量法)</h2>
<h3 id="exponentially-weighted-moving-averagesema">Exponentially weighted moving averages(EMA)</h3>
<p>EMA,指数加权移动平均数。</p>
<p>在GD中,如果学习率过大，会导致目标函数发散，而无法逼近最小值，如下图所示：<br>
<img loading="lazy" src="/posts/2019-02-28-Optimizer/images/EMA_1.png" alt="1"  />
<br>
如果学习率很低，那么会缓慢接近最优点，如下图红色轨迹：<br>
<img loading="lazy" src="/posts/2019-02-28-Optimizer/images/EMA_2.png" alt="2"  />
<br>
我们希望在学习率较小的时候可以更快逼近最优点，在学习率大的时候自变量可以不发散，即在正确的方向上加速下降并且抑制震荡，也就是达到如下的效果：<br>
<img loading="lazy" src="/posts/2019-02-28-Optimizer/images/EMA_3.png" alt="3"  />
</p>
<p>因此引入EMA。给定参数$0 \leq \gamma &lt; 1$,当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合。<br>
$$y_t = \gamma y_{t-1} + (1-\gamma) x_t$$<br>
展开上式:<br>
$$\begin{split}\begin{aligned}
y_t  &amp;= (1-\gamma) x_t + \gamma y_{t-1}\\
&amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + \gamma^2y_{t-2}\\
&amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + (1-\gamma) \cdot \gamma^2x_{t-2} + \gamma^3y_{t-3}\\
&amp;\ldots
\end{aligned}\end{split}$$<br>
上式可以看出当前时刻变量是对过去时刻变量做指数加权，离当前时刻越近，加权越大（越接近1）。<br>
在现实中，我们将$y_t$看作是最近$1/(1-\gamma)$个时间步的$x_t$的加权平均，当$\gamma = 0.95$时，是最近20个时间步的$x_t$值的加权平均。当$\gamma=0.9$时,可以看做是最近10个时间步加权平均。</p>
<h3 id="动量法">动量法</h3>
<p>$$\begin{split}\begin{aligned}
\boldsymbol{v}_t &amp;= \gamma \boldsymbol{v}_{t-1} + \eta_t \boldsymbol{g}_t, \\
\boldsymbol{x}_t &amp;= \boldsymbol{x}_{t-1} - \boldsymbol{v}_t,
\end{aligned}\end{split}$$<br>
其中$g_t = \nabla f_i(x)$上式可以看出，如果$\gamma=0$，则上式就是一个普通的随机梯度下降法。$0 \leq \gamma &lt; 1$. $\gamma$一般取0.9。<br>
一般，初始化$v_0=0$, 则<br>
$$v_1=\eta_t g_t \\ v_2=\gamma v_1+\eta_t g_t = \eta_t g_t(\gamma+1) \\ v_3 = \eta_t g_t (\gamma^2+\gamma+1) \\ v_{inf} = \frac{(\eta_t g_t)\cdot(1-\gamma^{inf+1})}{1-\gamma}\approx \frac{(\eta_t g_t)}{1-\gamma}$$</p>
<p>相比原始梯度下降算法，动量梯度下降算法有助于加速收敛。当梯度与动量方向一致时，动量项会增加，而相反时，动量项减少，因此动量梯度下降算法可以减少训练的震荡过程。</p>
<p>换种方式理解动量法：<br>
<img loading="lazy" src="/posts/2019-02-28-Optimizer/images/m.jpg" alt="4"  />
<br>
如上图所示，A点为起始点，首先计算A点的梯度$\nabla a$，下降到B点，<br>
$$\theta_{new} = \theta-\eta\nabla a$$<br>
其中$\theta$为参数， $\eta$为学习率<br>
到达B点后要加上A点的梯度，但是A点的梯度有个衰减值$\gamma$,推荐取0.9，相当于加上一个来自A点递减的加速度。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。所以B点的参数更新公式是这样的：<br>
$$v_t = \gamma v_{t-1}+\eta \nabla b$$<br>
$$\theta_{new} = \theta-v_t$$<br>
其中$v_{t-1}$表示之前所有步骤累计的动量和，$\nabla b$为B点的梯度方向。这样一步一步下去，带着初速度的小球就会极速的奔向谷底。</p>
<h1 id="adagrad">AdaGrad</h1>
<p>假设目标函数有两个参数分别为$x_1$,$x_2$,若梯度下降迭代过程中，始终使用相同的学习率$\eta$:<br>
$$x_{1_{new}} = x_1-\eta \frac{\partial f}{\partial x_1}$$
$$x_{2_{new}} = x_2-\eta \frac{\partial f}{\partial x_2}$$<br>
AdaGard算法根据自变量在每个维度的梯度值来调整各个维度上的学习率，避免学习率难以适应维度的问题。adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。<br>
$\nabla_{\theta_i} J(\theta)$表示第$i$个参数的梯度，其中$\theta=(\theta_1,\theta_2,&hellip;)$有$n$个参数。如果使用SGD来优化第$i$个参数，我们可以表示为:<br>
$$\theta_{i_new} = \theta_i-\eta \nabla_{\theta_i}J(\theta)$$<br>
如果使用Adagrad，则可以表示为这样:<br>
$$\theta_{i,t+1}=\theta_{i,t}-\frac{\eta}{\sqrt{G_{i,t}+\epsilon}} \nabla_{\theta_{i,t}}J(\theta)$$<br>
$i,t$ 表示优化参数$\theta_i$时的第$t$次迭代，$\epsilon$防止分母为0，可以取$10^{-6}$,$G_{i,t}$表示对参数$\theta_i$优化的前$t$步的梯度的累加：<br>
$$G_{i,t} = G_{i,t-1}+\nabla_{\theta_{i,t}}J(\theta) $$<br>
新公式可以简化成:<br>
$$\theta_{t+1}= \theta_t-\frac{\eta}{\sqrt{G_t+\epsilon}}\nabla_{\theta_t}J(\theta)$$<br>
可以从上式看出，随着迭代的推移，新的学习率$\frac{\eta}{\sqrt{G_t+\epsilon}}$在缩小，说明Adagrad一开始激励收敛，到了训练的后期惩罚收敛，收敛速度变慢</p>
<h1 id="rmsprop">RMSprop</h1>
<p>主要解决Adagrad学习率过快衰减问题，类似动量的思想，引入一个超参数，在积累梯度平方项进行衰减.<br>
$$s = \gamma \cdot s +(1-\gamma) \cdot \nabla J(\theta) \odot \nabla J(\theta) $$<br>
参数$\theta$的迭代目标函数可以改写为:<br>
$$\theta_{new} = \theta - \frac{\eta}{\sqrt{s+\varepsilon}} \odot \nabla J(\theta)$$<br>
可以看出$s$是梯度的平方的指数加权移动平均值，$\gamma$一般取0.9，有助于解决 Adagrad中学习率下降过快的情况。</p>
<h2 id="adaptive-moment-estimationadam">Adaptive moment estimation(Adam)</h2>
<p>Adam可以说是用的最多的优化算法，Adam通过计算一阶矩估计和二阶矩估计为不同的参数设计独立的自适应学习率。</p>
<h2 id="adabound">Adabound</h2>
<p>正在学习中</p>
<p>参考文献：<br>
<a href="https://zhuanlan.zhihu.com/p/32626442">https://zhuanlan.zhihu.com/p/32626442</a><br>
<a href="https://zhuanlan.zhihu.com/p/31630368">https://zhuanlan.zhihu.com/p/31630368</a><br>
<a href="https://zh.gluon.ai/">https://zh.gluon.ai/</a><br>
<a href="https://blog.csdn.net/tsyccnh/article/details/76270707">https://blog.csdn.net/tsyccnh/article/details/76270707</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>SIGIR2018 《HTNE Embedding Temporal Network via Neighborhood Formation》 Reading Notes</title>
      <link>https://JhuoW.github.io/posts/htne/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://JhuoW.github.io/posts/htne/</guid>
      <description>论文地址：HTNE
Introduction 本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。
另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。
因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。
通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。
另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。
值得注意的是，本文目标是优化邻域生成序列的极大似然估计即条件强度函数（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数
Model Definition 本文通过跟踪节点邻域的形成来捕获网络的形成过程。
Definition 1 : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \in E$ 被表示为按时间顺序的时间序列，例如， $\mathbf{a}_{x,y}={a_1\to{a_2}\to{…}}\subset\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。
因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。
Definition 2 : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2&amp;hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\to(y_2,t_2)\to&amp;hellip;\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。
Hawkes Process 点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。
对于一个给定的节点$x \in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：
$$ \tilde{\lambda}_{y|x}(t)=\mu_{x,y}+\sum_{t_h&amp;lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$$
其中，$\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\sum_{t_h&amp;lt;t}$表示遍历t时刻前$x$的所有邻居。$\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：
$$\kappa(t-t_h)=\exp(-\delta_s(t-t_h))$$
其中，减少率 $\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\delta_s(t-t_h)$越大, $\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。
综上所述，$\kappa$的具体意义是随时间衰减的影响，其中$\delta_s$参数表示对于不同的源节点，影响是不同的。
如果$\tilde{\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。
直观的来看，基本率（base rate）$\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\mu_{x,y}=f(\mathbf{e}_x,\mathbf{e}_y)=-||\mathbf{e}_x-\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\alpha_{h,y}=f(\mathbf{e}_h,\mathbf{e}_y)=-||\mathbf{e}_h-\mathbf{e}_y||^2$。
因为条件强度函数必须为正，所以使用如下公式: $\lambda_{y|x}(t)=\exp(\tilde\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$.</description>
      <content:encoded><![CDATA[<p>论文地址：<a href="https://dl.acm.org/citation.cfm?id=3220054">HTNE</a></p>
<h2 id="introduction">Introduction</h2>
<p>本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。</p>
<p><img loading="lazy" src="/posts/2019-01-17-HTNE/Fig1.png" alt="你想输入的替代文字"  />
</p>
<p>另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。</p>
<p>因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。</p>
<p>通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。</p>
<p>另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。</p>
<p>值得注意的是，本文目标是优化邻域生成序列的极大似然估计即<strong>条件强度函数</strong>（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数</p>
<h2 id="model">Model</h2>
<h3 id="definition">Definition</h3>
<p>本文通过跟踪节点邻域的形成来捕获网络的形成过程。<br>
<strong>Definition 1</strong> : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \in E$ 被表示为按时间顺序的时间序列，例如， $\mathbf{a}_{x,y}={a_1\to{a_2}\to{…}}\subset\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。</p>
<p>因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。</p>
<p><strong>Definition 2</strong> : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2&hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\to(y_2,t_2)\to&hellip;\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。</p>
<h3 id="hawkes-process">Hawkes Process</h3>
<p>点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。<br>
对于一个给定的节点$x \in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：<br>
$$ \tilde{\lambda}_{y|x}(t)=\mu_{x,y}+\sum_{t_h&lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$$<br>
其中，$\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\sum_{t_h&lt;t}$表示遍历t时刻前$x$的所有邻居。$\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：<br>
$$\kappa(t-t_h)=\exp(-\delta_s(t-t_h))$$<br>
其中，减少率 $\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\delta_s(t-t_h)$越大, $\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。<br>
综上所述，$\kappa$的具体意义是随时间衰减的影响，其中$\delta_s$参数表示对于不同的源节点，影响是不同的。</p>
<p>如果$\tilde{\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。</p>
<p>直观的来看，基本率（base rate）$\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\mu_{x,y}=f(\mathbf{e}_x,\mathbf{e}_y)=-||\mathbf{e}_x-\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\alpha_{h,y}=f(\mathbf{e}_h,\mathbf{e}_y)=-||\mathbf{e}_h-\mathbf{e}_y||^2$。<br>
因为条件强度函数必须为正，所以使用如下公式: $\lambda_{y|x}(t)=\exp(\tilde\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$. 这就与传统的NE方法差不多了。。。</p>
<h3 id="attention">Attention</h3>
<p>根据论文中（3）式，可以看出，$\sum_{t_h&lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$这一部分主要描述了历史邻居对当前邻居的影响，但是完全忽略了源节点$x$，因为源节点$x$的变化也会影响到历史邻居对当前邻居的亲近程度(affinity)。因此，本文引入了<strong>attention model</strong>。as follows：<br>
$$w_{h,x} = \frac{\exp(-||\mathbf{e}_x-\mathbf{e}_h||^2)}{\sum_{h&rsquo;}{\exp(-||\mathbf{e}_x-\mathbf{e}_{h&rsquo;}||^2)}}$$<br>
这是一个softmax函数 来根据源节点$x$的不同为它的邻居赋予不同权重。</p>
<p>最后， 历史邻居与当前邻居的连接紧密程度可以表示为:
$$\alpha_{h,y}=w_{h,x}f(\mathbf{e}_h,\mathbf{e}_y)$$</p>
<h3 id="optimization">Optimization</h3>
<p>目标函数即为给定节点$x$以及基于邻域形成序列的霍克斯过程, 生成节点$y$的条件概率。 公式如下：
$$p(y|x, \mathcal{H}_x(t)) = \frac{\lambda_{y|x}(t)}{\sum_{y&rsquo;}{\lambda_{y&rsquo;|x}(t)}}$$
目标函数即为所有节点对的极大似然：
$$\log \mathcal{L}=\sum_{x\in{\mathcal{V}}}{\sum_{y\in{\mathcal{H}_x}}}{\log{p(y|x,\mathcal{H}(t))}}$$</p>
<p>最后，由于softmax过程是calculating expensive，所以采用负采样优化损失函数。</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
