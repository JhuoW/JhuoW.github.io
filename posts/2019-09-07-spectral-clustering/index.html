<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var t=MathJax.Hub.getAllJax(),e;for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>Notes for Spectral Clustering | JhuoW‘s Notes</title><meta name=keywords content="Clustering,Spectral Clustering,algorithm"><meta name=description content="最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。
本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments
Introduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。
基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} > 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\displaystyle \left(\begin{array}{ccc}{d_{1}} & {\ldots} & {\ldots} \\\ {\ldots} & {d_{2}} & {\ldots} \\\ {\vdots} & {\vdots} & {\ddots} \\\ {\ldots} & {\ldots} & {d_{n}}\end{array}\right) ^{n\times n} $$ 是一个$n \times n$的对角阵，对角元素是每个节点的度和。
定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义： $$|A|=A 中的节点个数 $$
$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$"><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Notes for Spectral Clustering"><meta property="og:description" content="最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。
本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments
Introduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。
基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} > 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\displaystyle \left(\begin{array}{ccc}{d_{1}} & {\ldots} & {\ldots} \\\ {\ldots} & {d_{2}} & {\ldots} \\\ {\vdots} & {\vdots} & {\ddots} \\\ {\ldots} & {\ldots} & {d_{n}}\end{array}\right) ^{n\times n} $$ 是一个$n \times n$的对角阵，对角元素是每个节点的度和。
定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义： $$|A|=A 中的节点个数 $$
$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$"><meta property="og:type" content="article"><meta property="og:url" content="https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-09-07T09:11:09+00:00"><meta property="article:modified_time" content="2019-09-07T09:11:09+00:00"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Notes for Spectral Clustering"><meta name=twitter:description content="最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。
本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments
Introduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。
基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} > 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\displaystyle \left(\begin{array}{ccc}{d_{1}} & {\ldots} & {\ldots} \\\ {\ldots} & {d_{2}} & {\ldots} \\\ {\vdots} & {\vdots} & {\ddots} \\\ {\ldots} & {\ldots} & {d_{n}}\end{array}\right) ^{n\times n} $$ 是一个$n \times n$的对角阵，对角元素是每个节点的度和。
定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义： $$|A|=A 中的节点个数 $$
$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JhuoW.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Notes for Spectral Clustering","item":"https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes for Spectral Clustering","name":"Notes for Spectral Clustering","description":"最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。\n本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments\nIntroduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。\n基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} \u0026gt; 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \\sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\\displaystyle \\left(\\begin{array}{ccc}{d_{1}} \u0026amp; {\\ldots} \u0026amp; {\\ldots} \\\\\\ {\\ldots} \u0026amp; {d_{2}} \u0026amp; {\\ldots} \\\\\\ {\\vdots} \u0026amp; {\\vdots} \u0026amp; {\\ddots} \\\\\\ {\\ldots} \u0026amp; {\\ldots} \u0026amp; {d_{n}}\\end{array}\\right) ^{n\\times n} $$ 是一个$n \\times n$的对角阵，对角元素是每个节点的度和。\n定义图的邻接矩阵为$W \\in \\mathbb{R}^{n \\times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \\subset V$， 定义： $$|A|=A 中的节点个数 $$\n$$vol(A) = \\sum_{i \\in A} d_i \\quad 表示A中所有节点的权重之和$$","keywords":["Clustering","Spectral Clustering","algorithm"],"articleBody":"最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。\n本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments\nIntroduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。\n基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij}  0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \\sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\\displaystyle \\left(\\begin{array}{ccc}{d_{1}} \u0026 {\\ldots} \u0026 {\\ldots} \\\\\\ {\\ldots} \u0026 {d_{2}} \u0026 {\\ldots} \\\\\\ {\\vdots} \u0026 {\\vdots} \u0026 {\\ddots} \\\\\\ {\\ldots} \u0026 {\\ldots} \u0026 {d_{n}}\\end{array}\\right) ^{n\\times n} $$ 是一个$n \\times n$的对角阵，对角元素是每个节点的度和。\n定义图的邻接矩阵为$W \\in \\mathbb{R}^{n \\times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \\subset V$， 定义： $$|A|=A 中的节点个数 $$\n$$vol(A) = \\sum_{i \\in A} d_i \\quad 表示A中所有节点的权重之和$$\n基础2：相似矩阵 再提下谱聚类的基本思想： 距离较远的两个点之间权重值较低，距离较近的两个点之间权重值较高\n但是在谱聚类中，我们只能获得数据点的定义，无法给出邻接矩阵，因为是离散的分布在空间中，无法得知其中的连接关系。\n一般来说，通过样本点距离度量的相似矩阵$S$来获得邻接矩阵$W$.\n构建邻接矩阵$W$有两种方法: $\\epsilon$-邻近法， K邻近法和全连接法。\n$\\epsilon$-邻近法 $\\epsilon$为距离的阈值，欧式距离$S_{ij}$表示两点$v_i$,$v_j$的坐标$x_i$,$x_j$之间的距离。 即 $S_{ij} =||x_i-x_j||^2_2$为相似矩阵$S \\in \\mathbb{R}^{n \\times n}$的第$i$行第$j$个元素，则邻接矩阵$W$可以表示为：\n$$ w_{ij}=\\left\\{\\begin{array}{ll}{0} \u0026 {s_{i j}\\epsilon} \\\\ {\\epsilon} \u0026 {s_{i j} \\leq \\epsilon}\\end{array}\\right. $$\n意思是如果两点之间的距离大于$\\epsilon$，那么他们之间的权重为0， 如果他们之间的距离小于$\\epsilon$，他们之间的权重为$\\epsilon$。 这种方法的弊端在于点之间的距离只有两种情况，不够精确，故很少采用。\nK邻近法 利用KNN算法遍历所有样本点，取每个样本点最近的$k$个点作为近邻，只有和样本点距离最近的$k$个点的$w_{ij} 0$，但会出现一种情况， $v_i$的k个近邻中有$v_j$，但$v_j$的k个近邻中没有$v_i$，这样会造成邻接矩阵$W$的不对称， 因此提供两种解决办法\n第一种： 只要$v_j$在$v_i$的K邻域中，那么不管$v_i$在不在$v_j$的K邻域中，都把$v_i$加入$v_j$的邻域中，即：\n$$ w_{i j}=w_{j i}=\\left\\{\\begin{array}{ll}{0} \u0026 {x_{i} \\notin K N N\\left(x_{j}\\right) \\text { and } x_{j} \\notin K N N\\left(x_{i}\\right)} \\\\ {\\exp \\left(-\\frac{\\left||x_{i}-x_{j}\\right||^2_2}{2 \\sigma^{2}}\\right)} \u0026 {x_{i} \\in K N N\\left(x_{j}\\right) \\text { or } x_{j} \\in K N N\\left(x_{i}\\right)}\\end{array}\\right. $$ 第二种，必须$v_i$在$v_j$的K邻域中，且$v_j$在$v_i$的K邻域中，那么才保留两者间的权重，否则都为0，即：\n$$ w_{i j}=w_{j i}=\\left\\{\\begin{array}{ll}{0} \u0026 {x_{i} \\notin K N N\\left(x_{j}\\right) \\text { or } x_{j} \\notin K N N\\left(x_{i}\\right)} \\\\ {\\exp \\left(-\\frac{||x_{i}-x_{j}||^2_2}{2 \\sigma^{2}}\\right)} \u0026 {x_{i} \\in K N N\\left(x_{j}\\right) \\text { and } x_{j} \\in K N N\\left(x_{i}\\right)}\\end{array}\\right. $$\n全连接法 设所有点之间的权重都大于0，可以选择不同的核函数来定义边的权重，常用的如多项式核函数，高斯核函数， Sigmod核函数。 最常用的为高斯核函数RBF，将两点之间的距离带入高斯核函数RBF中，即： $$ w_{i j}=w_{ji}=s_{i j}=s_{ji}=\\exp \\left(-\\frac{\\left|x_{i}-x_{j}\\right|_{2}^{2}}{2 \\sigma^{2}}\\right) $$ 其中，$sigma$为为函数的宽度参数 , 控制了函数的径向作用范围。\n基础3：拉普拉斯矩阵 拉普拉斯矩阵定义为$L = D-W$ 如上文所示，$D$为度矩阵，$W$为邻接矩阵。\n拉普拉斯矩阵具有如下性质：\n  $L$是对称阵 （因为$D$和$W$都是对称阵）\n  $L$的所有特征值都是实数 （因为$L$是对称阵）\n  对于任意向量$f$， 有$f^TLf = \\displaystyle \\frac{1}{2} \\sum_{i = 1}^n \\sum_{j = 1}^n w_{ij} (f_i-f_j)^2$\n推导： $$ \\begin{aligned} f^TLf \u0026= f^TDf-f^TWf\\\\ \u0026= \\sum^n_{i = 1}d_if_i^2 - \\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij}\\\\ \u0026= \\frac{1}{2}\\left(\\sum^n_{i=1}d_if_i^2 -2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum^n_{i=1}d_if_i^2\\right)\\\\ \u0026由于d_i = \\sum_{j = 1}^nw_{ij}, 将d_i带入上式得\\\\ f^TLf \u0026= \\frac{1}{2} \\left(\\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2-2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2\\right)\\\\ \u0026 = \\frac{1}{2} \\left(\\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2-2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum_{j = 1}^n\\sum_{i =1}^n w_{ji}f_j^2\\right)\\\\ \u0026= \\frac{1}{2}\\left(\\sum_{i=1}^n\\sum_{j = 1}^n w_{ij} (f_i-f_j)^2\\right) \\end{aligned} $$\n  拉普帕斯矩阵是半正定的，且$n$个实数特征值都$\\geq$，即 $0=\\lambda_1 \\leq \\lambda_2 \\cdots \\leq \\lambda_n$，且最小的特征值为0。\n证明，因为$f^TLf \\geq 0$ 所以$L$半正定。\n  基础4：无向图切图 对于无向图$G$，目的是将图$G = (v,E)$切成相互没有连接的k个子图，每个子图是一个节点集合：$A_1,A_2,\\cdots, A_k$，满足$A_i \\cap A_j = \\phi$ 且$A_1 \\cup A_2 \\cup \\cdots \\cup A_k = V$，对于两个节点集合$A ,B \\subset V$, $A \\cap B = \\phi$，定义$A$,$B$之间的切图权重为： $$ W(A,B) = \\sum_{v_i\\in A, v_j \\in B} w_{ij} \\quad 表示A中节点到B中节点的权重和 $$ 对于$k$个子图节点集合$A_1,A_2,\\cdots, A_k$，定义切图$Cut$为： $$ Cut(A_1,A_2, \\cdots, A_k) = \\frac{1}{2}\\sum^k_{i = 1} W(A_i,\\overline{A_i}) $$ 其中$\\overline{A_i}$是$A_i$的补集，也就是除$A_i$外其他所有节点的集合，$Cut$计算的是$A_i$中每个节点到$\\overline{A_i}$中每个节点的权重总和，如果最小化$Cut$，就相当于最小化每个子集中节点到自己外节点的权重，但是会存在一个问题，如下图所示：\n如果按左边那条线分割，可以保证有边子图到左边子图的权重最小，但不是最佳分割。\n谱聚类：切图聚类 为了避免上述效果不佳的情况，提供两种切图方式，1.RatioCut, 2.Ncut.\nRatioCut 切图 最小化$Cut(A_1,A_2, \\cdots, A_k)$的同时，最大化每个子图中接待你的个数，即：A_{i}, \\overline{A}_{i}\n$$RatioCut\\left(A_{1}, A_{2}, \\ldots A_{k}\\right)=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\overline{A_i}\\right)}{\\left|A_{i}\\right|}$$\n目标是最小化$RatioCut\\left(A_{1}, A_{2}, \\ldots A_{k}\\right)$。\n为此，我们引入一个指示向量（indicator vector）$h_j \\in {h_1,h_2,\\cdots, h_k}$，其中$j = 1,2,\\cdots,k$，对于其中任意一个向量$h_j$，它是一个$n$维向量，即： $$ h_j = (h_{1j},h_{2j}, \\cdots, h_{nj})^T \\\\ h_{i j}=\\left\\{\\begin{array}{ll}{0} \u0026 {v_{i} \\notin A_{j}} \\\\ {\\frac{1}{\\sqrt{\\left|A_{j}\\right|}}} \u0026 {v_{i} \\in A_{j}}\\end{array}\\right. $$ $h_ij$表示节点$v_i$ 是否属于子图$A_j$, 如果属于，那么$h_{ij} = \\frac{1}{\\sqrt{\\left|A_{j}\\right|}}$，如果不属于，那么$h_{ij} = 0$。\n那么对于$h_i^TLh_i$有： $$ \\begin{aligned} h_i^T L h_i \u0026= \\frac{1}{2}\\sum_{m=1}\\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\\\ \u0026= \\frac{1}{2}\\left(\\sum_{m\\in A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2+\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\\\ \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2 + \\sum_{m\\notin A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2\\right)\\\\ \u0026 上式中的mn是图中任意选取的两个不同的节点， 针对子图A_i及其对应的指示向量h_i,\\\\ \u0026任意选取的节点对有四种情况。其中 根据h_{ij}的定义，第一项和第四项为0，所以\\\\ \u0026=\\frac{1}{2} \\left(\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2\\right) \\\\ \u0026=\\frac{1}{2} \\left(\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(\\frac{1}{\\sqrt{\\left|A_{i}\\right|}})^2 + \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(-\\frac{1}{\\sqrt{\\left|A_{i}\\right|}})^2\\right) \\\\ \u0026=\\frac{1}{2}\\left(\\frac{1}{|A_i|}Cut(A_i,\\overline{A_i}) + \\frac{1}{|A_i|}Cut(A_i,\\overline{A_i})\\right)\\\\ \u0026=\\frac{Cut(A_i,\\overline{A_i})}{|A_i|} = RatioCut(A_i) \\end{aligned} $$ 上式$h_i^TLh_i$可以看做是子图$A_i$的RatioCut，那么： $$ \\begin{aligned} RatioCut(A_1,A_2,\\cdots,A_k) \u0026=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\overline{A_i}\\right)}{\\left|A_{i}\\right|} = \\sum_{i = 1}^k \\frac{Cut(A_i,\\overline{A_i})}{|A_i|} \\\\ \u0026= \\sum_{i=1}^k h_i^TLh^i = \\sum_{i=1}^k (H^TLH)_{ii} = tr(H^TLH) \\end{aligned} $$ 每个$h_i^TLh^j$的值对应于矩阵$H^TLH$在位置$ij$处的值： $$ H=(h_1,h_2,\\cdots,h_k) \\in \\mathbb{R}^{n\\times k} $$ $$ h_i^TLh_j = (H^TLH)_{ij} \\to h^T_iLh_i = (H^TLH)_{ii} $$\n由于$h_i\\cdot h_j = 0, h_i \\cdot h_i = 1$, 所以$H^TH = I$是一个单位矩阵\n所以切图优化函数为： $$ \\underbrace{\\arg \\min }_{H} RatioCut\\left(A_1,A_2,\\cdots A_k\\right) = \\underbrace{\\arg \\min }_{H} \\operatorname{tr}\\left(H^{T} L H\\right) \\quad \\text { s.t. } \\quad H^{T} H=I $$ $H$中的每个指示向量$h$是$n$维的，每个向量中的元素有两种取值，分别是0和$\\frac{1}{\\sqrt{\\left|A_{j}\\right|}}$， 所以每个$h$有$2^n$种可能性，所以整个$H$有$k2^n$中，因此上述目标函数是个NP-hard问题。\n注意到$tr(H^TLH)$中每个优化子目标$h_i^TLh_i$，其中，$h$是单位正交基，基每个元素的平方和等于1，$L$为对称矩阵，此时$h^T_iLh_i$的最大值为$L$的最大特征值， $h^T_iLh_i$的最小值是$L$的最小特征值。在谱聚类中，我们的目标是找到目标函数的最小特征值，从而使目标函数最小，得到对应的特征向量。\n对于$h^T_iLh_i$，目标是找到$L$最小的特征值，这个值就是$h^T_iLh_i$的最小值，对于$tr(H^TLH) = \\sum^k_{i=1} h^T_iLh_i$，目标是找到$k$个最小的特征值，从而使$tr(H^TLH)$最小。\n通过找到$L$最小的$k$个特征值，可以对应得到$k$个特征向量，这$k$个特征向量可以组成一个$n \\times k$的矩阵，这个矩阵就是我们需要的指示向量矩阵$H$，里面包含了每个节点所属子图的信息。一般来说 我们需要对$H$按行做标准化： $$ h_{ij}^* = \\frac{h_{ij}}{\\sqrt{\\sum_{t=1}^kh^2_{it}}} $$ 注意到，$H$的每行是一个$k$维行向量，即$H_i = (H_{i1},H_{i2},\\cdots, H_{ik})$， 表示节点$i$属于每个子图的指标值， 我们可以把$H_i$当做节点$v_i$的表示向量， 由于归一化后的$H$还不能明确指示各个样本的归属， 我们还需要对$H$代表的所有节点做一次传统聚类，如K-Means。\nNCut切图 把$RatioCut$的分母从$|A_i|$换成$vol(A_i) = \\sum_{j \\in A_i}d_j$， 为$A_i$中所有节点的权重之和，一般来说$NCut$效果好于$RatioCut$: $$ NCut(A_1,A_2,\\cdots,A_k) = \\frac{1}{2}\\sum_{i=1}^k\\frac{W(A_i,\\overline{A_i})}{vol(A_i)} = \\sum^k_{i = 1}\\frac{Cut(A_i)}{vol(A_i)} $$ $NCut$指示向量$h$做了改进，$RatioCut$切图在指示向量中使用$\\frac{1}{\\sqrt{|A_i|}}$表示某节点归属于子图$A_i$，而$NCut$切图使用子图权重$\\frac{1}{\\sqrt{vol{A_i}}}$来表示某节点归属子图$A_i$如下： $$ h_{i j}=\\left\\{\\begin{array}{ll}{0} \u0026 {v_{i} \\notin A_{j}} \\\\ {\\frac{1}{\\sqrt{v o l\\left(A_{j}\\right)}}} \u0026 {v_{i} \\in A_{j}}\\end{array}\\right. $$ 上式表示如果节点$v_i$在子图$A_j$中，那么指示向量$h_j$的第$i$个元素为$\\frac{1}{\\sqrt{vol{A_j}}}$。\n那么对于$h_i^TLh_i$有： $$ h^T_iLh_i = \\frac{1}{2}\\sum_{m=1}\\sum_{n=1}w_{mn}(h_{im}-h_{in})^2 = \\frac{Cut(A_i)}{vol(A_i)} =NCut(A_i) $$ 目标函数： $$ NCut(A_i,A_2,\\cdots,A_k) = \\sum^k_{i = 1} NCut(A_i) = \\sum^k_{i=1}h^T_iLh_i =\\sum^k_{i=1}(H^TLH)_{ii} = tr(H^TLH) $$ 此时，$h_i \\cdot h_j = 0$，$h_i\\cdot h_i = \\frac{|A_i|}{vol(A_i)} \\neq 1$， 所以$H^TH \\neq I$。\n但是， 由于：$h^T_iDh_i = \\sum^n_{j = 1} h_{ij}^2d_j$, $d_j$为节点$v_j$的权重和，$h_{ij}$的值表示节点j是否在子图$A_i$中，如果在子图$A_i$中，那么$h_{ij}^2 = \\frac{1}{vol(A_i)}$，否则为0。\n$$h^T_iDh_i = \\frac{1}{vol(A_i)} \\sum_{v_j \\in A_i} d_j = \\frac{1}{vol(A_i)} \\cdot vol(A_i) = 1$$\n最终目标函数为： $$ \\underbrace{\\arg \\min } _{H}\\operatorname{tr}\\left(H^{T} L H\\right) \\quad \\text { s.t. }\\quad H^{T} D H=I $$ 由于$H$中的指示向量$h$不是标准正交基，所以RatioCut中加粗的定理不能直接使用，所以，令$H = D^{-\\frac{1}{2}}F$, $D^{-\\frac{1}{2}}$表示对$D$对角线上元素开方后求逆，那么： $$ H^TLH = F^TD^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}F $$\n$$ H^TDH = F^TD^{-\\frac{1}{2}}DD^{-\\frac{1}{2}}F = F^TF=I $$ 所以目标函数转化为： $$ \\underbrace{\\arg \\min }_{F} \\operatorname{tr}\\left(F^{T} D^{-1 / 2} L D^{-1 / 2} F\\right) \\quad \\text { s.t. } \\quad F^{T} F=I $$ 同$RatioCut$，$D^{-1 / 2} L D^{-1 / 2}$是对称矩阵，$F$中的每个向量为标准正交基，只需要求出$D^{-1 / 2} L D^{-1 / 2}$的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，最后得到特征矩阵$F$，然后对$F$的行向量做一次传统聚类，如K-means.\n一般来说， $D^{-1 / 2} L D^{-1 / 2}$相当于对拉普拉斯矩阵$L$做了一次标准化，即$\\frac{L_{i j}}{\\sqrt{d_{i} * d_{j}}}$.\n我把本文整理成了PDF\n","wordCount":"764","inLanguage":"en","datePublished":"2019-09-07T09:11:09Z","dateModified":"2019-09-07T09:11:09Z","author":{"@type":"Person","name":"JhuoW"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/"},"publisher":{"@type":"Organization","name":"JhuoW‘s Notes","logo":{"@type":"ImageObject","url":"https://JhuoW.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="Jhuo (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>Jhuo</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://JhuoW.github.io/gallery/ title=Gallery><span>Gallery</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://JhuoW.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://JhuoW.github.io/posts/>Posts</a></div><h1 class=post-title>Notes for Spectral Clustering</h1><div class=post-meta><span title="2019-09-07 09:11:09 +0000 UTC">September 7, 2019</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;JhuoW</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#%e5%9f%ba%e7%a1%801-%e6%97%a0%e5%90%91%e6%9d%83%e9%87%8d%e5%9b%be aria-label="基础1： 无向权重图">基础1： 无向权重图</a></li><li><a href=#%e5%9f%ba%e7%a1%802%e7%9b%b8%e4%bc%bc%e7%9f%a9%e9%98%b5 aria-label=基础2：相似矩阵>基础2：相似矩阵</a><ul><li><a href=#epsilon-%e9%82%bb%e8%bf%91%e6%b3%95 aria-label=$\epsilon$-邻近法>$\epsilon$-邻近法</a></li><li><a href=#k%e9%82%bb%e8%bf%91%e6%b3%95 aria-label=K邻近法>K邻近法</a></li><li><a href=#%e5%85%a8%e8%bf%9e%e6%8e%a5%e6%b3%95 aria-label=全连接法>全连接法</a></li></ul></li><li><a href=#%e5%9f%ba%e7%a1%803%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e7%9f%a9%e9%98%b5 aria-label=基础3：拉普拉斯矩阵>基础3：拉普拉斯矩阵</a></li><li><a href=#%e5%9f%ba%e7%a1%804%e6%97%a0%e5%90%91%e5%9b%be%e5%88%87%e5%9b%be aria-label=基础4：无向图切图>基础4：无向图切图</a></li><li><a href=#%e8%b0%b1%e8%81%9a%e7%b1%bb%e5%88%87%e5%9b%be%e8%81%9a%e7%b1%bb aria-label=谱聚类：切图聚类>谱聚类：切图聚类</a><ul><li><a href=#ratiocut-%e5%88%87%e5%9b%be aria-label="RatioCut 切图">RatioCut 切图</a></li><li><a href=#ncut%e5%88%87%e5%9b%be aria-label=NCut切图>NCut切图</a></li></ul></li></ul></div></details></div><div class=post-content><p>最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。</p><p>本文主要参考了：[1] <a href=https://www.cnblogs.com/pinard/p/6221564.html#!comments>https://www.cnblogs.com/pinard/p/6221564.html#!comments</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。</p><h1 id=基础1-无向权重图>基础1： 无向权重图<a hidden class=anchor aria-hidden=true href=#基础1-无向权重图>#</a></h1><p>对于边$(v_i,v_j)$, 它的权重$w_{ij} > 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即：
$$
d_i = \sum_{j=1}^n w_{ij}
$$
根据所有节点的度值，我们可以得到一个度矩阵$D$:
$$
D=\displaystyle \left(\begin{array}{ccc}{d_{1}} & {\ldots} & {\ldots} \\\ {\ldots} & {d_{2}} & {\ldots} \\\ {\vdots} & {\vdots} & {\ddots} \\\ {\ldots} & {\ldots} & {d_{n}}\end{array}\right) ^{n\times n}
$$
是一个$n \times n$的对角阵，对角元素是每个节点的度和。</p><p>定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义：
$$|A|=A 中的节点个数 $$</p><p>$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$</p><h1 id=基础2相似矩阵>基础2：相似矩阵<a hidden class=anchor aria-hidden=true href=#基础2相似矩阵>#</a></h1><p>再提下谱聚类的基本思想： 距离较远的两个点之间权重值较低，距离较近的两个点之间权重值较高</p><p>但是在谱聚类中，我们只能获得数据点的定义，无法给出邻接矩阵，因为是离散的分布在空间中，无法得知其中的连接关系。</p><p>一般来说，通过样本点距离度量的相似矩阵$S$来获得邻接矩阵$W$.</p><p>构建邻接矩阵$W$有两种方法: $\epsilon$-邻近法， K邻近法和全连接法。</p><h2 id=epsilon-邻近法>$\epsilon$-邻近法<a hidden class=anchor aria-hidden=true href=#epsilon-邻近法>#</a></h2><p>$\epsilon$为距离的阈值，欧式距离$S_{ij}$表示两点$v_i$,$v_j$的坐标$x_i$,$x_j$之间的距离。 即 $S_{ij} =||x_i-x_j||^2_2$为相似矩阵$S \in \mathbb{R}^{n \times n}$的第$i$行第$j$个元素，则邻接矩阵$W$可以表示为：</p><p>$$
w_{ij}=\left\{\begin{array}{ll}{0} & {s_{i j}>\epsilon} \\ {\epsilon} & {s_{i j} \leq \epsilon}\end{array}\right.
$$</p><p>意思是如果两点之间的距离大于$\epsilon$，那么他们之间的权重为0， 如果他们之间的距离小于$\epsilon$，他们之间的权重为$\epsilon$。 这种方法的弊端在于点之间的距离只有两种情况，不够精确，故很少采用。</p><h2 id=k邻近法>K邻近法<a hidden class=anchor aria-hidden=true href=#k邻近法>#</a></h2><p>利用<strong>KNN</strong>算法遍历所有样本点，取每个样本点最近的$k$个点作为近邻，只有和样本点距离最近的$k$个点的$w_{ij} >0$，但会出现一种情况， $v_i$的k个近邻中有$v_j$，但$v_j$的k个近邻中没有$v_i$，这样会造成邻接矩阵$W$的不对称， 因此提供两种解决办法</p><p>第一种： 只要$v_j$在$v_i$的K邻域中，那么不管$v_i$在不在$v_j$的K邻域中，都把$v_i$加入$v_j$的邻域中，即：</p><p>$$
w_{i j}=w_{j i}=\left\{\begin{array}{ll}{0} & {x_{i} \notin K N N\left(x_{j}\right) \text { and } x_{j} \notin K N N\left(x_{i}\right)} \\ {\exp \left(-\frac{\left||x_{i}-x_{j}\right||^2_2}{2 \sigma^{2}}\right)} & {x_{i} \in K N N\left(x_{j}\right) \text { or } x_{j} \in K N N\left(x_{i}\right)}\end{array}\right.
$$
第二种，必须$v_i$在$v_j$的K邻域中，且$v_j$在$v_i$的K邻域中，那么才保留两者间的权重，否则都为0，即：</p><p>$$
w_{i j}=w_{j i}=\left\{\begin{array}{ll}{0} & {x_{i} \notin K N N\left(x_{j}\right) \text { or } x_{j} \notin K N N\left(x_{i}\right)} \\ {\exp \left(-\frac{||x_{i}-x_{j}||^2_2}{2 \sigma^{2}}\right)} & {x_{i} \in K N N\left(x_{j}\right) \text { and } x_{j} \in K N N\left(x_{i}\right)}\end{array}\right.
$$</p><h2 id=全连接法>全连接法<a hidden class=anchor aria-hidden=true href=#全连接法>#</a></h2><p>设所有点之间的权重都大于0，可以选择不同的核函数来定义边的权重，常用的如多项式核函数，高斯核函数， Sigmod核函数。 最常用的为高斯核函数RBF，将两点之间的距离带入高斯核函数RBF中，即：
$$
w_{i j}=w_{ji}=s_{i j}=s_{ji}=\exp \left(-\frac{\left|x_{i}-x_{j}\right|_{2}^{2}}{2 \sigma^{2}}\right)
$$
其中，$sigma$为为函数的宽度参数 , 控制了函数的径向作用范围。</p><h1 id=基础3拉普拉斯矩阵>基础3：拉普拉斯矩阵<a hidden class=anchor aria-hidden=true href=#基础3拉普拉斯矩阵>#</a></h1><p>拉普拉斯矩阵定义为$L = D-W$ 如上文所示，$D$为度矩阵，$W$为邻接矩阵。</p><p>拉普拉斯矩阵具有如下性质：</p><ol><li><p>$L$是对称阵 （因为$D$和$W$都是对称阵）</p></li><li><p>$L$的所有特征值都是实数 （因为$L$是对称阵）</p></li><li><p>对于任意向量$f$， 有$f^TLf = \displaystyle \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n w_{ij} (f_i-f_j)^2$</p><p>推导：
$$
\begin{aligned}
f^TLf &= f^TDf-f^TWf\\
&= \sum^n_{i = 1}d_if_i^2 - \sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij}\\
&= \frac{1}{2}\left(\sum^n_{i=1}d_if_i^2 -2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum^n_{i=1}d_if_i^2\right)\\
&由于d_i = \sum_{j = 1}^nw_{ij}, 将d_i带入上式得\\
f^TLf &= \frac{1}{2} \left(\sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2-2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2\right)\\
& = \frac{1}{2} \left(\sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2-2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum_{j = 1}^n\sum_{i =1}^n w_{ji}f_j^2\right)\\
&= \frac{1}{2}\left(\sum_{i=1}^n\sum_{j = 1}^n w_{ij} (f_i-f_j)^2\right)
\end{aligned}
$$</p></li><li><p>拉普帕斯矩阵是半正定的，且$n$个实数特征值都$\geq$，即 $0=\lambda_1 \leq \lambda_2 \cdots \leq \lambda_n$，且最小的特征值为0。</p><p>证明，因为$f^TLf \geq 0$ 所以$L$半正定。</p></li></ol><h1 id=基础4无向图切图>基础4：无向图切图<a hidden class=anchor aria-hidden=true href=#基础4无向图切图>#</a></h1><p>对于无向图$G$，目的是将图$G = (v,E)$切成相互没有连接的k个子图，每个子图是一个节点集合：$A_1,A_2,\cdots, A_k$，满足$A_i \cap A_j = \phi$ 且$A_1 \cup A_2 \cup \cdots \cup A_k = V$，对于两个节点集合$A ,B \subset V$, $A \cap B = \phi$，定义$A$,$B$之间的切图权重为：
$$
W(A,B) = \sum_{v_i\in A, v_j \in B} w_{ij} \quad 表示A中节点到B中节点的权重和
$$
对于$k$个子图节点集合$A_1,A_2,\cdots, A_k$，定义切图$Cut$为：
$$
Cut(A_1,A_2, \cdots, A_k) = \frac{1}{2}\sum^k_{i = 1} W(A_i,\overline{A_i})
$$
其中$\overline{A_i}$是$A_i$的补集，也就是除$A_i$外其他所有节点的集合，$Cut$计算的是$A_i$中每个节点到$\overline{A_i}$中每个节点的权重总和，如果最小化$Cut$，就相当于最小化每个子集中节点到自己外节点的权重，但是会存在一个问题，如下图所示：</p><p><img loading=lazy src=1.jpg alt></p><p>如果按左边那条线分割，可以保证有边子图到左边子图的权重最小，但不是最佳分割。</p><h1 id=谱聚类切图聚类>谱聚类：切图聚类<a hidden class=anchor aria-hidden=true href=#谱聚类切图聚类>#</a></h1><p>为了避免上述效果不佳的情况，提供两种切图方式，1.RatioCut, 2.Ncut.</p><h2 id=ratiocut-切图>RatioCut 切图<a hidden class=anchor aria-hidden=true href=#ratiocut-切图>#</a></h2><p>最小化$Cut(A_1,A_2, \cdots, A_k)$的同时，最大化每个子图中接待你的个数，即：A_{i}, \overline{A}_{i}</p><p>$$RatioCut\left(A_{1}, A_{2}, \ldots A_{k}\right)=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \overline{A_i}\right)}{\left|A_{i}\right|}$$</p><p>目标是最小化$RatioCut\left(A_{1}, A_{2}, \ldots A_{k}\right)$。</p><p>为此，我们引入一个<strong>指示向量（indicator vector）</strong>$h_j \in {h_1,h_2,\cdots, h_k}$，其中$j = 1,2,\cdots,k$，对于其中任意一个向量$h_j$，它是一个$n$维向量，即：
$$
h_j = (h_{1j},h_{2j}, \cdots, h_{nj})^T \\
h_{i j}=\left\{\begin{array}{ll}{0} & {v_{i} \notin A_{j}} \\ {\frac{1}{\sqrt{\left|A_{j}\right|}}} & {v_{i} \in A_{j}}\end{array}\right.
$$
$h_ij$表示节点$v_i$ 是否属于子图$A_j$, 如果属于，那么$h_{ij} = \frac{1}{\sqrt{\left|A_{j}\right|}}$，如果不属于，那么$h_{ij} = 0$。</p><p>那么对于$h_i^TLh_i$有：
$$
\begin{aligned}
h_i^T L h_i &= \frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\
&= \frac{1}{2}\left(\sum_{m\in A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2+\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2 + \sum_{m\notin A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2\right)\\
& 上式中的mn是图中任意选取的两个不同的节点， 针对子图A_i及其对应的指示向量h_i,\\
&任意选取的节点对有四种情况。其中 根据h_{ij}的定义，第一项和第四项为0，所以\\
&=\frac{1}{2} \left(\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2 +
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2\right) \\
&=\frac{1}{2} \left(\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(\frac{1}{\sqrt{\left|A_{i}\right|}})^2 +
\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(-\frac{1}{\sqrt{\left|A_{i}\right|}})^2\right) \\
&=\frac{1}{2}\left(\frac{1}{|A_i|}Cut(A_i,\overline{A_i}) + \frac{1}{|A_i|}Cut(A_i,\overline{A_i})\right)\\
&=\frac{Cut(A_i,\overline{A_i})}{|A_i|} = RatioCut(A_i)
\end{aligned}
$$
上式$h_i^TLh_i$可以看做是子图$A_i$的RatioCut，那么：
$$
\begin{aligned}
RatioCut(A_1,A_2,\cdots,A_k) &=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \overline{A_i}\right)}{\left|A_{i}\right|} = \sum_{i = 1}^k \frac{Cut(A_i,\overline{A_i})}{|A_i|} \\
&= \sum_{i=1}^k h_i^TLh^i = \sum_{i=1}^k (H^TLH)_{ii} = tr(H^TLH)
\end{aligned}
$$
每个$h_i^TLh^j$的值对应于矩阵$H^TLH$在位置$ij$处的值：
$$
H=(h_1,h_2,\cdots,h_k) \in \mathbb{R}^{n\times k}
$$
$$
h_i^TLh_j = (H^TLH)_{ij} \to h^T_iLh_i = (H^TLH)_{ii}
$$</p><p>由于$h_i\cdot h_j = 0, h_i \cdot h_i = 1$, 所以$H^TH = I$是一个单位矩阵</p><p>所以切图优化函数为：
$$
\underbrace{\arg \min }_{H} RatioCut\left(A_1,A_2,\cdots A_k\right) = \underbrace{\arg \min }_{H} \operatorname{tr}\left(H^{T} L H\right) \quad \text { s.t. } \quad H^{T} H=I
$$
$H$中的每个指示向量$h$是$n$维的，每个向量中的元素有两种取值，分别是0和$\frac{1}{\sqrt{\left|A_{j}\right|}}$， 所以每个$h$有$2^n$种可能性，所以整个$H$有$k2^n$中，因此上述目标函数是个NP-hard问题。</p><p>注意到$tr(H^TLH)$中每个优化子目标$h_i^TLh_i$，其中，$h$是单位正交基，基每个元素的平方和等于1，$L$为对称矩阵，<strong>此时$h^T_iLh_i$的最大值为$L$的最大特征值， $h^T_iLh_i$的最小值是$L$的最小特征值</strong>。在谱聚类中，我们的目标是找到目标函数的最小特征值，从而使目标函数最小，得到对应的特征向量。</p><p>对于$h^T_iLh_i$，目标是找到$L$最小的特征值，这个值就是$h^T_iLh_i$的最小值，对于$tr(H^TLH) = \sum^k_{i=1} h^T_iLh_i$，目标是找到$k$个最小的特征值，从而使$tr(H^TLH)$最小。</p><p>通过找到$L$最小的$k$个特征值，可以对应得到$k$个特征向量，这$k$个特征向量可以组成一个$n \times k$的矩阵，这个矩阵就是我们需要的指示向量矩阵$H$，里面包含了每个节点所属子图的信息。一般来说 我们需要对$H$按行做标准化：
$$
h_{ij}^* = \frac{h_{ij}}{\sqrt{\sum_{t=1}^kh^2_{it}}}
$$
注意到，$H$的每行是一个$k$维行向量，即$H_i = (H_{i1},H_{i2},\cdots, H_{ik})$， 表示节点$i$属于每个子图的指标值， 我们可以把$H_i$当做节点$v_i$的表示向量， 由于归一化后的$H$还不能明确指示各个样本的归属， 我们还需要对$H$代表的所有节点做一次传统聚类，如K-Means。</p><h2 id=ncut切图>NCut切图<a hidden class=anchor aria-hidden=true href=#ncut切图>#</a></h2><p>把$RatioCut$的分母从$|A_i|$换成$vol(A_i) = \sum_{j \in A_i}d_j$， 为$A_i$中所有节点的权重之和，一般来说$NCut$效果好于$RatioCut$:
$$
NCut(A_1,A_2,\cdots,A_k) = \frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\overline{A_i})}{vol(A_i)} = \sum^k_{i = 1}\frac{Cut(A_i)}{vol(A_i)}
$$
$NCut$指示向量$h$做了改进，$RatioCut$切图在指示向量中使用$\frac{1}{\sqrt{|A_i|}}$表示某节点归属于子图$A_i$，而$NCut$切图使用子图权重$\frac{1}{\sqrt{vol{A_i}}}$来表示某节点归属子图$A_i$如下：
$$
h_{i j}=\left\{\begin{array}{ll}{0} & {v_{i} \notin A_{j}} \\ {\frac{1}{\sqrt{v o l\left(A_{j}\right)}}} & {v_{i} \in A_{j}}\end{array}\right.
$$
上式表示如果节点$v_i$在子图$A_j$中，那么指示向量$h_j$的第$i$个元素为$\frac{1}{\sqrt{vol{A_j}}}$。</p><p>那么对于$h_i^TLh_i$有：
$$
h^T_iLh_i = \frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2 = \frac{Cut(A_i)}{vol(A_i)} =NCut(A_i)
$$
目标函数：
$$
NCut(A_i,A_2,\cdots,A_k) = \sum^k_{i = 1} NCut(A_i) = \sum^k_{i=1}h^T_iLh_i =\sum^k_{i=1}(H^TLH)_{ii} = tr(H^TLH)
$$
此时，$h_i \cdot h_j = 0$，$h_i\cdot h_i = \frac{|A_i|}{vol(A_i)} \neq 1$， 所以$H^TH \neq I$。</p><p>但是， 由于：$h^T_iDh_i = \sum^n_{j = 1} h_{ij}^2d_j$, $d_j$为节点$v_j$的权重和，$h_{ij}$的值表示节点j是否在子图$A_i$中，如果在子图$A_i$中，那么$h_{ij}^2 = \frac{1}{vol(A_i)}$，否则为0。</p><p>$$h^T_iDh_i = \frac{1}{vol(A_i)} \sum_{v_j \in A_i} d_j = \frac{1}{vol(A_i)} \cdot vol(A_i) = 1$$</p><p>最终目标函数为：
$$
\underbrace{\arg \min } _{H}\operatorname{tr}\left(H^{T} L H\right) \quad \text { s.t. }\quad H^{T} D H=I
$$
由于$H$中的指示向量$h$不是标准正交基，所以RatioCut中加粗的定理不能直接使用，所以，令$H = D^{-\frac{1}{2}}F$, $D^{-\frac{1}{2}}$表示对$D$对角线上元素开方后求逆，那么：
$$
H^TLH = F^TD^{-\frac{1}{2}}LD^{-\frac{1}{2}}F
$$</p><p>$$
H^TDH = F^TD^{-\frac{1}{2}}DD^{-\frac{1}{2}}F = F^TF=I
$$
所以目标函数转化为：
$$
\underbrace{\arg \min }_{F} \operatorname{tr}\left(F^{T} D^{-1 / 2} L D^{-1 / 2} F\right) \quad \text { s.t. } \quad F^{T} F=I
$$
同$RatioCut$，$D^{-1 / 2} L D^{-1 / 2}$是对称矩阵，$F$中的每个向量为标准正交基，只需要求出$D^{-1 / 2} L D^{-1 / 2}$的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，最后得到特征矩阵$F$，然后对$F$的行向量做一次传统聚类，如K-means.</p><p>一般来说， $D^{-1 / 2} L D^{-1 / 2}$相当于对拉普拉斯矩阵$L$做了一次标准化，即$\frac{L_{i j}}{\sqrt{d_{i} * d_{j}}}$.</p><p>我把本文整理成了<a href=/posts/2019-09-07-spectral-clustering/Spectral-Cluster.pdf>PDF</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://JhuoW.github.io/tags/clustering/>Clustering</a></li><li><a href=https://JhuoW.github.io/tags/spectral-clustering/>Spectral Clustering</a></li><li><a href=https://JhuoW.github.io/tags/algorithm/>algorithm</a></li></ul><nav class=paginav><a class=prev href=https://JhuoW.github.io/posts/diffpool/><span class=title>« Prev Page</span><br><span>NIPS2018 《DiffPool： Hierarchical Graph Representation Learning with Differentiable Pooling》 Reading Notes</span></a>
<a class=next href=https://JhuoW.github.io/posts/m-nmf/><span class=title>Next Page »</span><br><span>AAAI2017 M-NMF:《Community Preserving Network Embedding》 Reading Notes</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Notes for Spectral Clustering on twitter" href="https://twitter.com/intent/tweet/?text=Notes%20for%20Spectral%20Clustering&url=https%3a%2f%2fJhuoW.github.io%2fposts%2f2019-09-07-spectral-clustering%2f&hashtags=Clustering%2cSpectralClustering%2calgorithm"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes for Spectral Clustering on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fJhuoW.github.io%2fposts%2f2019-09-07-spectral-clustering%2f&title=Notes%20for%20Spectral%20Clustering&summary=Notes%20for%20Spectral%20Clustering&source=https%3a%2f%2fJhuoW.github.io%2fposts%2f2019-09-07-spectral-clustering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes for Spectral Clustering on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fJhuoW.github.io%2fposts%2f2019-09-07-spectral-clustering%2f&title=Notes%20for%20Spectral%20Clustering"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes for Spectral Clustering on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fJhuoW.github.io%2fposts%2f2019-09-07-spectral-clustering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes for Spectral Clustering on whatsapp" href="https://api.whatsapp.com/send?text=Notes%20for%20Spectral%20Clustering%20-%20https%3a%2f%2fJhuoW.github.io%2fposts%2f2019-09-07-spectral-clustering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes for Spectral Clustering on telegram" href="https://telegram.me/share/url?text=Notes%20for%20Spectral%20Clustering&url=https%3a%2f%2fJhuoW.github.io%2fposts%2f2019-09-07-spectral-clustering%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://giscus.app/client.js data-repo=JhuoW/WebComments data-repo-id=R_kgDOHHz8Ug data-category=Announcements data-category-id=DIC_kwDOHHz8Us4COa5e data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>