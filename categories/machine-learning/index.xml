<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on JhuoW‘s Notes</title>
    <link>https://JhuoW.github.io/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on JhuoW‘s Notes</description>
    <image>
      <url>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Apr 2022 15:00:20 +0800</lastBuildDate><atom:link href="https://JhuoW.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Everything about Graph Laplacian</title>
      <link>https://JhuoW.github.io/posts/laplacian/</link>
      <pubDate>Sat, 02 Apr 2022 15:00:20 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/laplacian/</guid>
      <description>Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem.</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds.  The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem. Another application is spectral matching that solves for graph matching.</p>
<h1 id="basic-notations">Basic notations</h1>
<p>We consider simple graphs (no multiple edges or loops), $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ :</p>
<ul>
<li>
<p>$\mathcal{V}(\mathcal{G})=\left\{v_{1}, \ldots, v_{n}\right\}$ is called the vertex set with $n=|\mathcal{V}|$;</p>
</li>
<li>
<p>$\mathcal{E}(\mathcal{G})=\left\{e_{i j}\right\}$ is called the edge set with $m=|\mathcal{E}|$;</p>
</li>
<li>
<p>An edge $e_{i j}$ connects vertices $v_{i}$ and $v_{j}$ if they are adjacent or neighbors. One possible notation for adjacency is $v_{i} \sim v_{j}$;</p>
</li>
<li>
<p>The number of neighbors of a node $v$ is called the degree of $v$ and is denoted by $d(v), d\left(v_{i}\right)=\sum_{v_{i} \sim v_{j}} e_{i j}$. If all the nodes of a graph have the same degree, the graph is regular; The nodes of an Eulerian graph have even degree.</p>
</li>
<li>
<p>A graph is complete if there is an edge between every pair of vertices.</p>
</li>
</ul>
<h1 id="subgraph-of-a-graph">Subgraph of a graph</h1>
<ul>
<li>
<p>$\mathcal{H}$ is a subgraph of $\mathcal{G}$ if $\mathcal{V}(\mathcal{H}) \subseteq \mathcal{V}(\mathcal{G})$ and $\mathcal{E}(\mathcal{H}) \subseteq \mathcal{E}(\mathcal{G})$;</p>
</li>
<li>
<p>a subgraph $\mathcal{H}$ is an induced subgraph of $\mathcal{G}$ if two vertices of $\mathcal{V}(\mathcal{H})$ are adjacent if and only if they are adjacent in $\mathcal{G}$.</p>
</li>
<li>
<p>A clique is a complete subgraph of a graph.</p>
</li>
<li>
<p>A path of $k$ vertices is a sequence of $k$ distinct vertices such that consecutive vertices are adjacent.</p>
</li>
<li>
<p>A cycle is a connected subgraph where every vertex has exactly two neighbors.</p>
</li>
<li>
<p>A graph containing no cycles is a forest. A connected forest is a tree.</p>
</li>
</ul>
<h1 id="a-k-partite-graph">A k-partite graph</h1>
<ul>
<li>A graph is called k-partite if its set of vertices admits a partition into $k$ classes such that the vertices of the same class are not adjacent.</li>
<li>An example of a bipartite graph.</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/1.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-adjacency-matrix-of-a-graph">The adjacency matrix of a graph</h1>
<ul>
<li>For a graph with $n$ vertices, the entries of the $n \times n$ adjacency matrix are defined by:</li>
</ul>
<p>$$
\mathbf{A}:= \begin{cases}A_{i j}=1 &amp; \text { if there is an edge } e_{i j} \\ A_{i j}=0 &amp; \text { if there is no edge } \\ A_{i i}=0 &amp; \end{cases}
$$</p>
<p>$$
\begin{aligned}
&amp; \mathbf{A}=\left[\begin{array}{llll}0 &amp; 1 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 1 &amp; 1 \\1 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 &amp; 0\end{array}\right]
\end{aligned}
$$</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="eigenvalues-and-eigenvectors">Eigenvalues and eigenvectors</h1>
<ul>
<li>
<p>A is a real-symmetric matrix: it has $n$ real eigenvalues and its $n$ real eigenvectors form an orthonormal basis.</p>
</li>
<li>
<p>Let $\left\{\lambda_{1}, \ldots, \lambda_{i}, \ldots, \lambda_{r}\right\}$ be the set of distinct eigenvalues.</p>
</li>
<li>
<p>The eigenspace $S_{i}$ contains the eigenvectors associated with $\lambda_{i}$ :</p>
</li>
</ul>
<p>$$
S_{i}=\left\{\boldsymbol{x} \in \mathbb{R}^{n} \mid \mathbf{A} \boldsymbol{x}=\lambda_{i} \boldsymbol{x}\right\}
$$</p>
<ul>
<li>
<p>For real-symmetric matrices, the algebraic multiplicity is equal to the geometric multiplicity, for all the eigenvalues.</p>
</li>
<li>
<p>The dimension of $S_{i}$ (geometric multiplicity) is equal to the multiplicity of $\lambda_{i}$.</p>
</li>
<li>
<p>If $\lambda_{i} \neq \lambda_{j}$ then $S_{i}$ and $S_{j}$ are mutually orthogonal.</p>
</li>
</ul>
<h1 id="real-valued-functions-on-graphs">Real-valued functions on graphs</h1>
<ul>
<li>
<p>We consider real-valued functions on the set of the graph&rsquo;s vertices, $\boldsymbol{f}: \mathcal{V} \longrightarrow \mathbb{R}$. Such a function assigns a real number to each graph node.</p>
</li>
<li>
<p>$\boldsymbol{f}$ is a vector indexed by the graph&rsquo;s vertices, hence $\boldsymbol{f} \in \mathbb{R}^{n}$.</p>
</li>
<li>
<p>Notation: $\boldsymbol{f}=\left(f\left(v_{1}\right), \ldots, f\left(v_{n}\right)\right)=(f(1), \ldots, f(n))$.</p>
</li>
<li>
<p>The eigenvectors of the adjacency matrix, $\mathbf{A} \boldsymbol{x}=\lambda \boldsymbol{x}$, can be viewed as eigenfunctions.</p>
</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/3.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="matrix-a-as-an-operator-and-quadratic-form">Matrix A as an operator and quadratic form</h1>
<ul>
<li>The adjacency matrix can be viewed as an operator</li>
</ul>
<p>$$
\boldsymbol{g}=\mathbf{A} \boldsymbol{f} ; g(i)=\sum_{i \sim j} f(j)
$$</p>
<ul>
<li>It can also be viewed as a quadratic form:</li>
</ul>
<p>$$
\boldsymbol{f}^{\top} \mathbf{A} \boldsymbol{f}=\sum_{e_{i j}} f(i) f(j)
$$</p>
<h1 id="the-incidence-matrix-of-a-graph">The incidence matrix of a graph</h1>
<ul>
<li>
<p>Let each edge in the graph have an arbitrary but fixed orientation;</p>
</li>
<li>
<p>The incidence matrix of a graph is a $|\mathcal{E}| \times|\mathcal{V}|(m \times n)$ matrix defined as follows:</p>
</li>
</ul>
<p>$$
\nabla:= \begin{cases}\nabla_{e v}=-1 &amp; \text { if } v \text { is the initial vertex of edge } e \\ \nabla_{e v}=1 &amp; \text { if } v \text { is the terminal vertex of edge } e \\ \nabla_{e v}=0 &amp; \text { if } v \text { is not in } e\end{cases}
$$</p>
<p>$$
\begin{aligned}
&amp; \nabla=\left[\begin{array}{cccc}-1 &amp; 1 &amp; 0 &amp; 0 \\1 &amp; 0 &amp; -1 &amp; 0 \\0 &amp; -1 &amp; 1 &amp; 0 \\0 &amp; -1 &amp; 0 &amp; +1\end{array}\right]
\end{aligned}
$$</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/2.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-incidence-matrix-a-discrete-differential-operator">The incidence matrix: A discrete differential operator</h1>
<ul>
<li>
<p>The mapping $\boldsymbol{f} \longrightarrow \nabla \boldsymbol{f}$ is known as the co-boundary mapping of the graph.</p>
</li>
<li>
<p>$(\nabla \boldsymbol{f})\left(e_{i j}\right)=f\left(v_{j}\right)-f\left(v_{i}\right)$</p>
</li>
</ul>
<p>$$
\left(\begin{array}{c}
f(2)-f(1) \\
f(1)-f(3) \\
f(3)-f(2) \\
f(4)-f(2)
\end{array}\right)=\left[\begin{array}{cccc}
-1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; -1 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; +1
\end{array}\right]\left(\begin{array}{c}
f(1) \\
f(2) \\
f(3) \\
f(4)
\end{array}\right)
$$</p>
<h1 id="the-laplacian-matrix-of-a-graph">The Laplacian matrix of a graph</h1>
<ul>
<li>
<p>$\mathbf{L}=\nabla^{\top} \nabla$</p>
</li>
<li>
<p>$(\mathbf{L} \boldsymbol{f})\left(v_{i}\right)=\sum_{v_{j} \sim v_{i}}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)$</p>
</li>
<li>
<p>Connection between the Laplacian and the adjacency matrices:</p>
</li>
</ul>
<p>$$
\mathbf{L}=\mathbf{D}-\mathbf{A}
$$</p>
<ul>
<li>The degree matrix: $\mathbf{D}:=D_{i i}=d\left(v_{i}\right)$.</li>
</ul>
<p>$$
\mathbf{L}=\left[\begin{array}{cccc}
2 &amp; -1 &amp; -1 &amp; 0 \\
-1 &amp; 3 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 2 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 1
\end{array}\right]
$$</p>
<h1 id="the-laplacian-matrix-of-an-undirected-weighted-graph">The Laplacian matrix of an undirected weighted graph</h1>
<ul>
<li>
<p>We consider undirected weighted graphs: Each edge $e_{i j}$ is weighted by $w_{i j}&gt;0$.</p>
</li>
<li>
<p>The Laplacian as an operator:</p>
</li>
</ul>
<p>$$
(\mathbf{L} \boldsymbol{f})\left(v_{i}\right)=\sum_{v_{j} \sim v_{i}} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)
$$</p>
<ul>
<li>As a quadratic form:</li>
</ul>
<p>$$
\boldsymbol{f}^{\top} \mathbf{L} \boldsymbol{f}=\frac{1}{2} \sum_{e_{i j}} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)^{2}
$$</p>
<ul>
<li>
<p>L is symmetric and positive semi-definite.</p>
</li>
<li>
<p>L has $n$ non-negative, real-valued eigenvalues: $0=\lambda_{1} \leq \lambda_{2} \leq \ldots \leq \lambda_{n} .$</p>
</li>
</ul>
<h1 id="the-laplacian-of-a-3d-discrete-surface-mesh">The Laplacian of a 3D discrete surface (mesh)</h1>
<ul>
<li>
<p>A graph vertex $v_{i}$ is associated with a 3D point $\boldsymbol{v}_{i}$.</p>
</li>
<li>
<p>The weight of an edge $e_{i j}$ is defined by the Gaussian kernel:</p>
</li>
</ul>
<p>$$
w_{i j}=\exp \left(-\left|\boldsymbol{v}_{i}-\boldsymbol{v}_{j}\right|^{2} / \sigma^{2}\right)
$$</p>
<ul>
<li>
<p>$0 \leq w_{\min } \leq w_{i j} \leq w_{\max } \leq 1$</p>
</li>
<li>
<p>Hence, the geometric structure of the mesh is encoded in the weights.</p>
</li>
<li>
<p>Other weighting functions were proposed in the literature.</p>
</li>
</ul>
<h1 id="the-laplacian-of-a-cloud-of-points">The Laplacian of a cloud of points</h1>
<ul>
<li>
<p>3-nearest neighbor graph</p>
</li>
<li>
<p>$\varepsilon$-radius graph</p>
</li>
<li>
<p>KNN may guarantee that the graph is connected (depends on the implementation)</p>
</li>
<li>
<p>$\varepsilon$-radius does not guarantee that the graph has one connected component</p>
</li>
</ul>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/4.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="the-laplacian-of-a-graph-with-one-connected-component">The Laplacian of a graph with one connected component</h1>
<ul>
<li>
<p>$Lu =\lambda \boldsymbol{u}$.</p>
</li>
<li>
<p>$\mathbf{L} \mathbf{1}_{n}=\mathbf{0}, \lambda_{1}=0$ is the smallest eigenvalue.</p>
</li>
<li>
<p>The one vector: $\mathbf{1}_{n}=(1 \ldots 1)^{\top}$.</p>
</li>
<li>
<p>$0=\boldsymbol{u}^{\top} \mathbf{L} \boldsymbol{u}=\sum_{i, j=1}^{n} w_{i j}(u(i)-u(j))^{2}$.</p>
</li>
<li>
<p>If any two vertices are connected by a path, then $\boldsymbol{u}=(u(1), \ldots, u(n))$ needs to be constant at all vertices such that the quadratic form vanishes. Therefore, a graph with one connected component has the constant vector $\boldsymbol{u}_{1}=\mathbf{1}_{n}$ as the only eigenvector with eigenvalue 0 .</p>
</li>
</ul>
<h1 id="a-graph-with-k1-connected-components">A graph with $k&gt;1$ connected components</h1>
<ul>
<li>Each connected component has an associated Laplacian. Therefore, we can write matrix $\mathbf{L}$ as a block diagonal matrix:</li>
</ul>
<p>$$
\mathbf{L}=\left[\begin{array}{lll}
\mathbf{L}_{1} &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \mathbf{L}_{k}
\end{array}\right]
$$</p>
<ul>
<li>
<p>The spectrum of $\mathbf{L}$ is given by the union of the spectra of $\mathbf{L}_{i}$.</p>
</li>
<li>
<p>Each block corresponds to a connected component, hence each matrix $\mathbf{L}_{i}$ has an eigenvalue 0 with multiplicity 1 .</p>
</li>
<li>
<p>The spectrum of $\mathbf{L}$ is given by the union of the spectra of $\mathbf{L}_{i}$.</p>
</li>
<li>
<p>The eigenvalue $\lambda_{1}=0$ has multiplicity $k$.</p>
</li>
</ul>
<h1 id="the-eigenspace-of-lambda_10-with-multiplicity-k">The eigenspace of $\lambda_{1}=0$ with multiplicity $k$</h1>
<ul>
<li>The eigenspace corresponding to $\lambda_{1}=\ldots=\lambda_{k}=0$ is spanned by the $k$ mutually orthogonal vectors:</li>
</ul>
<p>$$
\begin{aligned}
\boldsymbol{u}_{1} &amp;=\mathbf{1}_{L_{1}} \\
&amp; \cdots \\
\boldsymbol{u}_{k} &amp;=\mathbf{1}_{L_{k}}
\end{aligned}
$$</p>
<ul>
<li>
<p>with $\mathbf{1}_{L_{i}}=(0000111110000)^{\top} \in \mathbb{R}^{n}$</p>
</li>
<li>
<p>These vectors are the indicator vectors of the graph&rsquo;s connected components.</p>
</li>
<li>
<p>Notice that $\mathbf{1}_{L_{1}}+\ldots+\mathbf{1}_{L_{k}}=\mathbf{1}_{n}$</p>
</li>
</ul>
<h1 id="the-fiedler-vector-of-the-graph-laplacian">The Fiedler vector of the graph Laplacian</h1>
<ul>
<li>
<p>The first non-null eigenvalue $\lambda_{k+1}$ is called the Fiedler value.</p>
</li>
<li>
<p>The corresponding eigenvector $\boldsymbol{u}_{k+1}$ is called the Fiedler vector.</p>
</li>
<li>
<p>The multiplicity of the Fiedler eigenvalue is always equal to $1 .$</p>
</li>
<li>
<p>The Fiedler value is the algebraic connectivity of a graph, the further from 0 , the more connected.</p>
</li>
<li>
<p>The Fidler vector has been extensively used for spectral bi-partioning</p>
</li>
<li>
<p>Theoretical results are summarized in Spielman &amp; Teng 2007: <a href="http://cs-www.cs.yale.edu/homes/spielman/">http://cs-www.cs.yale.edu/homes/spielman/</a></p>
</li>
</ul>
<h1 id="eigenvectors-of-the-laplacian-of-connected-graphs">Eigenvectors of the Laplacian of connected graphs</h1>
<ul>
<li>
<p>$\boldsymbol{u}_{1}=\mathbf{1}_{n}, \mathbf{L} \mathbf{1}_{n}=\mathbf{0}$.</p>
</li>
<li>
<p>$\boldsymbol{u}_{2}$ is the the Fiedler vector with multiplicity 1 .</p>
</li>
<li>
<p>The eigenvectors form an orthonormal basis: $\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{j}=\delta_{i j}$.</p>
</li>
<li>
<p>For any eigenvector $\boldsymbol{u}_{i}=\left(\boldsymbol{u}_{i}\left(v_{1}\right) \ldots \boldsymbol{u}_{i}\left(v_{n}\right)\right)^{\top}, 2 \leq i \leq n$ :</p>
</li>
</ul>
<p>$$
\boldsymbol{u}_{i}^{\top} \mathbf{1}_{n}=0
$$</p>
<ul>
<li>Hence the components of $\boldsymbol{u}_{i}, 2 \leq i \leq n$ satisfy:</li>
</ul>
<p>$$
\sum_{j=1}^{n} \boldsymbol{u}_{i}\left(v_{j}\right)=0
$$</p>
<ul>
<li>Each component is bounded by:</li>
</ul>
<p>$$
-1&lt;\boldsymbol{u}_{i}\left(v_{j}\right)&lt;1
$$</p>
<h1 id="laplacian-embedding-mapping-a-graph-on-a-line">Laplacian embedding: Mapping a graph on a line</h1>
<ul>
<li>Map a weighted graph onto a line such that connected nodes stay as close as possible, i.e., minimize $\sum_{i, j=1}^{n} w_{i j}\left(f\left(v_{i}\right)-f\left(v_{j}\right)\right)^{2}$, or:</li>
</ul>
<p>$$
\arg \min _{\boldsymbol{f}} \boldsymbol{f}^{\top} \mathbf{L} \boldsymbol{f} \text { with: } \boldsymbol{f}^{\top} \boldsymbol{f}=1 \text { and } \boldsymbol{f}^{\top} \mathbf{1}=0
$$</p>
<ul>
<li>
<p>The solution is the eigenvector associated with the smallest nonzero eigenvalue of the eigenvalue problem: $\mathbf{L} \boldsymbol{f}=\lambda \boldsymbol{f}$, namely the Fiedler vector $\boldsymbol{u}_{2}$.</p>
</li>
<li>
<p>For more details on this minimization see Golub &amp; Van Loan Matrix Computations, chapter 8 (The symmetric eigenvalue problem).</p>
</li>
</ul>
<p><em><strong>Example of mapping a graph on the Fiedler vector</strong></em>:</p>
<p><img loading="lazy" src="/posts/2022-04-02-laplacian/5.png#center" alt="你想输入的替代文字"  />
</p>
<h1 id="laplacian-embedding">Laplacian embedding</h1>
<ul>
<li>
<p>Embed the graph in a $k$-dimensional Euclidean space. The embedding is given by the $n \times k$ matrix $\mathbf{F}=\left[\boldsymbol{f}_{1} \boldsymbol{f}_{2} \ldots \boldsymbol{f}_{k}\right]$ where the $i$-th row of this matrix $-\boldsymbol{f}^{(i)}-$ corresponds to the Euclidean coordinates of the $i$-th graph node $v_{i}$.</p>
</li>
<li>
<p>We need to minimize:</p>
</li>
</ul>
<p>$$
\arg \min_{\boldsymbol{f}_{1} \ldots} \sum_{k}^{n} \sum_{i, j=1}^{n} w_{i j}\left|\left|\boldsymbol{f}^{(i)}-\boldsymbol{f}^{(j)}\right|\right|^{2} \text { with: } \mathbf{F}^{\top} \mathbf{F}=\mathbf{I}
$$</p>
<ul>
<li>The solution is provided by the matrix of eigenvectors corresponding to the $k$ lowest nonzero eigenvalues of the eigenvalue problem $\mathbf{L} \boldsymbol{f}=\lambda \boldsymbol{f}$.</li>
</ul>
<h1 id="spectral-embedding-using-the-unnormalized-laplacian">Spectral embedding using the unnormalized Laplacian</h1>
<ul>
<li>
<p>Compute the eigendecomposition $\mathbf{L}=\mathbf{D}-\mathbf{A}$.</p>
</li>
<li>
<p>Select the $k$ smallest non-null eigenvalues $\lambda_{2} \leq \ldots \leq \lambda_{k+1}$</p>
</li>
<li>
<p>$\lambda_{k+2}-\lambda_{k+1}=$ eigengap.</p>
</li>
<li>
<p>We obtain the $n \times k$ matrix $\mathbf{U}=\left[\boldsymbol{u}_{2} \ldots \boldsymbol{u}_{k+1}\right]$ :</p>
</li>
</ul>
<p>$$
\mathbf{U}=\left[\begin{array}{ccc}
\boldsymbol{u}_{2}\left(v_{1}\right) &amp; \ldots &amp; \boldsymbol{u}_{k+1}\left(v_{1}\right) \\
\vdots &amp; &amp; \vdots \\
\boldsymbol{u}_{2}\left(v_{n}\right) &amp; \ldots &amp; \boldsymbol{u}_{k+1}\left(v_{n}\right)
\end{array}\right]
$$</p>
<ul>
<li>
<p>$\boldsymbol{u}_{i}^{\top} \boldsymbol{u}_{j}=\delta_{i j}$ (orthonormal vectors), hence $\mathbf{U}^{\top} \mathbf{U}=\mathbf{I}_{k}$.</p>
</li>
<li>
<p>Column $i(2 \leq i \leq k+1)$ of this matrix is a mapping on the eigenvector $\boldsymbol{u}_{i}$.</p>
</li>
</ul>
<h1 id="euclidean-l-embedding-of-the-graphs-vertices">Euclidean L-embedding of the graph&rsquo;s vertices</h1>
<ul>
<li>(Euclidean) L-embedding of a graph:</li>
</ul>
<p>$$
\mathbf{X}=\boldsymbol{\Lambda}_{k}^{-\frac{1}{2}} \mathbf{U}^{\top}=\left[\begin{array}{llll}
\boldsymbol{x}_{1} &amp; \ldots &amp; \boldsymbol{x}_{j} \ldots &amp; \boldsymbol{x}_{n}
\end{array}\right]
$$</p>
<p>The coordinates of a vertex $v_{j}$ are:</p>
<p>$$
\boldsymbol{x}_{j}=\left(\begin{array}{c}
\frac{\boldsymbol{u}_{2}\left(v_{j}\right)}{\sqrt{\lambda_{2}}} \\
\vdots \\
\frac{\boldsymbol{u}_{k+1}\left(v_{j}\right)}{\sqrt{\lambda_{k+1}}}
\end{array}\right)
$$</p>
<h1 id="justification-for-choosing-the-l-embedding">Justification for choosing the L-embedding</h1>
<p>Both</p>
<ul>
<li>
<p>the commute-time distance (CTD) and</p>
</li>
<li>
<p>the principal-component analysis of a graph (graph PCA)</p>
</li>
</ul>
<p>are two important concepts; They allow to reason &ldquo;statistically&rdquo; on a graph. They are both associated with the unnormalized Laplacian matrix.</p>
<h1 id="the-commute-time-distance">The commute-time distance</h1>
<ul>
<li>
<p>The CTD is a well known quantity in Markov chains;</p>
</li>
<li>
<p>It is the average number of (weighted) edges that it takes, starting at vertex $v_{i}$, to randomly reach vertex $v_{j}$ for the first time and go back;</p>
</li>
<li>
<p>The CTD decreases as the number of connections between the two nodes increases;</p>
</li>
<li>
<p>It captures the connectivity structure of a small graph volume rather than a single path between the two vertices - such as the shortest-path geodesic distance.</p>
</li>
<li>
<p>The CTD can be computed in closed form:</p>
</li>
</ul>
<p>$$
\operatorname{CTD}^{2}\left(v_{i}, v_{j}\right)=\operatorname{vol}(\mathcal{G})\left|\left|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right|\right|^{2}
$$</p>
<h1 id="the-graph-pca">The graph PCA</h1>
<ul>
<li>The mean (remember that $\sum_{j=1}^{n} \boldsymbol{u}_{i}\left(v_{j}\right)=0$ ):</li>
</ul>
<p>$$
\overline{\boldsymbol{x}}=\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{x}_{j}=\boldsymbol{\Lambda}_{k}^{-\frac{1}{2}}\left(\begin{array}{c}
\sum_{j=1}^{n} \boldsymbol{u}_{2}\left(v_{j}\right) \\
\vdots \\
\sum_{j=1}^{n} \boldsymbol{u}_{k+1}\left(v_{j}\right)
\end{array}\right)=\left(\begin{array}{c}
0 \\
\vdots \\
0
\end{array}\right)
$$</p>
<ul>
<li>The covariance matrix:</li>
</ul>
<p>$$
\mathbf{S}=\frac{1}{n} \sum_{j=1}^{n} \boldsymbol{x}_{j} \boldsymbol{x}_{j}^{\top}=\frac{1}{n} \mathbf{X} \mathbf{X}^{\top}=\frac{1}{n} \boldsymbol{\Lambda}_{k}^{-\frac{1}{2}} \mathbf{U}^{\top} \mathbf{U} \boldsymbol{\Lambda}_{k}^{-\frac{1}{2}}=\frac{1}{n} \boldsymbol{\Lambda}_{k}^{-1}
$$</p>
<ul>
<li>The vectors $\boldsymbol{u}_{2}, \ldots, \boldsymbol{u}_{k+1}$ are the directions of maximum variance of the graph embedding, with $\lambda_{2}^{-1} \geq \ldots \geq \lambda_{k+1}^{-1}$.</li>
</ul>
<h1 id="other-laplacian-matrices">Other Laplacian matrices</h1>
<ul>
<li>The normalized graph Laplacian (symmetric and semi-definite positive):</li>
</ul>
<p>$$
\mathbf{L}_{n}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}}=\mathbf{I}-\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}
$$</p>
<ul>
<li>The transition matrix (allows an analogy with Markov chains):</li>
</ul>
<p>$$
\mathbf{L}_{t}=\mathbf{D}^{-1} \mathbf{A}
$$</p>
<ul>
<li>The random-walk graph Laplacian:</li>
</ul>
<p>$$
\mathbf{L}_{r}=\mathbf{D}^{-1} \mathbf{L}=\mathbf{I}-\mathbf{L}_{t}
$$</p>
<ul>
<li>These matrices are similar:</li>
</ul>
<p>$$
\mathbf{L}_{r}=\mathbf{D}^{-\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} \mathbf{D}^{\frac{1}{2}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L}_{n} \mathbf{D}^{\frac{1}{2}}
$$</p>
<h1 id="eigenvalues-and-eigenvectors-of-mathrml_n-and-mathrml_r">Eigenvalues and eigenvectors of $\mathrm{L}_{n}$ and $\mathrm{L}_{r}$</h1>
<ul>
<li>$\mathbf{L}_{r} \boldsymbol{w}=\lambda \boldsymbol{w} \Longleftrightarrow \mathbf{L} \boldsymbol{w}=\lambda \mathbf{D} \boldsymbol{w}$, hence:</li>
</ul>
<p>$$
\mathbf{L}_{r}: \quad \lambda_{1}=0 ; \quad \boldsymbol{w}_{1}=\mathbf{1}
$$</p>
<ul>
<li>$\mathbf{L}_{n} \boldsymbol{v}=\lambda \boldsymbol{v}$. By virtue of the similarity transformation between the two matrices:</li>
</ul>
<p>$$
\mathbf{L}_{n}: \quad \lambda_{1}=0 \quad \boldsymbol{v}_{1}=\mathbf{D}^{\frac{1}{2}} \mathbf{1}
$$</p>
<ul>
<li>More generally, the two matrices have the same eigenvalues:</li>
</ul>
<p>$$
0=\lambda_{1} \leq \ldots \leq \lambda_{i} \ldots \leq \lambda_{n}
$$</p>
<ul>
<li>Their eigenvectors are related by:</li>
</ul>
<p>$$
\boldsymbol{v}_{i}=\mathbf{D}^{\frac{1}{2}} \boldsymbol{w}_{i}, \forall i=1 \ldots n
$$</p>
<h1 id="spectral-embedding-using-the-random-walk-laplacian-mathbfl_r">Spectral embedding using the random-walk Laplacian $\mathbf{L}_{r}$</h1>
<ul>
<li>The $n \times k$ matrix contains the first $k$ eigenvectors of $\mathbf{L}_{r}$ :</li>
</ul>
<p>$$
\mathbf{W}=\left[\begin{array}{lll}
\boldsymbol{w}_{2} &amp; \ldots &amp; \boldsymbol{w}_{k+1}
\end{array}\right]
$$</p>
<ul>
<li>It is straightforward to obtain the following expressions, where $\boldsymbol{d}$ and $\mathbf{D}$ are the degree-vector and the degree-matrix:</li>
</ul>
<p>$$
\begin{gathered}
\boldsymbol{w}_{i}^{\top} \boldsymbol{d}=0, \forall i, 2 \leq i \leq n \\
\mathbf{W}^{\top} \mathbf{D W}=\mathbf{I}_{k}
\end{gathered}
$$</p>
<ul>
<li>The isometric embedding using the random-walk Laplacian:</li>
</ul>
<p>$$
\mathbf{Y}=\mathbf{W}^{\top}=\left[\begin{array}{lll}
\boldsymbol{y}_{1} &amp; \ldots &amp; \boldsymbol{y}_{n}
\end{array}\right]
$$</p>
<h1 id="the-normalized-additive-laplacian">The normalized additive Laplacian</h1>
<ul>
<li>Some authors use the following matrix:</li>
</ul>
<p>$$
\mathbf{L}_{a}=\frac{1}{d_{\max }}\left(\mathbf{A}+d_{\max } \mathbf{I}-\mathbf{D}\right)
$$</p>
<ul>
<li>This matrix is closely related to L:</li>
</ul>
<p>$$
\mathbf{L}_{a}=\frac{1}{d_{\max }}\left(d_{\max } \mathbf{I}-\mathbf{L}\right)
$$</p>
<ul>
<li>and we have:</li>
</ul>
<p>$$
\mathbf{L}_{a} \boldsymbol{u}=\mu \boldsymbol{u} \Longleftrightarrow \mathbf{L} \boldsymbol{u}=\lambda \boldsymbol{u}, \mu=1-\frac{\lambda}{d_{\max }}
$$</p>
<h1 id="the-graph-partitioning-problem">The graph partitioning problem</h1>
<ul>
<li>The graph-cut problem: Partition the graph such that:</li>
</ul>
<p>(1) Edges between groups have very low weight, and</p>
<p>(2) Edges within a group have high weight.</p>
<p>$\operatorname{cut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} W\left(A_{i}, \bar{A}_{i}\right)$ with $W(A, B)=\sum_{i \in A, j \in B} w_{i j}$</p>
<ul>
<li>Ratio cut:</li>
</ul>
<p>$$
\operatorname{RatioCut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\left|A_{i}\right|}
$$</p>
<ul>
<li>Normalized cut:</li>
</ul>
<p>$$
\operatorname{NCut}\left(A_{1}, \ldots, A_{k}\right):=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\operatorname{vol}\left(A_{i}\right)}
$$</p>
<h1 id="what-is-spectral-clustering">What is spectral clustering?</h1>
<p>See my <a href="https://jhuow.fun/posts/2019-09-07-spectral-clustering/">Blog</a> of Spectral Clustering (in Chinese).</p>
<ul>
<li>
<p>Both ratio-cut and normalized-cut minimizations are NP-hard problems</p>
</li>
<li>
<p>Spectral clustering is a way to solve relaxed versions of these problems:</p>
</li>
</ul>
<p>(1) The smallest non-null eigenvectors of the unnormalized Laplacian approximate the RatioCut minimization criterion, and</p>
<p>(2) The smallest non-null eigenvectors of the random-walk Laplacian approximate the NCut criterion.</p>
<h1 id="spectral-clustering-using-the-random-walk-laplacian">Spectral clustering using the random-walk Laplacian</h1>
<ul>
<li>
<p>For details see (von Luxburg &lsquo;07)</p>
</li>
<li>
<p>Input: Laplacian $\mathbf{L}_{r}$ and the number $k$ of clusters to compute.</p>
</li>
<li>
<p>Output: Cluster $C_{1}, \ldots, C_{k}$.</p>
</li>
</ul>
<p>(3) Compute W formed with the first $k$ eigenvectors of the random-walk Laplacian.</p>
<p>(2) Determine the spectral embedding $\mathbf{Y}=\mathbf{W}^{\top}$</p>
<p>(3) Cluster the columns $\boldsymbol{y}_{j}, j=1, \ldots, n$ into $k$ clusters using the K-means algorithm.</p>
<h1 id="k-means-clustering">K-means clustering</h1>
<p>See Bishop'2006 (pages 424-428) for more details.</p>
<ul>
<li>
<p>What is a cluster: a group of points whose inter-point distance are small compared to distances to points outside the cluster.</p>
</li>
<li>
<p>Cluster centers: $\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k}$.</p>
</li>
<li>
<p>Goal: find an assignment of points to clusters as well as a set of vectors $\mu_{i}$.</p>
</li>
<li>
<p>Notations: For each point $\boldsymbol{y}_{j}$ there is a binary indicator variable $r_{j i} \in{0,1}$.</p>
</li>
<li>
<p>Objective: minimize the following distorsion measure:</p>
</li>
</ul>
<p>$$
J=\sum_{j=1}^{n} \sum_{i=1}^{k} r_{j i}\left|\left|\boldsymbol{y}_{j}-\boldsymbol{\mu}_{i}\right|\right|^{2}
$$</p>
<h1 id="the-k-means-algorithm">The K-means algorithm</h1>
<p>(1) Initialization: Choose initial values for $\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k}$.</p>
<p>(2) First step: Assign the $j$-th point to the closest cluster center:</p>
<p>$$
r_{j i}= \begin{cases}1 &amp; \text { if } i=\arg \min_{l}\left|\left|\boldsymbol{y}_{j}-\mu_{l}\right|\right|^{2} \\ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>(3) Second Step: Minimize $J$ to estimate the cluster centers:</p>
<p>$$
\boldsymbol{\mu}_{i}=\frac{\sum_{j=1}^{n} r_{j i} \boldsymbol{y}_{j}}{\sum_{j=1}^{n} r_{j i}}
$$</p>
<p>(4) Convergence: Repeat until no more change in the assignments.</p>
<h1 id="the-laplacian-and-the-rayleigh-quotient">The Laplacian and the Rayleigh quotient</h1>
<p>As usual, for a graph $G=(V, E)$, let $A$ be its adjacency matrix and $D$ be the diagonal matrix with $D(v, v)=d_{v}$. Then, the random walk on $G$ will be taken according to the transition matrix $P=D^{-1} A$. We also define the stationary distribution $\pi$ with $\pi(x)=d_{x} / \operatorname{vol} G$.</p>
<p>Our discussion of random walks on $G$ left off with the result</p>
<p>$$
\left|\left|f P^{t}-\pi\right|\right|_{2} \leq \max_{i \neq 0}\left|\rho_{i}\right|^{t} \frac{\max_{x} \sqrt{d_{x}}}{\min_{y} \sqrt{d_{y}}}
$$</p>
<p>where $f$ is a probability distribution (i.e. $f \geq 0$ and $\sum_{x} f(x)=1$ ) and $1=\rho_{0} \geq \rho_{1} \geq \ldots \geq \rho_{n-1}$ are the eigenvalues of $P$. This inequality implies that convergence to the stationary distribution $\pi$ will follow if $\max \left\{\left|\rho_{1}\right|,\left|\rho_{n-1}\right|\right\}&lt;1$.</p>
<p>The transition probability matrix $P$ is similar to the matrix $M=D^{\frac{1}{2}} P D^{-\frac{1}{2}}$, so $P$ and $M$ have the same eigenvalues. We previously introduced the Laplacian of the graph as $\mathcal{L}=I-M$, so it has eigenvalues $0=\lambda_{0} \leq \lambda_{1} \leq \ldots \leq \lambda_{n-1}$ (where $\lambda_{i}=1-\rho_{i}$ ).</p>
<p>The main tool we&rsquo;ll use to study the spectrum of $\mathcal{L}$ is the Rayleigh quotient $R(f)$ of $\mathcal{L}$, defined (for our purposes) as</p>
<p>$$
R(f)=\frac{f L f^{*}}{f D f^{*}}
$$</p>
<p>where $L=D-A$ is the combinatorial Laplacian. This is the same as the usual sense of the Rayleigh quotient $g \mathcal{L} g^{*} / g g^{*}$ with the subtitution $f=g D^{-\frac{1}{2}}$. Following this equivalence, if the $\phi_{i}$ are the eigenvectors of $\mathcal{L}$, we&rsquo;ll call the $\psi_{i}=\phi_{i} D^{-\frac{1}{2}}$ the harmonic eigenvectors of $\mathcal{L}$.</p>
<p>Employing the Rayleigh quotient, we see that the eigenvalue $\lambda_{1}$ can be written as</p>
<p>$$
\lambda_{1}=\inf_{\substack{f \\ \sum_{x} f(x) d_{x}=0}} R(f) .
$$</p>
<p>Since the eigenvector associated with $\lambda_{0}$ is $\phi_{0}=1 D^{\frac{1}{2}}$, the condition $\sum_{x} f(x) d_{x}=0$ is an orthogonality condition. Such variational characterizations can also be made for the other eigenvalues:</p>
<p>$$
\lambda_{n-1}=\sup _{f} R(f)
$$</p>
<p>and, in general,
$$
\lambda_{i}=\sup_{h_{0}, h_{1}, \ldots, h_{i-1}}
\inf_{\substack{f: \\ \sum_{x} f(x) h_{j}(x) d_{x}=0 \\ \forall j \in{0, \ldots, i-1}}}  R(f)
$$
The following characterization of the Rayleigh quotient (demonstrated last time) will be useful later:
$$
R(f)=\frac{\sum_{x \sim y}(f(x)-f(y))^{2}}{\sum_{x} f^{2}(x) d_{x}} .
$$</p>
<p>To this point, we have done a lot of linear algebra. We are not here to teach linear algebra; we are here to take linear algebra one step further to understand what is happening in the graph.</p>
<h1 id="the-cheeger-ratio-and-the-cheeger-constant">The Cheeger Ratio and The Cheeger Constant</h1>
<p>In many areas of mathematics the questions of &ldquo;best&rdquo; comes into play. What is the best bound for a given constant? What is the best way of row reducing a certain matrix? In this section, we will describe a way to make the &ldquo;best possible cut&rdquo; of a graph $G=(V, E)$, where a cut may be either an edge-cut or a vertex-cut, and this cut will split $G$ into two disconnected pieces.</p>
<p>We would like a way to measure the quality of a cut that is made to $G$. That is, would it be better to cut 4 edges which cause us to lose 20 vertices, or is it better to cut 10 edges which would result in the removal of 120 vetices?</p>
<p>Suppose we are given a graph $G=(V, E)$ and a subset $S \subseteq V$. We wish to define the folling two sets:</p>
<p>$$
\partial S={{u, v} \mid u \in S, v \notin S}
$$</p>
<p>and</p>
<p>$$
\delta S={v \notin S \mid v \sim u, u \in S} .
$$</p>
<p>Definition 1 For any vertex set $W$, the volume of $W$ is given by</p>
<p>$$
\operatorname{vol}(W)=\sum_{x W} d_{x},
$$</p>
<p>where $d_{x}$ is the degree of $\mathrm{x}$ in $W$.</p>
<p>Definition 2 The Cheerger Ratio for a vertex set $S$ is</p>
<p>$$
h(S)=\frac{|\partial S|}{\min {\operatorname{vol}(S), \operatorname{vol}(\bar{S})}},
$$</p>
<p>where $\bar{S}=V-S$.</p>
<p>It is first worth noting that in terms of this defintion of the Cheeger ratio, we are gauging the quality of our cut by taking a measure of what&rsquo;s been cut off of $G$. There are other forms of the Cheeger ratio as well. For example, we can use $|\delta S|$ instead of $|\partial S|,|S|($ or $\bar{S})$ instead of $\operatorname{vol}(S)$ (or $\operatorname{vol}(\bar{S}))$, or $|S||\bar{S}|$ instead of $\min {\operatorname{vol}(S), \operatorname{vol}(\bar{S})}$.</p>
<p>Definition 3 For any graph $G=(V, E)$, the Cheeger Constant of $G$ is given by</p>
<p>$$
h_{G}=\min_S h(S) .
$$</p>
<p>Now, if we consider the case where $\operatorname{vol}(S) \leq \frac{1}{2} \operatorname{vol}(G)$, then we can see that</p>
<p>$$
|\partial S| \geq h_{G}(\operatorname{vol}(S)) .
$$</p>
<h1 id="the-cheeger-inequality">The Cheeger Inequality</h1>
<p>Given a graph $G$, we can define $\lambda_{1}$ to be the first nontrivial eignevalue of the Laplacian, $\mathcal{L}$, of $G$.</p>
<p>For any graph $G$,</p>
<p>$$
2 h_{G} \geq \lambda_{1} \geq \frac{h_{G}^{2}}{2}
$$</p>
<h1 id="reference">Reference</h1>
<p><a href="https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf">https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf</a></p>
<p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
<p><a href="https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf">https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf</a></p>
<p><a href="https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf">https://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Monte Carlo Tree Search</title>
      <link>https://JhuoW.github.io/posts/monte-carlo-tree-search/</link>
      <pubDate>Wed, 30 Mar 2022 18:09:58 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/monte-carlo-tree-search/</guid>
      <description>单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。
在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?
多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。
  利用（Exploitation）： 保证在过去决策中得到最佳回报
  探索（Exploration）：寄希望在未来能够得到更大的汇报
  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。
但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。
悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机
$I_t$: $t$时刻选择的赌博机
$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励
$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward
$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。
$R_n$越大，就代表$n$次决策的结果越差。
上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡
在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}.</description>
      <content:encoded><![CDATA[<h1 id="单一状态monte-carlo规划多臂赌博机multi-armed-bandits">单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits）</h1>
<p>单一状态$s$, $k$种action（$k$个摇臂）。</p>
<p>在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?</p>
<p>多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。</p>
<ul>
<li>
<p>利用（Exploitation）： 保证在过去决策中得到最佳回报</p>
</li>
<li>
<p>探索（Exploration）：寄希望在未来能够得到更大的汇报</p>
</li>
</ul>
<p>例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。</p>
<p>但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。</p>
<h2 id="悔值函数">悔值函数</h2>
<p>如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数：
$$
R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t}
$$
$i$: 第$i$个赌博机</p>
<p>$I_t$: $t$时刻选择的赌博机</p>
<p>$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励</p>
<p>$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward</p>
<p>$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。</p>
<p>$R_n$越大，就代表$n$次决策的结果越差。</p>
<h2 id="上置信区间upper-confidence-bound-ucb">上置信区间（Upper Confidence Bound, UCB）</h2>
<p>UCB旨在探索和利用间去的平衡</p>
<p>在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机：
$$
I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}.
$$
其中$I_{t}$为$t$时刻要摇的赌博机，</p>
<p>$\overline{X_{i, T_{i}(t-1)}}$要尽可能大：为$t$时刻之前赌博机$i$的平均reward 要尽可能大，</p>
<p>$C_{t-1, T_{i}(t-1)}$要尽可能小：$t$时刻之前$i$出现的次数要尽可能少</p>
<p>其中$C_{t,T_i(t)}$的取值定义如下：
$$
C_{t,T_i(t)}=\sqrt{\frac{2 \operatorname{In} t}{T_i(t)}}
$$
其中$T_i(t)$表示 $t$时刻以前（包括$t$），选到赌博机$i$的次数总和。</p>
<p>若赌博机在$t$时刻之前（包括$t$）时刻摇动的次数越少，$C_{t,T_i(t)}$越大</p>
<p>选出的$I_t$要满足在$t$之前的平均reward尽可能大（利用）， 且在$t$之前出现次数尽可能少（探索）。</p>
<p>也就是说，在第时刻，UCB算法一般会选择具有如下最大值的第$j$个赌博机：
$$
\begin{aligned}
U C B&amp;=\bar{X}_{j}+\sqrt{\frac{2 \operatorname{Inn}}{n_{j}}} \text { 或者 } U C B=\bar{X}_{j}+C \times \sqrt{\frac{2 \operatorname{In} n}{n_{j}}} \\
I_t &amp;= \mathrm{argmax}_j UCB(j)
\end{aligned}
$$
其中$\bar{X}_{j}$是第$j$个赌博机在过去时间内所获得的平均奖赏值，$n_j$是在过去时间内拉动第$j$个赌博机臂膀的总次数，$n$是过去时间内拉动所有赌博机臂膀的总次数。$C$是一个平衡因子，其决定着在选择时偏重探索还是利用。</p>
<p>从这里可看出UCB算法如何在探索-利用(exploration-exploitation)之间寻找平衡：既需要拉动在过去时间内获得最大平均奖赏的赌博机，又希望去选择拉动臂膀次数最少的赌博机。</p>
<h1 id="monte-carlo-tree-search">Monte Carlo Tree Search</h1>
<p>MCTS has four step:</p>
<ul>
<li>Selection 选择</li>
<li>Expansion 拓展</li>
<li>Simulation（rollout) 模拟</li>
<li>Backpropagation 回溯</li>
</ul>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/1.png#center" alt=""  />
</p>
<h2 id="选择">选择</h2>
<p>从根节点R开始，向下递归选择子节点，直至选择一个叶子节点L。
具体来说，通常用UCBl(Upper Confidence Bound,上限置信区间)选择最具“潜力”的后续节点：
$$
U C B=\bar{X}_{j}+\sqrt{\frac{2 \operatorname{In} n}{n_{j}}}
$$</p>
<h2 id="拓展">拓展</h2>
<p>如果L不是一个终止节点（即博弈游戏不)，则随机创建其后的一个未被访问节点，选择该节点作为后续子节点C。</p>
<h2 id="模拟">模拟</h2>
<p>从节点C出发，对游戏进行模拟，直到博弈游戏结束。</p>
<h2 id="反向传播">反向传播</h2>
<p>用模拟所得结果来回溯更新导致这个结果的每个节点中获胜次数和访问次数。</p>
<p><strong>包含两种策略学习机制：</strong></p>
<p><strong>搜索树策略</strong>：从已有的搜索树中选择或创建一个叶子结点（即蒙特卡洛中选择和拓展两个步骤).搜索树策略需要在利用和探索之间保持平衡。</p>
<p><strong>模拟策略</strong>：从非叶子结点出发模拟游戏，得到游戏仿真结果。</p>
<h1 id="例子-围棋">例子： 围棋</h1>
<ul>
<li>
<p>以围棋为例，假设根节点是执黑棋方。</p>
</li>
<li>
<p>图中每一个节点都代表一个局面，每一个局面记录两个值A/B</p>
</li>
</ul>
<p>A: 该局面被访问中黑棋胜利次数。对于黑棋表示己方胜利次数，对于白棋表示己方失败次数（对方胜利次数)；</p>
<p>B: 该局面被访问的总次数</p>
<p>初始状态：<img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>根节点（12/21）是当前局面，该局面被访问过21次，其中黑棋访问该局面后最终获胜12次。 黑执棋方遇到该局面（根节点局面）开始选择下一步action。</p>
<h3 id="选择-1">选择</h3>
<p>黑执棋方遇到（12/21）这个局面时有3个有效action, 黑执行这三个action会得到3种局面（7/10）,（5/8）, (0/3). 黑执棋方要选择其中一个action，是的棋局变为这三种中的一种，那么要如何选择action呢？我们先分别计算UCB1：</p>
<p>左一： 7/10对应的局面Reward为：
$$
\frac{7}{10} + \sqrt{\frac{\log (21)}{10}} = 1.252
$$
3中局面共21次被模拟到，其中，该局面被访问过10次（探索， 第二项）， 黑棋遇到该局面后最终获胜7次（利用， 第一项）。</p>
<p>左二：（5/8）对应局面Reward:
$$
\frac{5}{8} + \sqrt{\frac{\log(21)}{8}} = 1.243
$$
左三： （0/3）对应局面Reward:
$$
\frac{0}{3} + \sqrt{\frac{\log(21)}{3}} = 1.007
$$
由此可见，黑棋选择会导致局面（7/10）的action进行走琪。</p>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白棋遇到局面（7/10）时，有两个有效的action, 分别会导致局面A/B = 2/4和5/6, A为白棋访问该局面后失败的次数，所以白棋访问该局面最终胜利的次数应该是$B-A$：</p>
<p>左一： (2/4)对应的局面Reward (白棋尽可能获胜)为：
$$
(1-\frac{2}{4}) + \sqrt{\frac{\log(21)}{4}}=1.372
$$
左二：  (5/6)对应的局面Reward为：
$$
(1-\frac{5}{6}) + \sqrt{\frac{\log(21)}{4}}=0.879
$$
因此白棋会选择（2/4）局面</p>
<p>即<strong>每一步都寻找最佳应对方式，来最终评估更节点局面的好坏</strong></p>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白色执棋方选择action后到达局面（2/4）, 黑棋执棋方面对该局面，有两种action可选，分别会到达（1/3）和（1/1）， 分别计算UCB1 得：</p>
<p>左一： (1/3)对应reward 为：
$$
\frac{1}{3} + \sqrt{\frac{\log (21)}{3}} = 1.341
$$
左二：(1/1)对应reward为：
$$
\frac{1}{1} + \sqrt{\frac{\log (21)}{1}} = 2.745
$$
则黑棋选择action 使得局面变为(1/1)。右图中可见(1/1)为叶子节点， 白棋面对该局面暂时无候选局面（action）,则进入下一个步骤，拓展。</p>
<h3 id="拓展-1">拓展</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq1.png#center" alt=""  />
</p>
<p>白棋执棋方面对局面(1/1),因为已经是叶子节点， 所以(1/1)会随机产生一个未被访问过的新节点，初始化为(0/0),接着在该节点下进行模拟。 进入第三部， 模拟。</p>
<h3 id="模拟-1">模拟</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq3.png#center" alt=""  />
</p>
<p>黑棋执棋方面局面（0/0），开始进行游戏仿真， 假设经过一些列仿真后，最终白棋获胜，即从更节点到最终模拟结束的叶子节点黑棋失败了。 进入第四部，回溯</p>
<h3 id="回溯">回溯</h3>
<p><img loading="lazy" src="/posts/2022-03-30-MCTS/wq4.png#center" alt=""  />
</p>
<p>根据仿真结果来更新该仿真路径上的每个节点的A/B值。 因为该次模拟最终白棋获胜，所以向上回溯路径上的所有父节点，父节点的A不变，B+1。 若最终黑棋获胜，则父节点的A+1, B+1。</p>
<p><strong>在有限时间里， MCTS会不断重复4个步骤，所有节点的A,B 值会不断变化</strong></p>
<p><strong>到某个局面（节点）后如何下棋（action）, 会选择所有有效Action中UCB1值最大的节点，作为下棋的下一步。</strong></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>https://JhuoW.github.io/posts/rkhs_kernel/</link>
      <pubDate>Sat, 26 Mar 2022 22:39:20 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/rkhs_kernel/</guid>
      <description>Hilbert Space Definition 1 (Norm) Let $\mathcal{F}$ be a vector space over $\mathbb{R}$ (For example $\mathcal{F}=\mathbb{R}^n$ is a vector space). A function $||\cdot||_{\mathcal{F}}: \mathcal{F} \to [0, \inf)$ is said to be a norm on $\mathcal{F}$ if ($||\cdot||_{\mathcal{F}}$ 是一个有效norm算子要满足以下条件):
 For $f \in \mathcal{F}$, $||f||_{\mathcal{F}}=0$ if and only if $f=0$. (norm separates points) $|\lambda f|_{\mathcal{F}}=|\lambda||f|_{\mathcal{F}}$, $\forall \lambda \in \mathbb{R}, \forall f \in \mathcal{F}$ (positive homogeneity). $|f+g|_{\mathcal{F}} \leq|f|_{\mathcal{F}}+|g|_{\mathcal{F}}, \forall f, g \in \mathcal{F}$ (triangle inequality).</description>
      <content:encoded><![CDATA[<h1 id="hilbert-space">Hilbert Space</h1>
<p><em><strong>Definition 1</strong></em> (<a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf">Norm</a>) Let $\mathcal{F}$ be a vector space over $\mathbb{R}$ (For example $\mathcal{F}=\mathbb{R}^n$ is a vector space). A function $||\cdot||_{\mathcal{F}}: \mathcal{F} \to [0, \inf)$ is said to be a norm on $\mathcal{F}$ if ($||\cdot||_{\mathcal{F}}$ 是一个有效norm算子要满足以下条件):</p>
<ol>
<li>For $f \in \mathcal{F}$, $||f||_{\mathcal{F}}=0$ if and only if $f=0$. (norm separates points)</li>
<li>$|\lambda f|_{\mathcal{F}}=|\lambda||f|_{\mathcal{F}}$, $\forall \lambda \in \mathbb{R}, \forall f \in \mathcal{F}$ (positive homogeneity).</li>
<li>$|f+g|_{\mathcal{F}} \leq|f|_{\mathcal{F}}+|g|_{\mathcal{F}}, \forall f, g \in \mathcal{F}$ (triangle inequality).</li>
</ol>
<p>向$||\cdot||_{\mathcal{F}}$中输入任意一个向量，只要满足以上条件，那么$||\cdot||_{\mathcal{F}}$是一个valid norm operator.</p>
<h2 id="inner-product">Inner Product</h2>
<p>An <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">inner product</a> takes two elements of a vector space $\mathcal{X}$ and outputs a number. An inner product could be a usual dot product: $\langle\mathbf{u}, \mathbf{v}\rangle=\mathbf{u}^{\prime} \mathbf{v}=\sum_{i} u^{(i)} v^{(i)}$ (Inner Product can be Dot Product). Or the inner product could be something fancier（即内积不一定表示为点积的形式）. If an Inner Product $\langle \cdot,\cdot \rangle$ is valid, it <em><strong>MUST</strong></em>  satisfy the following conditions:</p>
<ol>
<li>
<p>Symmetry
$$\langle u, v\rangle=\langle v, u\rangle \quad \forall u, v \in \mathcal{X}$$</p>
</li>
<li>
<p>Bilinearity
$$\langle\alpha u+\beta v, w\rangle=\alpha\langle u, w\rangle+\beta\langle v, w\rangle \quad \forall u, v, w \in \mathcal{X}, \forall \alpha, \beta \in \mathbf{R}$$</p>
</li>
<li>
<p>Strict Positive Definiteness
$$
\begin{gathered}
\langle u, u\rangle \geq 0 \forall x \in \mathcal{X} \\
\langle u, u\rangle=0 \Longleftrightarrow u=0
\end{gathered}$$</p>
</li>
</ol>
<p>An  <em><strong>inner product space</strong></em> (or pre-Hilbert space) is a vector space together with an inner product. （包含内积运算的向量空间称为 内积空间，即可以定义内积运算的向量空间）。</p>
<p>Kernel is a kind of Inner Product. For example, the Gaussian kernel is defined as:
$$
\begin{equation}
\langle u, v \rangle = k(u,v) = \exp({-\frac{||u-v||^2}{2\sigma}}) \tag{1}
\end{equation}
$$</p>
<h2 id="hilbert-space-1">Hilbert Space</h2>
<p><em><strong>Definition 2</strong></em> (<a href="https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf">Hilbert Space</a>)  A Hilbert Space is an Inner Product space that is complete and separable with respect to the norm defined by the inner product.</p>
<p>&lsquo;Complete&rsquo; means sequences converge to elements of the space - there aren&rsquo;t any &ldquo;holes&rdquo; in the space.</p>
<h1 id="finite-states">Finite States</h1>
<p>Given finite input space ${x_1, x_2, \cdots x_m }$. I want to be able to take inner products between any two of them using my function $k$  as the inner product ($k$ is customized and satisfy three conditions. For example, $k$ is a Gaussian inner product as Eq.(1)). Inner products by definition are symmetric, so $k(x_i, x_j)=k(x_j, x_i)$ , which yields a symmetric matrix $\mathbf{K}$.</p>
<p>Since $\mathbf{K}$ is real symmetric, and this means we can diagonalize it （实对称阵可以对角化，即特征分解）, and the eigendecomposition takes this form:
$$
\begin{equation}
\begin{aligned}
\mathbf{K} &amp;=\mathbf{V} \Lambda \mathbf{V}^T \\
&amp;= \mathbf{V} 	\begin{bmatrix}
\lambda_1 &amp;   &amp; &amp;  \\
&amp; \lambda_2 &amp;  &amp; \\
&amp;   &amp; \cdots &amp;\
&amp;   &amp;  &amp;\lambda_m
\end{bmatrix} \mathbf{V}^T \\
&amp;= \begin{bmatrix}
v_1 &amp; v_2 &amp; \cdots v_m
\end{bmatrix} \begin{bmatrix}
\lambda_1 &amp;   &amp; &amp;  \\
&amp; \lambda_2 &amp;  &amp; \\
&amp;   &amp; \cdots &amp;\\
&amp;   &amp;  &amp;\lambda_m
\end{bmatrix}
\begin{bmatrix}
v_1^T\\
v_2^T\\
\cdots \\
v_m^T
\end{bmatrix}\\
&amp;=v_1\lambda_1 v_1^T + \cdots + v_m\lambda_m v_m^T = \sum_{t=1}^m v_t\lambda_tv_t^T
\end{aligned} \tag{2}
\end{equation}
$$
Let the $i$-th element of vector $v$ as $v^{(i)}$, then
$$
\begin{equation}
\begin{aligned}
\mathbf{K}_{ij} = k(x_i, x_j) &amp;= [\sum_{t=1}^m v_t\lambda_tv_t^T]_{ij}\\
&amp;=\sum^m_{t=1} v_t^{(i)} \lambda_t v_t^{(j)}
\end{aligned} \tag{3}
\end{equation}
$$
If $\mathbf{K}$ is a <strong>positive semi-definite</strong> (<strong>PSD</strong>) matrix, then $\lambda_1, \cdots \lambda_m \geq 0$.</p>
<blockquote>
<p><em><strong>Assumption 1</strong></em>. All $\lambda_t$ are nonnegative.</p>
</blockquote>
<p>We consider this feature map:
$$
\begin{equation}
\Phi\left(x_{i}\right)=\left[\sqrt{\lambda_{1}} v_{1}^{(i)}, \ldots, \sqrt{\lambda_{t}} v_{t}^{(i)}, \ldots, \sqrt{\lambda_{m}} v_{m}^{(i)}\right] \in \mathbb{R}^m \tag{4}
\end{equation}
$$
(writing it for $x_j$ too):
$$
\begin{equation}
\boldsymbol{\Phi}\left(x_{j}\right)=\left[\sqrt{\lambda_{1}} v_{1}^{(j)}, \ldots, \sqrt{\lambda_{t}} v_{t}^{(j)}, \ldots, \sqrt{\lambda_{m}} v_{m}^{(j)}\right]  \in \mathbb{R}^m \tag{5}
\end{equation}
$$
即 $\Phi: \mathcal{X} \to \mathbb{R}^m$ 将$x\in \mathcal{X}$映射到$m$维向量空间$\mathbb{R}^m$中的一个点。</p>
<p>With this choice, the inner product $k$ is just defined as a dot product in $\mathbb{R}^m$:
$$
\begin{equation}
\left\langle\Phi\left(x_{i}\right), \Phi\left(x_{j}\right)\right\rangle_{\mathbf{R}^{m}}=\sum_{t=1}^{m} \lambda_{t} v_{t}^{(i)} v_{t}^{(j)}=\left(\mathbf{V} \Lambda \mathbf{V}^{\prime}\right)_{i j}=K_{i j}=k\left(x_{i}, x_{j}\right)  \tag{6}
\end{equation}
$$
If there exists an eigenvalue $\lambda_s &lt;0$ (即$\sqrt{\lambda_s} = \sqrt{|\lambda_s|} i$). $\lambda_s$对应的特征向量$v_s$。用$v_s \in \mathbb{R}^m$的$m$个元素$v_s = [v_s^{(1)},\cdots, v_s^{(m)}]$, 来对$\Phi(x_1),\cdots, \Phi(x_m)$做线性组合：
$$
\begin{equation}
\mathbf{z}=\sum_{i=1}^{m} v_{s}^{(i)} \boldsymbol{\Phi}\left(x_{i}\right) \tag{7}
\end{equation}
$$</p>
<p>It is obvious that $\mathbf{z} \in \mathbb{R}^m$. Then calculate
$$
\begin{equation}
\begin{aligned}
|\mathbf{z}|_{2}^{2} &amp;=\langle\mathbf{z}, \mathbf{z}\rangle_{\mathbf{R}^{m}}=\sum_{i} \sum_{j} v_{s}^{(i)} \boldsymbol{\Phi}\left(x_{i}\right)^{T} \boldsymbol{\Phi}\left(x_{j}\right) v_{s}^{(j)}=\sum_{i} \sum_{j} v_{s}^{(i)} K_{i j} v_{s}^{(j)} \\
&amp;=\mathbf{v}_{s}^{T} \mathbf{K} \mathbf{v}_{s}=\lambda_{s}&lt;0
\end{aligned}  \tag{8}
\end{equation}
$$
which conflicts with the geometry of the feature space.</p>
<p>如果$\mathbf{K}$不是半正定，那么feature space $\mathbb{R}^m$存在小于0的值。所以假设Assumption不成立。即，若$k$表示有限集的内积，那么它的Gram Matrix一定半正定(PSD)，否则无法保证该空间中的norm大于0。</p>
<p>有效的内积对应的Gram Matrix 必定PSD.</p>
<h1 id="kernel">Kernel</h1>
<p><em><strong>Definition 3.</strong></em> (<a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">Kernel</a>) A function $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a kernel if</p>
<ol>
<li>$k$ is symmetric: $k(x,y) = k(y,x)$.</li>
<li>$k$ gives rise to a positive semi-definite &ldquo;Gram matrix,&rdquo; i.e., for any $m\in \mathbb{N}$ and any $x_1,\cdots,x_m$ chosen from $X$, the Gram matrix $\mathbf{K}$ defined by $\mathbf{K}_{ij} = k(x_i,x_j)$ is positive semi-definite.</li>
</ol>
<p>Another way to show that a matrix $\mathbf{K}$ is positive semi-definite is to show that
$$
\begin{equation}
\forall \mathbf{c} \in \mathbf{R}^{m}, \mathbf{c}^{T} \mathbf{K} \mathbf{c} \geq 0 \tag{9}
\end{equation}
$$
Here are some nice properties of $k$:</p>
<ul>
<li>$k(u,u) \geq 0$ (Think about the Gram matrix of $m = 1$.)</li>
<li>$k(u, v) \leq \sqrt{k(u, u) k(v, v)}$ (This is the Cauchy-Schwarz inequality.)</li>
</ul>
<h2 id="reproducing-kernel-hilbert-space-rkhs">Reproducing Kernel Hilbert Space (RKHS)</h2>
<p>给定一个kernel $k(\cdot, \cdot): \mathcal{X} \times \mathcal{X} \to \mathbb{R}$. 定义一个函数空间（space of functions）$\mathbf{R}^{\mathcal{X}}:={f: \mathcal{X} \rightarrow \mathbb{R}}$. $\mathbf{R}^{\mathcal{X}}$ 是一个 Hilbert Space， 该空间中的每个元素是一个$\mathcal{X}$映射到$\mathbb{R}$的函数。</p>
<p>令$k_x(\cdot) = k(x, \cdot)$, 假设$x$是一个定值（Constant），自变量（输入）用$\cdot$表示。那么$k(x, \cdot)$ 也是$\mathbf{R}^{\mathcal{X}}$空间中的一个函数。</p>
<p>每个函数$k_x(\cdot)$ 都与一个特定的$x \in \mathcal{X}$有关，即每个$x$对应于一个函数$k_x(\cdot) = k(\cdot, x)$. 这种对应关系表示为$\Phi(x) = k_x(\cdot) = k(x,\cdot)$, 即：
$$
\begin{equation}
\Phi: x \longmapsto k(\cdot, x) \tag{10}
\end{equation}
$$
即 $\Phi$的输入为$x\in \mathcal{X}$, 输出一个函数, 输出的函数属于$\mathbf{R}^{\mathcal{X}}$空间。</p>
<p>在连续空间$\mathcal{X}$中，$x \in \mathcal{X}$ 有无穷多种情况，那么$\Phi(x)=k_x(\cdot)=k(x, \cdot)$也有无穷多种情况，即无穷多种函数。 这些函数可以span 一个Hilbert Space:
$$
\begin{equation}
\mathcal{H}_k = \operatorname{span}({\Phi(x): x \in \mathcal{X}})=\left\{f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right): m \in \mathbf{N}, x_{i} \in \mathcal{X}, \alpha_{i} \in \mathbf{R}\right\}  \tag{11}
\end{equation}
$$
其中$k(x,\cdot)=\Phi(x)$可以理解为将$x$映射为一个函数（or vector）。上述Hilbert Space是由任意$k(x, \cdot)$线性组合而成的函数空间，该空间中的每个元素可以表示为
$$
\begin{equation}
f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)  \tag{12}
\end{equation}
$$
所以$\mathcal{H}$可以看作是kernel $k$对应的一个Hilbert Space。</p>
<p>给定$\mathcal{H}$中的任意两个函数$f(\cdot)=\sum_{i=1}^{m} \alpha_{i} k\left(\cdot, x_{i}\right)$, $g(\cdot)=\sum_{j=1}^{m^{\prime}} \beta_{j} k\left(\cdot, x_{j}^{\prime}\right)$。注意$f(\cdot)$和$g(\cdot)$可以表示$\mathcal{H}$中任意两个元素。我们将$\mathcal{H}$上的内积定义为：
$$
\begin{equation}
\langle f, g\rangle_{\mathcal{H}_{k}}=\sum_{i=1}^{m} \sum_{j=1}^{m^{\prime}} \alpha_{i} \beta_{j} k\left(x_{i}, x_{j}^{\prime}\right) \tag{13}
\end{equation}
$$
由<a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">Proof</a>证明了该内积符合三个条件，顾上式是$\mathcal{H}$空间中一个有效的内积算子。注：$\mathcal{H}_k$表示该Hilbert Space是由函数 $k(x,\cdot)$ span而成的，与Kernel $k$有关.</p>
<p>$k(x,\cdot)$也是$\mathcal{H}_k$中的一个函数，那么它与 $f$的内积为：
$$
\begin{equation}
\langle k(\cdot, x), f\rangle_{\mathcal{H}_{k}}=
\sum_{i=1}^m \alpha_i k(x,x_i)
=f(x) \tag{14}
\end{equation}
$$
<strong>Theorem 1.</strong>  $k(\cdot, \cdot)$ is a reproducing kernel of a Hilbert space $\mathcal{H}_k$ if $f(x)=\langle k(x, \cdot), f(\cdot)\rangle$.</p>
<p>$\mathcal{H}_k$ 为$k(\cdot, \cdot)$的再生核希尔伯特空间。</p>
<p>同理，$k(\cdot, x_i)$, $k(\cdot, x_j)$都为$\mathcal{H}_k$中的函数， 计算他们的内积:
$$
\begin{equation}
\left\langle k(\cdot, x_i), k\left(\cdot, x_j\right)\right\rangle_{\mathcal{H}_{k}}=k\left(x_i, x_j\right)  \tag{15}
\end{equation}
$$
因为$ k(\cdot, x_i) = \Phi(x_i)$, $ k(\cdot, x_j) = \Phi(x_j)$, 所以
$$
\begin{equation}
k\left(x_i, x_j\right) = \left\langle  \Phi(x_i), \Phi(x_j)\right\rangle_{\mathcal{H}_{k}} \tag{16}
\end{equation}
$$
表示将$x_i$和$x_j$ 映射成$\mathcal{H}_k$中的函数（向量）后再做内积。</p>
<h4 id="参考文献">参考文献</h4>
<p>[1] <a href="https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf">https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf</a></p>
<p>[2] <a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf">http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf</a></p>
<p>[3] <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf</a></p>
<p>我把本文整理成了<a href="/posts/rkhs/RKHS.pdf">PDF</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
