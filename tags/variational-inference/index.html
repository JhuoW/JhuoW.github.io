<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var t=MathJax.Hub.getAllJax(),e;for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>Variational Inference | JhuoW‘s Notes</title><meta name=keywords content><meta name=description content="Jhuo’s Notes"><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/tags/variational-inference/><meta name=google-site-verification content="G-6F49SGED6V"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://JhuoW.github.io/tags/variational-inference/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-6F49SGED6V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6F49SGED6V",{anonymize_ip:!1})}</script><meta property="og:title" content="Variational Inference"><meta property="og:description" content="Jhuo’s Notes"><meta property="og:type" content="website"><meta property="og:url" content="https://JhuoW.github.io/tags/variational-inference/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Variational Inference"><meta name=twitter:description content="Jhuo’s Notes"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="JhuoW's Notes (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>JhuoW's Notes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://JhuoW.github.io/gallery/ title=Gallery><span>Gallery</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://JhuoW.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://JhuoW.github.io/tags/>Tags</a></div><h1>Variational Inference</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2>ICML2022 《Local Augmentation for Graph Neural Networks》 Reading Notes</h2></header><section class=entry-content><p>paper
Introduction 在GNN的neighborhood aggregation中，对于拥有很少邻居的节点，在聚合过程中是否充分从邻居中获得了信息是一个问题。为解决该问题， 本文提出为每个节点做局部增强，即以中心节点为条件，学习邻居节点表示的分布。为了在局部邻域中生成一些样本来提升中心节点的neighborhood aggregation，本文提出一种数据增强框架：LA-GNNs， 以局部结构和中心节点特征为条件，生成neighborhood features。具体来说，在pre-training 阶段，通过一个生成模型，以中心节点的特征为条件来学习邻居特征的条件概率分布。然后利用这个邻居特征分布来生成中心节点的增强邻居特征。另外，通过pre-training来学习邻居增强特征生成器的过程是与下游任务无关的，所以该生成器生成的增强特征可以应用于其他GNN模型。
Local Augmentation for Graph Neural Networks (LAGNN) Motivation GNN在message passing的过程利用局部信息聚合来得到node representations。 但是对于邻居数量较少的节点，从邻居中得到的信息可能会不足。为了为节点$v$的邻域中$\mathcal{N}_v$生成更多样本，就需要知道邻居表示的分布。 由于一个节点邻居分布是与中心节点相关，所以我们要以中心节点$v$的representation为条件，学习它的邻居表示分布。
Approach 本文利用Conditional Variational Auto-Encoder (CVAE) 来学习给定中心节点$v$，邻居$u \in \mathcal{N}_v$的节点特征的条件分布。给定中心节点特征$\boldsymbol{X}_v$，关于中心节点的邻居分布为$p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v)$。定义隐变量$\mathbf{z}$，则先验可以定义为$p_\theta(\mathbf{z}|\boldsymbol{X}_v)$。结合隐变量$\mathbf{z}$，邻居特征$\boldsymbol{X}_u$的分布可以改写为如下形式： $$ \begin{aligned} \log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v) &= \log \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v)}{p_\theta( \boldsymbol{X}_v)}= \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}{p_\theta( \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} \\ &=\log \frac{p_\theta(\boldsymbol{X}_u , \boldsymbol{X}_v, \mathbf{z})}{p_\theta( \boldsymbol{X}_v)p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} \\ &= \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}\\ \end{aligned} $$ 假设隐变量$\mathbf{z}$的分布为$q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)$， 左右两边对分布$q_\phi$计算期望，左边： $$ \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v) dz = \log p_\theta(\boldsymbol{X}_u | \boldsymbol{X}_v) $$ 右边： $$ \begin{aligned} &\int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)} dz \\ =& \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \left(\frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \cdot \frac{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}\right) dz \\ =& \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} dz + \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \frac{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}{p_\theta(\mathbf{z}|\boldsymbol{X}_u , \boldsymbol{X}_v)}dz \\ =& \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta(\boldsymbol{X}_u , \mathbf{z}| \boldsymbol{X}_v)}{ q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} dz + K L\left(q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) || p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)\right) \\ \geq& \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} = ELBO \end{aligned} $$ Evidence Lower Bound (ELBO) 可以写为 $$ \begin{aligned} L_{ELBO} &= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\ &= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u, \boldsymbol{X}_v, \mathbf{z}\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) p_\theta\left(\boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\ &= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) p_\theta\left(\boldsymbol{X}_v, \mathbf{z}\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) p_\theta\left(\boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\ &= \int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log \frac{p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)}{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)} \mathrm{d} \mathbf{z} \\ &= -K L\left(q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) || p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)\right)+\int_z q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) \log p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) \mathrm{d} \mathbf{z} \\ &= -K L\left(\underbrace{q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right)}_{Encoder} || \underbrace{ p_\theta\left(\mathbf{z} \mid \boldsymbol{X}_v\right)}_{\text{Normal Distribution}}\right) + \mathbb{E}_{\mathbf{z} \sim q_\phi\left(\mathbf{z} \mid \boldsymbol{X}_u, \boldsymbol{X}_v\right) }\log p_\theta\left(\boldsymbol{X}_u \mid \boldsymbol{X}_v, \mathbf{z}\right) \end{aligned} $$ 在CVAE pre-training的过程中，第一项KL中CVAE Encoder 的一对邻接节点对，对于该节点对，输出一组分布参数均值$\mu$和方差$\sigma$，作为隐变量$z$的分布参数，第一项的优化目标使得编码器输出的分布接近Normal Distribution。然后利用reparameterization trick可微的从生成的$\mathbf{z}$分布中采样一个encoding:...</p></section><footer class=entry-footer><span title="2023-01-25 21:19:40 +0800 CST">January 25, 2023</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICML2022 《Local Augmentation for Graph Neural Networks》 Reading Notes" href=https://JhuoW.github.io/posts/lagcn/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes</h2></header><section class=entry-content><p>paper
Introduction 对于监督对比学习（Supervised Contrastive Learning, SupCon）, SupCon loss旨在表示空间中拉近属于同一个class的数据点，分离不同类的数据点。 但是SupCon难以处理高类内方差，类间相似度较大的数据集。为了解决该问题，本文提出了Cluster-aware supervised contrastive learning loss (ClusterSCL)。什么是高类内方差，高跨类相似度问题？如图1(a)所示，节点$u_1$和$u_3$ 是同类节点，$u_2$和$u_4$是同类节点。他们是同类节点但在不同的社区中，所以类内方差较大，即同一个类内的节点跨越了多个community。 另外$u_1$和$u_2$， $u_3$和$u_4$，是不同类的节点对， 但他们处在同一个社区中，导致在MPNN过程中，这些处在同一个community中的不同类节点被拉近，导致跨类相似度较高的问题。
如果对节点$u_2$计算SupCon时，如图1(b)所示，SupCon会使得同类节点被拉近，如$u_2$和$u_4$会被拉近。但是$u_3$和$u_4$处在同一个社区中（structurally similar）那么MPNN会使得$u_3$和$u_4$被拉近，所以SupCon在拉近$u_2$和$u_4$的同时，会间接拉近不同类节点$u_2$和$u_3$。同时，对于构成negative pairs的不同类节点，例如$u_1$和$u_2$，SupCon会推远$u_1$和$u_2$，但是$u_1$和$u_5$ structurally similar, 因此会推远$u_1$和$u_2$会间接导致$u_2$和$u_5$这两个同类节点被推远。因此对于一个cluster内节点不同类，且不同cluster中存在同类节点的情况，会导致复杂的决策边界，即在拉近同类但不同社区的节点时，也会间接拉近不同类不同社区的节点。在推远不同类同社区的节点时，也可能间接推远同类同社区的节点。
为了解决上述问题，最直接的方法是对于每个cluster，如图1(a)的Community 1，不考虑其他cluster，只对当前cluster内节点做SupCon。但是这么做忽略了跨cluster的同类节点交互，如$u_1$和$u_3$，$u_2$和$u_4$，这些跨cluster的positive pairs可能包含有益的信息。为了解决这个问题，本文提出cluster-aware data augmentation (CDA) 聚类感知的数据增强，来为每个节点生成augmented positives and negatives，如图1(b)中ClusterSCL所示。对于每个节点$u$，为它生成positive 和negative samples, 生成的samples 位于或接近$u$所在的cluster。Recall SupCon存在的问题：
SupCon会使得$u_2$和$u_4$被拉的太近，从而间接导致$u_2$和$u_3$被拉近，所以对于high intra-class variances，要求不同cluster的同类节点如$u_2$和$u_4$不要被拉太近； SupCon会使得$u_1$和$u_2$被推远，从而间接导致$u_2$和$u_5$被推远，所以对于high inter-class similarity，要求同一个cluster内的不同类节点如$u_1$和$u_2$不要被拉的太远。 Method Two stage training with Supervised Contrastive Loss SupCon encourages samples of the same class to have similar representations, while pushes apart samples of different classes in the embedding space....</p></section><footer class=entry-footer><span title="2022-11-17 01:33:20 +0800 CST">November 17, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes" href=https://JhuoW.github.io/posts/clusterscl/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Expectation Maximization</h2></header><section class=entry-content><p>最大似然估计MLE 数据 $X = \{x_1, \cdots x_N\}$, 模型参数为$\theta$，Likelihood 定义为 $P(X | \theta)$：当参数为$\theta$时，观测到给定数据$X$的概率。 $$ P(X|\theta) = L(\theta | X) = P_\theta(X) \tag{1} $$ 最大似然估计 （Maximum Likelihood Estimation, MLE）: $$ \theta_{\mathrm{MLE}} = \arg \max_\theta P(X|\theta) \tag{2} $$
最大似然估计：给定一组样本$X$，模型的参数$\theta$是研究对象。若能找到参数$\theta_{\mathrm{MLE}}$，使得样本发生的可能性最大，则此估计值$\theta_{\mathrm{MLE}}$为参数$\theta$的最大似然估计。
举例来说，如果模型是单个Gaussian Distribution下，参数为Gaussian Distribution的参数（均值$\mu$, 标准差$\Sigma$， $\theta = {\mu, \Sigma}$）. 给定一组数据$X$, 要计算$X$来自什么样的Gaussian，即：$P(\cdot | \theta) = f_\theta(\cdot) = \mathcal{N}(\cdot | \mu,\Sigma)$是一个Gaussian Distribution函数，目标为： $$ \theta_{\mathrm{MLE}} = \mu^\star, \Sigma^\star = \arg \max_{\mu,\Sigma} \sum^N_{i = 1} \log \mathcal{N}(x_i|\mu,\Sigma) \tag{3} $$ 即MLE的目标是找到最佳的高斯分布，是的从该分布中采样出数据$X$的概率最高。...</p></section><footer class=entry-footer><span title="2022-04-01 19:45:48 +0800 CST">April 1, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Expectation Maximization" href=https://JhuoW.github.io/posts/em-algo/></a></article></main><footer class=footer><span>Copyright &copy; 2023 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span></span><br><script>function siteTime(){var u=1e3,r=u*60,a=r*60,n=a*24,x=n*365,e=new Date,d=2019,O=1,w=16,_=19,y=15,m=11,l=e.getFullYear(),C=e.getMonth()+1,f=e.getDate(),p=e.getHours(),g=e.getMinutes(),v=e.getSeconds(),b=Date.UTC(d,O,w,_,y,m),j=Date.UTC(l,C,f,p,g,v),s=j-b,o=Math.floor(s/x),t=Math.floor(s/n-o*365),i=Math.floor((s-(o*365+t)*n)/a),c=Math.floor((s-(o*365+t)*n-i*a)/r),h=Math.floor((s-(o*365+t)*n-i*a-c*r)/u);d==l?document.getElementById("sitetime").innerHTML="本站已运行 "+t+" 天 "+i+" 小时 "+c+" 分钟 "+h+" 秒":document.getElementById("sitetime").innerHTML="本站已运行 "+o+" 年 "+t+" 天 "+i+" 小时 "+c+" 分钟 "+h+" 秒"}setInterval(siteTime,1e3)</script><span id=sitetime>载入运行时间...</span>
<script type=text/javascript id=clustrmaps src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=268&t=tt&d=YsONH-MzO6L7yrkA73Z_QW7LuMTfdUhk0uhb_KaBv-g&co=f5f5f5&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>