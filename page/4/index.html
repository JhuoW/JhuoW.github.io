<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.147.0"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>JhuoW‘s Notes</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="Jhuo’s Notes"><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/><meta name=google-site-verification content="G-6F49SGED6V"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://JhuoW.github.io/index.xml><link rel=alternate type=application/json href=https://JhuoW.github.io/index.json><link rel=alternate hreflang=en href=https://JhuoW.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-6F49SGED6V"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6F49SGED6V")}</script><meta property="og:title" content="JhuoW‘s Notes"><meta property="og:description" content="Jhuo’s Notes"><meta property="og:type" content="website"><meta property="og:url" content="https://JhuoW.github.io/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="JhuoW‘s Notes"><meta name=twitter:description content="Jhuo’s Notes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"JhuoW‘s Notes","url":"https://JhuoW.github.io/","description":"Jhuo’s Notes","thumbnailUrl":"https://JhuoW.github.io/favicon.ico","sameAs":["https://github.com/JhuoW","mailto:jhuow@proton.me"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="JhuoW's Notes (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>JhuoW's Notes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2>NeurIPS2021 《Representing Long-Range Context for Graph Neural Networks with Global Attention》 Reading Notes</h2></header><section class=entry-content><p>Paper
Introduction 加深GNN层数来增加感受野会导致优化不稳定性，比如梯度消失和oversmoothing。 因此本文采用Transformer-based self-attention来学习成对节点间的长距离关系，并且提出一种新型的readout池化机制来学习global graph embedding。即在一个GNN模块后接一个置换不变（permutation-invariant）Transformer, GNN模块捕获local信息，Transformer捕获global信息。
GNN作为一种专门的架构医学系节点直接邻域结构的局部表示， 而Transformer作为全局推理模块以位置无关的方式计算所有成对节点的交互。作者认为，一个没有positional encoding的Transformer是置换不变的，因此很适合图。
Motivation 强关系Inductive bias(我的理解是Homophily假设) 鼓励学习局部短距离的关联性。 而对于长距离相关性，结构化较低的模块（不需要过于考虑图的结构信息）更受欢迎。
GraphTrans leaves learning long-range dependencies to Transformer, 通过Transformer来学习图中所有节点对的依赖关系而不是只关注局部邻居。
下图中展示了一个子图的attention map。一共有17个节点，横坐标表示目标节点，纵坐标表示源节点，第$i$行第$j$列表示节点$i$在Transformer中聚合$j$的attention权重。第18行为一个特殊的$&lt;CLS>$token 作为图的readout embedding。结合本文的SOTA效果，表面在学习长距离依赖时不考虑图结构先验（spatial priors）对Graph summarization（graph-level representation）是有必要的
Model GNN Module 一个通用的GNN模块： $$ \boldsymbol{h}_{v}^{\ell}=f_{\ell}\left(\boldsymbol{h}_{v}^{\ell-1},\left\{\boldsymbol{h}_{u}^{\ell-1} \mid u \in \mathcal{N}(v)\right\}\right), \quad \ell=1, \ldots, L_{\mathrm{GNN}} $$
Transformer Module 通过上面的GNN模块，我们可以得到每个节点的embedding $h_{v}^{L_{\mathrm{GNN}}}$, 将所有节点作为Transformer的Input。传统Transformer中输入先计算Self-attention（当前输入的$Q$向量和所有节点的$K$向量做内积得到输入节点和其他节点的att值，再用这个att值来为当前输入节点加权聚合所有节点的$V$向量），聚合后再和自身相加做residual,然后在做Layer Norm, 即对节点$i$的表示做Layer Norm 为$x_i^\prime = \frac{x_i-m}{\sigma}$, 其中$m$为$x_i$的均值， $\sigma$为$x_i$的标准差。
这里的Transformer不同的是， 先对所有节点做一次MLP，然后直接计算Layer Norm: $$ \overline{\boldsymbol{h}}_{v}^{0}=\operatorname{LayerNorm}\left(\boldsymbol{W}^{\text {Proj }} \boldsymbol{h}_{v}^{L_{\mathrm{GNN}}}\right) $$ 其中$\boldsymbol{W}^{\text {Proj }} \in \mathbb{R}^{d_{\mathrm{TF}} \times d_{L_{\mathrm{GNN}}}}$， 把GNN的输出维度转为TF的输入维度$d_{\mathrm{TF}}$。将所有节点的GNN node embeddings作为Transformer的输入（无positional encoding）。 每个节点的$Q$, $K$和$V$向量分别用$\boldsymbol{W}_{\ell}^{Q}, \boldsymbol{W}_{\ell}^{K}, \boldsymbol{W}_{\ell}^{V} \in \mathbb{R}^{d_{\mathrm{TF}} / n_{\text {head }} \times d_{\mathrm{TF}} / n_{\text {head }}}$计算， 对于第$\ell$层 Transformer, 节点$v$ 的$Q$向量$Q_v = \boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}$和节点$u$的$K$向量$K_u = \boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}$做内积，得到两个节点之间的attention。然后用$\alpha_{v, u}^{\ell}$来为节点$v$聚合其他所有节点的$V$向量$V_u = \boldsymbol{W}_{\ell}^{V} \overline{\boldsymbol{h}}_{u}^{\ell-1}$, 如下所示: $$ a_{v, u}^{\ell}=\left(\boldsymbol{W}_{\ell}^{Q} \overline{\boldsymbol{h}}_{v}^{\ell-1}\right)^{\top}\left(\boldsymbol{W}_{\ell}^{K} \overline{\boldsymbol{h}}_{u}^{\ell-1}\right) / \sqrt{d_{\mathrm{TF}}} \tag{1} $$
...</p></section><footer class=entry-footer><span title='2022-03-30 10:37:41 +0800 +08'>March 30, 2022</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NeurIPS2021 《Representing Long-Range Context for Graph Neural Networks with Global Attention》 Reading Notes" href=https://JhuoW.github.io/posts/graphtrans/></a></article><article class=post-entry><header class=entry-header><h2>NeurIPS2021 《Not All Low-Pass Filters are Robust in Graph Convolutional Networks》 Reading Notes</h2></header><section class=entry-content><p>paper
Introduction 很多GNN易受图结构攻击的影响，本文首先证明了symmetric normalized laplacian的低频分量作为GCN的filter，在某个特征值区间内对于结构扰动更加robust。基于该理论提出GCN-LFR，通过一个辅助神经网络迁移低频分量的robustness。
Q: 对抗扰动边是否会对graph spectrum产生同等的影响？过去的研究显示来自结构攻击的扰动在图谱上表达了一种隐含的趋势。如下图所示，结构扰动后，小的特征值（低频）变化较小， 高频变化较大，即高频对扰动更加敏感。
本文证明了当normalized symmetric laplacian的特征值落于某个特定区间时，低频分量会更加robust。
Methodology Poisoning Attack是指训练前扰动：
Problem 1 （Poisoning Attack）: 给定一个扰动图 $\mathcal{G}^\prime$, 要对目标集合$\mathcal{T}$做对抗防御的目的是设计一个更加鲁棒的模型，使得模型在扰动图上训练后对$\mathcal{T}$中节点的预测结果和在原图上训练得到的预测结果相似： $$ \min_{\boldsymbol{\Theta}^{r *}} \sum_{u \in \mathcal{T}}\left|\left|\mathcal{M}_{u}^{r}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}^{r *}\right)-\mathcal{M}_{u}\left(\boldsymbol{A}, \boldsymbol{X} ; \boldsymbol{\Theta}^{*}\right)\right|\right| $$ 即模型可以尽可能避免扰动对预测带来的影响。 其中$\mathcal{M}_{u}^{r}\left(\boldsymbol{A}^{\prime}, \boldsymbol{X} ; \boldsymbol{\Theta}^{r *}\right)=\hat{\boldsymbol{y}}_{u}^{r}$模型在扰动图上训练过后对节点$u$的预测，$\mathcal{M}_{u}^{r}$是在扰动图上训练过后的模型，最佳参数为$\boldsymbol{\Theta}^{r *}$, $\mathcal{M}$是在原图上训练过后的模型。
观察$\hat{\boldsymbol{A}}$的特征值， 因为$\hat{\boldsymbol{A}} = I - L$, 所以$\hat{\boldsymbol{A}}$的大特征值对应于低频分量，小特征值对应于高频分量。 从图1可以看出$\hat{\boldsymbol{A}}$的大特征值对于结构扰动更加鲁棒, 因为扰动之后大特征值的变化较小。所以GCN-SVD只是用最低频的分量来做defense.
接下来本文证明了只有一条边被扰动的情况下，一定存在低频$\lambda_b$，比所有高频都robust, 鲁棒区间为： $$ \max \left(0, \frac{d_{b}-d_{a}+c_{a} \lambda_{a}}{c_{b}}\right)&lt;\lambda_{b}&lt;\min \left(\frac{d_{b}+d_{a}-c_{a} \lambda_{a}}{c_{b}}, 1\right) $$ 即，当特征值落于这个区间中时，它一定比高频更加鲁棒。
对于Non-targeted Perturbation, 图中有多条边被扰动，那么特征值的鲁棒区间为： $$ \max_{v \in \mathcal{P}_{u}, u \in \mathcal{T}}\left(0, \frac{d_{b u v}-d_{a u v}+c_{a u v} \lambda_{a}}{c_{b u v}}\right)&lt;\lambda_{b}&lt;\min_{v \in \mathcal{P}_{u}, u \in \mathcal{T}}\left(\frac{d_{b u v}+d_{a u v}-c_{a u v} \lambda_{a}}{c_{b u v}}, 1\right) $$ 在得到不同扰动情况下的鲁棒区间后，我们可以基于鲁棒区间来增强GCN的鲁棒性。
...</p></section><footer class=entry-footer><span title='2022-03-29 21:20:31 +0800 +08'>March 29, 2022</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NeurIPS2021 《Not All Low-Pass Filters are Robust in Graph Convolutional Networks》 Reading Notes" href=https://JhuoW.github.io/posts/gcn-lfr/></a></article><article class=post-entry><header class=entry-header><h2>Blog, Tools and Survey</h2></header><section class=entry-content><p>这篇笔记用于收藏别人的博客
Tech Blog Blog Author https://michael-bronstein.medium.com/ Michael Bronstein https://geometricdeeplearning.com/ Michael Bronstein https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c Vitaly Kurin (Many Paper Notes) https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html UvA DL Notebooks https://graph-neural-networks.github.io/index.html GNN Books http://prob140.org/sp17/textbook/ Probability for Data Science class at UC Berkeley https://graphreason.github.io/schedule.html Learning and Reasoning with Graph-Structured Representations ICML 2019 Workshop https://chuxuzhang.github.io/KDD21_Tutorial.html KDD2021 Tutorial: Data Efficient Learning on Graphs http://songcy.net/posts/ Changyue Song (Kernel) https://www.cs.mcgill.ca/~wlh/grl_book/ William L. Hamilton https://kexue.fm/ BoJone https://danielegrattarola.github.io/blog/ Daniele Grattarola (EPFL) https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html Google AI Blog https://zhiyuchen.com/blogs/ Zhiyu Chen https://andreasloukas.blog/ Andreas Loukas (EPFL) https://irhum.pubpub.org/pub/gnn/release/4 Understanding Graph Neural Networks https://lilianweng.github.io/ Lilian Weng https://www.zhihu.com/column/marlin 深度学习与图网络 https://github.com/roboticcam/machine-learning-notes Yida Xu https://www.dgl.ai/pages/index.html DGL https://www.kexinhuang.com/tech-blog Kexin Huang https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8 Rishabh Anand https://saashanair.com/blog Saasha Nair http://www.huaxiaozhuan.com/ 华校专 https://github.com/dglai/WWW20-Hands-on-Tutorial DGL https://blog.csdn.net/CSDNTianJi/article/details/104195306 Meng Liu https://www.chaitjo.com/post/ Chaitanya K. Joshi https://scottfreitas.medium.com/ Scott Freitas https://fabianfuchsml.github.io/ Fabian Fuchs https://medium.com/@pantelis.elinas Pantelis Elinas https://github.com/tianyicui/pack 背包9講 https://www.fenghz.xyz/ https://sakigami-yang.me/2017/08/13/about-kernel-01/ kernel https://davidham3.github.io/blog https://fenghz.github.io/index.html https://archwalker.github.io/ Awesome-Awesomes Repo Name https://github.com/naganandy/graph-based-deep-learning-literature links to conference publications in graph-based deep learning (Very, Very, Very Important) https://github.com/SherylHYX/pytorch_geometric_signed_directed PyTorch Geometric Signed Directed is a signed/directed graph neural network extension library for PyTorch Geometric. https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning Paper Lists for Fair Graph Learning https://github.com/thunlp/PromptPapers Must-read papers on prompt-based tuning for pre-trained language models. https://github.com/zhao-tong/graph-data-augmentation-papers A curated list of graph data augmentation papers. https://github.com/Thinklab-SJTU/ThinkMatch Code & pretrained models of novel deep graph matching methods. https://github.com/FLHonker/Awesome-Knowledge-Distillation Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)。 https://github.com/zlpure/awesome-graph-representation-learning A curated list for awesome graph representation learning resources. https://github.com/basiralab/GNNs-in-Network-Neuroscience A review of papers proposing novel GNN methods with application to brain connectivity published in 2017-2020. https://github.com/flyingdoog/awesome-graph-explainability-papers Papers about explainability of GNNs https://github.com/yuanqidu/awesome-graph-generation A curated list of graph generation papers and resources. https://github.com/benedekrozemberczki/awesome-decision-tree-papers A collection of research papers on decision, classification and regression trees with implementations. https://github.com/AstraZeneca/awesome-explainable-graph-reasoning A collection of research papers and software related to explainability in graph machine learning. https://github.com/LirongWu/awesome-graph-self-supervised-learning Awesome Graph Self-Supervised Learning https://github.com/Chen-Cai-OSU/awesome-equivariant-network Paper list for equivariant neural network https://github.com/mengliu1998/DL4DisassortativeGraphs Papers about developing DL methods on disassortative graphs https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers A curated list of graph reinforcement learning papers. https://github.com/ChandlerBang/awesome-self-supervised-gnn Papers about pretraining and self-supervised learning on Graph Neural Networks (GNN). https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks Paper Lists for Graph Neural Networks https://github.com/jwzhanggy/IFMLab_GNN Graph Neural Network Models from IFM Lab https://github.com/ChandlerBang/awesome-graph-attack-papers Adversarial attacks and defenses on Graph Neural Networks. https://github.com/safe-graph/graph-adversarial-learning-literature A curated list of adversarial attacks and defenses papers on graph-structured data. https://github.com/benedekrozemberczki/awesome-graph-classification A collection of important graph embedding, classification and representation learning papers with implementations. https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers A curated list of gradient boosting research papers with implementations. https://github.com/benedekrozemberczki/awesome-community-detection A curated list of community detection research papers with implementations. https://github.com/giannifranchi/awesome-uncertainty-deeplearning This repository contains a collection of surveys, datasets, papers, and codes, for predictive uncertainty estimation in deep learning models. https://sites.google.com/site/graphmatchingmethods/ Efficient Methods for Graph Matching and MAP Inference https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering Awesome Deep Graph Clustering is a collection of SOTA, novel deep graph clustering methods (papers, codes, and datasets). https://github.com/jwwthu/GNN4Traffic This is the repository for the collection of Graph Neural Network for Traffic Forecasting. https://github.com/zwt233/Awesome-Auto-GNNs A paper collection about automated graph learning https://github.com/chaitjo/awesome-efficient-gnn Efficient Graph Neural Networks - a curated list of papers and projects https://github.com/Radical3-HeZhang/Awesome-Trustworthy-GNNs Awesome Resources on Trustworthy Graph Neural Networks https://github.com/EdisonLeeeee/Awesome-Masked-Autoencoders A collection of literature after or concurrent with Masked Autoencoder (MAE) (Kaiming He el al.). https://github.com/XiaoxiaoMa-MQ/Awesome-Deep-Graph-Anomaly-Detection Awesome graph anomaly detection techniques built based on deep learning frameworks. https://github.com/mengliu1998/awesome-expressive-gnn A collection of papers studying/improving the expressiveness of graph neural networks (GNNs) Useful Repo/Tools Name Info http://acronymify.com/ Model Name https://csacademy.com/app/graph_editor/ Graph Editor https://github.com/guanyingc/python_plot_utils A simple code for plotting figure, colorbar, and cropping with python https://github.com/guanyingc/latex_paper_writing_tips Tips for Writing a Research Paper using LaTeX https://github.com/JhuoW/Pytorch_Program_Templete Pytorch Program Templete GNN https://github.com/graph4ai/graph4nlp Graph4nlp is the library for the easy use of Graph Neural Networks for NLP. Welcome to visit our DLG4NLP website (https://dlg4nlp.github.io/index.html) for various learning resources! https://github.com/benedekrozemberczki/pytorch_geometric_temporal PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models (CIKM 2021) https://github.com/ysig/GraKeL A scikit-learn compatible library for graph kernels https://github.com/jajupmochi/graphkit-learn A python package for graph kernels, graph edit distances, and graph pre-image problem. https://github.com/pliang279/awesome-phd-advice Collection of advice for prospective and current PhD students https://github.com/MLEveryday/100-Days-Of-ML-Code 100-Days-Of-ML-Code中文版 https://github.com/d2l-ai/d2l-zh 《动手学深度学习》 https://github.com/lukas-blecher/LaTeX-OCR pix2tex: Using a ViT to convert images of equations into LaTeX code. https://github.com/thunlp/OpenPrompt An Open-Source Framework for Prompt-Learning. https://github.com/snap-stanford/GraphGym Platform for designing and evaluating Graph Neural Networks (GNN) https://github.com/pygod-team/pygod A Python Library for Graph Outlier Detection (Anomaly Detection) https://github.com/MLNLP-World/Paper_Writing_Tips latex写作建议 https://github.com/dair-ai/ML-YouTube-Courses A place to discover the latest machine learning courses on YouTube. Miscellaneous Name Desc https://github.com/The-Run-Philosophy-Organization/run run学指南 https://10beasts.net/ 测评</p></section><footer class=entry-footer><span title='2022-03-29 11:03:50 +0000 UTC'>March 29, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Blog, Tools and Survey" href=https://JhuoW.github.io/posts/2019-04-28-paper-unscramble/></a></article><article class=post-entry><header class=entry-header><h2>ICLR2020 《Inductive and Unsupervised Representation Learning on Graph Structured Objects》 Reading Notes</h2></header><section class=entry-content><p>Paper
Code
Introduction 无监督图学习算法基于重构损失，不可避免的需要图相似度计算（重构embedding和输入embedding的loss), 计算复杂度较高。本文提出一种通用的归纳式无监督图学习算法SEED（Sampling, Encoding, and Embedding Distributions）。通过计算采样子图的重构损失来代替整个图的重构损失。 即 先采样子图，在用GNN编码子图，最后计算子图分布的embedding来作为整个图的representation. 过程如下图所示：
SEED: Sampling, Encoding, and Embedding Distributions Anonymous Random Walk Definition 1 (Random Anonymous Walks[1]): Given a random walk $\mathbf{w}=(w_1, \cdots, w_l)$ where $\langle w_i, w_{i+1} \rangle \in E$, the anonymous walk for $\mathbf{w}$ is defined as： $$ \mathrm{aw}(\mathbf{w}) = (\mathrm{DIS}(\mathbf{w}, w_1),\mathrm{DIS}(\mathbf{w}, w_2),\cdots, \mathrm{DIS}(\mathbf{w}, w_l) ) $$ where $\mathrm{DIS}(\mathbf{w}, w_i)$ denotes the number of distinct nodes in $\mathbf{w}$ when $w_i$ first appears in $\mathbf{w}$, i.e. $$ \mathrm{DIS}(\mathbf{w}, w_i) = |{w_1, \cdots w_p}|, \quad p = \min_j {w_j=w_i} $$ 匿名随机游走和随机游走的不同在于，匿名随机游走描述了随机游走的潜在“patterns”, 不管具体被访问的节点是什么。 距离来说，给定两条随机游走序列 $\mathbf{w_1}=(v_1, v_2, v_3, v_4, v_2)$ 和$w_2=(v_2, v_1, v_3, v_4, v_1)$, 这两个RW相关联的匿名随机游走是一样的，即$\mathrm{aw}(\mathbf{w_1}) = \mathrm{aw}(\mathbf{w_2}) = (1,2,3,4,2)$, 即使$\mathbf{w_1}$和$\mathbf{w_2}$访问不同的节点。即每个节点在RW中首次被访问时的位置就是这个点在ARW中的id,如在$\mathbf{w_2}$中，$v_1$首次访问是在第二个时刻，那么他的id就是2，在ARW中用2表示。
...</p></section><footer class=entry-footer><span title='2022-03-28 23:44:01 +0800 +08'>March 28, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICLR2020 《Inductive and Unsupervised Representation Learning on Graph Structured Objects》 Reading Notes" href=https://JhuoW.github.io/posts/seed/></a></article><article class=post-entry><header class=entry-header><h2>Awesome Barbell Graph with Networkx</h2></header><section class=entry-content><p>晕了 import networkx as nx
import matplotlib.pyplot as plt
n_clique, n_path = 10, 10
clique1 = nx.complete_graph(n_clique)
clique1_pos = nx.circular_layout(clique1)
clique2 = nx.complete_graph(n_clique)
clique2_mapping = {node: node + n_clique for node in clique2}
nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes
x_diff, y_diff = 8, -1
clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2}
path = nx.path_graph(n_path)
path_mapping = {node: node + 2 * n_clique for node in path}
nx.relabel_nodes(path, path_mapping, copy=False) # avoids repeated nodes
path_nodes = list(path.nodes)
path_half1_nodes = path_nodes[:n_path//2]
path_half2_nodes = path_nodes[n_path//2:]
path_dist = 0.9
clique2_entry = n_clique + n_clique // 2
path_half1_pos = {node: clique1_pos[0] + (path_dist + i * path_dist, 0) for i, node in enumerate(path_half1_nodes)}
path_half2_pos = {node: clique2_pos[clique2_entry] - (path_dist + i * path_dist, 0) for i, node in enumerate(path_half2_nodes[::-1])}
path_pos = {**path_half1_pos, **path_half2_pos}
barbell = nx.Graph()
barbell.add_edges_from(clique1.edges)
barbell.add_edges_from(clique2.edges)
barbell.add_edges_from(path.edges)
barbell.add_edges_from([(path_half1_nodes[0], 0), (path_half2_nodes[-1], clique2_entry)])
clique_pos = {**clique1_pos, **clique2_pos}
barbell_pos = {**clique_pos, **path_pos}
plt.figure(figsize=(20, 6))
nx.draw(barbell, pos=barbell_pos, with_labels=True) ...</p></section><footer class=entry-footer><span title='2022-03-27 15:38:50 +0800 +08'>March 27, 2022</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Awesome Barbell Graph with Networkx" href=https://JhuoW.github.io/posts/barbell_graph/></a></article><article class=post-entry><header class=entry-header><h2>Maximum Mean Discrepancy</h2></header><section class=entry-content><p>Mean Discrepancy (MD)均值差异 判断2个分布$p$ 和$q$是否相同。
$p$分布生成一个样本空间$\mathbb{P}$ (从$p$中采样$m$个样本)
$q$分布生成一个样本空间$\mathbb{Q}$（从$q$中采样$n$个样本）
函数$f$的输入为 分布生成的样本空间
如果 $$ \begin{equation} \begin{aligned} \mathrm{mean}(f(\mathbb{P})) == \mathrm{mean}(f(\mathbb{Q})) \\ i.e., \frac{1}{m}\sum^m_{i=1}f(p_i) = \frac{1}{n}\sum^n_{i=1}f(q_i) \end{aligned} \end{equation} $$
则$p$和$q$是同一分布。
MD can be defined as $$ \begin{equation} \begin{aligned} \mathrm{MD}&=|\mathrm{mean}(f(\mathbb{P})) -\mathrm{mean}(f(\mathbb{Q})) | \\ &= |\frac{1}{m}\sum^m_{i=1}f(p_i) - \frac{1}{n}\sum^n_{i=1}f(q_i)| \end{aligned} \end{equation} $$
Maximum Mean Discrepancy (MMD) 最大均值差异 定义 MMD: 在函数集$\mathcal{F}=\{f_1, f_2, \cdots \}$中， 找到一个函数$f^*$， 使得$|\mathrm{mean}(f^*(\mathbb{P})) -\mathrm{mean}(f^*(\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的最大均值差异（MMD）。MMD =0 表示两个分布相同。 $$ \operatorname{MMD}[\mathcal{F}, p, q]:=\sup _{f \in \mathcal{F}}\left(\mathbf{E}_{x \sim p}[f(x)]-\mathbf{E}_{y \sim q}[f(y)]\right) $$ 其中$\mathbf{E}_{x \sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\sup$为上确界直接理解为max就好。
...</p></section><footer class=entry-footer><span title='2022-03-27 10:45:08 +0800 +08'>March 27, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Maximum Mean Discrepancy" href=https://JhuoW.github.io/posts/mmd/></a></article><article class=post-entry><header class=entry-header><h2>Reproducing Kernel Hilbert Space</h2></header><section class=entry-content><p>Hilbert Space Definition 1 (Norm) Let $\mathcal{F}$ be a vector space over $\mathbb{R}$ (For example $\mathcal{F}=\mathbb{R}^n$ is a vector space). A function $||\cdot||_{\mathcal{F}}: \mathcal{F} \to [0, \inf)$ is said to be a norm on $\mathcal{F}$ if ($||\cdot||_{\mathcal{F}}$ 是一个有效norm算子要满足以下条件):
For $f \in \mathcal{F}$, $||f||_{\mathcal{F}}=0$ if and only if $f=0$. (norm separates points) $|\lambda f|_{\mathcal{F}}=|\lambda||f|_{\mathcal{F}}$, $\forall \lambda \in \mathbb{R}, \forall f \in \mathcal{F}$ (positive homogeneity). $|f+g|_{\mathcal{F}} \leq|f|_{\mathcal{F}}+|g|_{\mathcal{F}}, \forall f, g \in \mathcal{F}$ (triangle inequality). 向$||\cdot||_{\mathcal{F}}$中输入任意一个向量，只要满足以上条件，那么$||\cdot||_{\mathcal{F}}$是一个valid norm operator.
...</p></section><footer class=entry-footer><span title='2022-03-26 22:39:20 +0800 +08'>March 26, 2022</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Reproducing Kernel Hilbert Space" href=https://JhuoW.github.io/posts/rkhs_kernel/></a></article><article class=post-entry><header class=entry-header><h2>Monte Carlo Tree Search</h2></header><section class=entry-content><p>单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。
在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?
多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。
利用（Exploitation）： 保证在过去决策中得到最佳回报
探索（Exploration）：寄希望在未来能够得到更大的汇报
例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。
但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。
悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机
$I_t$: $t$时刻选择的赌博机
$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励
$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward
$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。
$R_n$越大，就代表$n$次决策的结果越差。
上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡
在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}. $$ 其中$I_{t}$为$t$时刻要摇的赌博机，
...</p></section><footer class=entry-footer><span title='2022-03-25 18:09:58 +0800 +08'>March 25, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Monte Carlo Tree Search" href=https://JhuoW.github.io/posts/monte-carlo-tree-search/></a></article><article class=post-entry><header class=entry-header><h2>NIPS2018 《DiffPool:Hierarchical Graph Representation Learning with Differentiable Pooling》 Reading Notes</h2></header><section class=entry-content><p>论文地址： DiffPool
Introduction 传统的GNN算法在Node-level的任务如节点分类、链路预测上有着较好的效果。但是，现有的GNN方法由于其存在平面化的局限性，因此无法学习图的层级表示（意味着无法预测整个图的标签），顾无法实现图分类任务。举个栗子，一个Graph，可以分成600个subgraph，每个节点都存在于其中的某个subgraph（一个节点只存在于一个subgraph中），每个subgraph拥有一个标签，如何预测subgraph的标签是这篇文章主要想解决的问题。传统的GNN的图分类方法都是为Graph中的所有节点生成Embedding，然后将对这些Embedding做全局聚合（池化），如简单的把属于同一个subgraph的节点求和或者输入到MLP中生成一个标签向量来表示整个subgraph，但是这样可能忽略的图的层级结构信息。
本文提出了一种端到端的可微可微图池化模块DiffPool，原理如下图所示：
在深度GNN中的每层中为节点学习可微的软簇分配，将节点映射到簇中，这些簇作为新的节点作为下一层GNN的输入。上图的Original Network部分是一个Subgraph，传统的方法是直接求出这个Subgraph中每个节点的Embedding，然后相加或输入到一个神经网络中，得到一个预测向量，这种方法可以称为“全局池化”。DiffPool中，假设第$l$层的输入是$1000$个簇（如果是第一层输入就是1000个节点），我们先设置第$l+1$层需要输入的簇的个数（假设为$100$），也就是第$l$层输出的簇个数，然后在$l$层中通过一个分配矩阵将$1000$个簇做合并，合并成100个“节点”，然后将这100个节点输入到$l+1$层中，最后图中的节点数逐渐减少，最后，图中的节点只有一个，这个节点的embedding就是整个图的表示，然后将图输入到一个多层感知机MLP中，得到预测向量，在于真值的one-hot向量做cross-entropy，得到Loss。
Model：DiffPool 一个Graph表示为$\mathcal{G} = (A,F)$，其中$A \in {0,1}^{n \times n}$是Graph的邻接矩阵，$F \in \mathbb{R}^{n \times d}$表示节点特征矩阵，每个节点有$d$维的特征。给定一个带标签的子图集$\mathcal{D}=\left\{\left(G_{1}, y_{1}\right),\left(G_{2}, y_{2}\right), \ldots\right\}$， 其中 $y_{i} \in \mathcal{Y}$表示每个子图$G_i \in \mathcal{G}$的标签，任务目标是寻找映射$f: \mathcal{G} \rightarrow \mathcal{Y}$，将图映射到标签集。我们需要一个过程来将每个子图转化为一个有限维度的向量$\mathbb{R}^D$。
Graph Neural Networks 一般，GNN可以表示成"Message Passing"框架： $$ H^{(k)}=M\left(A, H^{(k-1)} ; \theta^{(k)}\right) $$ 其中$H^{(k)} \in \mathbb{R}^{n \times d}$表示GNN迭代$k$次后的node embedding，$M$是一个Message扩散函数，由邻接矩阵$A$和一个可训练的参数$\theta^{(k)}$决定。$H^{(k-1)}$是由前一个message passing过程生成的node embedding。当$k = 1$时，第一个GNN的输入为$H^{(0)}$是原始的节点特征$H^{(0)} = F$。
GNN的一个主要目标是设计一个Message Passage函数$M$，GCN（kipf.2016）是一种流行的GNN，$M$的实现方式是将线性变换和ReLU非线性激活结合起来: $$ H^{(k)}=M\left(A, H^{(k-1)} ; W^{(k)}\right)=\operatorname{ReLU}\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(k-1)} W^{(k-1)}\right) $$ 其中，$\tilde{A} = A+I$是一个加上自环的邻接矩阵，$\tilde{D}=\sum_{j} \tilde{A}_{i j}$是$\tilde{A}$的度矩阵，$W^{(k)} \in \mathbb{R}^{d \times d}$是一个可训练的权重矩阵，$W$与节点个数以及每个节点的度无关，可以看做一个特征增强矩阵，用来规定GCN的输出维度。
...</p></section><footer class=entry-footer><span title='2019-12-19 19:32:36 +0000 UTC'>December 19, 2019</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NIPS2018 《DiffPool:Hierarchical Graph Representation Learning with Differentiable Pooling》 Reading Notes" href=https://JhuoW.github.io/posts/diffpool/></a></article><article class=post-entry><header class=entry-header><h2>Notes for Spectral Clustering</h2></header><section class=entry-content><p>最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。
本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments
Introduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。
基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} > 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\displaystyle \left(\begin{array}{ccc}{d_{1}} & {\ldots} & {\ldots} \\\ {\ldots} & {d_{2}} & {\ldots} \\\ {\vdots} & {\vdots} & {\ddots} \\\ {\ldots} & {\ldots} & {d_{n}}\end{array}\right) ^{n\times n} $$ 是一个$n \times n$的对角阵，对角元素是每个节点的度和。
定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义： $$|A|=A 中的节点个数 $$
$$vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和$$
...</p></section><footer class=entry-footer><span title='2019-09-07 09:11:09 +0000 UTC'>September 7, 2019</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Notes for Spectral Clustering" href=https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://JhuoW.github.io/page/3/>« Prev Page</a>
<a class=next href=https://JhuoW.github.io/page/5/>Next Page »</a></nav></footer></main><footer class=footer><span>Copyright &copy; 2025 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span></span><br><script>function siteTime(){var h=1e3,r=h*60,i=r*60,n=i*24,x=n*365,e=new Date,d=2019,w=1,_=16,y=19,b=15,C=11,l=e.getFullYear(),O=e.getMonth()+1,f=e.getDate(),p=e.getHours(),g=e.getMinutes(),v=e.getSeconds(),m=Date.UTC(d,w,_,y,b,C),j=Date.UTC(l,O,f,p,g,v),t=j-m,o=Math.floor(t/x),s=Math.floor(t/n-o*365),a=Math.floor((t-(o*365+s)*n)/i),c=Math.floor((t-(o*365+s)*n-a*i)/r),u=Math.floor((t-(o*365+s)*n-a*i-c*r)/h);d==l?document.getElementById("sitetime").innerHTML="本站已运行 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒":document.getElementById("sitetime").innerHTML="本站已运行 "+o+" 年 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒"}setInterval(siteTime,1e3)</script><span id=sitetime>载入运行时间...</span>
<script type=text/javascript id=clustrmaps src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=268&t=tt&d=YsONH-MzO6L7yrkA73Z_QW7LuMTfdUhk0uhb_KaBv-g&co=f5f5f5&cmo=3acc3a&cmn=ff5353&ct=808080"></script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>