<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>Posts | JhuoW‘s Notes</title>
<meta name=keywords content><meta name=description content="Posts - JhuoW‘s Notes"><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/posts/><meta name=google-site-verification content="G-6F49SGED6V"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://JhuoW.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://JhuoW.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-6F49SGED6V"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6F49SGED6V")}</script><meta property="og:title" content="Posts"><meta property="og:description" content="Jhuo’s Notes"><meta property="og:type" content="website"><meta property="og:url" content="https://JhuoW.github.io/posts/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Jhuo’s Notes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JhuoW.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="JhuoW's Notes (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>JhuoW's Notes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://JhuoW.github.io/>Home</a></div><h1>Posts</h1></header><article class=post-entry><header class=entry-header><h2>ICML2025《Towards Graph Foundation Models：Learning Generalities Across Graphs via Task-Trees》 Reading Notes</h2></header><section class=entry-content><p>和GFT类似，本文使用task-tree来作为统一不同图任务的学习实例。图中什么样的信息是可以实现跨图泛化的？过去的方法通常从2个角度出发：（1）一些方法利用graphon来作为跨图的可迁移pattern，如果两个图生成自相同的graphon，那么这两个图有相似的结构属性，但是这类方法依赖于强的生成假设，并且从大量图中计算graphon是计算复杂的；（2）可以通过寻找重复出现的motif来作为可迁移的pattern：在不同图中都可用于预测图属性的关键substructure，类似于自然语言中表示积极或者消极的词语就是关键子结构，在不同的句子中都可用于预测句子情感。但MPNN难以捕获这这些子结构。本文提出用task-tree来作为transferable pattern。
定义1（任务相关节点）节点级任务，task-relevant node就是节点本身 $v_i^t$；边级任务的task-relevant node是边的endpoints $\{v_i^t, v_j^t\}$ for target edge $e_{ij}$；对于图级任务，task-relevant nodes是 $\{v_i^t\}^{|V|}_{i=1}$为目标图中的所有节点。
定义2（计算树）对于一个节点 $v$，它的 $L$层计算树 $T^L_v$是以该节点为根节点的 $L$层subtree，其中 $T^1_v =v$。
定义3（Task-Trees）对于一个任务的一组task-relevant nodes ${v_i^t,\ldots,v_n^t}$和这些节点的计算树 $\{T_1, \ldots,T_n\}$。这些计算树可以通过引入连接到所有任务相关节点的虚拟节点构成一个更大的task-tree $T^t$。如下图所示，虚拟节点连接到任务相关的节点上，对于节点分类，目标节点被连接到虚拟节点；边任务上，endpoints被连接到虚拟节点。
对于一个task-tree $T^t$，它由一个虚拟节点 $v^t$和它的任务相关节点 $\{v_i^t,\ldots,v_n^t\}$，通过MEAN aggregator over computation trees来计算这个task-tree的表示：
$$ z^t=\phi\left(T^t\right)=\frac{1}{n} \sum_{i=1}^n \phi\left(T_i\right) $$
所有分类任务均通过任务目标task-tree的表示来做prediction。
GIT与GFT的区别：GFT通过实验证明计算树可以所谓跨任务的可迁移实例；GIT提供了理论基础。
Theoretical Analysis of Task-Trees Stability on Task-Trees 首先分析GNN在学习task-tree表示时的稳定性，如果GNN可以为具有相似subtree的task-tree和生成相似的embedding，那么就说明GNN关于task的输出时稳定的。即task-tree的子树越相似，那么他们的embedding越相似，那么具有相似subtree的task-tree可以用同一个GNN来学习。也就是说task-tree的相似度可以来决定两个任务是否可以用同一个GNN来学习。具体来说，用邻居的平均来表示每层subtree的信息： $\boldsymbol{x}_i^{(l)}=\frac{1}{\left|\left|\mathcal{N}_i\right|\right|} \sum_{j \in \mathcal{N}_i} \boldsymbol{x}_j^{(l-1)}$。给定两个 $L$层的task-tree $T_t^1$和 $T_t^2$，他们的task-relevant nodes分别是 $\{v_1, \ldots,v_n\}$和 $\{v_1, \ldots,v_m\}$，他们可能代表不同的任务，每个task-tree的subtree数量可能也不同，比如节点级任务的subtree和图级任务的subtree数量差别很大。用GNN为task的输出来定义task-tree之间的差异 $\Delta:=\left|\left|\phi\left(T_1^t\right)-\phi\left(T_2^t\right)\right|\right|$，它由如下bound：
$$ \begin{aligned}\Delta & =\left|\left|\phi\left(T_1^t\right)-\phi\left(T_2^t\right)\right|\right|=\left|\left|\frac{1}{n} \sum_{i=1}^n \phi\left(T_i\right)-\frac{1}{m} \sum_{j=1}^m \phi\left(T_j\right)\right|\right| \\ & \leq \frac{1}{n m} \sum_{i=1}^n \sum_{j=1}^m\left(\mathcal{C}_1\left|\left|\boldsymbol{x}_i^{(0)}-\boldsymbol{x}_j^{(0)}\right|\right|+\ldots\right. \\ & \left.+\mathcal{C}_1 \mathcal{C}_2^{L-1}\left|\left|\boldsymbol{x}_i^{(L-1)}-\boldsymbol{x}_j^{(L-1)}\right|\right|\right) \leq 2 \mathcal{B}_{\boldsymbol{x}} \cdot \mathcal{C}_1 \frac{\mathcal{C}_2^L-1}{\mathcal{C}_2-1}\end{aligned} $$
...</p></section><footer class=entry-footer><span title='2025-11-08 13:16:37 +0800 +08'>November 8, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICML2025《Towards Graph Foundation Models：Learning Generalities Across Graphs via Task-Trees》 Reading Notes" href=https://JhuoW.github.io/posts/2025-11-08-task-tree/></a></article><article class=post-entry><header class=entry-header><h2>ICML2025《AutoGFM：Automated Graph Foundation Model with Adaptive Architecture Customization》 Reading Notes</h2></header><section class=entry-content><p>Introduction GFM旨在实现跨领域和任务之间的知识共享来提升图机器学习，但是现有的GFM依赖于手工设计并且fixed的backbone GNN架构，不能根据任务或者领域来自适应调整最优的GNN backbone架构。对于此问题，本文通过发现跨领域和任务的不变“图-架构”关系（invariant graph-architecture relationship）来解决该问题。什么是invariant graph-architecture relationship，就是不管是什么领域，图和架构之间存在一种不变的关系，什么样的图就应该对应于什么样的架构。这会带来3个挑战：
如何捕获invariant和variant的pattern，其中invariant pattern指的是可以用来可靠预测图的架构的pattern，variant pattern指的是图中不能用来稳定预测图架构的pattern。换句话说，invariant pattern指的是图中可以决定架构的固有图结构，可以形成与对应GNN架构之间的一一对应的不变关系，而variant pattern指的是一个可变pattern，他可能对应于各种GNN架构，对于这类pattern，GNN架构是不确定的。 Invariant relationship between graph data and corresponding architecture 指的是一个架构适用于的固有图结构，即一个GNN架构对什么样的图结构一定适用。 如何为不同的领域和任务调整GFM中的GNN架构。 如何减轻架构搜索中的数据支配显现（Data domination）。 AutoGFM的核心思想是训练一个架构映射函数 $\pi: \mathcal{G} \to \mathcal{A}$，用于将图数据映射为特定GNN架构。
Architecture Inconsistency in GFM 当前GFM模型存在一个架构不一致（Architecture Inconsistency）问题，也就是说，不同领域不同任务的最优架构不一致。这边拿GFT这个foundation model为例，它在预训练图上预训练一个GNN模型，然后基于预训练图的embedding来构造一堆词汇表，然后下游任务的图基于这些预训练好的词汇表来作预测。其他GFM模型也都会在预训练阶段预训练好一个GNN模块。这个GNN模块的架构，比如可能是GCN也可能是GAT，是固定的。但这样就存在一个问题，就是下游任务的图不一定适用于这个固定的GNN架构。
Preliminary Problem Formulation TAG：所有图用Text-attributed graph $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{R})$来统一所有图，其中 $\mathcal{R}=r_1, \ldots, r_{|\mathcal{R}|}$是关系数量。
Node of Interest (NOI) Subgraph：用子图分类任务来统一节点级，边级，图级任务。NOI subgraph $G_h$定义为一个在NOI node周围的子图。用 $S_h(v)=\left\{\mathcal{V}_v^h, \mathcal{E}_v^h, \mathcal{R}_v^h\right\}$表示以节点 $v$为中心的 $h$-hop ego-subgraph。
节点级：NOI node 就是节点 $v$本身，因此 $\mathcal{T} = {v}$，NOI graph 表示为 $G_h(\mathcal{T}) = S_h(v)$。
...</p></section><footer class=entry-footer><span title='2025-10-26 13:59:35 +0800 +08'>October 26, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICML2025《AutoGFM：Automated Graph Foundation Model with Adaptive Architecture Customization》 Reading Notes" href=https://JhuoW.github.io/posts/2025-10-26-autogfm/></a></article><article class=post-entry><header class=entry-header><h2>NeurIPS2023《Prodigy：Enabling In-context Learning Over Graphs》 Reading Notes</h2></header><section class=entry-content><p>在llm里面，In-Context Learning就是用一些新任务的问答例子来提示LLM，使得LLM不需要更新参数，也能基于这些例子来对新的任务生成输出。
为LM的预训练任务实际上是next token prediction的任务，由于prompt和回答只是相邻的tokens，因此 Transformer 的自注意力可以计算要补全的句子和之前token之间的关系，从而让模型动态推断出新的决策规则。In-Context Learning实际上也是一种next token prediction的任务，所以In-Context Learning是和预训练任务相关的。
Prodigy和All in One (KDD 2023)是同期工作，目的都是为了去提示Pretrained graph model。 All in One 是已经有一个Pretrained graph model，然后用meta learning基于冻结的pretrained model，去学一个prompt graph，使得这个prompt graph和原图结合起来之后可以使预训练模型泛化到新的任务上。
All in One 存在的问题是，All in one要对测试图做finetune。但实际上我们知道foundation model是不需要fine tune的。遇到一个新任务，因为支持in-context learning，可以基于给出的prompt example 来对新任务直接做出预测。而Prodigy就是不需要fine tune的graph prompt方法，它是直接以in-context learning的方式来预训练模型。Prodigy在预训练图上构造一堆prompt example和queries，预训练的优化目标是使在给定prompt examples和query set的情况下，query set的预测损失最小，也就是说模型的参数由prompt examples来调整，优化目标是使得query set的损失最小。
Prompt Graph Representation 首先要基于预训练的图构造一个支持in-context learning的prompt graph。然后在这个prompt graph上以in-context learning的方式来预训练模型。具体来说，在预训练图上构造一个 $m$-way $k$-shot的分类任务。预训练图为MAG240M，该图中有122M个节点，1.3B条边，153个类。 在每次训练epoch中，采样 $m=30$个class，每个class选择 $k=3$个样本作为prompt examples $\mathcal{S}$，也就是每个epoch有90个prompt examples。每个类别取4个样本作为query set $\mathcal{Q}$。也就是 $\mathcal{S} = \{(x_i, y_i)\}^{mk}_{i = 1}$ 作为预训练阶段的support set。 $\mathcal{Q} = \{x_i\}^n_{i = 1}$作为预训练阶段的query set，也就是预训练阶段的优化目标是要在query set上的损失最小。
...</p></section><footer class=entry-footer><span title='2025-07-28 15:19:07 +0800 +08'>July 28, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NeurIPS2023《Prodigy：Enabling In-context Learning Over Graphs》 Reading Notes" href=https://JhuoW.github.io/posts/prodigy-icl/></a></article><article class=post-entry><header class=entry-header><h2>ICML2024《LLaGA：Large Language and Graph Assistant》 Reading Notes</h2></header><section class=entry-content><p>目的：将图结构调整为适配LLM的输入。具体来说，将图节点表示为图结构感知和属性感知的序列，然后将序列映射到token embedding space中，从而使LLM可以用处理text tokens的方式来对Graph token embeddings进行处理。为了实现这个目标，就要求node sequence必须充分保存中心节点的结构信息。
Structure-Aware Graph Translation LLaGA的目的是将graph翻译成可被LLM理解的token embedding sequence的形式。同时，这可以利用LLM固有的推理能力来处理图的任务，并且无需改变LLM的参数（foundation model）。为了实现这个目标，LLaGA将图结构转化为node embedding sequences，这些sequences融合了图的局部和更大范围的结构信息，然后将node embedding sequences通过一个projector转化为LLM可以处理的token embedding sequence。
第一步是将图转换为node embedding sequences。由于图分析的基本单位是节点，所以本工作开发了2个节点级templates，这些templates是多功能的，不仅可以用于节点级任务，也可以用于边级任务。分别是一个Neighborhood Detail Template，提供对中心节点及其周围环境的深入观察；Hop-Field Overview Template，提供了一个节点邻居的总结视角，可以拓展到更大的域。这两个模板都旨在编码节点周围的结构信息，为分析提供不同的视角。
Neighborhood Detail Template Neighborhood Detail Template用于描述节点和其周围邻居的详细信息。给定一个中心节点 $v$，需要构造一个形状固定的tree。对于中心节点 $v$的每跳邻居，定义一组邻居采样的size： $n_1$, $n_2$, …，其中 $n_i$表示第 $i$跳邻居的采样数量。对于 $1$-hop邻居集合 $\mathcal{N}_v^1$，从其中随机采样 $n_1$个邻居，表示为 $\widetilde{\mathcal{N}}_v^1$。如果 $\left|\mathcal{N}_v^1\right|&lt;n_1$，那么用placeholder nodes来补全 $n_1$个邻居。注意，这里定义的每跳邻居的采样数量 $n_1, n_2, \cdots$是应用于所有节点的。得到的 $v$-centered tree如下图所示：
这里定义 $n_2 = 3$，如果2跳邻居不满3个节点，那么用 placeholder node $[\text{pad}]$来占位。然后我们从tree的中心节点开始遍历，可以把这棵计算树转换为一个固定长度的node sequence。这个node sequence描述了以 $A$为中心的局部邻域内的节点相对结构位置（从近到远）。
上面的步骤将中心节点和它的结构信息编码成一个节点序列，然后我们需要把这个节点序列映射到embedding space中。对于TAG（Text-Attributed Graph），使用现成的语言模型 $\phi$，例如SBERT，RoBERTa或SimTeG来编码文本信息，placeholder node的特征被编码为 $0$向量。然后进一步融合结构信息，每个节点在tree中的结构信息用Laplacian Embedding来表示，也就是拉普拉斯矩阵的特征向量在node id的位置来表示这个node在Tree中的结构信息，也就是相邻的节点有相似的embedding：
...</p></section><footer class=entry-footer><span title='2025-07-28 15:09:11 +0800 +08'>July 28, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICML2024《LLaGA：Large Language and Graph Assistant》 Reading Notes" href=https://JhuoW.github.io/posts/llaga/></a></article><article class=post-entry><header class=entry-header><h2>NeurIPS2024《GFT：Graph Foundation Model with Transferable Tree Vocabulary》 Reading Notes</h2></header><section class=entry-content><p>本文的目的是在预训练图上训练一组通用的图词汇表（vocabulary）作为图上可迁移的通用pattern，这些词汇表可应用于跨领域的多任务中。
GNN的可迁移性 可迁移性指的是模型从source task中提取模式，这些模式的知识可以增强对目标任务的预测。近期的工作通过识别与各种任务相关的关键子结构，来研究图上可迁移的词汇表。例如，三元闭包（triadic closure），同质性，异质性对于节点分类至关重要；一些motif，比如三角形，k-cliques，和star，是图分类的基本组成成分，这些子结构可以作为通用可迁移的pattern，基于这些子结构在不同图中的形式，或者研究图具有怎样的子结构组成，可以推断图在各种任务上的表现。
拿自然语言来类比，positive sub-vocabulary 可以是 [happy, nice, great, … ]；negative sub-vocabulary 可以是[upset, bad, worse, … ]，对于一个句子，只需要用它来检索这个词汇表，就可以对这个句子是积极或者消极做出分类。
对于Graph也是一样，构建一些通用的子结构，下游graph去检索这些子结构，基于子结构的性质来对图的性质做出推理。
计算树（computation tree）作为可迁移的graph pattern Computation Tree：对于一个图 $\mathcal{G}(\mathcal{V}, \mathcal{E})$，关于节点 $v$，它的 $L$ 层计算树为 $\mathcal{T}_v^L$，且 $\mathcal{T}_v^1 = v$。这个tree通过递归融合邻居的子树得到。对于图 $\mathcal{G}$，它的 $L$层子树的集合可以表示为一个multiset： $\mathcal{T}_{\mathcal{G}}^L:=\left\{\mathcal{T}_v^L\right\}_{v \in \mathcal{V}}$ 。
为什么计算树比其他子结构（如motif）更适合做通用的vocabulary？ 首先，节点级、边级、图级任务都可以表示为计算树的分类任务。如下图所示：
计算树可以捕获图中重要的局部子树模式。如果两个节点的 $L$层计算树相似（同时考虑节点和特征），那么表明这两个节点共享相似的邻域信息，则这两个节点具有类似现象（analogous phenomena）。
两个具有相似计算树的图，是否以为这模型在这两个图上的迁移性更好？（Yes）
两个图具有相似的其他子结构（如motif），并不一定意味着两个图之间模型的迁移性更好
Theorem 2.2：
$$ \Delta \triangleq\left|\left|\phi\left(\mathcal{T}_{v_1}\right)-\phi\left(\mathcal{T}_{v_2}\right)\right|\right|_2 \leq \mathcal{C}_1 \left| \left| \mathbf{x}_{v_1}-\mathbf{x}_{v_2}\right|\right|_2+\mathcal{C}_2 \sum_{j \in \mathcal{N}(v)} \Delta_{v_1, v_2, j}^{L-1} \leq 2 \mathcal{B}_{\mathbf{x}}\left(\mathcal{C}_1+\sum_{l=1}^L \mathcal{C}_2^l D_l\right) $$
...</p></section><footer class=entry-footer><span title='2025-07-28 14:49:00 +0800 +08'>July 28, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NeurIPS2024《GFT：Graph Foundation Model with Transferable Tree Vocabulary》 Reading Notes" href=https://JhuoW.github.io/posts/gft/></a></article><article class=post-entry><header class=entry-header><h2>ICLR2025《GOFA：A Generative One-For-All Model for Joint Graph Language Modeling》 Reading Notes</h2></header><section class=entry-content><p>GOFA是OFA（ICLR 2024）的后续工作，用来解决OFA中存在的一些问题。
OFA用NOI Prompt Node来统一不同的分类任务，也就是用不同的NOI Prompt Node来表示不同的分类任务，然后用任务的文本描述作为这些表示任务的节点的文本特征，LLM 编码来数值化这个节点的初始特征，任务对应的label也用节点（Class Node）来表示，然后用每个class的文本描述作为这个Class Node的特征。GNN的训练目标就是训练NOI Prompt node的embedding 和class node的embedding，对于某个节点，如果我们要预测它关于某个任务的标签，只需要把它和这个任务对应的NOI Prompt Node连接起来，根据输出就可以知道这个节点在该任务下的标签。
但是，OFA存在以下问题：必须是已知任务，无法泛化到新的任务，因为新的任务NOI Prompt Node 未知，也不知道新任务的Class Node是什么， 因此下游任务必须得是预训练阶段见过的任务，不能是一个全新的任务。此外，可以看出由于OFA是一个supervised foundation model，所以他和参与训练的任务标签是强相关的，很难泛化到训练阶段没有见过的任务，GOFA就是用来解决OFA存在的问题，GOFA认为foundation model的训练过程应该不能有标签信息加入的，也就是需要是自监督的。
Unified Task Formats 模型的输入必须要统一，所以和过去的很多方法一样，GOFA用TAG来统一所有graph：
$$ G=\left\{V, E, X_V, X_E\right\} $$
自监督语言模型里面，模型训练的目标叫next-token prediction 通常是补全句子，比如给定一个句子，自监督语言模型的训练目标是基于这个句子来预测生下来的tokens，假设剩下来的token是apple，参数优化的目标就是使生成apple的likelihood要最大， 如下图所示：
GOFA提出的自监督图模型，也继承这种做法，模型学习的目标是补全TAG图，有个目标节点，叫node of generation，也就是自监督学习要补全的节点，把它的文本属性截断，然后自监督学习的目标就是要生成这个NOG node 的剩下内容。但是，如果要生成这个节点剩下的内容的话，不能无视这种图中的边信息，因为图的结构可以帮助这个自监督任务生成正确的信息，所以自监督图模型需要充分理解图结构。比如上图中的Target就需要在理解边关系的情况下补全句子，因此，模型需要充分学习图结构。
GOFA: Generative One-For-All Model 怎么才能在充分学习图结构的情况下，补全目标节点的text，是GOFA的主要目标。
GOFA主要由2个模块组成，一个是Graph Language Encoder，还有一个是LLM Decoder。其中Graph Language Encoder的目的是用来学习节点的表示向量， 其中交替训练了LLM compressor和GNNlayer，LLM compressor用来学习TAG graph中的语义信息，GNN用来学习结构关系信息（也就是NOG Node和其他节点的交互）。因为GNN已经可以学到图的结构信息了，LLM compressor的作用就是把文本信息压缩到GNN学习的节点表示中。 第二个模块是一个LLM decoder，它的训练目标是，基于节点的向量表示，预测下一个token。
Graph Language Encoder Graph Language Encoder 包含LLM compressors和GNN Layers。对于一个NOG Node的text attribute，若它由 $l$个tokens组成 $\{x_i\}_{i = 1}^l$，通过一个pretrained LLM 比如Mistral-7B，可以将每个token映射为一个token embedding $q(x_i)$。那么将NOG Node的所有token $\left\{q\left(x_i\right)\right\}_{i=1}^l$ pooling成一个向量可以作为NOG Node的node feature vector。但是这种做法丢失太多semantic information。
...</p></section><footer class=entry-footer><span title='2025-07-28 14:35:16 +0800 +08'>July 28, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICLR2025《GOFA：A Generative One-For-All Model for Joint Graph Language Modeling》 Reading Notes" href=https://JhuoW.github.io/posts/gofa/></a></article><article class=post-entry><header class=entry-header><h2>Fair Graph Learning</h2></header><section class=entry-content><p>Overview The following two works reduce prediction discrimination by optimizing adjacency matrices, which can improve fairness for link prediction tasks:
On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections （ICLR 2021） All of the Fairness for Edge Prediction with Optimal Transport (AISTATS 2021) 通过修改原图的敏感属性，使用对比学习来实现模型对敏感属性的鲁棒性，即敏感属性的修改不会影响模型的输出:
Towards a Unified Framework for Fair and Stable Graph Representation Learning (UAI 2021) 使用对抗训练策略来增强图，使得增强图与敏感属性的关系（MI）最小，基于增强图训练的representation可以实现fairness:
Learning Fair Graph Representations via Automated Data Augmentations (ICLR 2023) 证明了message passing的neighbor aggregation会使得拓扑偏差积累到node representation中，在GNN的signal denoising优化框架中加入fairness regularization，使得学习到的节点表示向量要满足，不同敏感group具有相同的期望logits:
...</p></section><footer class=entry-footer><span title='2025-05-21 12:13:13 +0800 +08'>May 21, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to Fair Graph Learning" href=https://JhuoW.github.io/posts/fairnessgnn/></a></article><article class=post-entry><header class=entry-header><h2>ICLR2024《One for All：Towards Training One Graph Model for All Classification Tasks》 Reading Notes</h2></header><section class=entry-content><p>What is in-context learning?
“In-Context Learning is a way to use LLMs to learn tasks given only a few examples. During in-context learning, we give the LM a prompt that consists of a list of input-output pairs that demonstrate how to perform a task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens. For example, to answer the two prompts below, the model needs to examine the training examples to figure out the input distribution (financial or general news), output distribution (Positive/Negative or topic), input-output mapping (sentiment or topic classification), and the formatting. “
...</p></section><footer class=entry-footer><span title='2025-05-21 11:57:35 +0800 +08'>May 21, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to ICLR2024《One for All：Towards Training One Graph Model for All Classification Tasks》 Reading Notes" href=https://JhuoW.github.io/posts/ofa/></a></article><article class=post-entry><header class=entry-header><h2>KDD2023《All in One：Multi-Task Prompting for Graph Neural Networks》 Reading Notes</h2></header><section class=entry-content><p>图神经网络的预训练任务和下游任务之间可能存在较大gap，直接将预训练模型应用在下游任务上可能会产生负迁移现象（“negative transfer”）。例如，binary edge prediction经常用于pretrain graph model。这样的预训练模型使得有边连接的节点在representation space中接近。但是下游任务可能是node-level 或graph-level tasks，下游的任务如果是节点分类任务，那么预训练模型需要针对额外的节点类别标签搜索更高维度的参数空间。如果图中相连节点的类别不同（heterophilic），那么基于edge prediction pretrained的模型会对下游任务参数负面效果。
为了解决上述问题，一个潜在的方向是将“pretraining and fine-tuning”拓展为“pretraining, prompting, and fine-tuning”。例如在自然语言处理中，如果要赋予预训练语言模型预测句子情感的能力（sentiment analysis），可以通过prompt来完成，而不需要优化pretrained model。
以上图为例，对于一个fronzen LLM（参数固定），如果要为这个模型赋予情感分析的能力，我们可以额外训练一个最佳的prompt，训练数据为prompt parameters，要求这个prompt tokens在tasker $\phi$的优化下，生成的下一个token是正确的情感（label为excited）。即训练得到一个最佳的prompt tokens，比如训练得到的最佳prompt tokens是“I feel so [mask]”，使得frozen LLM应用在“KDD2023 will witness many high-quality papers. I feel so ” 这个句子上时，可以将下一个词预测为情感词，这样LLM在输入包含prompt tokens的情况下，可以具备预测句子情感的能力。也就是说， Prompt Learning的目的是训练得到一堆tokens，使得这些tokens与原来的context拼起来可以使得LLM具备新的能力。
这篇文章的目的是在图上做Prompt Learning，也就是在训练一个prompt graph，使得现有图拼上这个prompt graph后，预训练的GNN可以在新的任务上（预训练阶段没有接触过的任务上）也表现的较好。 但是在graph上做Prompt Learning存在以下挑战：
自然语言处理中，prompt tokens是一个一维线性的句子，可以放在content的开头或结尾，但是在graph中，节点是非欧结构，因此如何组织prompt tokens，以及如何将graph prompts与input graph结合是一个挑战。 在自然语言处理中，类似于情感分析，和问答任务，这些任务都可以简单的重构为next token prediction（单词预测）的任务，所以只需要使用单词预测来训练prompt就可以。但是在图中，节点级任务、边级任务和图级任务难以统一成一种形式。因此如何将各种prompt任务统一来训练graph prompt也是一个挑战。 训练好prompt token的向量化信息、连接结构、以及插入到原图的方式，然后Frozen Pretrained Model应用在这个combined graph上后，就可以为Pretrained Model赋予处理新任务的能力。
Reformulating Downstream Tasks 将节点级和边级的下游任务统一为induced graph的标签预测问题。
...</p></section><footer class=entry-footer><span title='2025-05-20 14:22:08 +0800 +08'>May 20, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to KDD2023《All in One：Multi-Task Prompting for Graph Neural Networks》 Reading Notes" href=https://JhuoW.github.io/posts/allinone/></a></article><article class=post-entry><header class=entry-header><h2>NeurIPS2023《Evaluating GNN Performance On Unseen Graphs Without Labels》 Reading Notes</h2></header><section class=entry-content><p>Paper
目的：在一个图上训练好的GNN，在未知的testing graph上的结果由于training和test数据分布的不同，可能存在很大的不确定性。通常来说，in-service的GNN在已知graph with labels的图上训练好后，需要部署在label未知的testing graph上，但是由于label未知，无法估计GNN在testing graph上的效果。
如上图所示，对于一个已经在训练图 $\mathcal{S}=(\mathbf{X}, \mathbf{A}, \mathbf{Y})$上well-trained & fixed GNN model $\mathrm{GNN}_{\mathcal{S}}^\star$，并且将它deploy in service。对于一个不知道label的测试图 $\mathcal{T}$，由于不知道label，如何评估GNN在该测试图上的性能是一个挑战。由于可见的训练graph和不可见的test graph的分布差异可能很大，因此GNN评估器GNNEvaluator需要充分学习多样化的图结构with diverse node context and graph structure distributions，从而可以评估不同数据分布的测试图潜在的效果。
假设：Covariate shift between the training graph $\mathcal{S}$ and the label-unlabeled graph $\mathcal{T}$ with respect to the label space。 即无论输入图是什么样的，输出的label space相同。
如何生成数量足够的graph set来训练GNNEvaluator $f_\theta$? Solution：采样一个seed subgraph $\mathcal{S}_{seed}$ from the observed training graph $\mathcal{S}$。采样原则是seed graph 要和 training graph之间满足相同的label space，所以原图中采样可以使采样图$\mathcal{S}_{seed}$的分布尽可能少的偏离原图，从而满足Covariate shift。如下图左边所示。
在得到采样seed graph $\mathcal{S}_{seed}$后，对 $\mathcal{S}_{seed}$做 $K$次增强，其中涉及的增强包括 $\texttt{EdgeDrop}$， $\texttt{NodeDrop(Subgraph)}$， $\texttt{AttrMask}$和 $\texttt{NodeMix}$。选择哪种增强有特定的概率 $\epsilon$。基于seed graph $\mathcal{S}_{seed}$，可以得到一个 meta-graph集合 $\mathcal{G}_{\text{meta}}=\left\{g_{\text {meta }}^i\right\}_{i=1}^K$，其中的每个meta-graph都是由seed graph 扰动而来，和原图具有相同的label space。每个meta-graph $g_{\text {meta }}^i=\left\{\mathbf{X}_{\text {meta }}^i, \mathbf{A}_{\text {meta }}^i, \mathbf{Y}_{\text {meta }}^i\right\}$，分别表示meta-graph的节点特征，结构和标签。通过这种方式可以为原图生成label space相同，但结构/特征都不相同图，拓展了差异性，基于这些图学习到的GNNEvaluator可以评估不同分布图的效果。
...</p></section><footer class=entry-footer><span title='2025-05-20 13:45:31 +0800 +08'>May 20, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</footer><a class=entry-link aria-label="post link to NeurIPS2023《Evaluating GNN Performance On Unseen Graphs Without Labels》 Reading Notes" href=https://JhuoW.github.io/posts/gnn-evaluator/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://JhuoW.github.io/posts/page/2/>Next Page »</a></nav></footer></main><footer class=footer><span>Copyright &copy; 2025 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span></span><br><script>function siteTime(){var h=1e3,r=h*60,i=r*60,n=i*24,x=n*365,e=new Date,d=2019,w=1,_=16,y=19,b=15,C=11,l=e.getFullYear(),O=e.getMonth()+1,f=e.getDate(),p=e.getHours(),g=e.getMinutes(),v=e.getSeconds(),m=Date.UTC(d,w,_,y,b,C),j=Date.UTC(l,O,f,p,g,v),t=j-m,o=Math.floor(t/x),s=Math.floor(t/n-o*365),a=Math.floor((t-(o*365+s)*n)/i),c=Math.floor((t-(o*365+s)*n-a*i)/r),u=Math.floor((t-(o*365+s)*n-a*i-c*r)/h);d==l?document.getElementById("sitetime").innerHTML="本站已运行 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒":document.getElementById("sitetime").innerHTML="本站已运行 "+o+" 年 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒"}setInterval(siteTime,1e3)</script><span id=sitetime>载入运行时间...</span>
<script type=text/javascript id=clustrmaps src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=268&t=tt&d=YsONH-MzO6L7yrkA73Z_QW7LuMTfdUhk0uhb_KaBv-g&co=f5f5f5&cmo=3acc3a&cmn=ff5353&ct=808080"></script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>