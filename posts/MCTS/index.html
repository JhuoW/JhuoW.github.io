<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var t=MathJax.Hub.getAllJax(),e;for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>Monte Carlo Tree Search (MCTS) | JhuoW‘s Notes</title><meta name=keywords content="MCTS"><meta name=description content="单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。
在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?
多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。
  利用（Exploitation）： 保证在过去决策中得到最佳回报
  探索（Exploration）：寄希望在未来能够得到更大的汇报
  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。
但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。
悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机
$I_t$: $t$时刻选择的赌博机
$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励
$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward
$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。
$R_n$越大，就代表$n$次决策的结果越差。
上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡
在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}."><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/posts/mcts/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Monte Carlo Tree Search (MCTS)"><meta property="og:description" content="单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。
在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?
多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。
  利用（Exploitation）： 保证在过去决策中得到最佳回报
  探索（Exploration）：寄希望在未来能够得到更大的汇报
  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。
但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。
悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机
$I_t$: $t$时刻选择的赌博机
$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励
$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward
$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。
$R_n$越大，就代表$n$次决策的结果越差。
上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡
在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}."><meta property="og:type" content="article"><meta property="og:url" content="https://JhuoW.github.io/posts/mcts/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-30T18:09:58+08:00"><meta property="article:modified_time" content="2022-03-30T18:09:58+08:00"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Monte Carlo Tree Search (MCTS)"><meta name=twitter:description content="单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。
在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?
多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。
  利用（Exploitation）： 保证在过去决策中得到最佳回报
  探索（Exploration）：寄希望在未来能够得到更大的汇报
  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。
但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。
悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机
$I_t$: $t$时刻选择的赌博机
$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励
$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward
$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。
$R_n$越大，就代表$n$次决策的结果越差。
上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡
在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JhuoW.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Monte Carlo Tree Search (MCTS)","item":"https://JhuoW.github.io/posts/mcts/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Monte Carlo Tree Search (MCTS)","name":"Monte Carlo Tree Search (MCTS)","description":"单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。\n在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?\n多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。\n  利用（Exploitation）： 保证在过去决策中得到最佳回报\n  探索（Exploration）：寄希望在未来能够得到更大的汇报\n  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。\n但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。\n悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \\cdots (i = 1,\\cdots, k)$。 在时刻$t=1,2,\\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \\max_{i=1,\\cdots,k} \\sum^n_{t = 1} X_{i,t}-\\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机\n$I_t$: $t$时刻选择的赌博机\n$\\max_{i=1,\\cdots,k} \\sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励\n$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward\n$\\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。\n$R_n$越大，就代表$n$次决策的结果越差。\n上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡\n在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\\max_{i \\in{1, \\ldots, k}}\\left\\{\\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \\right\\}.","keywords":["MCTS"],"articleBody":"单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。\n在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?\n多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。\n  利用（Exploitation）： 保证在过去决策中得到最佳回报\n  探索（Exploration）：寄希望在未来能够得到更大的汇报\n  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。\n但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。\n悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \\cdots (i = 1,\\cdots, k)$。 在时刻$t=1,2,\\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \\max_{i=1,\\cdots,k} \\sum^n_{t = 1} X_{i,t}-\\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机\n$I_t$: $t$时刻选择的赌博机\n$\\max_{i=1,\\cdots,k} \\sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励\n$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward\n$\\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。\n$R_n$越大，就代表$n$次决策的结果越差。\n上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡\n在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\\max_{i \\in{1, \\ldots, k}}\\left\\{\\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \\right\\}. $$ 其中$I_{t}$为$t$时刻要摇的赌博机，\n$\\overline{X_{i, T_{i}(t-1)}}$要尽可能大：为$t$时刻之前赌博机$i$的平均reward 要尽可能大，\n$C_{t-1, T_{i}(t-1)}$要尽可能小：$t$时刻之前$i$出现的次数要尽可能少\n其中$C_{t,T_i(t)}$的取值定义如下： $$ C_{t,T_i(t)}=\\sqrt{\\frac{2 \\operatorname{In} t}{T_i(t)}} $$ 其中$T_i(t)$表示 $t$时刻以前（包括$t$），选到赌博机$i$的次数总和。\n若赌博机在$t$时刻之前（包括$t$）时刻摇动的次数越少，$C_{t,T_i(t)}$越大\n选出的$I_t$要满足在$t$之前的平均reward尽可能大（利用）， 且在$t$之前出现次数尽可能少（探索）。\n也就是说，在第时刻，UCB算法一般会选择具有如下最大值的第$j$个赌博机： $$ \\begin{aligned} U C B\u0026=\\bar{X}_{j}+\\sqrt{\\frac{2 \\operatorname{Inn}}{n_{j}}} \\text { 或者 } U C B=\\bar{X}_{j}+C \\times \\sqrt{\\frac{2 \\operatorname{In} n}{n_{j}}} \\\\ I_t \u0026= \\mathrm{argmax}_j UCB(j) \\end{aligned} $$ 其中$\\bar{X}_{j}$是第$j$个赌博机在过去时间内所获得的平均奖赏值，$n_j$是在过去时间内拉动第$j$个赌博机臂膀的总次数，$n$是过去时间内拉动所有赌博机臂膀的总次数。$C$是一个平衡因子，其决定着在选择时偏重探索还是利用。\n从这里可看出UCB算法如何在探索-利用(exploration-exploitation)之间寻找平衡：既需要拉动在过去时间内获得最大平均奖赏的赌博机，又希望去选择拉动臂膀次数最少的赌博机。\nMonte Carlo Tree Search MCTS has four step:\n Selection 选择 Expansion 拓展 Simulation（rollout) 模拟 Backpropagation 回溯  选择 从根节点R开始，向下递归选择子节点，直至选择一个叶子节点L。 具体来说，通常用UCBl(Upper Confidence Bound,上限置信区间)选择最具“潜力”的后续节点： $$ U C B=\\bar{X}_{j}+\\sqrt{\\frac{2 \\operatorname{In} n}{n_{j}}} $$\n拓展 如果L不是一个终止节点（即博弈游戏不)，则随机创建其后的一个未被访问节点，选择该节点作为后续子节点C。\n模拟 从节点C出发，对游戏进行模拟，直到博弈游戏结束。\n反向传播 用模拟所得结果来回溯更新导致这个结果的每个节点中获胜次数和访问次数。\n包含两种策略学习机制：\n搜索树策略：从已有的搜索树中选择或创建一个叶子结点（即蒙特卡洛中选择和拓展两个步骤).搜索树策略需要在利用和探索之间保持平衡。\n模拟策略：从非叶子结点出发模拟游戏，得到游戏仿真结果。\n例子： 围棋   以围棋为例，假设根节点是执黑棋方。\n  图中每一个节点都代表一个局面，每一个局面记录两个值A/B\n  A: 该局面被访问中黑棋胜利次数。对于黑棋表示己方胜利次数，对于白棋表示己方失败次数（对方胜利次数)；\nB: 该局面被访问的总次数\n初始状态：根节点（12/21）是当前局面，该局面被访问过21次，其中黑棋访问该局面后最终获胜12次。 黑执棋方遇到该局面（根节点局面）开始选择下一步action。\n选择 黑执棋方遇到（12/21）这个局面时有3个有效action, 黑执行这三个action会得到3种局面（7/10）,（5/8）, (0/3). 黑执棋方要选择其中一个action，是的棋局变为这三种中的一种，那么要如何选择action呢？我们先分别计算UCB1：\n左一： 7/10对应的局面Reward为： $$ \\frac{7}{10} + \\sqrt{\\frac{\\log (21)}{10}} = 1.252 $$ 3中局面共21次被模拟到，其中，该局面被访问过10次（探索， 第二项）， 黑棋遇到该局面后最终获胜7次（利用， 第一项）。\n左二：（5/8）对应局面Reward: $$ \\frac{5}{8} + \\sqrt{\\frac{\\log(21)}{8}} = 1.243 $$ 左三： （0/3）对应局面Reward: $$ \\frac{0}{3} + \\sqrt{\\frac{\\log(21)}{3}} = 1.007 $$ 由此可见，黑棋选择会导致局面（7/10）的action进行走琪。\n白棋遇到局面（7/10）时，有两个有效的action, 分别会导致局面A/B = 2/4和5/6, A为白棋访问该局面后失败的次数，所以白棋访问该局面最终胜利的次数应该是$B-A$：\n左一： (2/4)对应的局面Reward (白棋尽可能获胜)为： $$ (1-\\frac{2}{4}) + \\sqrt{\\frac{\\log(21)}{4}}=1.372 $$ 左二： (5/6)对应的局面Reward为： $$ (1-\\frac{5}{6}) + \\sqrt{\\frac{\\log(21)}{4}}=0.879 $$ 因此白棋会选择（2/4）局面\n即每一步都寻找最佳应对方式，来最终评估更节点局面的好坏\n白色执棋方选择action后到达局面（2/4）, 黑棋执棋方面对该局面，有两种action可选，分别会到达（1/3）和（1/1）， 分别计算UCB1 得：\n左一： (1/3)对应reward 为： $$ \\frac{1}{3} + \\sqrt{\\frac{\\log (21)}{3}} = 1.341 $$ 左二：(1/1)对应reward为： $$ \\frac{1}{1} + \\sqrt{\\frac{\\log (21)}{1}} = 2.745 $$ 则黑棋选择action 使得局面变为(1/1)。右图中可见(1/1)为叶子节点， 白棋面对该局面暂时无候选局面（action）,则进入下一个步骤，拓展。\n拓展 白棋执棋方面对局面(1/1),因为已经是叶子节点， 所以(1/1)会随机产生一个未被访问过的新节点，初始化为(0/0),接着在该节点下进行模拟。 进入第三部， 模拟。\n模拟 黑棋执棋方面局面（0/0），开始进行游戏仿真， 假设经过一些列仿真后，最终白棋获胜，即从更节点到最终模拟结束的叶子节点黑棋失败了。 进入第四部，回溯\n回溯 根据仿真结果来更新该仿真路径上的每个节点的A/B值。 因为该次模拟最终白棋获胜，所以向上回溯路径上的所有父节点，父节点的A不变，B+1。 若最终黑棋获胜，则父节点的A+1, B+1。\n在有限时间里， MCTS会不断重复4个步骤，所有节点的A,B 值会不断变化\n到某个局面（节点）后如何下棋（action）, 会选择所有有效Action中UCB1值最大的节点，作为下棋的下一步。\n","wordCount":"267","inLanguage":"en","datePublished":"2022-03-30T18:09:58+08:00","dateModified":"2022-03-30T18:09:58+08:00","author":{"@type":"Person","name":"JhuoW"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://JhuoW.github.io/posts/mcts/"},"publisher":{"@type":"Organization","name":"JhuoW‘s Notes","logo":{"@type":"ImageObject","url":"https://JhuoW.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="Jhuo (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>Jhuo</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://github.com/996icu/996.ICU title=996.ICU><span>996.ICU</span></a></li><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://JhuoW.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://JhuoW.github.io/posts/>Posts</a></div><h1 class=post-title>Monte Carlo Tree Search (MCTS)</h1><div class=post-meta><span title="2022-03-30 18:09:58 +0800 CST">March 30, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;JhuoW</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%8d%95%e4%b8%80%e7%8a%b6%e6%80%81monte-carlo%e8%a7%84%e5%88%92%e5%a4%9a%e8%87%82%e8%b5%8c%e5%8d%9a%e6%9c%bamulti-armed-bandits aria-label="单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits）">单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits）</a><ul><li><a href=#%e6%82%94%e5%80%bc%e5%87%bd%e6%95%b0 aria-label=悔值函数>悔值函数</a></li><li><a href=#%e4%b8%8a%e7%bd%ae%e4%bf%a1%e5%8c%ba%e9%97%b4upper-confidence-bound-ucb aria-label="上置信区间（Upper Confidence Bound, UCB）">上置信区间（Upper Confidence Bound, UCB）</a></li></ul></li><li><a href=#monte-carlo-tree-search aria-label="Monte Carlo Tree Search">Monte Carlo Tree Search</a><ul><li><a href=#%e9%80%89%e6%8b%a9 aria-label=选择>选择</a></li><li><a href=#%e6%8b%93%e5%b1%95 aria-label=拓展>拓展</a></li><li><a href=#%e6%a8%a1%e6%8b%9f aria-label=模拟>模拟</a></li><li><a href=#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad aria-label=反向传播>反向传播</a></li></ul></li><li><a href=#%e4%be%8b%e5%ad%90-%e5%9b%b4%e6%a3%8b aria-label="例子： 围棋">例子： 围棋</a><ul><ul><li><a href=#%e9%80%89%e6%8b%a9-1 aria-label=选择>选择</a></li><li><a href=#%e6%8b%93%e5%b1%95-1 aria-label=拓展>拓展</a></li><li><a href=#%e6%a8%a1%e6%8b%9f-1 aria-label=模拟>模拟</a></li><li><a href=#%e5%9b%9e%e6%ba%af aria-label=回溯>回溯</a></li></ul></ul></li></ul></div></details></div><div class=post-content><h1 id=单一状态monte-carlo规划多臂赌博机multi-armed-bandits>单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits）<a hidden class=anchor aria-hidden=true href=#单一状态monte-carlo规划多臂赌博机multi-armed-bandits>#</a></h1><p>单一状态$s$, $k$种action（$k$个摇臂）。</p><p>在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?</p><p>多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。</p><ul><li><p>利用（Exploitation）： 保证在过去决策中得到最佳回报</p></li><li><p>探索（Exploration）：寄希望在未来能够得到更大的汇报</p></li></ul><p>例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。</p><p>但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。</p><h2 id=悔值函数>悔值函数<a hidden class=anchor aria-hidden=true href=#悔值函数>#</a></h2><p>如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \cdots (i = 1,\cdots, k)$。 在时刻$t=1,2,\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\cdots,I_n$后，可如下定义悔值函数：
$$
R_n = \max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}-\sum^n_{t=1}X_{I_t, t}
$$
$i$: 第$i$个赌博机</p><p>$I_t$: $t$时刻选择的赌博机</p><p>$\max_{i=1,\cdots,k} \sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励</p><p>$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward</p><p>$\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。</p><p>$R_n$越大，就代表$n$次决策的结果越差。</p><h2 id=上置信区间upper-confidence-bound-ucb>上置信区间（Upper Confidence Bound, UCB）<a hidden class=anchor aria-hidden=true href=#上置信区间upper-confidence-bound-ucb>#</a></h2><p>UCB旨在探索和利用间去的平衡</p><p>在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机：
$$
I_{t}=\max_{i \in{1, \ldots, k}}\left\{\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \right\}.
$$
其中$I_{t}$为$t$时刻要摇的赌博机，</p><p>$\overline{X_{i, T_{i}(t-1)}}$要尽可能大：为$t$时刻之前赌博机$i$的平均reward 要尽可能大，</p><p>$C_{t-1, T_{i}(t-1)}$要尽可能小：$t$时刻之前$i$出现的次数要尽可能少</p><p>其中$C_{t,T_i(t)}$的取值定义如下：
$$
C_{t,T_i(t)}=\sqrt{\frac{2 \operatorname{In} t}{T_i(t)}}
$$
其中$T_i(t)$表示 $t$时刻以前（包括$t$），选到赌博机$i$的次数总和。</p><p>若赌博机在$t$时刻之前（包括$t$）时刻摇动的次数越少，$C_{t,T_i(t)}$越大</p><p>选出的$I_t$要满足在$t$之前的平均reward尽可能大（利用）， 且在$t$之前出现次数尽可能少（探索）。</p><p>也就是说，在第时刻，UCB算法一般会选择具有如下最大值的第$j$个赌博机：
$$
\begin{aligned}
U C B&=\bar{X}_{j}+\sqrt{\frac{2 \operatorname{Inn}}{n_{j}}} \text { 或者 } U C B=\bar{X}_{j}+C \times \sqrt{\frac{2 \operatorname{In} n}{n_{j}}} \\
I_t &= \mathrm{argmax}_j UCB(j)
\end{aligned}
$$
其中$\bar{X}_{j}$是第$j$个赌博机在过去时间内所获得的平均奖赏值，$n_j$是在过去时间内拉动第$j$个赌博机臂膀的总次数，$n$是过去时间内拉动所有赌博机臂膀的总次数。$C$是一个平衡因子，其决定着在选择时偏重探索还是利用。</p><p>从这里可看出UCB算法如何在探索-利用(exploration-exploitation)之间寻找平衡：既需要拉动在过去时间内获得最大平均奖赏的赌博机，又希望去选择拉动臂膀次数最少的赌博机。</p><h1 id=monte-carlo-tree-search>Monte Carlo Tree Search<a hidden class=anchor aria-hidden=true href=#monte-carlo-tree-search>#</a></h1><p>MCTS has four step:</p><ul><li>Selection 选择</li><li>Expansion 拓展</li><li>Simulation（rollout) 模拟</li><li>Backpropagation 回溯</li></ul><p><img loading=lazy src=/posts/MCTS/1.png#center alt></p><h2 id=选择>选择<a hidden class=anchor aria-hidden=true href=#选择>#</a></h2><p>从根节点R开始，向下递归选择子节点，直至选择一个叶子节点L。
具体来说，通常用UCBl(Upper Confidence Bound,上限置信区间)选择最具“潜力”的后续节点：
$$
U C B=\bar{X}_{j}+\sqrt{\frac{2 \operatorname{In} n}{n_{j}}}
$$</p><h2 id=拓展>拓展<a hidden class=anchor aria-hidden=true href=#拓展>#</a></h2><p>如果L不是一个终止节点（即博弈游戏不)，则随机创建其后的一个未被访问节点，选择该节点作为后续子节点C。</p><h2 id=模拟>模拟<a hidden class=anchor aria-hidden=true href=#模拟>#</a></h2><p>从节点C出发，对游戏进行模拟，直到博弈游戏结束。</p><h2 id=反向传播>反向传播<a hidden class=anchor aria-hidden=true href=#反向传播>#</a></h2><p>用模拟所得结果来回溯更新导致这个结果的每个节点中获胜次数和访问次数。</p><p><strong>包含两种策略学习机制：</strong></p><p><strong>搜索树策略</strong>：从已有的搜索树中选择或创建一个叶子结点（即蒙特卡洛中选择和拓展两个步骤).搜索树策略需要在利用和探索之间保持平衡。</p><p><strong>模拟策略</strong>：从非叶子结点出发模拟游戏，得到游戏仿真结果。</p><h1 id=例子-围棋>例子： 围棋<a hidden class=anchor aria-hidden=true href=#例子-围棋>#</a></h1><ul><li><p>以围棋为例，假设根节点是执黑棋方。</p></li><li><p>图中每一个节点都代表一个局面，每一个局面记录两个值A/B</p></li></ul><p>A: 该局面被访问中黑棋胜利次数。对于黑棋表示己方胜利次数，对于白棋表示己方失败次数（对方胜利次数)；</p><p>B: 该局面被访问的总次数</p><p>初始状态：<img loading=lazy src=/posts/MCTS/wq1.png alt></p><p>根节点（12/21）是当前局面，该局面被访问过21次，其中黑棋访问该局面后最终获胜12次。 黑执棋方遇到该局面（根节点局面）开始选择下一步action。</p><h3 id=选择-1>选择<a hidden class=anchor aria-hidden=true href=#选择-1>#</a></h3><p>黑执棋方遇到（12/21）这个局面时有3个有效action, 黑执行这三个action会得到3种局面（7/10）,（5/8）, (0/3). 黑执棋方要选择其中一个action，是的棋局变为这三种中的一种，那么要如何选择action呢？我们先分别计算UCB1：</p><p>左一： 7/10对应的局面Reward为：
$$
\frac{7}{10} + \sqrt{\frac{\log (21)}{10}} = 1.252
$$
3中局面共21次被模拟到，其中，该局面被访问过10次（探索， 第二项）， 黑棋遇到该局面后最终获胜7次（利用， 第一项）。</p><p>左二：（5/8）对应局面Reward:
$$
\frac{5}{8} + \sqrt{\frac{\log(21)}{8}} = 1.243
$$
左三： （0/3）对应局面Reward:
$$
\frac{0}{3} + \sqrt{\frac{\log(21)}{3}} = 1.007
$$
由此可见，黑棋选择会导致局面（7/10）的action进行走琪。</p><p><img loading=lazy src=/posts/MCTS/wq1.png#center alt></p><p>白棋遇到局面（7/10）时，有两个有效的action, 分别会导致局面A/B = 2/4和5/6, A为白棋访问该局面后失败的次数，所以白棋访问该局面最终胜利的次数应该是$B-A$：</p><p>左一： (2/4)对应的局面Reward (白棋尽可能获胜)为：
$$
(1-\frac{2}{4}) + \sqrt{\frac{\log(21)}{4}}=1.372
$$
左二： (5/6)对应的局面Reward为：
$$
(1-\frac{5}{6}) + \sqrt{\frac{\log(21)}{4}}=0.879
$$
因此白棋会选择（2/4）局面</p><p>即<strong>每一步都寻找最佳应对方式，来最终评估更节点局面的好坏</strong></p><p><img loading=lazy src=/posts/MCTS/wq1.png#center alt></p><p>白色执棋方选择action后到达局面（2/4）, 黑棋执棋方面对该局面，有两种action可选，分别会到达（1/3）和（1/1）， 分别计算UCB1 得：</p><p>左一： (1/3)对应reward 为：
$$
\frac{1}{3} + \sqrt{\frac{\log (21)}{3}} = 1.341
$$
左二：(1/1)对应reward为：
$$
\frac{1}{1} + \sqrt{\frac{\log (21)}{1}} = 2.745
$$
则黑棋选择action 使得局面变为(1/1)。右图中可见(1/1)为叶子节点， 白棋面对该局面暂时无候选局面（action）,则进入下一个步骤，拓展。</p><h3 id=拓展-1>拓展<a hidden class=anchor aria-hidden=true href=#拓展-1>#</a></h3><p><img loading=lazy src=/posts/MCTS/wq1.png alt></p><p>白棋执棋方面对局面(1/1),因为已经是叶子节点， 所以(1/1)会随机产生一个未被访问过的新节点，初始化为(0/0),接着在该节点下进行模拟。 进入第三部， 模拟。</p><h3 id=模拟-1>模拟<a hidden class=anchor aria-hidden=true href=#模拟-1>#</a></h3><p><img loading=lazy src=/posts/MCTS/wq3.png alt></p><p>黑棋执棋方面局面（0/0），开始进行游戏仿真， 假设经过一些列仿真后，最终白棋获胜，即从更节点到最终模拟结束的叶子节点黑棋失败了。 进入第四部，回溯</p><h3 id=回溯>回溯<a hidden class=anchor aria-hidden=true href=#回溯>#</a></h3><p><img loading=lazy src=/posts/MCTS/wq4.png alt></p><p>根据仿真结果来更新该仿真路径上的每个节点的A/B值。 因为该次模拟最终白棋获胜，所以向上回溯路径上的所有父节点，父节点的A不变，B+1。 若最终黑棋获胜，则父节点的A+1, B+1。</p><p><strong>在有限时间里， MCTS会不断重复4个步骤，所有节点的A,B 值会不断变化</strong></p><p><strong>到某个局面（节点）后如何下棋（action）, 会选择所有有效Action中UCB1值最大的节点，作为下棋的下一步。</strong></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://JhuoW.github.io/tags/mcts/>MCTS</a></li></ul><nav class=paginav><a class=next href=https://JhuoW.github.io/posts/neo-gnns/><span class=title>Next Page »</span><br><span>NeurIPS2021 《Neo-GNNs:Neighborhood Overlap-aware Graph Neural Networks for Link Prediction》 Reading Notes</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Tree Search (MCTS) on twitter" href="https://twitter.com/intent/tweet/?text=Monte%20Carlo%20Tree%20Search%20%28MCTS%29&url=https%3a%2f%2fJhuoW.github.io%2fposts%2fmcts%2f&hashtags=MCTS"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Tree Search (MCTS) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fJhuoW.github.io%2fposts%2fmcts%2f&title=Monte%20Carlo%20Tree%20Search%20%28MCTS%29&summary=Monte%20Carlo%20Tree%20Search%20%28MCTS%29&source=https%3a%2f%2fJhuoW.github.io%2fposts%2fmcts%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Tree Search (MCTS) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fJhuoW.github.io%2fposts%2fmcts%2f&title=Monte%20Carlo%20Tree%20Search%20%28MCTS%29"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Tree Search (MCTS) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fJhuoW.github.io%2fposts%2fmcts%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Tree Search (MCTS) on whatsapp" href="https://api.whatsapp.com/send?text=Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20-%20https%3a%2f%2fJhuoW.github.io%2fposts%2fmcts%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Tree Search (MCTS) on telegram" href="https://telegram.me/share/url?text=Monte%20Carlo%20Tree%20Search%20%28MCTS%29&url=https%3a%2f%2fJhuoW.github.io%2fposts%2fmcts%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>