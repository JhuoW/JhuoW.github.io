<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script type=text/javascript async src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><title>WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes | JhuoW‘s Notes</title>
<meta name=keywords content="GNN,Self-Supervised Learning,Variational Inference"><meta name=description content='WWW2022 "ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs" 阅读笔记'><meta name=author content="JhuoW"><link rel=canonical href=https://JhuoW.github.io/posts/clusterscl/><meta name=google-site-verification content="G-6F49SGED6V"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=preload href=/apple-touch-icon.png as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.2eadbb982468c11a433a3e291f01326f2ba43f065e256bf792dbd79640a92316.js integrity="sha256-Lq27mCRowRpDOj4pHwEybyukPwZeJWv3ktvXlkCpIxY=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://JhuoW.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JhuoW.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JhuoW.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JhuoW.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JhuoW.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://JhuoW.github.io/posts/clusterscl/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6F49SGED6V"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6F49SGED6V")}</script><meta property="og:title" content="WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes"><meta property="og:description" content='WWW2022 "ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs" 阅读笔记'><meta property="og:type" content="article"><meta property="og:url" content="https://JhuoW.github.io/posts/clusterscl/"><meta property="og:image" content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-17T01:33:20+08:00"><meta property="article:modified_time" content="2022-11-17T01:33:20+08:00"><meta property="og:site_name" content="JhuoW"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes"><meta name=twitter:description content='WWW2022 "ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs" 阅读笔记'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JhuoW.github.io/posts/"},{"@type":"ListItem","position":2,"name":"WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes","item":"https://JhuoW.github.io/posts/clusterscl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes","name":"WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes","description":"WWW2022 \"ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs\" 阅读笔记","keywords":["GNN","Self-Supervised Learning","Variational Inference"],"articleBody":"paper\nIntroduction 对于监督对比学习（Supervised Contrastive Learning, SupCon）, SupCon loss旨在表示空间中拉近属于同一个class的数据点，分离不同类的数据点。 但是SupCon难以处理高类内方差，类间相似度较大的数据集。为了解决该问题，本文提出了Cluster-aware supervised contrastive learning loss (ClusterSCL)。什么是高类内方差，高跨类相似度问题？如图1(a)所示，节点$u_1$和$u_3$ 是同类节点，$u_2$和$u_4$是同类节点。他们是同类节点但在不同的社区中，所以类内方差较大，即同一个类内的节点跨越了多个community。 另外$u_1$和$u_2$， $u_3$和$u_4$，是不同类的节点对， 但他们处在同一个社区中，导致在MPNN过程中，这些处在同一个community中的不同类节点被拉近，导致跨类相似度较高的问题。\n如果对节点$u_2$计算SupCon时，如图1(b)所示，SupCon会使得同类节点被拉近，如$u_2$和$u_4$会被拉近。但是$u_3$和$u_4$处在同一个社区中（structurally similar）那么MPNN会使得$u_3$和$u_4$被拉近，所以SupCon在拉近$u_2$和$u_4$的同时，会间接拉近不同类节点$u_2$和$u_3$。同时，对于构成negative pairs的不同类节点，例如$u_1$和$u_2$，SupCon会推远$u_1$和$u_2$，但是$u_1$和$u_5$ structurally similar, 因此会推远$u_1$和$u_2$会间接导致$u_2$和$u_5$这两个同类节点被推远。因此对于一个cluster内节点不同类，且不同cluster中存在同类节点的情况，会导致复杂的决策边界，即在拉近同类但不同社区的节点时，也会间接拉近不同类不同社区的节点。在推远不同类同社区的节点时，也可能间接推远同类同社区的节点。\n为了解决上述问题，最直接的方法是对于每个cluster，如图1(a)的Community 1，不考虑其他cluster，只对当前cluster内节点做SupCon。但是这么做忽略了跨cluster的同类节点交互，如$u_1$和$u_3$，$u_2$和$u_4$，这些跨cluster的positive pairs可能包含有益的信息。为了解决这个问题，本文提出cluster-aware data augmentation (CDA) 聚类感知的数据增强，来为每个节点生成augmented positives and negatives，如图1(b)中ClusterSCL所示。对于每个节点$u$，为它生成positive 和negative samples, 生成的samples 位于或接近$u$所在的cluster。Recall SupCon存在的问题：\nSupCon会使得$u_2$和$u_4$被拉的太近，从而间接导致$u_2$和$u_3$被拉近，所以对于high intra-class variances，要求不同cluster的同类节点如$u_2$和$u_4$不要被拉太近； SupCon会使得$u_1$和$u_2$被推远，从而间接导致$u_2$和$u_5$被推远，所以对于high inter-class similarity，要求同一个cluster内的不同类节点如$u_1$和$u_2$不要被拉的太远。 Method Two stage training with Supervised Contrastive Loss SupCon encourages samples of the same class to have similar representations, while pushes apart samples of different classes in the embedding space.\nFirst Stage: 计算node embeddings $H = g_\\theta(G)$， 然后用SupCon Loss来训练 $g_\\theta$ 。即已经知道训练集中的节点label，基于这些节点label，SupCon在embedding space中把同label的节点拉近，不同label的节点分开。$g_\\theta$用于得到node embeddings.\nSecond Stage：基于学习好的$g_\\theta$， 用$\\hat{Y}=f_\\phi\\left(g_\\theta(G)\\right)$来得到logits/prediction。即用cross-entropy loss来训练$f_\\theta$。\nSupCon 从同一个类中采样的节点构成positive pairs。batch中随机采样的节点对为negative pairs。给定$N$个随机采样的节点，对于每个节点，从其对应的class中随机采样一个不为它的节点作为positive pairs。所以一个batch有$N$对positive pairs，共$2N$个节点。\n用$I \\equiv\\{1,2, \\ldots, 2 N\\}$ 表示一个batch中的node indices。$s_i \\in I$表示这$2N$个节点中与节点$v_i$属于同一类的节点的indices。如下式所示， 在一个batch中，令$S_i \\subset I$表示$2N$个节点中可以与节点$v_i$构成positive pairs的节点集合。相比其他节点，SupCon的objective是拉近positive pairs。 $$ \\max \\sum_{i \\in I} \\frac{1}{\\left|S_i\\right|} \\sum_{s_i \\in S_i} \\log \\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{h}_{s_i} / \\tau\\right)}{\\sum_{j \\in I \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{h}_j / \\tau\\right)} $$ 但是如果batch中与$v_i$同类的节点$v_j$和它不属于同一个cluster，$v_j$所属的cluster不同类的节点较多，那么拉近他们的距离也会间接拉近$v_i$与不同类节点间的距离。\nClusterSCL 为了解决上述问题，本文提出ClusterSCL。\nCluster-aware Data Augmentation (CDA) 定义隐变量 $c_i$，该隐变量取值范围为$c_i \\in \\{1,2, \\ldots, M\\}$ 表示节点$v_i$属于哪一个cluster。给定两个anchor node $v_i$,$v_j$，CDA使用线性插值法为$v_j$生成augmentation： $$ \\tilde{\\mathbf{h}}_j=\\alpha \\mathbf{h}_j+(1-\\alpha) \\mathbf{w}_{c_i} \\tag{4} $$ 其中$c_i$指示了节点$v_i$所在的cluster。$\\mathbf{w}=\\left\\{\\mathbf{w}_m\\right\\}_{m=1}^M$ 表示每个cluster的cluster prototypes，即每个cluster的中心，serve to characterize the cluster。$\\mathbf{w}_{c_i}$表示节点$v_i$所在cluster的中心表示。$\\tilde{\\mathbf{h}}_j$包含了节点$v_j$的信息。并且，由于$\\mathbf{w}_{c_i}$是$v_i$所在cluster的prtotype，所以通过调整$\\alpha$，可以使$\\tilde{\\mathbf{h}}_j$位于$v_i$所在cluster的附近或内部。\n（1）如果$(v_i, v_j)$是一个batch中的positive pair，$v_i$是anchor节点，如果$v_j$位于$v_i$所在的cluster $c_i$内，那么就需要学到的$\\mathbf{h}_j$与$\\mathbf{h}_i$尽可能靠近。在SupCon中，需要设置较大的$\\alpha$使得$\\tilde{\\mathbf{h}}_j$保留更多$\\mathbf{h}_j$ 。对于anchor节点$\\mathbf{h}_i$，将它与$\\tilde{\\mathbf{h}}_j$拉近的时候，由于$\\tilde{\\mathbf{h}}_j$保留了更多$v_j$特征，所以$\\mathbf{h}_j$也会被和$\\mathbf{h}_i$拉近。如图2(a)所示。\n（2）如果$(v_i, v_j)$是positive pair，如果$v_j$位于$v_i$所在的cluster $c_i$外时，如果$v_j$周围有negative samples （不一定在该batch中），那么直接拉近$(v_i, v_j)$会间接导致潜在的negative samples也会被拉近，因此对于位于$v_i$所在cluster外的节点$v_j$，要求它最终的表示$\\mathbf{h}_j$不能被拉的太近，此时就需要小一些的$\\alpha$，使得$\\tilde{\\mathbf{h}}_j$保留少一些$v_j$的信息，那么在SupCon拉近$\\mathbf{h}_i$和$\\tilde{\\mathbf{h}}_j$的过程不会导致$\\mathbf{h}_j$被拉近太多。因为$ \\mathbf{w}_{c_i}$占据了$\\tilde{\\mathbf{h}}_j$的大部分，且它与$\\mathbf{h}_i$已经很接近。如图2(b)所示。\n对于negative pair $(v_i, v_j)$。如果$v_j$位于$v_i$所在的cluster $c_i$内，如果直接推远$\\mathbf{h}_i$和$\\mathbf{h}_j$，会导致如果$v_j$的邻居有$v_i$的positive sample，那么这个positive sample也会被间接推远。所以$\\tilde{\\mathbf{h}}_j$应该保留较少的$\\mathbf{h}_j$，即$\\alpha$应该小。但是本文不考虑negative pairs的这种情况了，直接套用posiive的CDA原则。\n综合上面的（1）（2）即对于close positive pairs，要让他们尽可能接近，即$\\alpha$要大，对于distant positive pairs，要让他们不要太接近，$\\alpha$要小一些。所以$\\alpha$应与positive pair之间的相似度相关。所以$\\alpha$定义如下： $$ \\alpha=\\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{h}_j\\right)}{\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{h}_j\\right)+\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{w}_{c_i}\\right)} $$ 上式分子越大$\\alpha$越大，说明对于anchor node $v_i$， 如果的positive sample $v_j$位于它的cluster内（$v_i$与$v_j$相似）,$\\mathbf{h}_j$的augmentation $\\tilde{\\mathbf{h}}_j$要保留越多自身信息。\nIntegraging Clustering and CDA into SupCon Learning 目标是给定节点$v_i$以及它所在的cluster 隐变量$c_i$，$v_i$的positive samples $s_i$的cluster-aware SupCon定义为条件概率： $$ \\begin{aligned} p\\left(s_i \\mid v_i, c_i\\right) \u0026=\\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_{s_i} / \\tau\\right)}{\\sum_{j \\in V \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_j / \\tau\\right)} \\\\ \u0026=\\frac{\\exp \\left(\\mathbf{h}_i^{\\top}\\left(\\alpha \\mathbf{h}_{s_i}+(1-\\alpha) \\mathbf{w}_{c_i}\\right) / \\tau\\right)}{\\sum_{j \\in V \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top}\\left(\\alpha \\mathbf{h}_j+(1-\\alpha) \\mathbf{w}_{c_i}\\right) / \\tau\\right)} \\end{aligned} \\tag{6} $$ 即对于positive pair $(v_i, s_i)$，最大化$\\mathbf{h}_i$和$s_i$的augmentation $\\tilde{\\mathbf{h}}_{s_i}$之间的一致性，$\\alpha$可以依据$s_i$和$c_i$的关系来调整$\\mathbf{h}_{s_i}$对于SupCon的贡献，使得$\\mathbf{h}_{i}$与$\\mathbf{h}_{s_i}$在位于不同cluster的情况下不会被拉的太近。其中$c_i$是隐变量。\n首先定义关于节点$v_i$的cluster分布，即$v_i$属于每个$c_i$的概率。给定anchor node $v_i$，它属于cluster $c_i \\in \\{1,2, \\ldots, M\\}$的概率定义为： $$ p\\left(c_i \\mid v_i\\right)=\\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{w}_{c_i} / \\kappa\\right)}{\\sum_{m=1}^M \\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{w}_m / \\kappa\\right)} \\tag{7} $$ $\\mathbf{w}_{c_i}$是cluster $c_i$的prototype表示，$v_i$属于在embedding空间中与它相似的cluster的概率更高。ClusterSCL旨在最大化给定锚节点$v_i$，锚节点与其positive sample $s_i$的likelihood： $$ p\\left(s_i \\mid v_i\\right)=\\int_{c_i} p\\left(c_i \\mid v_i\\right) p\\left(s_i \\mid v_i, c_i\\right) d c_i= \\sum^M_{m=1} p(m|v_i)p(s_i|v_i,m) \\tag{8} $$ 即给定anchor $v_i$，$s_i$是$v_i$的postive sample的概率为 当$v_i$属于cluster $m$的情况下，$s_i$是其positive sample的概率， over all clusters $m \\in M$。\n在likelihood Eq.(8)中，anchor node $v_i$ 为待优化参数，它的positive sample $s_i$为观测数据，$c_i$为隐变量。\nMaximize likelihood Eq.(8) via EM Objective: $\\mathrm{maximize} \\log p(s_i| v_i)$，即最大化positive pairs的条件概率 given anchor node $v_i$。\nE-step：$\\mathbb{E}_{p(c_i|s_i, v_i)} \\log p(s_i, c_i|v_i)$\nM-step: $\\widehat{v}_i = \\arg \\max_{v_i} \\mathbb{E}_{p(c_i|s_i, v_i)} \\log p(s_i, c_i|v_i)$\n可见，如果要通过EM算法来优化得到anchor node $v_i$的表示，需要计算后验 $p(c_i|s_i, v_i)$： $$ \\begin{aligned} p(c_i|s_i, v_i) \u0026 = \\frac{p(c_i,v_i, s_i)}{p(s_i,v_i)} \\\\ \u0026 = \\frac{p(v_i)p(s_i,c_i | v_i)}{p(s_i |v_i) p(v_i)}\\\\ \u0026 = \\frac{p(s_i,c_i|v_i)}{p(s_i |v_i)} \\\\ \u0026 = \\frac{p(s_i,c_i|v_i)}{\\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}\\\\ \u0026 = \\frac{\\frac{p(s_i,c_i,v_i)}{p(v_i)} = \\frac{p(c_i,v_i)}{p(v_i)} \\frac{p(s_i,c_i,v_i)}{p(c_i,v_i)} = p(c_i | v_i)p(s_i|c_i,v_i)}{\\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)} \\\\ \u0026 = \\frac{p(c_i | v_i)p(s_i|c_i,v_i)}{\\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}\n\\end{aligned}\\tag{9} $$ 后验中，$p(c_i | v_i)$，$p(m|v_i)$是$v_i$的cluster 分布，在Eq.(7)中给出定义。但是，对于$p(s_i|c_i,v_i)$和$p(s_i | v_i, m)$，Eq.(6)中给出了它的定义，$p\\left(s_i \\mid v_i, c_i\\right) =\\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_{s_i} / \\tau\\right)}{\\sum_{j \\in V \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_j / \\tau\\right)}$， 可以看出分母部分需要计算$\\mathbf{h}_i$与所有节点，并且还要over all $M$ cluster，因此后验难以计算。为了解决这个问题，我们可以maximize evidence lower bound (ELBO) of $\\log p(s_i | v_i)$： $$ \\begin{aligned} \\log p(s_i | v_i) \u0026= \\log \\frac{p(s_i,v_i)}{p(v_i)} = \\log \\frac{p(s_i,v_i)p(c_i | v_i,s_i)}{p(v_i)p(c_i | v_i,s_i)} \\\\ \u0026= \\log \\frac{p(v_i,s_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} \\\\ \u0026= \\log \\frac{p(s_i|v_i,c_i)p(v_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} = \\log \\frac{p(s_i|v_i,c_i)p(c_i|v_i)p(v_i)}{p(v_i)p(c_i | v_i,s_i)} \\\\ \u0026 = \\log \\frac{p(s_i|v_i,c_i)p(c_i|v_i)}{p(c_i | v_i,s_i)} \\\\ \u0026 = \\log p(s_i|v_i,c_i)-\\log p(c_i | v_i,s_i) + \\log p(c_i|v_i) \\\\ \u0026 \\text{引进一个关于隐变量$c_i$的分布$q(c_i)$，可以是任意形式，这里定义为$q(c_i|v_i,s_i)$}\\\\ \u0026 = \\log p(s_i|v_i,c_i) - \\log \\frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} + \\log \\frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} \\\\ \u0026\\text{左右两边分别对$q(c_i|v_i,s_i)$求期望} \\\\ \\text{左边} \u0026= \\int q(c_i|v_i,s_i) \\log p(s_i | v_i) d c_i = \\int q(c_i|v_i,s_i) d c_i \\cdot \\log p(s_i | v_i) = \\log p(s_i | v_i) \\\\ \\text{右边} \u0026= \\int q(c_i|v_i,s_i) \\log p(s_i|v_i,c_i) dc_i - \\underbrace{\\int q(c_i|v_i,s_i) \\log \\frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} dc_i}_{-\\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i))} + \\underbrace{\\int q(c_i|v_i,s_i) \\log \\frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} dc_i}_{-\\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))} \\\\ \u0026= \\int q(c_i|v_i,s_i) \\log p(s_i|v_i,c_i) dc_i + \\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i)) - \\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \\\\ \u0026 \\geq \\underbrace{\\mathbb{E}_{q(c_i|v_i,s_i)} \\log p(s_i|v_i,c_i) - \\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))}_{ELBO} \\end{aligned} \\tag{10} $$ 因此， we have： $$ \\log p(s_i | v_i) \\geq ELBO = \\mathbb{E}_{q(c_i|v_i,s_i)} \\log p(s_i|v_i,c_i) - \\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \\tag{10} $$ 所以目标为找到node embeddings $\\mathbf{h}_i, \\forall i$， 最大化ELBO。接下来可以定义任意关于隐变量$c_i$的vartational distribution $q(c_i)$。 这里将关于$c_i$的variational distribution定义为用mini-batch近似后验的形式： $$ q\\left(c_i \\mid v_i, s_i\\right)=\\frac{p\\left(c_i \\mid v_i\\right) \\tilde{p}\\left(s_i \\mid v_i, c_i\\right)}{\\sum_{m=1}^M p\\left(m \\mid v_i\\right) \\tilde{p}\\left(s_i \\mid v_i, m\\right)} \\tag{11} $$ 其中$\\tilde{p}\\left(s_i \\mid v_i, c_i\\right)=\\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_{s_i} / \\tau\\right) / \\sum_{j \\in I \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_j / \\tau\\right)$，与Eq.(6)不同的是$\\tilde{p}\\left(s_i \\mid v_i, c_i\\right)$的分母只计算mini-batch $I$内的negative samples。并且用$\\tilde{p}\\left(s_i \\mid v_i, c_i\\right)$来替换了Eq.10 中的$ p(s_i|v_i,c_i)$，文中证明了这种替代的合理性。\n在E-step，定义了variational distribution $q\\left(c_i \\mid v_i, s_i\\right)$和likelihood objective的ELBO，在M-step最大化ELBO。给定一个mini-batch $I$，目标函数为最大化$I$中每对positive pairs 的ELBO： $$ \\max \\quad \\mathcal{L}_{\\mathrm{ELBO}}(\\theta, \\mathbf{w} ; I) \\approx \\frac{1}{|I|} \\sum_{i \\in I} \\frac{1}{\\left|S_i\\right|} \\sum_{s_i \\in S_i} \\mathcal{L}_{\\mathrm{ELBO}}\\left(\\theta, \\mathbf{w} ; v_i, s_i\\right) $$\n","wordCount":"699","inLanguage":"en","datePublished":"2022-11-17T01:33:20+08:00","dateModified":"2022-11-17T01:33:20+08:00","author":{"@type":"Person","name":"JhuoW"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://JhuoW.github.io/posts/clusterscl/"},"publisher":{"@type":"Organization","name":"JhuoW‘s Notes","logo":{"@type":"ImageObject","url":"https://JhuoW.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JhuoW.github.io/ accesskey=h title="JhuoW's Notes (Alt + H)"><img src=https://JhuoW.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>JhuoW's Notes</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://JhuoW.github.io/about/ title=About><span>About</span></a></li><li><a href=https://JhuoW.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://JhuoW.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://JhuoW.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://JhuoW.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://JhuoW.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://JhuoW.github.io/posts/>Posts</a></div><h1 class=post-title>WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes</h1><div class=post-description>WWW2022 "ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs" 阅读笔记</div><div class=post-meta><span title='2022-11-17 01:33:20 +0800 +08'>November 17, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;JhuoW</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#method aria-label=Method>Method</a><ul><li><a href=#two-stage-training-with-supervised-contrastive-loss aria-label="Two stage training with Supervised Contrastive Loss">Two stage training with Supervised Contrastive Loss</a></li><li><a href=#supcon aria-label=SupCon>SupCon</a></li><li><a href=#clusterscl aria-label=ClusterSCL>ClusterSCL</a><ul><li><a href=#cluster-aware-data-augmentation-cda aria-label="Cluster-aware Data Augmentation (CDA)">Cluster-aware Data Augmentation (CDA)</a></li><li><a href=#integraging-clustering-and-cda-into-supcon-learning aria-label="Integraging Clustering and CDA into SupCon Learning">Integraging Clustering and CDA into SupCon Learning</a></li><li><a href=#maximize-likelihood-eq8-via-em aria-label="Maximize likelihood Eq.(8) via EM">Maximize likelihood Eq.(8) via EM</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><p><a href=https://xiaojingzi.github.io/publications/WWW22-Wang-et-al-ClusterSCL.pdf>paper</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>对于<strong>监督对比学习</strong>（Supervised Contrastive Learning, SupCon）, SupCon loss旨在表示空间中拉近属于同一个class的数据点，分离不同类的数据点。 但是SupCon难以处理高类内方差，类间相似度较大的数据集。为了解决该问题，本文提出了Cluster-aware supervised contrastive learning loss (ClusterSCL)。什么是高类内方差，高跨类相似度问题？如图1(a)所示，节点$u_1$和$u_3$ 是同类节点，$u_2$和$u_4$是同类节点。他们是同类节点但在不同的社区中，所以类内方差较大，即同一个类内的节点跨越了多个community。 另外$u_1$和$u_2$， $u_3$和$u_4$，是不同类的节点对， 但他们处在同一个社区中，导致在MPNN过程中，这些处在同一个community中的不同类节点被拉近，导致跨类相似度较高的问题。</p><p>如果对节点$u_2$计算SupCon时，如图1(b)所示，SupCon会使得同类节点被拉近，如$u_2$和$u_4$会被拉近。但是$u_3$和$u_4$处在同一个社区中（structurally similar）那么MPNN会使得$u_3$和$u_4$被拉近，所以SupCon在拉近$u_2$和$u_4$的同时，会间接拉近不同类节点$u_2$和$u_3$。同时，对于构成negative pairs的不同类节点，例如$u_1$和$u_2$，SupCon会推远$u_1$和$u_2$，但是$u_1$和$u_5$ structurally similar, 因此会推远$u_1$和$u_2$会间接导致$u_2$和$u_5$这两个同类节点被推远。因此对于一个cluster内节点不同类，且不同cluster中存在同类节点的情况，会导致复杂的决策边界，即<strong>在拉近同类但不同社区的节点时，也会间接拉近不同类不同社区的节点</strong>。<strong>在推远不同类同社区的节点时，也可能间接推远同类同社区的节点</strong>。</p><p><img loading=lazy src=/posts/2022-11-15-ClusterSCL/1.png#center alt></p><p>为了解决上述问题，最直接的方法是对于每个cluster，如图1(a)的Community 1，不考虑其他cluster，只对当前cluster内节点做SupCon。但是这么做忽略了跨cluster的同类节点交互，如$u_1$和$u_3$，$u_2$和$u_4$，这些跨cluster的positive pairs可能包含有益的信息。为了解决这个问题，本文提出<strong>cluster-aware data augmentation (CDA)</strong> 聚类感知的数据增强，来为每个节点生成augmented positives and negatives，如图1(b)中ClusterSCL所示。对于每个节点$u$，为它生成positive 和negative samples, 生成的samples 位于或接近$u$所在的cluster。Recall SupCon存在的问题：</p><ul><li>SupCon会使得$u_2$和$u_4$被拉的太近，从而间接导致$u_2$和$u_3$被拉近，所以对于high intra-class variances，要求不同cluster的同类节点如$u_2$和$u_4$不要被拉太近；</li><li>SupCon会使得$u_1$和$u_2$被推远，从而间接导致$u_2$和$u_5$被推远，所以对于high inter-class similarity，要求同一个cluster内的不同类节点如$u_1$和$u_2$不要被拉的太远。</li></ul><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><h2 id=two-stage-training-with-supervised-contrastive-loss>Two stage training with Supervised Contrastive Loss<a hidden class=anchor aria-hidden=true href=#two-stage-training-with-supervised-contrastive-loss>#</a></h2><p><strong>SupCon encourages samples of the same class to have similar representations, while pushes apart samples of different classes in the embedding space.</strong></p><p>First Stage: 计算node embeddings $H = g_\theta(G)$， 然后用SupCon Loss来训练 $g_\theta$ 。即已经知道训练集中的节点label，基于这些节点label，SupCon在embedding space中把同label的节点拉近，不同label的节点分开。$g_\theta$用于得到node embeddings.</p><p>Second Stage：基于学习好的$g_\theta$， 用$\hat{Y}=f_\phi\left(g_\theta(G)\right)$来得到logits/prediction。即用cross-entropy loss来训练$f_\theta$。</p><h2 id=supcon>SupCon<a hidden class=anchor aria-hidden=true href=#supcon>#</a></h2><p>从同一个类中采样的节点构成positive pairs。batch中随机采样的节点对为negative pairs。给定$N$个随机采样的节点，对于每个节点，从其对应的class中随机采样一个不为它的节点作为positive pairs。所以一个batch有$N$对positive pairs，共$2N$个节点。</p><p>用$I \equiv\{1,2, \ldots, 2 N\}$ 表示一个batch中的node indices。$s_i \in I$表示这$2N$个节点中与节点$v_i$属于同一类的节点的indices。如下式所示， 在一个batch中，令$S_i \subset I$表示$2N$个节点中可以与节点$v_i$构成positive pairs的节点集合。相比其他节点，SupCon的objective是拉近positive pairs。
$$
\max \sum_{i \in I} \frac{1}{\left|S_i\right|} \sum_{s_i \in S_i} \log \frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_{s_i} / \tau\right)}{\sum_{j \in I \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j / \tau\right)}
$$
但是如果batch中与$v_i$同类的节点$v_j$和它不属于同一个cluster，$v_j$所属的cluster不同类的节点较多，那么拉近他们的距离也会间接拉近$v_i$与不同类节点间的距离。</p><h2 id=clusterscl>ClusterSCL<a hidden class=anchor aria-hidden=true href=#clusterscl>#</a></h2><p>为了解决上述问题，本文提出ClusterSCL。</p><h3 id=cluster-aware-data-augmentation-cda>Cluster-aware Data Augmentation (CDA)<a hidden class=anchor aria-hidden=true href=#cluster-aware-data-augmentation-cda>#</a></h3><p>定义隐变量 $c_i$，该隐变量取值范围为$c_i \in \{1,2, \ldots, M\}$ 表示节点$v_i$属于哪一个cluster。给定两个anchor node $v_i$,$v_j$，CDA使用线性插值法为$v_j$生成augmentation：
$$
\tilde{\mathbf{h}}_j=\alpha \mathbf{h}_j+(1-\alpha) \mathbf{w}_{c_i} \tag{4}
$$
其中$c_i$指示了节点$v_i$所在的cluster。$\mathbf{w}=\left\{\mathbf{w}_m\right\}_{m=1}^M$ 表示每个cluster的cluster prototypes，即每个cluster的中心，serve to characterize the cluster。$\mathbf{w}_{c_i}$表示节点$v_i$所在<strong>cluster</strong>的中心表示。$\tilde{\mathbf{h}}_j$包含了节点$v_j$的信息。并且，由于$\mathbf{w}_{c_i}$是$v_i$所在cluster的prtotype，所以通过调整$\alpha$，可以使$\tilde{\mathbf{h}}_j$位于$v_i$所在cluster的附近或内部。</p><p>（1）如果$(v_i, v_j)$是一个batch中的positive pair，$v_i$是anchor节点，如果$v_j$位于$v_i$所在的cluster $c_i$内，那么就需要学到的$\mathbf{h}_j$与$\mathbf{h}_i$尽可能靠近。在SupCon中，需要设置较大的$\alpha$使得$\tilde{\mathbf{h}}_j$保留更多$\mathbf{h}_j$ 。对于anchor节点$\mathbf{h}_i$，将它与$\tilde{\mathbf{h}}_j$拉近的时候，由于$\tilde{\mathbf{h}}_j$保留了更多$v_j$特征，所以$\mathbf{h}_j$也会被和$\mathbf{h}_i$拉近。如图2(a)所示。</p><p>（2）如果$(v_i, v_j)$是positive pair，如果$v_j$位于$v_i$所在的cluster $c_i$外时，如果$v_j$周围有negative samples （不一定在该batch中），那么直接拉近$(v_i, v_j)$会间接导致潜在的negative samples也会被拉近，因此对于位于$v_i$所在cluster外的节点$v_j$，要求它最终的表示$\mathbf{h}_j$不能被拉的太近，此时就需要小一些的$\alpha$，使得$\tilde{\mathbf{h}}_j$保留少一些$v_j$的信息，那么在SupCon拉近$\mathbf{h}_i$和$\tilde{\mathbf{h}}_j$的过程不会导致$\mathbf{h}_j$被拉近太多。因为$ \mathbf{w}_{c_i}$占据了$\tilde{\mathbf{h}}_j$的大部分，且它与$\mathbf{h}_i$已经很接近。如图2(b)所示。</p><p><img loading=lazy src=/posts/2022-11-15-ClusterSCL/2.png#center alt></p><p>对于negative pair $(v_i, v_j)$。如果$v_j$位于$v_i$所在的cluster $c_i$内，如果直接推远$\mathbf{h}_i$和$\mathbf{h}_j$，会导致如果$v_j$的邻居有$v_i$的positive sample，那么这个positive sample也会被间接推远。所以$\tilde{\mathbf{h}}_j$应该保留较少的$\mathbf{h}_j$，即$\alpha$应该小。但是本文不考虑negative pairs的这种情况了，直接套用posiive的CDA原则。</p><p>综合上面的（1）（2）即对于close positive pairs，要让他们尽可能接近，即$\alpha$要大，对于distant positive pairs，要让他们不要太接近，$\alpha$要小一些。所以$\alpha$应与positive pair之间的相似度相关。所以$\alpha$定义如下：
$$
\alpha=\frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j\right)}{\exp \left(\mathbf{h}_i^{\top} \mathbf{h}_j\right)+\exp \left(\mathbf{h}_i^{\top} \mathbf{w}_{c_i}\right)}
$$
上式分子越大$\alpha$越大，说明对于anchor node $v_i$， 如果的positive sample $v_j$位于它的cluster内（$v_i$与$v_j$相似）,$\mathbf{h}_j$的augmentation $\tilde{\mathbf{h}}_j$要保留越多自身信息。</p><h3 id=integraging-clustering-and-cda-into-supcon-learning>Integraging Clustering and CDA into SupCon Learning<a hidden class=anchor aria-hidden=true href=#integraging-clustering-and-cda-into-supcon-learning>#</a></h3><p>目标是给定节点$v_i$以及它所在的cluster 隐变量$c_i$，$v_i$的positive samples $s_i$的cluster-aware SupCon定义为条件概率：
$$
\begin{aligned}
p\left(s_i \mid v_i, c_i\right) &=\frac{\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)} \\
&=\frac{\exp \left(\mathbf{h}_i^{\top}\left(\alpha \mathbf{h}_{s_i}+(1-\alpha) \mathbf{w}_{c_i}\right) / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top}\left(\alpha \mathbf{h}_j+(1-\alpha) \mathbf{w}_{c_i}\right) / \tau\right)}
\end{aligned} \tag{6}
$$
即对于positive pair $(v_i, s_i)$，最大化$\mathbf{h}_i$和$s_i$的augmentation $\tilde{\mathbf{h}}_{s_i}$之间的一致性，$\alpha$可以依据$s_i$和$c_i$的关系来调整$\mathbf{h}_{s_i}$对于SupCon的贡献，使得$\mathbf{h}_{i}$与$\mathbf{h}_{s_i}$在位于不同cluster的情况下不会被拉的太近。其中$c_i$是隐变量。</p><p>首先定义关于节点$v_i$的cluster分布，即$v_i$属于每个$c_i$的概率。给定anchor node $v_i$，它属于cluster $c_i \in \{1,2, \ldots, M\}$的概率定义为：
$$
p\left(c_i \mid v_i\right)=\frac{\exp \left(\mathbf{h}_i^{\top} \mathbf{w}_{c_i} / \kappa\right)}{\sum_{m=1}^M \exp \left(\mathbf{h}_i^{\top} \mathbf{w}_m / \kappa\right)} \tag{7}
$$
$\mathbf{w}_{c_i}$是cluster $c_i$的prototype表示，$v_i$属于在embedding空间中与它相似的cluster的概率更高。ClusterSCL旨在最大化给定锚节点$v_i$，锚节点与其positive sample $s_i$的likelihood：
$$
p\left(s_i \mid v_i\right)=\int_{c_i} p\left(c_i \mid v_i\right) p\left(s_i \mid v_i, c_i\right) d c_i= \sum^M_{m=1} p(m|v_i)p(s_i|v_i,m) \tag{8}
$$
即给定anchor $v_i$，$s_i$是$v_i$的postive sample的概率为 当$v_i$属于cluster $m$的情况下，$s_i$是其positive sample的概率， over all clusters $m \in M$。</p><p>在likelihood Eq.(8)中，anchor node $v_i$ 为待优化参数，它的positive sample $s_i$为观测数据，$c_i$为隐变量。</p><h3 id=maximize-likelihood-eq8-via-em>Maximize likelihood Eq.(8) via EM<a hidden class=anchor aria-hidden=true href=#maximize-likelihood-eq8-via-em>#</a></h3><p>Objective: $\mathrm{maximize} \log p(s_i| v_i)$，即最大化positive pairs的条件概率 given anchor node $v_i$。</p><p>E-step：$\mathbb{E}_{p(c_i|s_i, v_i)} \log p(s_i, c_i|v_i)$</p><p>M-step: $\widehat{v}_i = \arg \max_{v_i} \mathbb{E}_{p(c_i|s_i, v_i)} \log p(s_i, c_i|v_i)$</p><p>可见，如果要通过EM算法来优化得到anchor node $v_i$的表示，需要计算后验 $p(c_i|s_i, v_i)$：
$$
\begin{aligned}
p(c_i|s_i, v_i) & = \frac{p(c_i,v_i, s_i)}{p(s_i,v_i)} \\
& = \frac{p(v_i)p(s_i,c_i | v_i)}{p(s_i |v_i) p(v_i)}\\
& = \frac{p(s_i,c_i|v_i)}{p(s_i |v_i)} \\
& = \frac{p(s_i,c_i|v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}\\
& = \frac{\frac{p(s_i,c_i,v_i)}{p(v_i)} = \frac{p(c_i,v_i)}{p(v_i)} \frac{p(s_i,c_i,v_i)}{p(c_i,v_i)} = p(c_i | v_i)p(s_i|c_i,v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)} \\
& = \frac{p(c_i | v_i)p(s_i|c_i,v_i)}{\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}<br>\end{aligned}\tag{9}
$$
后验中，$p(c_i | v_i)$，$p(m|v_i)$是$v_i$的cluster 分布，在Eq.(7)中给出定义。但是，对于$p(s_i|c_i,v_i)$和$p(s_i | v_i, m)$，Eq.(6)中给出了它的定义，$p\left(s_i \mid v_i, c_i\right) =\frac{\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right)}{\sum_{j \in V \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)}$， 可以看出分母部分需要计算$\mathbf{h}_i$与所有节点，并且还要over all $M$ cluster，因此后验难以计算。为了解决这个问题，我们可以maximize evidence lower bound (ELBO) of $\log p(s_i | v_i)$：
$$
\begin{aligned}
\log p(s_i | v_i) &= \log \frac{p(s_i,v_i)}{p(v_i)} = \log \frac{p(s_i,v_i)p(c_i | v_i,s_i)}{p(v_i)p(c_i | v_i,s_i)} \\
&= \log \frac{p(v_i,s_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} \\
&= \log \frac{p(s_i|v_i,c_i)p(v_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} = \log \frac{p(s_i|v_i,c_i)p(c_i|v_i)p(v_i)}{p(v_i)p(c_i | v_i,s_i)} \\
& = \log \frac{p(s_i|v_i,c_i)p(c_i|v_i)}{p(c_i | v_i,s_i)} \\
& = \log p(s_i|v_i,c_i)-\log p(c_i | v_i,s_i) + \log p(c_i|v_i) \\
& \text{引进一个关于隐变量$c_i$的分布$q(c_i)$，可以是任意形式，这里定义为$q(c_i|v_i,s_i)$}\\
& = \log p(s_i|v_i,c_i) - \log \frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} + \log \frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} \\
&\text{左右两边分别对$q(c_i|v_i,s_i)$求期望} \\
\text{左边} &= \int q(c_i|v_i,s_i) \log p(s_i | v_i) d c_i = \int q(c_i|v_i,s_i) d c_i \cdot \log p(s_i | v_i) = \log p(s_i | v_i) \\
\text{右边} &= \int q(c_i|v_i,s_i) \log p(s_i|v_i,c_i) dc_i - \underbrace{\int q(c_i|v_i,s_i) \log \frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} dc_i}_{-\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i))} + \underbrace{\int q(c_i|v_i,s_i) \log \frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} dc_i}_{-\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))} \\
&= \int q(c_i|v_i,s_i) \log p(s_i|v_i,c_i) dc_i + \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i)) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \\
& \geq \underbrace{\mathbb{E}_{q(c_i|v_i,s_i)} \log p(s_i|v_i,c_i) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))}_{ELBO}
\end{aligned} \tag{10}
$$
因此， we have：
$$
\log p(s_i | v_i) \geq ELBO = \mathbb{E}_{q(c_i|v_i,s_i)} \log p(s_i|v_i,c_i) - \mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \tag{10}
$$
所以目标为找到node embeddings $\mathbf{h}_i, \forall i$， 最大化ELBO。接下来可以定义任意关于隐变量$c_i$的vartational distribution $q(c_i)$。 这里将关于$c_i$的variational distribution定义为用mini-batch近似后验的形式：
$$
q\left(c_i \mid v_i, s_i\right)=\frac{p\left(c_i \mid v_i\right) \tilde{p}\left(s_i \mid v_i, c_i\right)}{\sum_{m=1}^M p\left(m \mid v_i\right) \tilde{p}\left(s_i \mid v_i, m\right)} \tag{11}
$$
其中$\tilde{p}\left(s_i \mid v_i, c_i\right)=\exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_{s_i} / \tau\right) / \sum_{j \in I \backslash\{i\}} \exp \left(\mathbf{h}_i^{\top} \tilde{\mathbf{h}}_j / \tau\right)$，与Eq.(6)不同的是$\tilde{p}\left(s_i \mid v_i, c_i\right)$的分母只计算mini-batch $I$内的negative samples。并且用$\tilde{p}\left(s_i \mid v_i, c_i\right)$来替换了Eq.10 中的$ p(s_i|v_i,c_i)$，文中证明了这种替代的合理性。</p><p>在E-step，定义了variational distribution $q\left(c_i \mid v_i, s_i\right)$和likelihood objective的ELBO，在M-step最大化ELBO。给定一个mini-batch $I$，目标函数为最大化$I$中每对positive pairs 的ELBO：
$$
\max \quad \mathcal{L}_{\mathrm{ELBO}}(\theta, \mathbf{w} ; I) \approx \frac{1}{|I|} \sum_{i \in I} \frac{1}{\left|S_i\right|} \sum_{s_i \in S_i} \mathcal{L}_{\mathrm{ELBO}}\left(\theta, \mathbf{w} ; v_i, s_i\right)
$$</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://JhuoW.github.io/tags/gnn/>GNN</a></li><li><a href=https://JhuoW.github.io/tags/self-supervised-learning/>Self-Supervised Learning</a></li><li><a href=https://JhuoW.github.io/tags/variational-inference/>Variational Inference</a></li></ul><nav class=paginav><a class=prev href=https://JhuoW.github.io/posts/progcl/><span class=title>« Prev Page</span><br><span>ICML2022 《ProGCL：Rethinking Hard Negative Mining in Graph Contrastive Learning》 Reading Note</span>
</a><a class=next href=https://JhuoW.github.io/posts/gcond/><span class=title>Next Page »</span><br><span>ICLR2022 《Graph Condensation for Graph Neural Networks》 Reading Notes</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes on twitter" href="https://twitter.com/intent/tweet/?text=WWW2022%20%e3%80%8aClusterSCL%ef%bc%9aCluster-Aware%20Supervised%20Contrastive%20Learning%20on%20Graphs%e3%80%8b%20Reading%20Notes&amp;url=https%3a%2f%2fJhuoW.github.io%2fposts%2fclusterscl%2f&amp;hashtags=GNN%2cSelf-SupervisedLearning%2cVariationalInference"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fJhuoW.github.io%2fposts%2fclusterscl%2f&amp;title=WWW2022%20%e3%80%8aClusterSCL%ef%bc%9aCluster-Aware%20Supervised%20Contrastive%20Learning%20on%20Graphs%e3%80%8b%20Reading%20Notes&amp;summary=WWW2022%20%e3%80%8aClusterSCL%ef%bc%9aCluster-Aware%20Supervised%20Contrastive%20Learning%20on%20Graphs%e3%80%8b%20Reading%20Notes&amp;source=https%3a%2f%2fJhuoW.github.io%2fposts%2fclusterscl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fJhuoW.github.io%2fposts%2fclusterscl%2f&title=WWW2022%20%e3%80%8aClusterSCL%ef%bc%9aCluster-Aware%20Supervised%20Contrastive%20Learning%20on%20Graphs%e3%80%8b%20Reading%20Notes"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fJhuoW.github.io%2fposts%2fclusterscl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes on whatsapp" href="https://api.whatsapp.com/send?text=WWW2022%20%e3%80%8aClusterSCL%ef%bc%9aCluster-Aware%20Supervised%20Contrastive%20Learning%20on%20Graphs%e3%80%8b%20Reading%20Notes%20-%20https%3a%2f%2fJhuoW.github.io%2fposts%2fclusterscl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes on telegram" href="https://telegram.me/share/url?text=WWW2022%20%e3%80%8aClusterSCL%ef%bc%9aCluster-Aware%20Supervised%20Contrastive%20Learning%20on%20Graphs%e3%80%8b%20Reading%20Notes&amp;url=https%3a%2f%2fJhuoW.github.io%2fposts%2fclusterscl%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://giscus.app/client.js data-repo=JhuoW/WebComments data-repo-id=R_kgDOHHz8Ug data-category=Announcements data-category-id=DIC_kwDOHHz8Us4COa5e data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>Copyright &copy; 2025 <a href=https://JhuoW.github.io/>JhuoW‘s Notes</a></span>
<span></span><br><script>function siteTime(){var h=1e3,r=h*60,i=r*60,n=i*24,x=n*365,e=new Date,d=2019,w=1,_=16,y=19,b=15,C=11,l=e.getFullYear(),O=e.getMonth()+1,f=e.getDate(),p=e.getHours(),g=e.getMinutes(),v=e.getSeconds(),m=Date.UTC(d,w,_,y,b,C),j=Date.UTC(l,O,f,p,g,v),t=j-m,o=Math.floor(t/x),s=Math.floor(t/n-o*365),a=Math.floor((t-(o*365+s)*n)/i),c=Math.floor((t-(o*365+s)*n-a*i)/r),u=Math.floor((t-(o*365+s)*n-a*i-c*r)/h);d==l?document.getElementById("sitetime").innerHTML="本站已运行 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒":document.getElementById("sitetime").innerHTML="本站已运行 "+o+" 年 "+s+" 天 "+a+" 小时 "+c+" 分钟 "+u+" 秒"}setInterval(siteTime,1e3)</script><span id=sitetime>载入运行时间...</span>
<script type=text/javascript id=clustrmaps src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=268&t=tt&d=YsONH-MzO6L7yrkA73Z_QW7LuMTfdUhk0uhb_KaBv-g&co=f5f5f5&cmo=3acc3a&cmn=ff5353&ct=808080"></script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>