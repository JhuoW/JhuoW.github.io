<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Fraud Detection on JhuoW‘s Notes</title>
    <link>https://JhuoW.github.io/tags/fraud-detection/</link>
    <description>Recent content in Fraud Detection on JhuoW‘s Notes</description>
    <image>
      <url>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://JhuoW.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 13 May 2023 16:14:56 +0800</lastBuildDate><atom:link href="https://JhuoW.github.io/tags/fraud-detection/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fraud Detection based on Graph Neural Networks</title>
      <link>https://JhuoW.github.io/posts/fd/</link>
      <pubDate>Sat, 13 May 2023 16:14:56 +0800</pubDate>
      
      <guid>https://JhuoW.github.io/posts/fd/</guid>
      <description>基于图神经网络的异常检测</description>
      <content:encoded><![CDATA[<h1 id="1-label-information-enhanced-fraud-detection-against-low-homophily-in-graphs-www-23">1. Label Information Enhanced Fraud Detection against Low Homophily in Graphs (WWW &lsquo;23)</h1>
<h2 id="introduction">Introduction</h2>
<p>GNN4FD存在问题： 大多数基于GNN的欺诈检测器难以泛化到low homophily网络中，除此之外，如何充分利用label信息也是Fraud detection的重要因素。即如果一个Fraud node的邻居都是benign nodes，那么这样的图就是heterophily or low homophily graph，由于GNN的neighborhood aggregation机制，target node的表示会和它的邻居相似，无论他们的label是否不同，这样会使得GNN难以区分位于异质邻域内的Fraud nodes。另外， 现有的GNN4FD方法利用label信息的能力有限，这些方法仅在训练阶段使用label信息作为监督信号，但是在设计message passing 机制的过程中并没有使用label信息。</p>
<p>为了解决上述2个挑战，本文提出GAGA: 基于分组聚合的Transformer。 GAGA首先提出了一种预处理策略Group Aggregation (GA, 分组聚合)，然后每个节点的原始邻居特特征被分组为序列数据。 然后提出一种科学系的编码方式来编码structural，relational 和label信息 （全局），即整个图的relational encoding，group encoding 和 hop encoding （图中又几个relation就有几个relational embedding，取几个hop就又几个hop embedding..）。 最后用多头attention为每个节点聚合embedding sequence.</p>
<h2 id="preliminaries">Preliminaries</h2>
<p><strong>Multi-relational fraud graph construction</strong>  Multi-relational fraud graph $\mathcal{G}(\mathcal{V}, \mathcal{E}, \mathcal{X}, \mathcal{Y})$, 其中节点集$\mathcal{V}=\left\{v_1, v_2, \ldots, v_N\right\}(N=|\mathcal{V}|)$，$R$ 个邻接矩阵$\mathcal{E}=\left\{\mathbf{A}_1, \mathbf{A}_2, \ldots, \mathbf{A}_R\right\}(R=|\mathcal{E}|)$的多关系图 （$R$个关系）。节点feature vectors $X=\left\{\mathbf{x}_1, \mathrm{x}_2, \ldots, \mathrm{x}_N\right\}$以及节点的label集合$\mathcal{Y}$。 对于一个relation的邻接矩阵$\mathbf{A}_r$，如果$\mathbf{A}_1[u,v]=1$，那么在关系$r$下节点$u$和$v$被连接。每个节点$v \in \mathcal{V}$ 有一个$d$维feature vector $\mathbf{x}_v \in \mathbb{R}^d$。 在基于Graph的fraud detection中，我们考虑半监督场景，其中一小部分节点 $\hat{\mathcal{V}} \supset \mathcal{V}$是有label的 （$y=1$表示该节点为fraud node，$y=0$表示该节点为benign node）所以对于fraud graph，节点class数为2。</p>
<h2 id="gaga">GAGA</h2>
<p><img loading="lazy" src="/posts/2023-05-13-FD/1.png#center" alt=""  />
</p>
<p>上图为GAGA的框架。第一步为Group Aggregation，为预处理过程，为每个节点计算多条邻居信息，并且每跳内的信息分组表示（一跳内 label=0，label=1，label=None的节点分别聚合）。这样会为每个节点生成一系列embeddings。第二步中，定义三种类型可学习的embeddings：hop embeddings, relation embeddings,group embeddings，即如果每个节点有$K$-hop邻居参与聚合，那么hop embeddings 是一个$K \times d_H$ 矩阵，每个hop（结构特征）用一个$d_H$维向量表示。 同理relational embedding是一个$R \times d_H$矩阵，每个relation 用一个$d_H$维向量表示。一共存在3种group（$y=1$的group， $y=0$的group， 无标签邻居的group），所以group embeddings是一个$3 \times d_H$的矩阵，每种group 表示为一个$1 \times d_H$的向量。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/2.png#center" alt=""  />
</p>
<p>对于第一步得到的某一个节点$v$在relation 0的邻接矩阵$\mathbf{A}_0$下的第<strong>2</strong>跳邻居的fraud neighbors ($y=1$)的group embedding $g_v (r=0, h=2, y=1)$，为这个embedding融合hop信息 + relation信息+ group 信息得到 $x_v = g_v (r=0, h=2, y=1) + E_h(1) + E_r(0) + E_g(1)$    关于节点$v$的某个group的融合结构，关系特征的表示向量。最后一步将一个节点的所有多关系多hop的group向量用transformer合并然后输入MLP种来预测节点label。</p>
<p>即一个节点会生成 $\#relation (\#hop * (\#class+1) + 1)$个group 向量，每个group向量属于某个relation下的某个hop，这个group向量属于那个relation就加上这个relation的一维encoding，属于那是个hop就加上这个hop的1维encoding，这个group是0/1/None group就再加上对应group的encoding，从而得到这个group的最终encoding。</p>
<h3 id="group-aggregation">Group Aggregation</h3>
<p>对于Fraud detection任务，每个节点的label有3种情况，分别为 benign node $y=0$, Fraud node $y=1$, unlabeled node $y = None$，所以对于每个节点，它的第$k$hop邻居可以被分为3个group，每个group的节点做聚合：
$$
\mathbf{H}_g^{(k)}=\left[\mathbf{h}^{-}, \mathbf{h}^{+}, \mathbf{h}^*\right]^{(k)} \text { given } \hat{\mathcal{N}}_k(v)
$$
表示节点$k$ hop内的3个group表示向量 （由每个group节点取平均得到）。那么对于关系$r$下的所有$K$个hop内的group embedding可以表示为：
$$
\mathbf{H}_r=||_{k=1}^K \mathbf{H}_g^{(k)},
$$
那么对于所有$R$个关系，所涉及的group embedding sequence表示为：
$$
\mathbf{H}_s=||_{r=1}^R \mathbf{H}_{v, r} .
$$
关于每个节点，共有$S=R \times(P \times K+1)$个group embeddings。其中$R$为relation数， $K$为hop数，$K = \#class +1$ 为label数+1 （有多少种group）。</p>
<h3 id="learnable-encoding-for-graph-transformer">Learnable Encoding for Graph Transformer</h3>
<p>先将每个节点的所有$S$个group embeddings过一下MLP得到$\mathbf{X}_s \in \mathbb{R}^ {S\times d_H}$。用nn.Embedding来定义一个$K \times d_H$的可训练的Hop encoding 矩阵$E_h(\cdot)$，每行表示一种hop的embedding。 对于节点的$S$个group 向量，每个group向量都属于一个hop种，那么这个group embedding 就+对应hop的embedding，从而融合结构特征。 比如$X_s[3]$是hop 2的 group embedding，那么这个group embedding 就要加上 $E_h(1)$ 来保留hop结构特征。所有$S$个group embedding 都要融合各自的hop特征，他们的hop 特征为：
$$
\begin{gathered}
\mathbf{X}_h=[\underbrace{\mathbf{E}_h(0), \overbrace{\mathbf{E}_h(1), \mathrm{E}_h(1), \mathrm{E}_h(1)}^{1 \text { st hop }}, \ldots, \overbrace{\mathrm{E}_h(K), \mathrm{E}_h(K), \mathbf{E}_h(K)}^{K-\text { th hop }}}_{1 \text { st relation }}, \\
\ldots, \underbrace{\mathbf{E}_h(0), \mathrm{E}_h(1), \mathrm{E}_h(1), \mathrm{E}_h(1), \ldots, \mathrm{E}_h(K), \mathrm{E}_h(K), \mathrm{E}_h(K)}_{R \text {-th relation }}]
\end{gathered}
$$
$S$中1-st到R-th relation的所有1hop group embedding都要加上hop 1 的encoding $E_h(1)$，对于其他hop的group embedding 同理。$\mathbf{X}_s$ 表示一个节点的所有$S$个group embedding，每个group embedding 要加上它所在的relation encoding $E_r(\text{relation of group})$，hop encoding $E_h(\text{hop of group})$ 以及它属于那个group $E_g (\text{label of group})$:
$$
\mathrm{X}_{i n}=\mathrm{X}_s+\mathrm{X}_h+\mathrm{X}_r+\mathrm{X}_g
$$
$\mathbf{X}_in$为一个节点新的group embeddings。每个节点的每个group embedding 都要融合它所在的hop 特征，所在的relation特征和所在的label特征（group 特征）然后用transformer将一个节点所有$S$个融合丰富特征的group embedding 做聚合，从而得到这个节点的最终embedding，用这个最终embedding来计算binary classification loss。</p>
<h1 id="2-gccad-graph-contrastive-coding-for-anomaly-detection-tkde">2. GCCAD: Graph Contrastive Coding for Anomaly Detection （TKDE）</h1>
<p>本文的目标：拉近normal nodes和global embedding的距离，拉远fraud nodes和global embedding的距离。inference阶段通过计算testing node和global embedding的距离来判断节点是否为fraud node。</p>
<h2 id="preliminary-observations">Preliminary Observations</h2>
<p><img loading="lazy" src="/posts/2023-05-13-FD/3.png#center" alt=""  />
</p>
<p>上图中N-N表示Normal nodes之间的相似度，AB-AB表示Abnormal nodes之间的相似度，N-AB表示Normal nodes和Abnormal nodes之间的相似度，从图（a）中可以发现N-N节点原始之间的相似度差别很大，即normal nodes之间的相似度差别很大，相似度范围在$[0.2,0.8]$， 而abnormal nodes之间的相似度差别也很大。从图(a)中还可以看出，normal nodes (N)和abnormal nodes （AB）原始特征之间有很大一部分是相似的。从图(b)中可以看出，当使用GCN学习到新的节点feature vectors后，normal nodes之间的相似度(N-N)得到了提升，即从$[0.2,0.8]$改善到$[0.4,1.0]$，但是N-N，AB-AB内部的相似度依然变化较大，并且依然存在大量高相似度的N-AB。</p>
<p>从图（c）可以看出normal nodes的原始特征和global embedding （N-GL）之间相似度较高，并且相似度变化范围小。而Abnormal nodes和global embedding (AB-GL)之间的相似度较低，并且N-GL相似度和AB-GL相似度更好区分。所以通过与global embedding之间的相似度来区分normal nodes和abnormal nodes可能更加有效。而本文提出的GCCAD会进一步提升normal nodes和global emb之间的相似度，并且更加容易区分N-GL相似度与AB-GL相似度。</p>
<h2 id="gccad-model">GCCAD Model</h2>
<p>GCCAD基于监督对比学习来优化node embeddings和global embeddings，目标函数如下：
$$
\mathcal{L}_{\text {con }}=\underset{\substack{i: y_i=0 \\ j: y_j=1}}{\mathbb{E}}\left[-\log \frac{\exp \left(\mathbf{q}^{\top} \boldsymbol{h}_i / \tau\right)}{\sum_j \exp \left(\boldsymbol{q}^{\top} \boldsymbol{h}_j / \tau\right)+\exp \left(\boldsymbol{q}^{\top} \boldsymbol{h}_i / \tau\right)}\right]
$$
其中$\boldsymbol{q}$为global embedding，$\boldsymbol{h}_i$为normal node $v_i$的embedding。基于上述supervised contrastive loss，训练目标为增大训练集中normal nodes和global embedding的相似度，减少abnormal nodes和global embedding的相似度。即使得global embedding尽可能不受abnormal nodes的影响。</p>
<p><img loading="lazy" src="/posts/2023-05-13-FD/4.png#center" alt=""  />
</p>
<p>另外 本文不直接在原图上训练contrastive loss，而是先优化图结构，然后在优化的图结构上训练contrastive loss。由于message passing过程中会使得节点特征局部平滑，而abnormal node通常和位于normal node中，所以MPNN无论如何都会使得abnormal node和周围的normal node变得相似。所以在MPNN前先使用<strong>Edge Update</strong>模块对图更新，移除潜在的可以links，使得abnormal nodes尽可能少的接受到normal node的信息。本文提出Context-Aware Link Predictor来衡量原图中两个节点的边的保留概率：
$$
\begin{array}{r}
p_{i j}^{(l)}=\operatorname{MLP}\left(\left(\boldsymbol{h}_i^{(l-1)}-\boldsymbol{h}_j^{(l-1)}\right) \oplus\left(\boldsymbol{h}_i^{(l-1)}-\boldsymbol{q}^{(l-1)}\right)\right.
\left.\oplus\left(\boldsymbol{h}_j^{(l-1)}-\boldsymbol{q}^{(l-1)}\right)\right)
\end{array}
$$
两个节点间边的保留概率和两个节点间的embedding相似度有关（第1项），也和节点与global emb相似度有关。然后用训练集中标注好的normal nodes和abnormal nodes来训练$p_{i j}^{(l)}$：
$$
\mathcal{L}_{\text {link }}=\mathbb{E}\left[\sum_{i, j: y_i=y_j=0}-\log p_{i j}^{(l)}-\sum_{i, j: y_i \neq y_j=0}\left(1-\log p_{i j}^{(l)}\right)\right]
$$
通过这种方式，来将潜在的与abnormal nodes连接的边移除，从而使得abnormal node在message passing过程减少收到normal node的影响。基于每条边的保留概率，基于Bernoulli 分布来采样edges从而得到边mask 矩阵 $I^{(l)}$。新的图结构定义为：
$$
A_{i j}^{(l)}=\left(\alpha A_{i j}^{(l-1)}+(1-\alpha) p_{i j}^{(l)}\right) \odot I_{i j}^{(l)}
$$
反向传播时$I$视为常量，梯度从$p_{ij}$走。得到新的图结构后用GNN学习图的node embeddings。最后基于得到的node embeddings计算每个node embedding 和global embedding $\boldsymbol{q}$ 的相似度 （$\boldsymbol{q}$初始化为所有节点初始feature的均值）：
$$
s_i^{(l)}=\operatorname{cosine}\left(\boldsymbol{h}_i^{(l)}, \boldsymbol{m}\right)
$$
然后基于每个节点和global emb之间的相似度来计算聚合权重：
$$
\alpha_i^{(l)}=\frac{\exp \left(s_i^{(l)}\right)}{\sum_{j=1}^N \exp \left(s_j^{(l)}\right)}
$$
聚合node emb得到global emb:
$$
\boldsymbol{q}^{(l)}=\sum_{i=1}^N \alpha_i^{(l)} \cdot \boldsymbol{h}_i^{(l)}
$$
<strong>Training and Inference</strong></p>
<p>每个epoch 基于得到的global emb $\boldsymbol{q}$以及node embeddings $\boldsymbol{h}$ 来计算supervised contrastive loss，从而同时优化node embs和global embs。测试阶段，将测试节点的emb计算和global emb之间的相似度，相似度越低，测试节点是abnormal nodes的可能性越大。</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
