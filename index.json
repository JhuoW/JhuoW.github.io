[{"content":"1. Label Information Enhanced Fraud Detection against Low Homophily in Graphs (WWW \u0026lsquo;23) Introduction GNN4FD存在问题： 大多数基于GNN的欺诈检测器难以泛化到low homophily网络中，除此之外，如何充分利用label信息也是Fraud detection的重要因素。即如果一个Fraud node的邻居都是benign nodes，那么这样的图就是heterophily or low homophily graph，由于GNN的neighborhood aggregation机制，target node的表示会和它的邻居相似，无论他们的label是否不同，这样会使得GNN难以区分位于异质邻域内的Fraud nodes。另外， 现有的GNN4FD方法利用label信息的能力有限，这些方法仅在训练阶段使用label信息作为监督信号，但是在设计message passing 机制的过程中并没有使用label信息。\n为了解决上述2个挑战，本文提出GAGA: 基于分组聚合的Transformer。 GAGA首先提出了一种预处理策略Group Aggregation (GA, 分组聚合)，然后每个节点的原始邻居特特征被分组为序列数据。 然后提出一种科学系的编码方式来编码structural，relational 和label信息 （全局），即整个图的relational encoding，group encoding 和 hop encoding （图中又几个relation就有几个relational embedding，取几个hop就又几个hop embedding..）。 最后用多头attention为每个节点聚合embedding sequence.\nPreliminaries Multi-relational fraud graph construction Multi-relational fraud graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E}, \\mathcal{X}, \\mathcal{Y})$, 其中节点集$\\mathcal{V}=\\left\\{v_1, v_2, \\ldots, v_N\\right\\}(N=|\\mathcal{V}|)$，$R$ 个邻接矩阵$\\mathcal{E}=\\left\\{\\mathbf{A}_1, \\mathbf{A}_2, \\ldots, \\mathbf{A}_R\\right\\}(R=|\\mathcal{E}|)$的多关系图 （$R$个关系）。节点feature vectors $X=\\left\\{\\mathbf{x}_1, \\mathrm{x}_2, \\ldots, \\mathrm{x}_N\\right\\}$以及节点的label集合$\\mathcal{Y}$。 对于一个relation的邻接矩阵$\\mathbf{A}_r$，如果$\\mathbf{A}_1[u,v]=1$，那么在关系$r$下节点$u$和$v$被连接。每个节点$v \\in \\mathcal{V}$ 有一个$d$维feature vector $\\mathbf{x}_v \\in \\mathbb{R}^d$。 在基于Graph的fraud detection中，我们考虑半监督场景，其中一小部分节点 $\\hat{\\mathcal{V}} \\supset \\mathcal{V}$是有label的 （$y=1$表示该节点为fraud node，$y=0$表示该节点为benign node）所以对于fraud graph，节点class数为2。\nGAGA 上图为GAGA的框架。第一步为Group Aggregation，为预处理过程，为每个节点计算多条邻居信息，并且每跳内的信息分组表示（一跳内 label=0，label=1，label=None的节点分别聚合）。这样会为每个节点生成一系列embeddings。第二步中，定义三种类型可学习的embeddings：hop embeddings, relation embeddings,group embeddings，即如果每个节点有$K$-hop邻居参与聚合，那么hop embeddings 是一个$K \\times d_H$ 矩阵，每个hop（结构特征）用一个$d_H$维向量表示。 同理relational embedding是一个$R \\times d_H$矩阵，每个relation 用一个$d_H$维向量表示。一共存在3种group（$y=1$的group， $y=0$的group， 无标签邻居的group），所以group embeddings是一个$3 \\times d_H$的矩阵，每种group 表示为一个$1 \\times d_H$的向量。\n对于第一步得到的某一个节点$v$在relation 0的邻接矩阵$\\mathbf{A}_0$下的第2跳邻居的fraud neighbors ($y=1$)的group embedding $g_v (r=0, h=2, y=1)$，为这个embedding融合hop信息 + relation信息+ group 信息得到 $x_v = g_v (r=0, h=2, y=1) + E_h(1) + E_r(0) + E_g(1)$ 关于节点$v$的某个group的融合结构，关系特征的表示向量。最后一步将一个节点的所有多关系多hop的group向量用transformer合并然后输入MLP种来预测节点label。\n即一个节点会生成 $\\#relation (\\#hop * (\\#class+1) + 1)$个group 向量，每个group向量属于某个relation下的某个hop，这个group向量属于那个relation就加上这个relation的一维encoding，属于那是个hop就加上这个hop的1维encoding，这个group是0/1/None group就再加上对应group的encoding，从而得到这个group的最终encoding。\nGroup Aggregation 对于Fraud detection任务，每个节点的label有3种情况，分别为 benign node $y=0$, Fraud node $y=1$, unlabeled node $y = None$，所以对于每个节点，它的第$k$hop邻居可以被分为3个group，每个group的节点做聚合： $$ \\mathbf{H}_g^{(k)}=\\left[\\mathbf{h}^{-}, \\mathbf{h}^{+}, \\mathbf{h}^*\\right]^{(k)} \\text { given } \\hat{\\mathcal{N}}_k(v) $$ 表示节点$k$ hop内的3个group表示向量 （由每个group节点取平均得到）。那么对于关系$r$下的所有$K$个hop内的group embedding可以表示为： $$ \\mathbf{H}_r=||_{k=1}^K \\mathbf{H}_g^{(k)}, $$ 那么对于所有$R$个关系，所涉及的group embedding sequence表示为： $$ \\mathbf{H}_s=||_{r=1}^R \\mathbf{H}_{v, r} . $$ 关于每个节点，共有$S=R \\times(P \\times K+1)$个group embeddings。其中$R$为relation数， $K$为hop数，$K = \\#class +1$ 为label数+1 （有多少种group）。\nLearnable Encoding for Graph Transformer 先将每个节点的所有$S$个group embeddings过一下MLP得到$\\mathbf{X}_s \\in \\mathbb{R}^ {S\\times d_H}$。用nn.Embedding来定义一个$K \\times d_H$的可训练的Hop encoding 矩阵$E_h(\\cdot)$，每行表示一种hop的embedding。 对于节点的$S$个group 向量，每个group向量都属于一个hop种，那么这个group embedding 就+对应hop的embedding，从而融合结构特征。 比如$X_s[3]$是hop 2的 group embedding，那么这个group embedding 就要加上 $E_h(1)$ 来保留hop结构特征。所有$S$个group embedding 都要融合各自的hop特征，他们的hop 特征为： $$ \\begin{gathered} \\mathbf{X}_h=[\\underbrace{\\mathbf{E}_h(0), \\overbrace{\\mathbf{E}_h(1), \\mathrm{E}_h(1), \\mathrm{E}_h(1)}^{1 \\text { st hop }}, \\ldots, \\overbrace{\\mathrm{E}_h(K), \\mathrm{E}_h(K), \\mathbf{E}_h(K)}^{K-\\text { th hop }}}_{1 \\text { st relation }}, \\\\ \\ldots, \\underbrace{\\mathbf{E}_h(0), \\mathrm{E}_h(1), \\mathrm{E}_h(1), \\mathrm{E}_h(1), \\ldots, \\mathrm{E}_h(K), \\mathrm{E}_h(K), \\mathrm{E}_h(K)}_{R \\text {-th relation }}] \\end{gathered} $$ $S$中1-st到R-th relation的所有1hop group embedding都要加上hop 1 的encoding $E_h(1)$，对于其他hop的group embedding 同理。$\\mathbf{X}_s$ 表示一个节点的所有$S$个group embedding，每个group embedding 要加上它所在的relation encoding $E_r(\\text{relation of group})$，hop encoding $E_h(\\text{hop of group})$ 以及它属于那个group $E_g (\\text{label of group})$: $$ \\mathrm{X}_{i n}=\\mathrm{X}_s+\\mathrm{X}_h+\\mathrm{X}_r+\\mathrm{X}_g $$ $\\mathbf{X}_in$为一个节点新的group embeddings。每个节点的每个group embedding 都要融合它所在的hop 特征，所在的relation特征和所在的label特征（group 特征）然后用transformer将一个节点所有$S$个融合丰富特征的group embedding 做聚合，从而得到这个节点的最终embedding，用这个最终embedding来计算binary classification loss。\n2. GCCAD: Graph Contrastive Coding for Anomaly Detection （TKDE） 本文的目标：拉近normal nodes和global embedding的距离，拉远fraud nodes和global embedding的距离。inference阶段通过计算testing node和global embedding的距离来判断节点是否为fraud node。\nPreliminary Observations 上图中N-N表示Normal nodes之间的相似度，AB-AB表示Abnormal nodes之间的相似度，N-AB表示Normal nodes和Abnormal nodes之间的相似度，从图（a）中可以发现N-N节点原始之间的相似度差别很大，即normal nodes之间的相似度差别很大，相似度范围在$[0.2,0.8]$， 而abnormal nodes之间的相似度差别也很大。从图(a)中还可以看出，normal nodes (N)和abnormal nodes （AB）原始特征之间有很大一部分是相似的。从图(b)中可以看出，当使用GCN学习到新的节点feature vectors后，normal nodes之间的相似度(N-N)得到了提升，即从$[0.2,0.8]$改善到$[0.4,1.0]$，但是N-N，AB-AB内部的相似度依然变化较大，并且依然存在大量高相似度的N-AB。\n从图（c）可以看出normal nodes的原始特征和global embedding （N-GL）之间相似度较高，并且相似度变化范围小。而Abnormal nodes和global embedding (AB-GL)之间的相似度较低，并且N-GL相似度和AB-GL相似度更好区分。所以通过与global embedding之间的相似度来区分normal nodes和abnormal nodes可能更加有效。而本文提出的GCCAD会进一步提升normal nodes和global emb之间的相似度，并且更加容易区分N-GL相似度与AB-GL相似度。\nGCCAD Model GCCAD基于监督对比学习来优化node embeddings和global embeddings，目标函数如下： $$ \\mathcal{L}_{\\text {con }}=\\underset{\\substack{i: y_i=0 \\\\ j: y_j=1}}{\\mathbb{E}}\\left[-\\log \\frac{\\exp \\left(\\mathbf{q}^{\\top} \\boldsymbol{h}_i / \\tau\\right)}{\\sum_j \\exp \\left(\\boldsymbol{q}^{\\top} \\boldsymbol{h}_j / \\tau\\right)+\\exp \\left(\\boldsymbol{q}^{\\top} \\boldsymbol{h}_i / \\tau\\right)}\\right] $$ 其中$\\boldsymbol{q}$为global embedding，$\\boldsymbol{h}_i$为normal node $v_i$的embedding。基于上述supervised contrastive loss，训练目标为增大训练集中normal nodes和global embedding的相似度，减少abnormal nodes和global embedding的相似度。即使得global embedding尽可能不受abnormal nodes的影响。\n另外 本文不直接在原图上训练contrastive loss，而是先优化图结构，然后在优化的图结构上训练contrastive loss。由于message passing过程中会使得节点特征局部平滑，而abnormal node通常和位于normal node中，所以MPNN无论如何都会使得abnormal node和周围的normal node变得相似。所以在MPNN前先使用Edge Update模块对图更新，移除潜在的可以links，使得abnormal nodes尽可能少的接受到normal node的信息。本文提出Context-Aware Link Predictor来衡量原图中两个节点的边的保留概率： $$ \\begin{array}{r} p_{i j}^{(l)}=\\operatorname{MLP}\\left(\\left(\\boldsymbol{h}_i^{(l-1)}-\\boldsymbol{h}_j^{(l-1)}\\right) \\oplus\\left(\\boldsymbol{h}_i^{(l-1)}-\\boldsymbol{q}^{(l-1)}\\right)\\right. \\left.\\oplus\\left(\\boldsymbol{h}_j^{(l-1)}-\\boldsymbol{q}^{(l-1)}\\right)\\right) \\end{array} $$ 两个节点间边的保留概率和两个节点间的embedding相似度有关（第1项），也和节点与global emb相似度有关。然后用训练集中标注好的normal nodes和abnormal nodes来训练$p_{i j}^{(l)}$： $$ \\mathcal{L}_{\\text {link }}=\\mathbb{E}\\left[\\sum_{i, j: y_i=y_j=0}-\\log p_{i j}^{(l)}-\\sum_{i, j: y_i \\neq y_j=0}\\left(1-\\log p_{i j}^{(l)}\\right)\\right] $$ 通过这种方式，来将潜在的与abnormal nodes连接的边移除，从而使得abnormal node在message passing过程减少收到normal node的影响。基于每条边的保留概率，基于Bernoulli 分布来采样edges从而得到边mask 矩阵 $I^{(l)}$。新的图结构定义为： $$ A_{i j}^{(l)}=\\left(\\alpha A_{i j}^{(l-1)}+(1-\\alpha) p_{i j}^{(l)}\\right) \\odot I_{i j}^{(l)} $$ 反向传播时$I$视为常量，梯度从$p_{ij}$走。得到新的图结构后用GNN学习图的node embeddings。最后基于得到的node embeddings计算每个node embedding 和global embedding $\\boldsymbol{q}$ 的相似度 （$\\boldsymbol{q}$初始化为所有节点初始feature的均值）： $$ s_i^{(l)}=\\operatorname{cosine}\\left(\\boldsymbol{h}_i^{(l)}, \\boldsymbol{m}\\right) $$ 然后基于每个节点和global emb之间的相似度来计算聚合权重： $$ \\alpha_i^{(l)}=\\frac{\\exp \\left(s_i^{(l)}\\right)}{\\sum_{j=1}^N \\exp \\left(s_j^{(l)}\\right)} $$ 聚合node emb得到global emb: $$ \\boldsymbol{q}^{(l)}=\\sum_{i=1}^N \\alpha_i^{(l)} \\cdot \\boldsymbol{h}_i^{(l)} $$ Training and Inference\n每个epoch 基于得到的global emb $\\boldsymbol{q}$以及node embeddings $\\boldsymbol{h}$ 来计算supervised contrastive loss，从而同时优化node embs和global embs。测试阶段，将测试节点的emb计算和global emb之间的相似度，相似度越低，测试节点是abnormal nodes的可能性越大。\n3. H2-FDetector: A GNN-based Fraud Detector with Homophilic and Heterophilic Connections (WWW \u0026lsquo;22) Introduction Fraud graph通常包含2种类型的实体关联：1. homophilic connections: 相同label的节点被连接。 2. heterophilic connections: 不同label的节点被连接（fraudster 和 benign）。对于同时存在homophily 和heterophily的fraud graph，存在以下挑战：（1）如何学习一个边判别器，来判断图中的边是homophilic （两端都是benign 或 fraud） 还是 heterophilic (边一端是fraud一端是benign)。（2）如何为同时包含homophilic和heterophilic connections的fraud graph设计GNN。 (3) 如何利用整个类别的特征来判别新的fraud node? 即fraud node除了捕获与其邻居中benign nodes不相似的信息，还要捕获其他fraudster的信息，所以本方法让每个节点的表示和它所在类别的category feature相似，来捕获其他fraud 节点的特征。\nMethodology H$^2$-connection Identification 训练一个边判别器来预测图中任意一条边是homophilic edge还是heterophilic edge，基于训练集中的节点label。对于第$l$层的node embedding $H^{(l-1)}=\\left\\{h_1^{(l-1)}, h_2^{(l-1)}, \\ldots, h_N^{(l-1)}\\right\\}$。对于图中的每条边 $e_{uv}$，定义一个可训练的判别器来判断该边是homophilic还是heterophilic。 首先： $$ \\begin{aligned} \u0026amp; \\bar{h}_u^{(l)}=\\sigma\\left(W_t^{(l)} h_u^{(l-1)}\\right) \\\\ \u0026amp; \\bar{h}_v^{(l)}=\\sigma\\left(W_t^{(l)} h_v^{(l-1)}\\right) \\end{aligned} $$ 其中$W_t^{(l)} \\in \\mathbb{R}^{d_l \\times d_{l-1}}$是边判别器的可学习参数。然后基于$\\bar{h}_u^{(l)}$和$\\bar{h}_v^{(l)}$来计算边$e_{uv}$的homophilic分数。边的homophilic分数通过对两个节点的拼接 以及两个节点的不同来计算，$W_c^{(l)}$也是边判别器的参数，用于输出边分数： $$ m_{u v}^{(l)}=\\tanh \\left(W_c^{(l)}\\left[\\bar{h}_u^{(l)}||\\bar{h}_v^{(l)}||\\left(\\bar{h}_u^{(l)}-\\bar{h}_v^{(l)}\\right)\\right]\\right) $$ 其中 $\\mathrm{tanh}(\\cdot) \\in (-1,1)$。根据$m_{u v}^{(l)}$的符号来判断$e_{uv}$是homo还是hetero： $$ c_{u v}^{(l)}=\\operatorname{SIGN}\\left(m_{u v}^{(l)}\\right) $$ 基于第$l$层的embedding输入边判别器中，可以得到所有边是homophilic还是heterophilic： $$ C^{(l)}=\\left\\{c_{u v}^{(l)}\\right\\}_{e_{u v} \\in \\mathcal{E}} $$ 因为边判别器的输出是$\\{-1,1\\}$，所以对于训练集中的homophilic边，$y_{uv}=1$，那么$m_{u v}^{(l)}$要逼近1。同理对于heterophilic边，$y_{uv}=-1$，那么$m_{u v}^{(l)}$要逼近-1。即最小化以下目标： $$ \\mathcal{L}_{H I}^{(l)}=\\frac{1}{\\mathcal{E}_t} \\sum_{e_{u v}}^{\\mathcal{E}_t} \\max \\left(0,1-y_{u v} m_{u v}^{(l)}\\right) $$\nH$^2$-connection Aggregation 对于第$r$个relation下的图$\\mathcal{G}_r=\\left\\{\\mathcal{V}, X,\\left\\{\\mathcal{E}_r\\right\\}, Y\\right\\}$，$\\mathcal{N}_r(v)$表示关系$r$下节点$v$的邻居，$u \\in \\mathcal{N}_r(v)$。计算$u$对中心节点$v$重要性分数时考虑他们之间的边是homo边还是hetero边，所以在计算边$e_{vu}$间的重要性系数时考虑$c_{uv}^{(l)}$: $$ e_{u v}^{(l), r}=a^{(l), r}\\left[W_r^{{l}} h_v^{(l-1)} || c_{u v}^{(l)} W_r^{(l)} h_u^{(l-1)}\\right] $$ 其中 attention mechanism权重向量$a^{(l), r} \\in \\mathbb{R}^{1 \\times 2d_l}$。类似于GAT，邻居聚合的attention系数如下： $$ \\alpha_{u, v}^{(l), r}=\\frac{\\exp \\left\\{\\operatorname{LeakyReLU}\\left(e_{u v}^{(l), r}\\right)\\right\\}}{\\sum_{k \\in \\mathcal{N}_r(v)} \\exp \\left\\{\\operatorname{LeakyReLU}\\left(e_{k v}^{(l), r}\\right)\\right\\}} $$ 考虑多头attention，并且在邻居聚合的时候考虑边类型： $$ h_v^{(l), r}=||_{k=1}^K \\sigma\\left(\\sum_{u \\in \\mathcal{N}_r(v)} \\alpha_{u, v}^{(l), r, k} c_{u v}^{(l)} W_r^{(l), k} h_u^{(l-1)}\\right) $$ 对于$R$个relation，将每个节点每层输出$h_v^{(l), r}$的所有$R$个关系拼接后做特征变换，得到融合多关系的节点embedding： $$ \\begin{aligned} \u0026amp; h_v^{(l), \\text { all }}=||_{r=1}^R h_v^{(l), r} \\\\ \u0026amp; h_v^{(l)}=W_d^{(l)} h_v^{(l), \\text { all }} \\end{aligned} $$ 其中$W_d^{(l)} \\in \\mathbb{R}^{d_l \\times R d_l}$。最后一层输出维度为2，并做softmax： $$ p_v=\\operatorname{softmax}\\left(h_v^{(L)}\\right) $$ 用cross-entropy 训练GNN： $$ \\mathcal{L}_o=-\\sum_{v \\in \\mathcal{V}_t}\\left[y_v \\log \\left(p_v\\right)+\\left(1-y_v\\right) \\log \\left(1-p_v\\right)\\right] $$\nPrototype Extraction 除了训练边类型判别器H$^2$-connection Identification $\\mathcal{L}_{H I}$，节点embedding类型判别器$\\mathcal{L}_o$外，节点的每层embedding要和该节点所属的类embedding（prototype embedding）相似。类的prototype embedding: $$ \\begin{aligned} \\operatorname{prototype}_{\\text {fraud }}^{(l)} \u0026amp; =\\frac{1}{\\left|\\mathcal{V}_f\\right|} \\sum_{v \\in \\mathcal{V}_f} h_v^{(l)} \\\\ \\operatorname{prototype}_{\\text {benign }}^{(l)} \u0026amp; =\\frac{1}{\\left|\\mathcal{V}_b\\right|} \\sum_{v \\in \\mathcal{V}_b} h_v^{(l)} \\end{aligned} $$ distance between node $v$ and two prototype: $$ \\begin{aligned} \u0026amp; \\mathcal{D}_f^{{l}}(v)=|| h_v^{(l)}-\\text { prototype }_{f r a u d}^{(l)} ||_2 \\\\ \u0026amp; \\mathcal{D}_b^{{l}}(v)=|| h_v^{(l)}-\\text { prototype }_{\\text {benign }}^{(l)} ||_2 \\end{aligned} $$ $v$到两个prototype 的距离可以用softmax来输出一个2维概率向量，用来匹配他的ground truth one-hot label: $$ \\begin{gathered} \\mathcal{L}_{P E}^{(l)}=-\\sum_{v \\in \\mathcal{V}_t}\\left[y_v \\log \\left(q_v^{(l)}\\right)+\\left(1-y_v\\right) \\log \\left(1-q_v^{(l)}\\right)\\right] \\\\ q_v^{(l)}=\\operatorname{softmax}\\left(-\\mathcal{D}_{C(v)}^{(l)}(v)\\right) \\end{gathered} $$ 最终的训练目标为： $$ \\mathcal{L}=\\mathcal{L}_o+\\gamma_1 \\sum_{l-1}^L \\mathcal{L}_{H I}^{(l)}+\\gamma_2 \\sum_{l=1}^L \\mathcal{L}_{P E}^{(l)} $$\n","permalink":"https://JhuoW.github.io/posts/fd/","summary":"1. Label Information Enhanced Fraud Detection against Low Homophily in Graphs (WWW \u0026lsquo;23) Introduction GNN4FD存在问题： 大多数基于GNN的欺诈检测器难以泛化到low homophily网络中，除此之外，如何充分利用label信息也是Fraud detection的重要因素。即如果一个Fraud node的邻居都是benign nodes，那么这样的图就是heterophily or low homophily graph，由于GNN的neighborhood aggregation机制，target node的表示会和它的邻居相似，无论他们的label是否不同，这样会使得GNN难以区分位于异质邻域内的Fraud nodes。另外， 现有的GNN4FD方法利用label信息的能力有限，这些方法仅在训练阶段使用label信息作为监督信号，但是在设计message passing 机制的过程中并没有使用label信息。\n为了解决上述2个挑战，本文提出GAGA: 基于分组聚合的Transformer。 GAGA首先提出了一种预处理策略Group Aggregation (GA, 分组聚合)，然后每个节点的原始邻居特特征被分组为序列数据。 然后提出一种科学系的编码方式来编码structural，relational 和label信息 （全局），即整个图的relational encoding，group encoding 和 hop encoding （图中又几个relation就有几个relational embedding，取几个hop就又几个hop embedding..）。 最后用多头attention为每个节点聚合embedding sequence.\nPreliminaries Multi-relational fraud graph construction Multi-relational fraud graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E}, \\mathcal{X}, \\mathcal{Y})$, 其中节点集$\\mathcal{V}=\\left\\{v_1, v_2, \\ldots, v_N\\right\\}(N=|\\mathcal{V}|)$，$R$ 个邻接矩阵$\\mathcal{E}=\\left\\{\\mathbf{A}_1, \\mathbf{A}_2, \\ldots, \\mathbf{A}_R\\right\\}(R=|\\mathcal{E}|)$的多关系图 （$R$个关系）。节点feature vectors $X=\\left\\{\\mathbf{x}_1, \\mathrm{x}_2, \\ldots, \\mathrm{x}_N\\right\\}$以及节点的label集合$\\mathcal{Y}$。 对于一个relation的邻接矩阵$\\mathbf{A}_r$，如果$\\mathbf{A}_1[u,v]=1$，那么在关系$r$下节点$u$和$v$被连接。每个节点$v \\in \\mathcal{V}$ 有一个$d$维feature vector $\\mathbf{x}_v \\in \\mathbb{R}^d$。 在基于Graph的fraud detection中，我们考虑半监督场景，其中一小部分节点 $\\hat{\\mathcal{V}} \\supset \\mathcal{V}$是有label的 （$y=1$表示该节点为fraud node，$y=0$表示该节点为benign node）所以对于fraud graph，节点class数为2。","title":"Fraud Detection based on Graph Neural Networks"},{"content":"paper\nIntroduction 在GNN的neighborhood aggregation中，对于拥有很少邻居的节点，在聚合过程中是否充分从邻居中获得了信息是一个问题。为解决该问题， 本文提出为每个节点做局部增强，即以中心节点为条件，学习邻居节点表示的分布。为了在局部邻域中生成一些样本来提升中心节点的neighborhood aggregation，本文提出一种数据增强框架：LA-GNNs， 以局部结构和中心节点特征为条件，生成neighborhood features。具体来说，在pre-training 阶段，通过一个生成模型，以中心节点的特征为条件来学习邻居特征的条件概率分布。然后利用这个邻居特征分布来生成中心节点的增强邻居特征。另外，通过pre-training来学习邻居增强特征生成器的过程是与下游任务无关的，所以该生成器生成的增强特征可以应用于其他GNN模型。\nLocal Augmentation for Graph Neural Networks (LAGNN) Motivation GNN在message passing的过程利用局部信息聚合来得到node representations。 但是对于邻居数量较少的节点，从邻居中得到的信息可能会不足。为了为节点$v$的邻域中$\\mathcal{N}_v$生成更多样本，就需要知道邻居表示的分布。 由于一个节点邻居分布是与中心节点相关，所以我们要以中心节点$v$的representation为条件，学习它的邻居表示分布。\nApproach 本文利用Conditional Variational Auto-Encoder (CVAE) 来学习给定中心节点$v$，邻居$u \\in \\mathcal{N}_v$的节点特征的条件分布。给定中心节点特征$\\boldsymbol{X}_v$，关于中心节点的邻居分布为$p_\\theta(\\boldsymbol{X}_u | \\boldsymbol{X}_v)$。定义隐变量$\\mathbf{z}$，则先验可以定义为$p_\\theta(\\mathbf{z}|\\boldsymbol{X}_v)$。结合隐变量$\\mathbf{z}$，邻居特征$\\boldsymbol{X}_u$的分布可以改写为如下形式： $$ \\begin{aligned} \\log p_\\theta(\\boldsymbol{X}_u | \\boldsymbol{X}_v) \u0026amp;= \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\boldsymbol{X}_v)}{p_\\theta( \\boldsymbol{X}_v)}= \\frac{p_\\theta(\\boldsymbol{X}_u , \\boldsymbol{X}_v)p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)}{p_\\theta( \\boldsymbol{X}_v)p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)} \\\\ \u0026amp;=\\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\boldsymbol{X}_v, \\mathbf{z})}{p_\\theta( \\boldsymbol{X}_v)p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)} \\\\ \u0026amp;= \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)}\\\\ \\end{aligned} $$ 假设隐变量$\\mathbf{z}$的分布为$q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)$， 左右两边对分布$q_\\phi$计算期望，左边： $$ \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log p_\\theta(\\boldsymbol{X}_u | \\boldsymbol{X}_v) dz = \\log p_\\theta(\\boldsymbol{X}_u | \\boldsymbol{X}_v) $$ 右边： $$ \\begin{aligned} \u0026amp;\\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)} dz \\\\ =\u0026amp; \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\left(\\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} \\cdot \\frac{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)}{p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)}\\right) dz \\\\ =\u0026amp; \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} dz + \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\frac{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)}{p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)}dz \\\\ =\u0026amp; \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} dz + K L\\left(q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) || p_\\theta\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)\\right) \\\\ \\geq\u0026amp; \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u, \\mathbf{z} \\mid \\boldsymbol{X}_v\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} = ELBO \\end{aligned} $$ Evidence Lower Bound (ELBO) 可以写为 $$ \\begin{aligned} L_{ELBO} \u0026amp;= \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u, \\mathbf{z} \\mid \\boldsymbol{X}_v\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u, \\boldsymbol{X}_v, \\mathbf{z}\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) p_\\theta\\left(\\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u \\mid \\boldsymbol{X}_v, \\mathbf{z}\\right) p_\\theta\\left(\\boldsymbol{X}_v, \\mathbf{z}\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) p_\\theta\\left(\\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u \\mid \\boldsymbol{X}_v, \\mathbf{z}\\right) p_\\theta\\left(\\mathbf{z} \\mid \\boldsymbol{X}_v\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= -K L\\left(q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) || p_\\theta\\left(\\mathbf{z} \\mid \\boldsymbol{X}_v\\right)\\right)+\\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log p_\\theta\\left(\\boldsymbol{X}_u \\mid \\boldsymbol{X}_v, \\mathbf{z}\\right) \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= -K L\\left(\\underbrace{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)}_{Encoder} || \\underbrace{ p_\\theta\\left(\\mathbf{z} \\mid \\boldsymbol{X}_v\\right)}_{\\text{Normal Distribution}}\\right) + \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) }\\log p_\\theta\\left(\\boldsymbol{X}_u \\mid \\boldsymbol{X}_v, \\mathbf{z}\\right) \\end{aligned} $$ 在CVAE pre-training的过程中，第一项KL中CVAE Encoder 的一对邻接节点对，对于该节点对，输出一组分布参数均值$\\mu$和方差$\\sigma$，作为隐变量$z$的分布参数，第一项的优化目标使得编码器输出的分布接近Normal Distribution。然后利用reparameterization trick可微的从生成的$\\mathbf{z}$分布中采样一个encoding:\ndef reparameterize(means, log_var): std = torch.exp(0.5 * log_var) eps = torch.randn_like(std) return means + eps * std // z 若当前输入节点对为$(\\boldsymbol{X}_v, \\boldsymbol{X}_u)$，从输出的分布中采样一个encoding $\\mathbf{z}$然后将$\\mathbf{z}$输入decoding中，使得用$\\mathbf{z}$和中心节点$\\boldsymbol{X}_v$可以重构邻接节点$\\boldsymbol{X}_u$。用所有邻接节点对训练encoder参数$\\phi$和generator参数$\\phi$。 这样在下游预测任务前，直接从Normal Distribution 随机采样$\\mathbf{z}$，拼接中心节点$\\boldsymbol{X}_v$输入generator中就可以为节点$v$生成增强邻居。过程如下图所示：\n","permalink":"https://JhuoW.github.io/posts/lagcn/","summary":"paper\nIntroduction 在GNN的neighborhood aggregation中，对于拥有很少邻居的节点，在聚合过程中是否充分从邻居中获得了信息是一个问题。为解决该问题， 本文提出为每个节点做局部增强，即以中心节点为条件，学习邻居节点表示的分布。为了在局部邻域中生成一些样本来提升中心节点的neighborhood aggregation，本文提出一种数据增强框架：LA-GNNs， 以局部结构和中心节点特征为条件，生成neighborhood features。具体来说，在pre-training 阶段，通过一个生成模型，以中心节点的特征为条件来学习邻居特征的条件概率分布。然后利用这个邻居特征分布来生成中心节点的增强邻居特征。另外，通过pre-training来学习邻居增强特征生成器的过程是与下游任务无关的，所以该生成器生成的增强特征可以应用于其他GNN模型。\nLocal Augmentation for Graph Neural Networks (LAGNN) Motivation GNN在message passing的过程利用局部信息聚合来得到node representations。 但是对于邻居数量较少的节点，从邻居中得到的信息可能会不足。为了为节点$v$的邻域中$\\mathcal{N}_v$生成更多样本，就需要知道邻居表示的分布。 由于一个节点邻居分布是与中心节点相关，所以我们要以中心节点$v$的representation为条件，学习它的邻居表示分布。\nApproach 本文利用Conditional Variational Auto-Encoder (CVAE) 来学习给定中心节点$v$，邻居$u \\in \\mathcal{N}_v$的节点特征的条件分布。给定中心节点特征$\\boldsymbol{X}_v$，关于中心节点的邻居分布为$p_\\theta(\\boldsymbol{X}_u | \\boldsymbol{X}_v)$。定义隐变量$\\mathbf{z}$，则先验可以定义为$p_\\theta(\\mathbf{z}|\\boldsymbol{X}_v)$。结合隐变量$\\mathbf{z}$，邻居特征$\\boldsymbol{X}_u$的分布可以改写为如下形式： $$ \\begin{aligned} \\log p_\\theta(\\boldsymbol{X}_u | \\boldsymbol{X}_v) \u0026amp;= \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\boldsymbol{X}_v)}{p_\\theta( \\boldsymbol{X}_v)}= \\frac{p_\\theta(\\boldsymbol{X}_u , \\boldsymbol{X}_v)p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)}{p_\\theta( \\boldsymbol{X}_v)p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)} \\\\ \u0026amp;=\\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\boldsymbol{X}_v, \\mathbf{z})}{p_\\theta( \\boldsymbol{X}_v)p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)} \\\\ \u0026amp;= \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)}\\\\ \\end{aligned} $$ 假设隐变量$\\mathbf{z}$的分布为$q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)$， 左右两边对分布$q_\\phi$计算期望，左边： $$ \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log p_\\theta(\\boldsymbol{X}_u | \\boldsymbol{X}_v) dz = \\log p_\\theta(\\boldsymbol{X}_u | \\boldsymbol{X}_v) $$ 右边： $$ \\begin{aligned} \u0026amp;\\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)} dz \\\\ =\u0026amp; \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\left(\\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} \\cdot \\frac{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)}{p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)}\\right) dz \\\\ =\u0026amp; \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} dz + \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\frac{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)}{p_\\theta(\\mathbf{z}|\\boldsymbol{X}_u , \\boldsymbol{X}_v)}dz \\\\ =\u0026amp; \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta(\\boldsymbol{X}_u , \\mathbf{z}| \\boldsymbol{X}_v)}{ q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} dz + K L\\left(q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) || p_\\theta\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)\\right) \\\\ \\geq\u0026amp; \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u, \\mathbf{z} \\mid \\boldsymbol{X}_v\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} = ELBO \\end{aligned} $$ Evidence Lower Bound (ELBO) 可以写为 $$ \\begin{aligned} L_{ELBO} \u0026amp;= \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u, \\mathbf{z} \\mid \\boldsymbol{X}_v\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u, \\boldsymbol{X}_v, \\mathbf{z}\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) p_\\theta\\left(\\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u \\mid \\boldsymbol{X}_v, \\mathbf{z}\\right) p_\\theta\\left(\\boldsymbol{X}_v, \\mathbf{z}\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) p_\\theta\\left(\\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= \\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log \\frac{p_\\theta\\left(\\boldsymbol{X}_u \\mid \\boldsymbol{X}_v, \\mathbf{z}\\right) p_\\theta\\left(\\mathbf{z} \\mid \\boldsymbol{X}_v\\right)}{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)} \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= -K L\\left(q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) || p_\\theta\\left(\\mathbf{z} \\mid \\boldsymbol{X}_v\\right)\\right)+\\int_z q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) \\log p_\\theta\\left(\\boldsymbol{X}_u \\mid \\boldsymbol{X}_v, \\mathbf{z}\\right) \\mathrm{d} \\mathbf{z} \\\\ \u0026amp;= -K L\\left(\\underbrace{q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right)}_{Encoder} || \\underbrace{ p_\\theta\\left(\\mathbf{z} \\mid \\boldsymbol{X}_v\\right)}_{\\text{Normal Distribution}}\\right) + \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi\\left(\\mathbf{z} \\mid \\boldsymbol{X}_u, \\boldsymbol{X}_v\\right) }\\log p_\\theta\\left(\\boldsymbol{X}_u \\mid \\boldsymbol{X}_v, \\mathbf{z}\\right) \\end{aligned} $$ 在CVAE pre-training的过程中，第一项KL中CVAE Encoder 的一对邻接节点对，对于该节点对，输出一组分布参数均值$\\mu$和方差$\\sigma$，作为隐变量$z$的分布参数，第一项的优化目标使得编码器输出的分布接近Normal Distribution。然后利用reparameterization trick可微的从生成的$\\mathbf{z}$分布中采样一个encoding:","title":"ICML2022 《Local Augmentation for Graph Neural Networks》 Reading Notes"},{"content":"paper\nIntroduction Contrastive Learning 受益于区分hard negatives (最相似的negative pairs)， 但是其他领域的hard negative mining方法不适用于graph。 对于GCL来说大量embedding之后的hard negatives实际上是false negatives。如左图所示，对于CV上的SimCLR，它所学到的高相似度的negatives中，True negatives 和False negatives数量相当，那么从高相似度的negatives中采样到true negatives的概率更大。然而对于GCL方法GCA来说，是每个anchor节点将其他所有（inter/intra）节点作为negatives，使得在训练过程中与它同类的节点也变成anchor的negatives，这些negatives是false negatives。对于GCA，高相似度的negatives中false negatives的数量远多于true negatives，所以直接采样高相似度的negatives作为hard negatives来针对性的判别他们，会导致同类节点的embedding相互远离。这是传统的hard negatives mining方法在graph domain失效的原因。\n为了解决这个问题， 本文提出利用Beta mixture model来估计对于一个anchor node，它的一个negatve是true negative的概率，结合相似度，来衡量该negative的hardness。即与anchor node相似度越高，且它是true negative的概率越大，那么该节点的hardness越高。\nMethodology GCL 如上图所示，InfoNCE将跨图same node视为positives，其他节点对视为negatives，GCL的目标函数如下： $$ \\begin{aligned} \\ell\\left(\\boldsymbol{u}_{i},\\boldsymbol{v}_{i}\\right)= \\log \\frac{e^{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{i}\\right) / \\tau}}{\\underbrace{e^{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{i}\\right) / \\tau}}_{\\text{positive pair }}+\\underbrace{\\sum_{k\\neq i}e^{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{k}\\right) / \\tau}}_{\\text{inter-view negative pairs}}+\\underbrace{\\sum_{k\\neq i}e^{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{u}_{k}\\right) / \\tau}}_{\\text{intra-view negative pairs}}}, \\end{aligned} $$ Overall objective定义在所有跨图same node pairs上： $$ \\mathcal{J}=-\\frac{1}{2 N} \\sum_{i=1}^N\\left[\\ell\\left(\\boldsymbol{u}_{\\boldsymbol{i}}, \\boldsymbol{v}_{\\boldsymbol{i}}\\right)+\\ell\\left(\\boldsymbol{v}_{\\boldsymbol{i}}, \\boldsymbol{u}_{\\boldsymbol{i}}\\right)\\right] $$ 如果将GCA中的2层shared GNN替换为MLP，那么contrastive learning将不存在Message Passing，这样得到的true/false negative分布如(b)所示，可以看出Message Passing是GCL和CL之间产生区别关键因素。直观上，MP将anchor与相邻的negatives拉近，而相邻的negatives大多为False negatives（Homophily），所以GCL得到的高相似度negatives中false negatives要远多于True negatives。\nTheorem 3.1: $\\mathcal{G}$ is a non-bipartile and connected graph with $N$ nodes $\\mathcal{V}=\\left\\{v_1, \\ldots, v_N\\right\\}$ and $\\boldsymbol{X}_i^{(\\tau)}$ is the embedding of node $v_i$ after $\\tau$ tims message passing. For large enough $\\tau$, $$ \\left|\\left|\\boldsymbol{X}_i^{(\\tau)}-\\boldsymbol{X}_j^{(\\tau)}\\right|\\right|_2 \\leq\\left|\\left|\\boldsymbol{X}_i^{(0)}-\\boldsymbol{X}_j^{(0)}\\right|\\right|_2 $$ 该定理说明了Message passing之后，不同节点间的距离会变小。\nProGCL Figure 4描述了negatives相似度的直方图分布，即每个相似度下true/false negatives的相对数量。例如Figure 4（a），对于true negatives，高相似度的true negatives较少，中等相似度的true negatives较多；对于False negatives，大多相似度较高。所以直接采样高相似度的negatives作为hard negatives很容易采样到false negatives。那么随机采样一个相似度$s$的概率有该相似度在true negatives中的采样概率和在false negatives采样概率共同决定。因此相似度分布可以建模为mixture model。本文用Beta distribution来建模true/false negatives的相似度分布。beta distribution的pdf为： $$ p(s \\mid \\alpha, \\beta)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} s^{\\alpha-1}(1-s)^{\\beta-1}, $$ 其中$\\alpha, \\beta \u0026gt;0$ 是beta distribution的参数，$\\Gamma(\\cdot)$ 是gamma function。对于$C$个beta distribution的mixture model，相似度$s$在该mixture model中的概率为$s$在true negatives 相似度分布中的概率和false negatives相似度分布中的概率的加权： $$ p(s)=\\sum_{c=1}^{C} \\lambda_{c} p(s \\mid \\alpha_c, \\beta_c), $$ 其中$\\lambda_c$是mixture coefficients。下面要做的就是通过EM算法来优化两beta mixture model的参数，包括两个beta distribution的$\\alpha, \\beta$参数以及mixture coefficient，使得GCL学习得到的negatives similarities从改mixture model中采样的概率最大，即优化mixture model这个混合概率分布，使其符合所有negatives similarity分布。例如总共有10000个negatives（true+false），这一万个similarity分布要符合分布$p(s)$。用EM算法来优化mixture model $p(s)$时，E-step要计算后验： $$ p(c\\mid s)=\\frac{\\lambda_{c} p\\left(s \\mid \\alpha_{c}, \\beta_{c}\\right)}{\\sum_{j=1}^{C} \\lambda_{j} p\\left(s \\mid \\alpha_{j}, \\beta_{j}\\right)}. $$ 其中$c$为latent variable。然而优化Beta Mixture Model使其你和所有negatives的similarity分布是计算量巨大的，为了解决这个问题每次迭代只采样$M$ ($M \\ll N^2$)个相似度来的分布来优化BMM，使其你和这$M$个相似度的分布。首先计算$M$个相似度在每个beta distribution上的weighted average $\\bar{s}_c$以及variance $v_c^2$： $$ \\bar{s}_c=\\frac{\\sum_{i=1}^M p\\left(c \\mid s_i\\right) s_i}{\\sum_{i=1}^M p\\left(c \\mid s_i\\right)}, \\quad v_c^2=\\frac{\\sum_{i=1}^M p\\left(c \\mid s_i\\right)\\left(s_i-\\bar{s}_c\\right)^2}{\\sum_{i=1}^M p\\left(c \\mid s_i\\right)} . $$ 在M-step，如下优化BMM的参数 $\\alpha_c, \\beta_c, \\lambda_c$使得BMM符合$M$个similarity的分布： $$ \\alpha_c=\\bar{s}_c\\left(\\frac{\\bar{s}_c\\left(1-\\bar{s}_c\\right)}{v_c^2}-1\\right), \\quad \\beta_c=\\frac{\\alpha_c\\left(1-\\bar{s}_c\\right)}{\\bar{s}_c}，\\quad \\lambda_c=\\frac{1}{M} \\sum_{i=1}^M p\\left(c \\mid s_i\\right) $$ 这样通过E-step计算后验和average, variance，M-step基于average,variance和后验更新BMM的参数，循环迭代$I=10$次后，可以得到拟合输入negatives similarity的BMM分布的参数。最终，得到关于相似度$s$的BMM$p(s)$后，构成$p(s)$的两个分布即给定negative similarity $s$，该negative是true negative/false negative的概率为： $$ p(c \\mid s)=\\frac{\\lambda_c p\\left(s \\mid \\alpha_c, \\beta_c\\right)}{p(s)} $$\nProGCL-weight 对于一对negative pair，$\\boldsymbol{u}_i$为anchor和他的inter-view $\\boldsymbol{v}_k$，他们的相似度$s_{ik}$在true negative分布中的概率为$p(c_t|s_{ik})$，概率越大越可能是true negative，同时$s_{ik}$越大越hardness。所以在Contrastive loss中，越hard的true negative要被赋予越大的权重，使得被判别区分。negative pair权重为： $$ w(i, k)=\\frac{p\\left(c_t \\mid s_{i k}\\right) s_{i k}}{\\frac{1}{N-1} \\sum_{j \\neq i}\\left[p\\left(c_t \\mid s_{i j}\\right) s_{i j}\\right]} $$ 在Contrastive objective中对negatives加权： $$ \\ell_w\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{i}\\right)= \\log \\frac{e^{\\frac{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{i}\\right)}{ \\tau}}}{\\underbrace{e^{\\frac{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{i}\\right)}{\\tau}}}_{\\text{positive pair }}+\\underbrace{\\sum_{k\\neq i}w(i,k)e^{\\frac{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{k}\\right)}{\\tau}}}_{\\text{inter-view negative pairs}}+\\underbrace{\\sum_{k\\neq i}w(i,k)e^{\\frac{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{u}_{k}\\right)} {\\tau}}}_{\\text{intra-view negative pairs}}}, $$\n","permalink":"https://JhuoW.github.io/posts/progcl/","summary":"paper\nIntroduction Contrastive Learning 受益于区分hard negatives (最相似的negative pairs)， 但是其他领域的hard negative mining方法不适用于graph。 对于GCL来说大量embedding之后的hard negatives实际上是false negatives。如左图所示，对于CV上的SimCLR，它所学到的高相似度的negatives中，True negatives 和False negatives数量相当，那么从高相似度的negatives中采样到true negatives的概率更大。然而对于GCL方法GCA来说，是每个anchor节点将其他所有（inter/intra）节点作为negatives，使得在训练过程中与它同类的节点也变成anchor的negatives，这些negatives是false negatives。对于GCA，高相似度的negatives中false negatives的数量远多于true negatives，所以直接采样高相似度的negatives作为hard negatives来针对性的判别他们，会导致同类节点的embedding相互远离。这是传统的hard negatives mining方法在graph domain失效的原因。\n为了解决这个问题， 本文提出利用Beta mixture model来估计对于一个anchor node，它的一个negatve是true negative的概率，结合相似度，来衡量该negative的hardness。即与anchor node相似度越高，且它是true negative的概率越大，那么该节点的hardness越高。\nMethodology GCL 如上图所示，InfoNCE将跨图same node视为positives，其他节点对视为negatives，GCL的目标函数如下： $$ \\begin{aligned} \\ell\\left(\\boldsymbol{u}_{i},\\boldsymbol{v}_{i}\\right)= \\log \\frac{e^{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{i}\\right) / \\tau}}{\\underbrace{e^{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{i}\\right) / \\tau}}_{\\text{positive pair }}+\\underbrace{\\sum_{k\\neq i}e^{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{v}_{k}\\right) / \\tau}}_{\\text{inter-view negative pairs}}+\\underbrace{\\sum_{k\\neq i}e^{\\theta\\left(\\boldsymbol{u}_{i}, \\boldsymbol{u}_{k}\\right) / \\tau}}_{\\text{intra-view negative pairs}}}, \\end{aligned} $$ Overall objective定义在所有跨图same node pairs上： $$ \\mathcal{J}=-\\frac{1}{2 N} \\sum_{i=1}^N\\left[\\ell\\left(\\boldsymbol{u}_{\\boldsymbol{i}}, \\boldsymbol{v}_{\\boldsymbol{i}}\\right)+\\ell\\left(\\boldsymbol{v}_{\\boldsymbol{i}}, \\boldsymbol{u}_{\\boldsymbol{i}}\\right)\\right] $$ 如果将GCA中的2层shared GNN替换为MLP，那么contrastive learning将不存在Message Passing，这样得到的true/false negative分布如(b)所示，可以看出Message Passing是GCL和CL之间产生区别关键因素。直观上，MP将anchor与相邻的negatives拉近，而相邻的negatives大多为False negatives（Homophily），所以GCL得到的高相似度negatives中false negatives要远多于True negatives。","title":"ICML2022 《ProGCL：Rethinking Hard Negative Mining in Graph Contrastive Learning》 Reading Note"},{"content":"paper\nIntroduction 对于监督对比学习（Supervised Contrastive Learning, SupCon）, SupCon loss旨在表示空间中拉近属于同一个class的数据点，分离不同类的数据点。 但是SupCon难以处理高类内方差，类间相似度较大的数据集。为了解决该问题，本文提出了Cluster-aware supervised contrastive learning loss (ClusterSCL)。什么是高类内方差，高跨类相似度问题？如图1(a)所示，节点$u_1$和$u_3$ 是同类节点，$u_2$和$u_4$是同类节点。他们是同类节点但在不同的社区中，所以类内方差较大，即同一个类内的节点跨越了多个community。 另外$u_1$和$u_2$， $u_3$和$u_4$，是不同类的节点对， 但他们处在同一个社区中，导致在MPNN过程中，这些处在同一个community中的不同类节点被拉近，导致跨类相似度较高的问题。\n如果对节点$u_2$计算SupCon时，如图1(b)所示，SupCon会使得同类节点被拉近，如$u_2$和$u_4$会被拉近。但是$u_3$和$u_4$处在同一个社区中（structurally similar）那么MPNN会使得$u_3$和$u_4$被拉近，所以SupCon在拉近$u_2$和$u_4$的同时，会间接拉近不同类节点$u_2$和$u_3$。同时，对于构成negative pairs的不同类节点，例如$u_1$和$u_2$，SupCon会推远$u_1$和$u_2$，但是$u_1$和$u_5$ structurally similar, 因此会推远$u_1$和$u_2$会间接导致$u_2$和$u_5$这两个同类节点被推远。因此对于一个cluster内节点不同类，且不同cluster中存在同类节点的情况，会导致复杂的决策边界，即在拉近同类但不同社区的节点时，也会间接拉近不同类不同社区的节点。在推远不同类同社区的节点时，也可能间接推远同类同社区的节点。\n为了解决上述问题，最直接的方法是对于每个cluster，如图1(a)的Community 1，不考虑其他cluster，只对当前cluster内节点做SupCon。但是这么做忽略了跨cluster的同类节点交互，如$u_1$和$u_3$，$u_2$和$u_4$，这些跨cluster的positive pairs可能包含有益的信息。为了解决这个问题，本文提出cluster-aware data augmentation (CDA) 聚类感知的数据增强，来为每个节点生成augmented positives and negatives，如图1(b)中ClusterSCL所示。对于每个节点$u$，为它生成positive 和negative samples, 生成的samples 位于或接近$u$所在的cluster。Recall SupCon存在的问题：\n SupCon会使得$u_2$和$u_4$被拉的太近，从而间接导致$u_2$和$u_3$被拉近，所以对于high intra-class variances，要求不同cluster的同类节点如$u_2$和$u_4$不要被拉太近； SupCon会使得$u_1$和$u_2$被推远，从而间接导致$u_2$和$u_5$被推远，所以对于high inter-class similarity，要求同一个cluster内的不同类节点如$u_1$和$u_2$不要被拉的太远。  Method Two stage training with Supervised Contrastive Loss SupCon encourages samples of the same class to have similar representations, while pushes apart samples of different classes in the embedding space.\nFirst Stage: 计算node embeddings $H = g_\\theta(G)$， 然后用SupCon Loss来训练 $g_\\theta$ 。即已经知道训练集中的节点label，基于这些节点label，SupCon在embedding space中把同label的节点拉近，不同label的节点分开。$g_\\theta$用于得到node embeddings.\nSecond Stage：基于学习好的$g_\\theta$， 用$\\hat{Y}=f_\\phi\\left(g_\\theta(G)\\right)$来得到logits/prediction。即用cross-entropy loss来训练$f_\\theta$。\nSupCon 从同一个类中采样的节点构成positive pairs。batch中随机采样的节点对为negative pairs。给定$N$个随机采样的节点，对于每个节点，从其对应的class中随机采样一个不为它的节点作为positive pairs。所以一个batch有$N$对positive pairs，共$2N$个节点。\n用$I \\equiv\\{1,2, \\ldots, 2 N\\}$ 表示一个batch中的node indices。$s_i \\in I$表示这$2N$个节点中与节点$v_i$属于同一类的节点的indices。如下式所示， 在一个batch中，令$S_i \\subset I$表示$2N$个节点中可以与节点$v_i$构成positive pairs的节点集合。相比其他节点，SupCon的objective是拉近positive pairs。 $$ \\max \\sum_{i \\in I} \\frac{1}{\\left|S_i\\right|} \\sum_{s_i \\in S_i} \\log \\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{h}_{s_i} / \\tau\\right)}{\\sum_{j \\in I \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{h}_j / \\tau\\right)} $$ 但是如果batch中与$v_i$同类的节点$v_j$和它不属于同一个cluster，$v_j$所属的cluster不同类的节点较多，那么拉近他们的距离也会间接拉近$v_i$与不同类节点间的距离。\nClusterSCL 为了解决上述问题，本文提出ClusterSCL。\nCluster-aware Data Augmentation (CDA) 定义隐变量 $c_i$，该隐变量取值范围为$c_i \\in \\{1,2, \\ldots, M\\}$ 表示节点$v_i$属于哪一个cluster。给定两个anchor node $v_i$,$v_j$，CDA使用线性插值法为$v_j$生成augmentation： $$ \\tilde{\\mathbf{h}}_j=\\alpha \\mathbf{h}_j+(1-\\alpha) \\mathbf{w}_{c_i} \\tag{4} $$ 其中$c_i$指示了节点$v_i$所在的cluster。$\\mathbf{w}=\\left\\{\\mathbf{w}_m\\right\\}_{m=1}^M$ 表示每个cluster的cluster prototypes，即每个cluster的中心，serve to characterize the cluster。$\\mathbf{w}_{c_i}$表示节点$v_i$所在cluster的中心表示。$\\tilde{\\mathbf{h}}_j$包含了节点$v_j$的信息。并且，由于$\\mathbf{w}_{c_i}$是$v_i$所在cluster的prtotype，所以通过调整$\\alpha$，可以使$\\tilde{\\mathbf{h}}_j$位于$v_i$所在cluster的附近或内部。\n（1）如果$(v_i, v_j)$是一个batch中的positive pair，$v_i$是anchor节点，如果$v_j$位于$v_i$所在的cluster $c_i$内，那么就需要学到的$\\mathbf{h}_j$与$\\mathbf{h}_i$尽可能靠近。在SupCon中，需要设置较大的$\\alpha$使得$\\tilde{\\mathbf{h}}_j$保留更多$\\mathbf{h}_j$ 。对于anchor节点$\\mathbf{h}_i$，将它与$\\tilde{\\mathbf{h}}_j$拉近的时候，由于$\\tilde{\\mathbf{h}}_j$保留了更多$v_j$特征，所以$\\mathbf{h}_j$也会被和$\\mathbf{h}_i$拉近。如图2(a)所示。\n（2）如果$(v_i, v_j)$是positive pair，如果$v_j$位于$v_i$所在的cluster $c_i$外时，如果$v_j$周围有negative samples （不一定在该batch中），那么直接拉近$(v_i, v_j)$会间接导致潜在的negative samples也会被拉近，因此对于位于$v_i$所在cluster外的节点$v_j$，要求它最终的表示$\\mathbf{h}_j$不能被拉的太近，此时就需要小一些的$\\alpha$，使得$\\tilde{\\mathbf{h}}_j$保留少一些$v_j$的信息，那么在SupCon拉近$\\mathbf{h}_i$和$\\tilde{\\mathbf{h}}_j$的过程不会导致$\\mathbf{h}_j$被拉近太多。因为$ \\mathbf{w}_{c_i}$占据了$\\tilde{\\mathbf{h}}_j$的大部分，且它与$\\mathbf{h}_i$已经很接近。如图2(b)所示。\n对于negative pair $(v_i, v_j)$。如果$v_j$位于$v_i$所在的cluster $c_i$内，如果直接推远$\\mathbf{h}_i$和$\\mathbf{h}_j$，会导致如果$v_j$的邻居有$v_i$的positive sample，那么这个positive sample也会被间接推远。所以$\\tilde{\\mathbf{h}}_j$应该保留较少的$\\mathbf{h}_j$，即$\\alpha$应该小。但是本文不考虑negative pairs的这种情况了，直接套用posiive的CDA原则。\n综合上面的（1）（2）即对于close positive pairs，要让他们尽可能接近，即$\\alpha$要大，对于distant positive pairs，要让他们不要太接近，$\\alpha$要小一些。所以$\\alpha$应与positive pair之间的相似度相关。所以$\\alpha$定义如下： $$ \\alpha=\\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{h}_j\\right)}{\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{h}_j\\right)+\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{w}_{c_i}\\right)} $$ 上式分子越大$\\alpha$越大，说明对于anchor node $v_i$， 如果的positive sample $v_j$位于它的cluster内（$v_i$与$v_j$相似）,$\\mathbf{h}_j$的augmentation $\\tilde{\\mathbf{h}}_j$要保留越多自身信息。\nIntegraging Clustering and CDA into SupCon Learning 目标是给定节点$v_i$以及它所在的cluster 隐变量$c_i$，$v_i$的positive samples $s_i$的cluster-aware SupCon定义为条件概率： $$ \\begin{aligned} p\\left(s_i \\mid v_i, c_i\\right) \u0026amp;=\\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_{s_i} / \\tau\\right)}{\\sum_{j \\in V \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_j / \\tau\\right)} \\\\ \u0026amp;=\\frac{\\exp \\left(\\mathbf{h}_i^{\\top}\\left(\\alpha \\mathbf{h}_{s_i}+(1-\\alpha) \\mathbf{w}_{c_i}\\right) / \\tau\\right)}{\\sum_{j \\in V \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top}\\left(\\alpha \\mathbf{h}_j+(1-\\alpha) \\mathbf{w}_{c_i}\\right) / \\tau\\right)} \\end{aligned} \\tag{6} $$ 即对于positive pair $(v_i, s_i)$，最大化$\\mathbf{h}_i$和$s_i$的augmentation $\\tilde{\\mathbf{h}}_{s_i}$之间的一致性，$\\alpha$可以依据$s_i$和$c_i$的关系来调整$\\mathbf{h}_{s_i}$对于SupCon的贡献，使得$\\mathbf{h}_{i}$与$\\mathbf{h}_{s_i}$在位于不同cluster的情况下不会被拉的太近。其中$c_i$是隐变量。\n首先定义关于节点$v_i$的cluster分布，即$v_i$属于每个$c_i$的概率。给定anchor node $v_i$，它属于cluster $c_i \\in \\{1,2, \\ldots, M\\}$的概率定义为： $$ p\\left(c_i \\mid v_i\\right)=\\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{w}_{c_i} / \\kappa\\right)}{\\sum_{m=1}^M \\exp \\left(\\mathbf{h}_i^{\\top} \\mathbf{w}_m / \\kappa\\right)} \\tag{7} $$ $\\mathbf{w}_{c_i}$是cluster $c_i$的prototype表示，$v_i$属于在embedding空间中与它相似的cluster的概率更高。ClusterSCL旨在最大化给定锚节点$v_i$，锚节点与其positive sample $s_i$的likelihood： $$ p\\left(s_i \\mid v_i\\right)=\\int_{c_i} p\\left(c_i \\mid v_i\\right) p\\left(s_i \\mid v_i, c_i\\right) d c_i= \\sum^M_{m=1} p(m|v_i)p(s_i|v_i,m) \\tag{8} $$ 即给定anchor $v_i$，$s_i$是$v_i$的postive sample的概率为 当$v_i$属于cluster $m$的情况下，$s_i$是其positive sample的概率， over all clusters $m \\in M$。\n在likelihood Eq.(8)中，anchor node $v_i$ 为待优化参数，它的positive sample $s_i$为观测数据，$c_i$为隐变量。\nMaximize likelihood Eq.(8) via EM Objective: $\\mathrm{maximize} \\log p(s_i| v_i)$，即最大化positive pairs的条件概率 given anchor node $v_i$。\nE-step：$\\mathbb{E}_{p(c_i|s_i, v_i)} \\log p(s_i, c_i|v_i)$\nM-step: $\\widehat{v}_i = \\arg \\max_{v_i} \\mathbb{E}_{p(c_i|s_i, v_i)} \\log p(s_i, c_i|v_i)$\n可见，如果要通过EM算法来优化得到anchor node $v_i$的表示，需要计算后验 $p(c_i|s_i, v_i)$： $$ \\begin{aligned} p(c_i|s_i, v_i) \u0026amp; = \\frac{p(c_i,v_i, s_i)}{p(s_i,v_i)} \\\\ \u0026amp; = \\frac{p(v_i)p(s_i,c_i | v_i)}{p(s_i |v_i) p(v_i)}\\\\ \u0026amp; = \\frac{p(s_i,c_i|v_i)}{p(s_i |v_i)} \\\\ \u0026amp; = \\frac{p(s_i,c_i|v_i)}{\\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}\\\\ \u0026amp; = \\frac{\\frac{p(s_i,c_i,v_i)}{p(v_i)} = \\frac{p(c_i,v_i)}{p(v_i)} \\frac{p(s_i,c_i,v_i)}{p(c_i,v_i)} = p(c_i | v_i)p(s_i|c_i,v_i)}{\\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)} \\\\ \u0026amp; = \\frac{p(c_i | v_i)p(s_i|c_i,v_i)}{\\sum^M_{m=1}p(m|v_i)p(s_i | v_i, m)}\n\\end{aligned}\\tag{9} $$ 后验中，$p(c_i | v_i)$，$p(m|v_i)$是$v_i$的cluster 分布，在Eq.(7)中给出定义。但是，对于$p(s_i|c_i,v_i)$和$p(s_i | v_i, m)$，Eq.(6)中给出了它的定义，$p\\left(s_i \\mid v_i, c_i\\right) =\\frac{\\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_{s_i} / \\tau\\right)}{\\sum_{j \\in V \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_j / \\tau\\right)}$， 可以看出分母部分需要计算$\\mathbf{h}_i$与所有节点，并且还要over all $M$ cluster，因此后验难以计算。为了解决这个问题，我们可以maximize evidence lower bound (ELBO) of $\\log p(s_i | v_i)$： $$ \\begin{aligned} \\log p(s_i | v_i) \u0026amp;= \\log \\frac{p(s_i,v_i)}{p(v_i)} = \\log \\frac{p(s_i,v_i)p(c_i | v_i,s_i)}{p(v_i)p(c_i | v_i,s_i)} \\\\ \u0026amp;= \\log \\frac{p(v_i,s_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} \\\\ \u0026amp;= \\log \\frac{p(s_i|v_i,c_i)p(v_i,c_i)}{p(v_i)p(c_i | v_i,s_i)} = \\log \\frac{p(s_i|v_i,c_i)p(c_i|v_i)p(v_i)}{p(v_i)p(c_i | v_i,s_i)} \\\\ \u0026amp; = \\log \\frac{p(s_i|v_i,c_i)p(c_i|v_i)}{p(c_i | v_i,s_i)} \\\\ \u0026amp; = \\log p(s_i|v_i,c_i)-\\log p(c_i | v_i,s_i) + \\log p(c_i|v_i) \\\\ \u0026amp; \\text{引进一个关于隐变量$c_i$的分布$q(c_i)$，可以是任意形式，这里定义为$q(c_i|v_i,s_i)$}\\\\ \u0026amp; = \\log p(s_i|v_i,c_i) - \\log \\frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} + \\log \\frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} \\\\ \u0026amp;\\text{左右两边分别对$q(c_i|v_i,s_i)$求期望} \\\\ \\text{左边} \u0026amp;= \\int q(c_i|v_i,s_i) \\log p(s_i | v_i) d c_i = \\int q(c_i|v_i,s_i) d c_i \\cdot \\log p(s_i | v_i) = \\log p(s_i | v_i) \\\\ \\text{右边} \u0026amp;= \\int q(c_i|v_i,s_i) \\log p(s_i|v_i,c_i) dc_i - \\underbrace{\\int q(c_i|v_i,s_i) \\log \\frac{p(c_i | v_i,s_i)}{q(c_i|v_i,s_i)} dc_i}_{-\\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i))} + \\underbrace{\\int q(c_i|v_i,s_i) \\log \\frac{p(c_i|v_i)}{q(c_i|v_i,s_i)} dc_i}_{-\\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))} \\\\ \u0026amp;= \\int q(c_i|v_i,s_i) \\log p(s_i|v_i,c_i) dc_i + \\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i | v_i,s_i)) - \\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \\\\ \u0026amp; \\geq \\underbrace{\\mathbb{E}_{q(c_i|v_i,s_i)} \\log p(s_i|v_i,c_i) - \\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i))}_{ELBO} \\end{aligned} \\tag{10} $$ 因此， we have： $$ \\log p(s_i | v_i) \\geq ELBO = \\mathbb{E}_{q(c_i|v_i,s_i)} \\log p(s_i|v_i,c_i) - \\mathrm{KL}(q(c_i|v_i,s_i)||p(c_i|v_i)) \\tag{10} $$ 所以目标为找到node embeddings $\\mathbf{h}_i, \\forall i$， 最大化ELBO。接下来可以定义任意关于隐变量$c_i$的vartational distribution $q(c_i)$。 这里将关于$c_i$的variational distribution定义为用mini-batch近似后验的形式： $$ q\\left(c_i \\mid v_i, s_i\\right)=\\frac{p\\left(c_i \\mid v_i\\right) \\tilde{p}\\left(s_i \\mid v_i, c_i\\right)}{\\sum_{m=1}^M p\\left(m \\mid v_i\\right) \\tilde{p}\\left(s_i \\mid v_i, m\\right)} \\tag{11} $$ 其中$\\tilde{p}\\left(s_i \\mid v_i, c_i\\right)=\\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_{s_i} / \\tau\\right) / \\sum_{j \\in I \\backslash\\{i\\}} \\exp \\left(\\mathbf{h}_i^{\\top} \\tilde{\\mathbf{h}}_j / \\tau\\right)$，与Eq.(6)不同的是$\\tilde{p}\\left(s_i \\mid v_i, c_i\\right)$的分母只计算mini-batch $I$内的negative samples。并且用$\\tilde{p}\\left(s_i \\mid v_i, c_i\\right)$来替换了Eq.10 中的$ p(s_i|v_i,c_i)$，文中证明了这种替代的合理性。\n在E-step，定义了variational distribution $q\\left(c_i \\mid v_i, s_i\\right)$和likelihood objective的ELBO，在M-step最大化ELBO。给定一个mini-batch $I$，目标函数为最大化$I$中每对positive pairs 的ELBO： $$ \\max \\quad \\mathcal{L}_{\\mathrm{ELBO}}(\\theta, \\mathbf{w} ; I) \\approx \\frac{1}{|I|} \\sum_{i \\in I} \\frac{1}{\\left|S_i\\right|} \\sum_{s_i \\in S_i} \\mathcal{L}_{\\mathrm{ELBO}}\\left(\\theta, \\mathbf{w} ; v_i, s_i\\right) $$\n","permalink":"https://JhuoW.github.io/posts/clusterscl/","summary":"paper\nIntroduction 对于监督对比学习（Supervised Contrastive Learning, SupCon）, SupCon loss旨在表示空间中拉近属于同一个class的数据点，分离不同类的数据点。 但是SupCon难以处理高类内方差，类间相似度较大的数据集。为了解决该问题，本文提出了Cluster-aware supervised contrastive learning loss (ClusterSCL)。什么是高类内方差，高跨类相似度问题？如图1(a)所示，节点$u_1$和$u_3$ 是同类节点，$u_2$和$u_4$是同类节点。他们是同类节点但在不同的社区中，所以类内方差较大，即同一个类内的节点跨越了多个community。 另外$u_1$和$u_2$， $u_3$和$u_4$，是不同类的节点对， 但他们处在同一个社区中，导致在MPNN过程中，这些处在同一个community中的不同类节点被拉近，导致跨类相似度较高的问题。\n如果对节点$u_2$计算SupCon时，如图1(b)所示，SupCon会使得同类节点被拉近，如$u_2$和$u_4$会被拉近。但是$u_3$和$u_4$处在同一个社区中（structurally similar）那么MPNN会使得$u_3$和$u_4$被拉近，所以SupCon在拉近$u_2$和$u_4$的同时，会间接拉近不同类节点$u_2$和$u_3$。同时，对于构成negative pairs的不同类节点，例如$u_1$和$u_2$，SupCon会推远$u_1$和$u_2$，但是$u_1$和$u_5$ structurally similar, 因此会推远$u_1$和$u_2$会间接导致$u_2$和$u_5$这两个同类节点被推远。因此对于一个cluster内节点不同类，且不同cluster中存在同类节点的情况，会导致复杂的决策边界，即在拉近同类但不同社区的节点时，也会间接拉近不同类不同社区的节点。在推远不同类同社区的节点时，也可能间接推远同类同社区的节点。\n为了解决上述问题，最直接的方法是对于每个cluster，如图1(a)的Community 1，不考虑其他cluster，只对当前cluster内节点做SupCon。但是这么做忽略了跨cluster的同类节点交互，如$u_1$和$u_3$，$u_2$和$u_4$，这些跨cluster的positive pairs可能包含有益的信息。为了解决这个问题，本文提出cluster-aware data augmentation (CDA) 聚类感知的数据增强，来为每个节点生成augmented positives and negatives，如图1(b)中ClusterSCL所示。对于每个节点$u$，为它生成positive 和negative samples, 生成的samples 位于或接近$u$所在的cluster。Recall SupCon存在的问题：\n SupCon会使得$u_2$和$u_4$被拉的太近，从而间接导致$u_2$和$u_3$被拉近，所以对于high intra-class variances，要求不同cluster的同类节点如$u_2$和$u_4$不要被拉太近； SupCon会使得$u_1$和$u_2$被推远，从而间接导致$u_2$和$u_5$被推远，所以对于high inter-class similarity，要求同一个cluster内的不同类节点如$u_1$和$u_2$不要被拉的太远。  Method Two stage training with Supervised Contrastive Loss SupCon encourages samples of the same class to have similar representations, while pushes apart samples of different classes in the embedding space.","title":"WWW2022 《ClusterSCL：Cluster-Aware Supervised Contrastive Learning on Graphs》 Reading Notes"},{"content":"paper\nIntroduction 本文提出图浓缩技术（Graph Condensation），旨在将大图浓缩为一个小图，使得在小图上训练的GNN可以得到和大图相当的效果。通过优化gradient matching loss来模拟GNN在原图上的训练轨迹，从而解决图浓缩问题。\n通常有两个策略来简化图：Graph Sparsification(图稀疏化)和Graph Coarsening(图粗化)。图稀疏化通过减少边数来近似一个图； 图粗化旨在减少节点数量。（1）当节点具有属性特征时，由于稀疏化不会减少节点数量，因此属性量不会减少。 （2）图粗化的目的是保存一些图属性比如主特征值，这可能对下游任务不是最优的保存属性。\n本文提出图浓缩，来学习生成图的结构和节点属性，从这两方面同时进行浓缩。对于Reddit数据集，GCond可以将节点数浓缩至0.1%，并且在浓缩图上可以得到和原图相当的效果。如下图所示：\n本文解决了图浓缩面临的两个挑战：1. 构建目标函数， 2. 参数化可学习的节点特征和图结构。为了解决上述挑战，本文使用gradient matching loss来匹配每一个training step上原图与浓缩图的GNN参数梯度，使得GNN在浓缩图上的训练趋势与原图相匹配。为了参数化节点特征和图结构，本文将浓缩图的Feature Matrix设为自由参数矩阵，将浓缩图结构设为关于Feature matrix 的 函数（基于结构与特征相关联假设），使得计算开销降低。\nMethodology A graph $\\mathcal{T}=\\{\\mathbf{A}, \\mathbf{X}, \\mathbf{Y}\\}$，其中$\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$是$d$维节点特征，$\\mathbf{Y} \\in\\{0, \\ldots, C-1\\}^N$ 表示$N$个节点的labels，共有$C$个class。图浓缩旨在学习一个小的生成图$\\mathcal{S}=\\left\\{\\mathbf{A}^{\\prime}, \\mathbf{X}^{\\prime}, \\mathbf{Y}^{\\prime}\\right\\}$，其中$\\mathbf{A}^{\\prime} \\in \\mathbb{R}^{N^{\\prime} \\times N^{\\prime}}$是浓缩图的邻接矩阵，$\\mathbf{X}^{\\prime} \\in \\mathbb{R}^{N^{\\prime} \\times D}$是浓缩图的特征矩阵，$\\mathbf{Y}^{\\prime} \\in\\{0, \\ldots, C-1\\}^{N^{\\prime}}$是浓缩图的node labels 其中$N^{\\prime} \\ll N$，特征维度从$d$变为$D$。图浓缩的目标是基于原图训练过程学习浓缩图$\\mathcal{S}$，使得在$\\mathcal{S}$上训练的GNN应用在原图上的loss最小： $$ \\min_{\\mathcal{S}} \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}_{\\mathcal{S}}}(\\mathbf{A}, \\mathbf{X}), \\mathbf{Y}\\right) \\quad \\text { s.t } \\quad \\boldsymbol{\\theta}_{\\mathcal{S}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}}\\left(\\mathbf{A}^{\\prime}, \\mathbf{X}^{\\prime}\\right), \\mathbf{Y}^{\\prime}\\right), $$ Outer：固定GNN参数，优化小图。 Inner: 固定小图，在小图上训练GNN参数。\n由于如果就用一个固定的初始化参数来初始化GNN，小图的训练参数${\\boldsymbol{\\theta}_{\\mathcal{S}}}$可能会过拟合一个特定初始化的GNN。 因此为了使得浓缩data可以泛化到随机初始化的GNN $P_{\\boldsymbol{\\theta}_0}$，上面的目标函数可以改写为： $$ \\min_{\\mathcal{S}} \\mathrm{E}_{\\boldsymbol{\\theta}_0 \\sim P_{\\theta_0}}\\left[\\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}_{\\mathcal{S}}}(\\mathbf{A}, \\mathbf{X}), \\mathbf{Y}\\right)\\right] \\quad \\text { s.t. } \\quad \\boldsymbol{\\theta}_{\\mathcal{S}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}\\left(\\boldsymbol{\\theta}_0\\right)}\\left(\\mathbf{A}^{\\prime}, \\mathbf{X}^{\\prime}\\right), \\mathbf{Y}^{\\prime}\\right) $$ 具体实现就是在用一个GNN训练${\\boldsymbol{\\theta}_{\\mathcal{S}}}$后，初始化GNN继续训练${\\boldsymbol{\\theta}_{\\mathcal{S}}}$（${\\boldsymbol{\\theta}_{\\mathcal{S}}}$不用初始化）。也就是在不同的初始化GNN情况下训练${\\boldsymbol{\\theta}_{\\mathcal{S}}}$。\nGraph Condensation via Gradient Matching 通过优化bi-level问题来求解参数过于困难，因此使用gradient matching方法来匹配在不同数据上每次迭代的参数梯度。通过这种方式，模型在浓缩图$\\mathcal{S}$上的训练轨迹可以用来模拟原图$\\mathcal{T}$上的训练轨迹。模型的参数匹配可以表示为： $$ \\begin{gathered} \\min_{\\mathcal{S}} \\mathrm{E}_{\\boldsymbol{\\theta}_0 \\sim P_{\\boldsymbol{\\theta}_0}}\\left[\\sum_{t=0}^{T-1} D\\left(\\boldsymbol{\\theta}_t^{\\mathcal{S}}, \\boldsymbol{\\theta}_t^{\\mathcal{T}}\\right)\\right] \\quad \\text { with } \\\\ \\boldsymbol{\\theta}_{t+1}^{\\mathcal{S}}=\\operatorname{opt}_{\\boldsymbol{\\theta}}\\left(\\mathcal{L}\\left(\\operatorname{GNN}_{\\boldsymbol{\\theta}_t^{\\mathcal{S}}}\\left(\\mathbf{A}^{\\prime}, \\mathbf{X}^{\\prime}\\right), \\mathbf{Y}^{\\prime}\\right)\\right) \\text { and } \\boldsymbol{\\theta}_{t+1}^{\\mathcal{T}}=\\operatorname{opt}_{\\boldsymbol{\\theta}}\\left(\\mathcal{L}\\left(\\operatorname{GNN}_{\\boldsymbol{\\theta}_t^{\\mathcal{T}}}(\\mathbf{A}, \\mathbf{X}), \\mathbf{Y}\\right)\\right) \\end{gathered} $$ 表示第$t$次迭代时，原图上训练的GNN参数$\\boldsymbol{\\theta}_t^{\\mathcal{T}}$和小图上训练的GNN参数$\\boldsymbol{\\theta}_t^{\\mathcal{S}}$要接近。由于两个GNN初始化参数一致，如果将同一个GNN应用于两个图，要使他们的每一步训练轨迹一致，那么他们每一步的参数梯度应该一致，对于$\\mathrm{GNN}_{\\theta_t}$，所有$T$步的梯度匹配可以写为： $$ \\min_{\\mathcal{S}}\\mathrm{E}_{\\boldsymbol{\\theta}_0 \\sim P_{\\theta_0}}\\left [\\sum_{t=0}^{T-1} D\\left(\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}_t}\\left(\\mathbf{A}^{\\prime}, \\mathbf{X}^{\\prime}\\right), \\mathbf{Y}^{\\prime}\\right), \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}_t}(\\mathbf{A}, \\mathbf{X}), \\mathbf{Y}\\right)\\right)\\right] $$ 其中$D$为参数梯度矩阵之间的距离，若$\\mathbf{G}^{\\mathcal{S}}, \\mathbf{G}^{\\mathcal{T}} \\in \\mathbb{R}^{d_1 \\times d_2}$， 那么梯度矩阵的差异定义为： $$ \\operatorname{dis}\\left(\\mathbf{G}^{\\mathcal{S}}, \\mathbf{G}^{\\mathcal{T}}\\right)=\\sum_{i=1}^{d_2}\\left(1-\\frac{\\mathbf{G}_{\\mathbf{i}}^{\\mathcal{S}} \\cdot \\mathbf{G}_{\\mathbf{i}}^{\\mathcal{T}}}{\\left|\\left|\\mathbf{G}_{\\mathbf{i}}^{\\mathcal{S}}\\right|\\right|\\left|\\left|\\mathbf{G}_{\\mathbf{i}}^{\\mathcal{T}}\\right|\\right|}\\right) $$\nModeling Condensed Graph Data 要使得浓缩图可学习，直接参数化三个矩阵$\\mathbf{A}^{\\prime}, \\mathbf{X}^{\\prime}, \\mathbf{Y}^{\\prime}$并优化是很困难的。 因此本文先确定浓缩图的label矩阵$\\mathbf{Y}^{\\prime}$，具体来说，对于每个class的训练节点，选取特定比例的节点。例如训练集有3个类，每个类选取一定比例的训练节点，这些节点label作为浓缩图的node labels，features作为浓缩图的初始化node features $\\mathbf{X}^{\\prime}$。注意，这里$\\mathbf{X}^{\\prime}$是自由可训练参数。 由于在社交网络中，结构通常与节点特征相关，因此将结构$\\mathbf{A}^{\\prime}$设置成关于特征$\\mathbf{X}^{\\prime}$的函数，这样减少了参数量： $$ \\mathbf{A}^{\\prime}=g_{\\Phi}\\left(\\mathbf{X}^{\\prime}\\right), \\quad \\text { with } \\mathbf{A}_{i j}^{\\prime}=\\operatorname{Sigmoid}\\left(\\frac{\\operatorname{MLP}_{\\Phi}\\left(\\left[\\mathbf{x}_i^{\\prime} ; \\mathbf{x}_j^{\\prime}\\right]\\right)+\\operatorname{MLP}_{\\Phi}\\left(\\left[\\mathbf{x}_j^{\\prime} ; \\mathbf{x}_i^{\\prime}\\right]\\right)}{2}\\right) $$ 带入gradient matching loss中： $$ \\min_{\\mathbf{X}^{\\prime}, \\Phi} \\mathrm{E}_{\\boldsymbol{\\theta}_0 \\sim P_{\\theta_0}}\\left[\\sum_{t=0}^{T-1} D\\left(\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}_t}\\left(g_{\\Phi}\\left(\\mathbf{X}^{\\prime}\\right), \\mathbf{X}^{\\prime}\\right), \\mathbf{Y}^{\\prime}\\right), \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}_t}(\\mathbf{A}, \\mathbf{X}), \\mathbf{Y}\\right)\\right)\\right] $$ 其中：GNN的参数会重复初始化，增强$\\theta_\\mathcal{S}$的泛化效果。\n","permalink":"https://JhuoW.github.io/posts/gcond/","summary":"paper\nIntroduction 本文提出图浓缩技术（Graph Condensation），旨在将大图浓缩为一个小图，使得在小图上训练的GNN可以得到和大图相当的效果。通过优化gradient matching loss来模拟GNN在原图上的训练轨迹，从而解决图浓缩问题。\n通常有两个策略来简化图：Graph Sparsification(图稀疏化)和Graph Coarsening(图粗化)。图稀疏化通过减少边数来近似一个图； 图粗化旨在减少节点数量。（1）当节点具有属性特征时，由于稀疏化不会减少节点数量，因此属性量不会减少。 （2）图粗化的目的是保存一些图属性比如主特征值，这可能对下游任务不是最优的保存属性。\n本文提出图浓缩，来学习生成图的结构和节点属性，从这两方面同时进行浓缩。对于Reddit数据集，GCond可以将节点数浓缩至0.1%，并且在浓缩图上可以得到和原图相当的效果。如下图所示：\n本文解决了图浓缩面临的两个挑战：1. 构建目标函数， 2. 参数化可学习的节点特征和图结构。为了解决上述挑战，本文使用gradient matching loss来匹配每一个training step上原图与浓缩图的GNN参数梯度，使得GNN在浓缩图上的训练趋势与原图相匹配。为了参数化节点特征和图结构，本文将浓缩图的Feature Matrix设为自由参数矩阵，将浓缩图结构设为关于Feature matrix 的 函数（基于结构与特征相关联假设），使得计算开销降低。\nMethodology A graph $\\mathcal{T}=\\{\\mathbf{A}, \\mathbf{X}, \\mathbf{Y}\\}$，其中$\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$是$d$维节点特征，$\\mathbf{Y} \\in\\{0, \\ldots, C-1\\}^N$ 表示$N$个节点的labels，共有$C$个class。图浓缩旨在学习一个小的生成图$\\mathcal{S}=\\left\\{\\mathbf{A}^{\\prime}, \\mathbf{X}^{\\prime}, \\mathbf{Y}^{\\prime}\\right\\}$，其中$\\mathbf{A}^{\\prime} \\in \\mathbb{R}^{N^{\\prime} \\times N^{\\prime}}$是浓缩图的邻接矩阵，$\\mathbf{X}^{\\prime} \\in \\mathbb{R}^{N^{\\prime} \\times D}$是浓缩图的特征矩阵，$\\mathbf{Y}^{\\prime} \\in\\{0, \\ldots, C-1\\}^{N^{\\prime}}$是浓缩图的node labels 其中$N^{\\prime} \\ll N$，特征维度从$d$变为$D$。图浓缩的目标是基于原图训练过程学习浓缩图$\\mathcal{S}$，使得在$\\mathcal{S}$上训练的GNN应用在原图上的loss最小： $$ \\min_{\\mathcal{S}} \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}_{\\mathcal{S}}}(\\mathbf{A}, \\mathbf{X}), \\mathbf{Y}\\right) \\quad \\text { s.t } \\quad \\boldsymbol{\\theta}_{\\mathcal{S}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\mathcal{L}\\left(\\mathrm{GNN}_{\\boldsymbol{\\theta}}\\left(\\mathbf{A}^{\\prime}, \\mathbf{X}^{\\prime}\\right), \\mathbf{Y}^{\\prime}\\right), $$ Outer：固定GNN参数，优化小图。 Inner: 固定小图，在小图上训练GNN参数。","title":"ICLR2022 《Graph Condensation for Graph Neural Networks》 Reading Notes"},{"content":"最近一些工作通过解耦Message-Passing 和 Feature Learning的方式来提升GNN的可拓展性，这里对一小部分相关工作做一个小总结。\n1. Combining Label Propagation and Simple Models Out-performs Graph Neural Networks （ICLR2021） 模型首先忽略图结构，用简单模型（MLP），只使用节点特征预测label：\n$$ \\min \\sum_{i \\in L_{t}} \\ell\\left(f\\left(x_{i}\\right), y_{i}\\right) $$ 考虑一个inductive bias：预测误差与邻近度关系强相关，对图中所有节点的误差做校正。\n具体来说，首先计算一个初始的误差矩阵$E$，其中训练集误差如下 $$ E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:} $$ 其他节点的误差未知：$E_{L_{v},:}=0, \\quad E_{U,:}=0$。然后通过Label Propagation将误差矩阵在图上做平滑，使得相邻节点的误差相似：\n$$ \\hat{E}=\\underset{W \\in \\mathbb{R}^{n \\times c}}{\\arg \\min } \\operatorname{trace}\\left(W^{T}(I-S) W\\right)+\\mu||W-E||_{F}^{2} $$ 由此得到所有节点的误差矩阵$\\hat{E}$。然后用$\\hat{E}$对基础MLP预测做校正，这个post-processing过程不涉及训练参数，校正后的预测为： $$ Z^{(r)} = Z + \\hat{E} $$ 考虑homophily：校正的预测label要满足相邻节点label相似。 注意，这里不直接对$Z^{(r)}$做Label Propagation，而是构造了一个label矩阵$H \\in \\mathbb{R}^{n \\times c}$，其中将训练集真实label和验证+测试集校正label加入$H$中，然后对$H$做label propagation： $$ \\begin{aligned} H_{L_{t},:}\u0026amp;=Y_{L_{t},:} \\\\ H_{L_{v} \\cup U,:}\u0026amp;=Z_{L_{v} \\cup U,:}^{(r)} \\end{aligned} $$ Label Prop: $$ H^{(t+1)}=(1-\\alpha) H+\\alpha S H^{(t)} $$ 最后直接用收敛的$H$做预测，即$\\hat{Y} = H^{\\infty}$，node $i$ 的预测class为： $$ y_i = \\arg \\max _{j \\in\\{1, \\ldots, c\\}} \\hat{Y}_{i j} $$\n2. Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation (ICLR2022) MLPs相对于GNN更易于部署，并且避开了在线预测过程中的冷启动问题，即在线预测时，新节点的加入，它的邻居可能不能立即获得，MLP无需依赖于图结构，相比于GNN，MLP的延迟低。而GNN依赖于节点上下文，准确性更高。如何融合GNN和MLP的有点是本文解决的挑战。\n本文的核心发现为：可以在不显著损失性能的情况下将知识从 GNN 提取到 MLP，从而大大减少节点分类的推理时间。 知识蒸馏（Knowledge Distillation, KD）可以离线完成，并且与模型训练相耦合，即将推理阶段的耗时迁移到训练阶段，因为训练阶段可以容忍高耗时，而推理通常需要较大的时间减少。 也就是 训练阶段不仅要训练GNN模型，还要将模型的知识迁移到MLP上，而推理阶段直接用MLP做预测。\nMotivation： GNN虽然可以取得较好的性能，但是由于它依赖于图结构，所以有较大的推理延迟。 每增加一层GNN，就要为每个节点多捕获一跳邻居。对于一个平均度为$R$的图，在用$L$层GNN预测一个节点时，需要先获取的邻居数量为$\\mathcal{O}(R^L)$。 同时，由于层是sequential，所以要逐层获取邻居，总的延迟会随层数加深而增大，每层需要融合的邻居数也成指数上升趋势，是的层数越深，预测延迟越高。\n相反，MLP不利用图结构，使得推理时间远小于GNN，但是损害了节点分类的预测性能，因此 如何同时兼顾GNN和MLP的优势，是的模型以获得高精度和低延迟是一个待解决问题，因此本文提出了GNN和MLP的跨模型Knowledge Distillation。\n本文提出了GLNN，如上图所示，训练一个“Boosted” MLP，他的Knowledge来自于一个Teacher GNN。如上图所示，首先训练好一个GNN模型（这里用GraphSAGE+GCN Aggregation）作为Teacher，GNN的为节点集$v \\in V$的预测输出为$\\boldsymbol{z}_{v}$。 然后训练一个student MLP，predictions为$\\hat{\\boldsymbol{y}}_{v}$。loss由两部分组成，第一部分直接用MLP做半监督预测的损失；第二部分为KD，使得MLP对所有节点的预测与GNN的预测接近： $$ \\mathcal{L}=\\lambda \\Sigma_{v \\in \\mathcal{V}^{L}} \\mathcal{L}_{\\text {label }}\\left(\\hat{\\boldsymbol{y}}_{v}, \\boldsymbol{y}_{v}\\right)+(1-\\lambda) \\Sigma_{v \\in \\mathcal{V}} \\mathcal{L}_{\\text {teacher }}\\left(\\hat{\\boldsymbol{y}}_{v}, \\boldsymbol{z}_{v}\\right) $$ 其中$\\mathcal{L}_{\\text {label }}$为cross-entropy loss， $\\mathcal{L}_{\\text {teacher }}$为KL-divergence。在推理阶段直接使用MLP来预测测试集节点label，无需依赖图结构。预测阶段，直接用MLP做预测。实做中直接去掉了第一部分，只保留KD部分。\n3. Node Dependent Local Smoothing for Scalable Graph Learning （NeurIPS2021） 以往的工作已经证明了，简单的MLP+Label Smoothing的性能可以超过vanilla GCN。但是如何控制模型的平滑程度（extent of smoothness）依然是个问题。 太少的平滑迭代会造成欠平滑（under-smoothing）问题，而太多的迭代会造成过平滑（oversmoothing）问题。另外，不同节点应有特定的平滑程度。大多数现有的GNN使用一个统一的迭代次数$k$，即每个节点都聚合它的$k$阶邻居。 这种统一的聚合方式存在问题，因为迭代次数应与每个节点的度和局部结构相关。如下图所示，两个红色节点有完全不同的局部结构。 左边的红色节点位于Dense region中，因此它的传播速度更快，即很少的step就可以扩散到很多节点，因此，对于这类节点，需要较少次的propagation，因为小的iteration足以聚合足够多的节点。而右边红色节点需要更多次的propagation来聚合足够的信息。\n本文提出了Node-dependent Local Smoothing (NDLS)， 计算每个节点的特定迭代次数（LSI），使得节点只会聚合它特定LSI以内的节点。\nOver-Smoothing issue: The convolution matrix is defined as $\\widetilde{\\mathbf{D}}^{r-1} \\tilde{\\mathbf{A}} \\widetilde{\\mathbf{D}}^{-r}$. By continually smoothing the node feature with infinite number of propagation in SGC, the final smoothed feature $\\mathbf{X}^{(\\infty)}$ is： $$ \\mathbf{X}^{(\\infty)}=\\hat{\\mathbf{A}}^{\\infty} \\mathbf{X}, \\quad \\hat{\\mathbf{A}}_{i, j}^{\\infty}=\\frac{\\left(d_{i}+1\\right)^{r}\\left(d_{j}+1\\right)^{1-r}}{2 m+n} $$ 可以看到，无限多层的SGC，卷积矩阵$\\hat{\\mathbf{A}}_{i, j}^{\\infty}$每个元素只和两个节点的度，节点数$n$以及边数$m$有关，即$\\hat{\\mathbf{A}}^{\\infty} \\mathbf{X}$表示节点聚合图中所有节点，聚合权重只与两个节点的度有关，与节点位置，距离根节点距离无关。因此度越大的节点被赋予更大的聚合权重，无论两个节点的相对位置如何。\nLocal Smoothing Iteration (LSI)： 对于SGC，第$k$次迭代为$\\mathbf{X}^{(k)}=\\hat{\\mathbf{A}}^{k} \\mathbf{X}$。对于第$h$个feature，定义Influence matrix $I_{h}(k)$： $$ I_{h}(k)_{i j}=\\frac{\\partial \\hat{\\mathbf{X}}_{i h}^{(k)}}{\\partial \\hat{\\mathbf{X}}_{j h}^{(0)}} $$ $I_{h}(k)_{i j}$ 表示在第$h$个feature处，节点$j$的变化对于节点$i$第$k$层输出的影响。因为$\\mathbf{X}^{(k)}_{ih}=\\hat{\\mathbf{A}}^{k}_i \\mathbf{X}_{:,h}$， 所以$\\frac{\\partial \\hat{\\mathbf{X}}_{i h}^{(k)}}{\\partial \\hat{\\mathbf{X}}_{j h}^{(0)}} = \\hat{\\mathbf{A}}^{k}_{ij}$， 与特征$h$无关，因此节点$j$的输入特征对于节点$i$的第$k$层表示的影响为$\\hat{\\mathbf{A}}^{k}_{ij}$。那么第$k$次迭代的影响力矩阵可以写为： $$ I(k)=\\hat{\\mathbf{A}}^{k} $$\n$$ \\tilde{I} = I(\\infty)=\\hat{\\mathbf{A}}^{\\infty} $$\n影响力矩阵$I(\\infty)$收敛于稳态分布，即无限多层GNN时，节点$j$对$i$的representation的影响只与两个节点的度有关，与他们之间的结构关系无关。\n$\\tilde{I}_i$为节点$v_i$的over-smoothing stationarity，LSI衡量了其他节点对$v_i$的影响力到达over-smoothing所需最少的迭代次数： $$ K(i, \\epsilon)=\\min \\left\\{k:\\left|\\left|\\tilde{I}_{i}-I(k)_{i}\\right|\\right|_{2}\u0026lt;\\epsilon\\right\\} $$ 上式可以通过迭代次数来控制节点$v_i$的平滑程度，是node-specific的。\nNDLS Pipeline 1). 节点依赖的局部平滑 （NDLS-F） 2). 基于平滑特征的base prediction 3). 节点依赖的标签平滑（NDLS-L）。其中，第一步为pre-processing，第三部为post-processing。图结构仅用于第一部和第三部，参数的训练过程不涉及图结构，因此更加scalable。\nLSI和参数$\\epsilon$ 使得每个节点可以与oversmoothing保持一个合适的距离。NDLS-F和NDLS-L利用label smoothing和node smoothing, 具体如下:\n  NDLS-F\n对于节点$i$， 计算它的LSI$K(i,\\epsilon)$，对节点$i$做$K(i,\\epsilon)$次propagation，然后做multi-scale features residual connection: $$ \\widetilde{\\mathbf{X}}_{i}(\\epsilon)=\\frac{1}{K(i, \\epsilon)+1} \\sum_{k=0}^{K(i, \\epsilon)} \\mathbf{X}_{i}^{(k)} $$ 上式的矩阵形式可写为： $$ \\tilde{\\mathbf{X}}(\\epsilon)=\\sum_{k=0}^{\\max_{i} K(i, \\epsilon)} \\mathbf{M}^{(k)} \\mathbf{X}^{(k)}, \\quad \\mathbf{M}^{(\\mathbf{k})}{ }_{i j}=\\left\\{\\begin{array}{l} \\frac{1}{K(i, \\epsilon)+1}, \\quad i=j \\quad \\text { and } \\quad k \\leq K(i, \\epsilon) \\\\ 0, \\quad \\text { otherwise } \\end{array}\\right. $$\n  Base Prediction\n基于NDLS-F的得到的smoothed features $\\tilde{\\mathbf{X}}$训练一个base predictor，$\\hat{\\mathbf{Y}}=f(\\widetilde{\\mathbf{X}})$。 $\\hat{\\mathbf{Y}}$ 为模型预测的soft label (softmax output)。\n  NDLS-L\n将预测的soft label $\\hat{\\mathbf{Y}}$ 再做Label Propagation，得到最终的预测结果。依然先计算在做Label Prop时，每个节点的LSI。 $\\hat{\\mathbf{Y}}^{(k)}=\\hat{\\mathbf{A}}^{k} \\hat{\\mathbf{Y}}$， 同理，影响力矩阵为$J_h(k) = I_h(k)$，LP for node $i$： $$ \\tilde{\\mathbf{Y}}_{i}(\\epsilon)=\\frac{1}{K(i, \\epsilon)+1} \\sum_{k=0}^{K(i, \\epsilon)} \\hat{\\mathbf{Y}}_{i}^{(k)} $$\n  ","permalink":"https://JhuoW.github.io/posts/glnn/","summary":"最近一些工作通过解耦Message-Passing 和 Feature Learning的方式来提升GNN的可拓展性，这里对一小部分相关工作做一个小总结。\n1. Combining Label Propagation and Simple Models Out-performs Graph Neural Networks （ICLR2021） 模型首先忽略图结构，用简单模型（MLP），只使用节点特征预测label：\n$$ \\min \\sum_{i \\in L_{t}} \\ell\\left(f\\left(x_{i}\\right), y_{i}\\right) $$ 考虑一个inductive bias：预测误差与邻近度关系强相关，对图中所有节点的误差做校正。\n具体来说，首先计算一个初始的误差矩阵$E$，其中训练集误差如下 $$ E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:} $$ 其他节点的误差未知：$E_{L_{v},:}=0, \\quad E_{U,:}=0$。然后通过Label Propagation将误差矩阵在图上做平滑，使得相邻节点的误差相似：\n$$ \\hat{E}=\\underset{W \\in \\mathbb{R}^{n \\times c}}{\\arg \\min } \\operatorname{trace}\\left(W^{T}(I-S) W\\right)+\\mu||W-E||_{F}^{2} $$ 由此得到所有节点的误差矩阵$\\hat{E}$。然后用$\\hat{E}$对基础MLP预测做校正，这个post-processing过程不涉及训练参数，校正后的预测为： $$ Z^{(r)} = Z + \\hat{E} $$ 考虑homophily：校正的预测label要满足相邻节点label相似。 注意，这里不直接对$Z^{(r)}$做Label Propagation，而是构造了一个label矩阵$H \\in \\mathbb{R}^{n \\times c}$，其中将训练集真实label和验证+测试集校正label加入$H$中，然后对$H$做label propagation： $$ \\begin{aligned} H_{L_{t},:}\u0026amp;=Y_{L_{t},:} \\\\ H_{L_{v} \\cup U,:}\u0026amp;=Z_{L_{v} \\cup U,:}^{(r)} \\end{aligned} $$ Label Prop: $$ H^{(t+1)}=(1-\\alpha) H+\\alpha S H^{(t)} $$ 最后直接用收敛的$H$做预测，即$\\hat{Y} = H^{\\infty}$，node $i$ 的预测class为： $$ y_i = \\arg \\max _{j \\in\\{1, \\ldots, c\\}} \\hat{Y}_{i j} $$","title":"MLP and GNNs"},{"content":"paper\nIntroduction 本文研究了结合更简单的模型来处理transductive node classification任务。 主要包括1个预测模块和两个后处理（post-processing）模块：\n Base predictor：忽略图结构，用简单模型（如MLP或线性模型）使用节点特征预测label Error correction：校正步骤，将训练数据中的不确定性（误差）传播到图上，来校正Base predictor的预测 Smoothing：在图上平滑预测  其中只有第一步base predictor的参数是可学习的，即涉及图结构的操作（Correction和Smoothing）无需参数学习，这种简单的模型使得参数数量减少了几个数量级，训练时间也减少了几个数量级，并且可以轻松扩展到大规模图。\n相比于过去的GNN+LP的方法，C\u0026amp;S更加高效：1）C\u0026amp;S首先只使用节点特征进行低成本的base prediction；2）然后再使用标签传播对基础预测进行校正 ；3）最后对最终预测进行平滑。 第一步是预测操作，后两部是后处理操作，也就是第一步为一个独立的端到端模型，后两部基于一个inductive bias来调整节点的表示。即homophily假设：相连节点的误差和label是相似的（正相关）。训练节点的误差和它相连节点的误差应相似，那么就用训练节点的误差来校正邻居节点。\n因此，将标签更加直接的整合到GNN的学习算法中是本文性能的关键，并且发现LP与node features是相互互补的信号。实验表明，在OGB-Products上，参数量比GNN少了2个数量级，训练时间也减少2个数量级。\nCorrect and Smooth (C\u0026amp;S) Model 给定无向图$G=(V,E)$，$A$为邻接矩阵，$S=D^{-1 / 2} A D^{-1 / 2}$为归一化邻接矩阵。节点集划分为labeled nodes $V_L$和unlabeled nodes $V_U$，其中$V = V_L \\cup V_U$。进一步，labeled nodes可以分为训练节点集$V_{L_t}$和验证节点集$V_{L_v}$。训练集和验证集的label分别为$Y_{L_t:}$和$Y_{L_v:}$， 每行为label的one-hot向量。\nSimple Base Predictor $$ \\min \\sum_{i \\in L_{t}} \\ell\\left(f\\left(x_{i}\\right), y_{i}\\right) $$\n$f(\\cdot)$为简单的训练模型+softmax，如浅层MLP， $\\ell$为cross-entropy loss。 基于训练节点$V_{L_t}$特征的模型$f$可以得到输出预测$Z \\in \\mathbb{R}^{n\\times c}$， 其中$Z$的每行是softmax得到的分类概率分布。Simple Base Predictor是一个独立训练的端到端模型。\nCorrecting Base Prediction with Error Correlation (使用邻居误差关联来纠正基础预测） 通过融合标签信息来提高base prediction $Z$的准确率。 本文期望base prediction中的误差沿着图中的边正相关，即节点$i$出的预测误差在它的邻居处也会出现相似的误差。为了实现这个目的，首先定义一个误差矩阵$E \\in \\mathbb{R}^{n \\times c}$用来保存每个节点的预测误差，其中误差为训练数据集上的残差（只有训练节点由误差）其他没有训练过程中不知道label的节点误差设为0： $$ E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:} \\quad 为训练集节点 V_{L_t}的误差 $$\n$$ E_{L_{v},:}=0, \\quad E_{U,:}=0 \\quad 验证集和测试集节点的误差设为0 $$\n若base predictor做出perfect prediction时，$E$将是一个全0矩阵。\n然后要在$E$中填补图中其他节点（验证集和测试集节点）的误差。依据homophily假设，相邻接节点的误差相似，因此使用标签扩散技术来平滑误差，即优化一下函数： $$ \\hat{E}=\\underset{W \\in \\mathbb{R}^{n \\times c}}{\\arg \\min } \\operatorname{trace}\\left(W^{T}(I-S) W\\right)+\\mu||W-E||_{F}^{2} $$ 实际上就是Laplacian Smoothing，$W \\in \\mathbb{R}^{n \\times c}$表示$c$个信号，最小化第一项用来保证$W$每一列在图上平滑，即相邻的节点的误差向量$W_i \\in \\mathbb{R}^c$相似。第二项要求$W$要尽量接近$E$。最优的$W$表示为$\\hat{E}$，上式可以通过： $$ E^{(t+1)}=(1-\\alpha) E+\\alpha S E^{(t)} $$ 迭代求解，其中$\\alpha = 1/(1+\\mu)$。得到的$\\hat{E}$称为smoothed errors。Base predictor中得到的$E_{L_{t},:}$只包含训练节点的误差，而通过在图上的误差平滑后，基于homophily 假设可以得到图中所有的误差（平滑误差）。已知图中所有节点在base predictor中的预测为$Z$，它的误差矩阵为$\\hat{E}$，然后用误差矩阵来校正预测结果： $$ Z^{(r)} = Z + \\hat{E} $$ 通过这种方式对图中所有节点的误差做校正会存在一个问题，已知训练集的总误差为$||E||_2$， 通过迭代计算得到的总误差为$||E^{(t)}||_2$，且$||S||_2 = 1$, 所以下式成立： $$ ||E^{(t+1)}||_2 = ||(1-\\alpha)E + \\alpha S E^{(t)}||_2 \\leq (1-\\alpha)||E||_2+\\alpha ||S||_2||E^{(t)}||_2 = (1-\\alpha)||E||_2+\\alpha ||E^{(t)}||_2 $$ 因为 $||E^{(1)}||_2 \\leq (1-\\alpha)||E||_2 + \\alpha ||E||_2 = ||E||_2$， 可以推出$||E^{(2)}||_2\\leq (1-\\alpha)||E||_2 + \\alpha ||E^{(1)}||_2 \\leq (1-\\alpha)||E||_2 + \\alpha ||E||_2 = ||E||_2$。因此，可以得到： $$ ||E^{(t)}||_2 \\leq ||E||_2 $$ 可以看出，传播之后的总error小了，因此不能完全纠正所有节点上的error。并且实验发现，对残差做放缩可以取得实质上的帮助。因此，本文提出两种方式对误差（残差）做放缩。\n  Autoscale. 希望平滑后的总误差$\\hat{E}$可以放缩到和训练集误差$E$差不多大小。 由于我们只知道训练节点上的真实误差，所以用训练集节点上的平均误差来缩放。形式上，令$e^T_j \\in \\mathbb{R}^c$表示$E$的第$j$行，即节点$j$的误差，用$\\hat{e}^T_j \\in \\mathbb{R}^c$表示平滑之后的节点$j$误差，即$\\hat{E}$的第$j$行。 定义： $$ \\sigma=\\frac{1}{\\left|L_{t}\\right|} \\sum_{j \\in L_{t}}\\left|\\left|e_{j}\\right|\\right|_{1} $$ $\\sigma$为训练集节点的平均误差，对于每个unlabeled node $i \\in V_U$，它的校正prediction为： $$ Z_{i,:}^{(r)}=Z_{i,:}+\\frac{\\sigma}{\\left|\\left|\\hat{e}_{i}\\right|\\right|_{1}} \\cdot \\hat{e}_{i}^{T} $$ 其中校正误差为 $\\frac{\\hat{e}_{i}^{T}}{\\left|\\left|\\hat{e}_{i}\\right|\\right|_{1}} \\cdot \\sigma$， 表示对校正的误差做放缩，使得unlabeled node每个节点的校正误差为训练集节点的平均校正误差。\n  Scaled Fixed Diffusion (FDiff-scale). 每次传播完，把training node的误差设为真实误差再进行下一次传播。另外，本文发现用超参数来放缩误差校正也是有效的：$Z^{(r)}=Z+s \\hat{E}$。\n  Smoothing Final Predictions with Prediction Correlation 在用$\\hat{E}$校正base prediction $Z$后得到校正预测矩阵$Z^{(r)}$。为了得到最后的预测，本文进一步对校正预测做平滑处理。 动机是：图中相邻的顶点可能具有相似的标签，即homophily假设。而在对base prediction做校正后，仅是的相邻的节点具有相似的误差（误差正相关），为了使其进一步满足homophily假设（即标签正相关），本文通过另一个LP来使得label在图上是平滑的：定义一个预测矩阵$H \\in \\mathbb{R}^{n \\times c}$, 将训练集ground-truth label赋值给对应位置： $$ H_{L_{t},:}=Y_{L_{t},:} $$ 然后将验证集和测试集节点的位置赋值为校正预测 （平滑误差后的预测）： $$ H_{L_{v} \\cup U,:}=Z_{L_{v} \\cup U,:}^{(r)} $$ 然后对矩阵$H$做LP: $$ H^{(t+1)}=(1-\\alpha) H+\\alpha S H^{(t)} $$ 其中 $H^{(0)} = H$，直到收敛，即收敛的$H^{(T)}$会尽可能保持平滑，并且和$H^{(0)}$接近，即最优$H$会依据训练集label使得相邻节点的label尽可能一样的同时，对图中节点做校正。\n与APPNP的关系 APPNP也可以视为先特征变换，再平滑的过程，但APPNP是端到端的过程，label信息没有被加入平滑过程中。\n","permalink":"https://JhuoW.github.io/posts/c_and_s/","summary":"paper\nIntroduction 本文研究了结合更简单的模型来处理transductive node classification任务。 主要包括1个预测模块和两个后处理（post-processing）模块：\n Base predictor：忽略图结构，用简单模型（如MLP或线性模型）使用节点特征预测label Error correction：校正步骤，将训练数据中的不确定性（误差）传播到图上，来校正Base predictor的预测 Smoothing：在图上平滑预测  其中只有第一步base predictor的参数是可学习的，即涉及图结构的操作（Correction和Smoothing）无需参数学习，这种简单的模型使得参数数量减少了几个数量级，训练时间也减少了几个数量级，并且可以轻松扩展到大规模图。\n相比于过去的GNN+LP的方法，C\u0026amp;S更加高效：1）C\u0026amp;S首先只使用节点特征进行低成本的base prediction；2）然后再使用标签传播对基础预测进行校正 ；3）最后对最终预测进行平滑。 第一步是预测操作，后两部是后处理操作，也就是第一步为一个独立的端到端模型，后两部基于一个inductive bias来调整节点的表示。即homophily假设：相连节点的误差和label是相似的（正相关）。训练节点的误差和它相连节点的误差应相似，那么就用训练节点的误差来校正邻居节点。\n因此，将标签更加直接的整合到GNN的学习算法中是本文性能的关键，并且发现LP与node features是相互互补的信号。实验表明，在OGB-Products上，参数量比GNN少了2个数量级，训练时间也减少2个数量级。\nCorrect and Smooth (C\u0026amp;S) Model 给定无向图$G=(V,E)$，$A$为邻接矩阵，$S=D^{-1 / 2} A D^{-1 / 2}$为归一化邻接矩阵。节点集划分为labeled nodes $V_L$和unlabeled nodes $V_U$，其中$V = V_L \\cup V_U$。进一步，labeled nodes可以分为训练节点集$V_{L_t}$和验证节点集$V_{L_v}$。训练集和验证集的label分别为$Y_{L_t:}$和$Y_{L_v:}$， 每行为label的one-hot向量。\nSimple Base Predictor $$ \\min \\sum_{i \\in L_{t}} \\ell\\left(f\\left(x_{i}\\right), y_{i}\\right) $$\n$f(\\cdot)$为简单的训练模型+softmax，如浅层MLP， $\\ell$为cross-entropy loss。 基于训练节点$V_{L_t}$特征的模型$f$可以得到输出预测$Z \\in \\mathbb{R}^{n\\times c}$， 其中$Z$的每行是softmax得到的分类概率分布。Simple Base Predictor是一个独立训练的端到端模型。\nCorrecting Base Prediction with Error Correlation (使用邻居误差关联来纠正基础预测） 通过融合标签信息来提高base prediction $Z$的准确率。 本文期望base prediction中的误差沿着图中的边正相关，即节点$i$出的预测误差在它的邻居处也会出现相似的误差。为了实现这个目的，首先定义一个误差矩阵$E \\in \\mathbb{R}^{n \\times c}$用来保存每个节点的预测误差，其中误差为训练数据集上的残差（只有训练节点由误差）其他没有训练过程中不知道label的节点误差设为0： $$ E_{L_{t},:}=Y_{L_{t},:}-Z_{L_{t},:} \\quad 为训练集节点 V_{L_t}的误差 $$","title":"ICLR2021 《Combining Label Propagation and Simple Models Out-performs Graph Neural Networks》 Reading Notes"},{"content":"Paper\nIntroduction SubGNN在学习子图representation时保留子图的三种属性：Position，Neighborhood，Structure，每种属性包含Internal 和Border两方面，并且要精心设计不同的anchor patch，所以过于复杂。通过分析SubGNN和普通GNN，作者发现子图表示的核心可能是区分子图内部和外部节点。基于此发现，本文提出了一种labeling trick, 即max-zero-one，来提升子图GNN的表达能力和可拓展性。\nSubgraph representation task 如上图所示，目标子图$\\mathbb{S}$被嵌入在整个图中，并且可能拥有多个连通分量，目标是学习子图的表示向量，使其可以预测子图的属性。NeurIPS2020文章SubGNN提出子图级message-passing来代替节点级的message passing，并且设计了三个message passing通道，每个通道分为内部和边界模块，分别捕获子图分量间的交互，以及子图与图的其他部分之间的交互。尽管取得了比普通GNN更好的效果，但是SubGNN需要繁琐的预计算，因为SubGNN通过不同采样规则的anchor patch来传递子图分量之间，以及子图分量与图其他部分之间的相关性，而三个通道共6个aspects需要不同的采样规则，以及各自的message passing，计算十分冗长（这里有解读）。另外 SubGNN对每个aspects需要使用不同的anchor patch随机采样策略，无法保证采样的anchor patch是最优的，因此效果的方差较大，使得鲁棒性堪忧。\n通过对比SubGNN相较于普通GNN的优势，作者发现对于子图任务来说，区分子图内部节点和外部节点非常重要。基于这个发现，本文提出了一种labeling trick，即max-zero-one labeling trick，来标注每个节点是否在子图外或者子图内。\nLabeling Trick [1]: 使用GNN生成multi-node representations （即，为一组节点，例如子图生成表示向量），该方法说明了为高阶结构生成有表达能力的representation，需要捕获结构内不同节点间的交互。在实现上，labeling trick通过一个专门设计的label来表示节点的结构特征，这个label与node feature 结合作为新的feature输入GNN中。\n注： 本文只考虑诱导子图，即每个子图的每个连通分量保留原图中的所有边。\nPlain GNN and SubGNN 如上图所示，$G$是一个regular graph, 所以在没有节点feature的情况下，每个节点的embedding相同，所以GNN无法区分子图$\\mathcal{S}$和$\\mathcal{S}^\\prime$。如下图所示，Plain GNN 在message passing中子图$\\mathcal{S}$内部节点1同时接收来自子图内和子图外的邻居信息，并不会加以区分。同样$\\mathcal{S}^\\prime$中节点3也同时接收子图内外节点，因此对于Plain GNN ，它无法区分节点1和3，因此无法区分两个子图。\n而SubGNN引入了3个通道：position (P)，neighborhood (N), 和structure (S) 每个通道分别学习Internal 和Border两方面，共6个属性融入子图表示学习中。对于子图$\\mathcal{S}$，为了捕获某个通道$i$的属性，SubGNN首先随机采样$n_A$个anchor patches： $\\mathbb{A}_{i}=\\left\\{\\mathcal{A}_{i}^{(1)}, \\ldots, \\mathcal{A}_{i}^{\\left(n_{A}\\right)}\\right\\}$，然后学习$\\mathcal{S}$中的每个连通分量在这个属性$i$下的表示向量，通过子图内部连通分量和anchor patches之间的消息传递，来捕获子图内部连通分量的相对位置/邻域/结构信息，以及子图连通分量相对于子图外部分的位置/邻域/结构信息。如图2右边所示。对于通道$i$，它的Internal和border两方面采样的anchor patches表示为$\\mathbb{A}_{i}=\\left\\{\\mathcal{A}_{i}^{(1)}, \\ldots, \\mathcal{A}_{i}^{\\left(n_{A}\\right)}\\right\\}$，对于子图$\\mathcal{S}$的一个连通分量$\\mathcal{S}^{(c)}$，要学习该连通分量的表示，可使用一下subgraph-level message passing layer: $$ \\begin{aligned} \u0026amp;\\boldsymbol{a}_{i, \\mathcal{S}^{(c)}}=\\sum_{\\mathcal{A}_{i} \\in \\mathbb{A}_{i}} \\gamma_{i}\\left(\\mathcal{S}^{(c)}, \\mathcal{A}_{i}\\right) \\boldsymbol{g}_{\\mathcal{A}_{i}}, \\\\ \u0026amp;\\boldsymbol{h}_{i, \\mathcal{S}^{(c)}}^{(k)}=\\sigma\\left(W_{i} \\cdot\\left[\\boldsymbol{a}_{i, \\mathcal{S}^{(c)}}, \\boldsymbol{h}_{i, \\mathcal{S}^{(c)}}^{(k-1)}\\right]\\right) \\end{aligned} $$ 其中$\\gamma_{i}\\left(\\mathcal{S}^{(c)}, \\mathcal{A}_{i}\\right)$是子图分量$\\mathcal{S}^{(c)}$和一个anchor patch $\\mathcal{A}_{i}$的相似度。即每个子图分量依照与anchor patch 的相似度聚合来自anchor的信息。由于相似度函数的存在，SubGNN实际上是使用与子图分量$\\mathcal{S}^{(c)}$接近或结构相似的anchor patch对$\\mathcal{S}^{(c)}$的representation做平滑，即$\\mathcal{S}^{(c)}$聚合更多与它结构相似的anchor patches的信息。通过精心设计的anchor和subgraph-level message passing，6个属性可以被各自保留，然后在融合。\nPlain GNN 存在的问题在于不能很好的表示内部结构和外部结构，即Plain GNN在message passing过程中不能为子图中的节点判断它的邻居是在子图内还是子图外。 而SubGNN如Figure 2右边所示， 子图内节点1接收Internal消息和border消息在两个独立的message passing中，回味每个节点生成2个表示向量，分别表示内部MP和外部MP，因为节点1和3内外部节点不一样，所以SubGNN可以为这两个节点生成不同的representations。\nGLASS：Gnns with LAbeling trickS for Subgraph 首先介绍zero-one label trick：\nDefinition 1 （zero-one label trick）: 给定一个图$\\mathcal{G}$和它的一个子图$\\mathcal{S}$，对与子图$\\mathcal{S}$， 图$\\mathcal{G}$中的任意一个节点$v$的zero-one标签为： $$ l_{v}^{(\\mathcal{S})}= \\begin{cases}1 \u0026amp; \\text { if } v \\in \\mathbb{V}_{\\mathcal{S}} \\\\ 0 \u0026amp; \\text { if } v \\notin \\mathbb{V}_{\\mathcal{S}}\\end{cases} $$ 即对于一个子图$\\mathcal{S}$，对图中所有节点赋予一个node label，用来区分节点在$\\mathcal{S}$内外。\nMax-Zero-One Labeling Trick 对每个节点做zero-one labeling trick可以区分一个子图的内外节点，因此zero-one labeling trick难以做batch training。因为为一个子图标记内部节点和外部节点，可以得到一个labeled graph （子图内节点为1，子图外节点为0），也就是对于一个graph $\\mathcal{G}$，每个子图都需要专门生成一个该子图的labeled graph, 不同子图的labeled graph 也不同。如果要为每个子图都构造labeled graph, 再各自在每个labeled graph上做独立的message passing，得到每个labeled graph对应的子图embedding，这样过于耗时。\n为了减轻上述每个组图对应一个特定的labeled graph 问题，本文认为可以为一个batch 子图生成一个labeled graph，这样的话，通过一次message passing就可以计算一个batch subgraphs的representations。为了结合一个batch 子图的zero-one labels，从而生成一个公共的labeled graph， 本文进一步提出了Max-Zero-One Labeling Trick。具体来说，一个batch的所有子图只生成一个labeled graph，其中，该batch内所有子图内节点都被赋予label 1, 所有子图外节点都被赋予label 0， 然后在labeled graph 上做GNN，就可以一次性学习一个batch子图的representations。\n作者认为如果目标子图稀疏的分布在图中的话，一个子图外有其他节点被赋予1标签的影响是微不足道的，因为浅层GNN也不会聚合到远距离的节点。另外这么做还可以避免对一个子图的过拟合。\nImplementation Input: Graph $\\mathcal{G}$ , 所有子图subG_node = [[subgraph 1], [subgraph 2], [subgraph 3], ...]， $z=[0,0,1,0,1,1,0,0,1, \\cdots]$ 为batch subgraphs对应的labeled graph, 在batch subgraphs中的节点为1，不在的为0。\n以下为一层GLASS：\n  对于每个节点特征，分别做两个线性变换\nx1 = MLP_1(x) # 节点充当batch subgraphs内节点时的embedding x0 = MLP_0(x) # 节点充当batch subgraphs外节点时的embedding   对于label=1的节点 （batch 子图内的节点）\n特征 $x = \\alpha x_1 + (1-\\alpha)x_0$， 对于ppi_bp数据集，$\\alpha = 0.95$为超参数，若节点是batch子图内的节点，保留更多$x_1$。\n对于label=0的节点 （batch 子图外的节点）\n$x = (1-\\alpha)x_1 + \\alpha x_0$，对于不在batch子图中的节点，保留更多$x_0$。\n通过这种方式，子图内外的节点得以区分\n  Message Passing:\n$x = (D^{-1}A)X$\nGraphNorm:\n$x = \\mathrm{GraphNorm}(x)$\nResidual:\n$x = \\mathrm{cat}(x_, x)$ //和初始特征拼接\n  再次区分batch subgraph 内外节点：\nx1 = MLP_2(x) x0 = MLP_3(x)   再对子图内外节点做不同的组合\n$x = \\alpha x_1 + (1-\\alpha)x_0$： 子图内节点\n$x = (1-\\alpha)x_1 + \\alpha x_0$： 子图外节点\n  可以发现，如果$\\alpha = 1$，那么相当于子图内节点用$x_1$， 子图外节点用$x_0$，这样就彻底区别了子图内外的节点。即， 对于邻居聚合操作来说，如果聚合到了子图外邻居，那么子图外邻居使用$\\mathrm{MLP}_0$变换过的特征，如果聚合到子图内的节点，使用$\\mathrm{MLP}_1$变换过的特征。\n一点理论 Proposition 1: 给定图$G$，$\\mathcal{S}$和$\\mathcal{S}^\\prime$，如果Plain GNN可以区分的子图，GLASS也一定可以区分。但是存在Plain GNN不能区分但GLASS可以区分的子图。\nProof: 首先证明给定任意Plain GNN model $m_1$，存在一个GLASS模型$m_2$，使得对于目标子图$\\mathcal{S}$，$m_1$和$m_2$的输出相同，也就是GLASS至少可以和Plain GNN一样expressive。\n假设Plain GNN $m_1$ 的第$k$层 $\\mathrm{AGGREGATE}$函数为$f^{(k)}_1$，$\\mathrm{COMBINE}$函数为$g^{(k)}_1$， 第$k$层$\\mathrm{READOUT}$函数为$\\phi_1$，那么Plain GNN可以表达为： $$ \\begin{aligned} \u0026amp;\\boldsymbol{a}_{v}^{(k)}=\\operatorname{AGGREGATE}^{(k)}\\left(\\left\\{\\boldsymbol{h}_{u}^{(k-1)} \\mid u \\in N(v)\\right\\}\\right) = f^{(k)}_1 \\left(\\left\\{\\boldsymbol{h}_{u}^{(k-1)} \\mid u \\in N(v)\\right\\}\\right) \\\\ \u0026amp;\\boldsymbol{h}_{v}^{(k)}=\\operatorname{COMBINE}^{(k)}\\left(\\boldsymbol{h}_{v}^{(k-1)}, \\boldsymbol{a}_{v}^{(k)}\\right) = g^{(k)}_1\\left(\\boldsymbol{h}_{v}^{(k-1)}, \\boldsymbol{a}_{v}^{(k)}\\right) \\\\ \u0026amp;\\boldsymbol{h}_{\\mathcal{S}}=\\operatorname{READOUT}\\left(\\left\\{\\boldsymbol{h}_{u} \\mid u \\in \\mathbb{V}_{\\mathcal{S}}\\right\\}\\right) = \\phi_1\\left(\\left\\{\\boldsymbol{h}_{u} \\mid u \\in \\mathbb{V}_{\\mathcal{S}}\\right\\}\\right) \\end{aligned} $$ 接下来设计GLASS，将每层节点特征定义为$\\operatorname{CONCATENATE}\\left(\\boldsymbol{h}_{u}^{(k-1)}, \\boldsymbol{l}^{(\\mathcal{S})}\\right)$，即每层拼接该节点的label （是否在子图中），基于universal approximation theorem，一定存在一个函数$\\theta$, 使得$\\theta\\left(\\operatorname{CONCATENATE}\\left(\\boldsymbol{h}_{u}^{(k-1)}, \\boldsymbol{l}^{(\\mathcal{S})}\\right)\\right)=\\boldsymbol{h}_{u}^{(k-1)}$，那么GLASS可以定义为： $$ \\begin{aligned} \\boldsymbol{h}_{u}^{\\prime(k-1)} \u0026amp;=\\operatorname{CONCATENATE}\\left(\\boldsymbol{h}_{u}^{(k-1)}, \\boldsymbol{l}^{(\\mathcal{S})}\\right), \\\\ \\boldsymbol{a}_{v}^{(k)} \u0026amp;=f_{1}^{(k)}\\left(\\left\\{\\theta \\left(\\boldsymbol{h}_{u}^{\\prime (k-1)}\\right) \\mid u \\in N(v)\\right\\}\\right) \\\\ \\boldsymbol{h}_{v}^{(k)} \u0026amp;=g_{1}^{(k)}\\left(\\boldsymbol{h}_{v}^{(k-1)}, \\boldsymbol{a}_{v}^{(k)}\\right) \\end{aligned} $$ 因为$\\theta \\left(\\boldsymbol{h}_{u}^{\\prime (k-1)}\\right) = \\theta\\left(\\operatorname{CONCATENATE}\\left(\\boldsymbol{h}_{u}^{(k-1)}, \\boldsymbol{l}^{(\\mathcal{S})}\\right)\\right)=\\boldsymbol{h}_{u}^{(k-1)}$， Plain GNN 是上述特定形式GLASS的特例，所以GLASS使得至少与Plain GNN 表达能力相同。\nFigure 2给出了Plain GNN不能区分但GLASS可以区分的子图实例。\nTheorem 1： 给定任意图$\\mathcal{G}$，存在一个GLASS model，可以准确预测$\\mathcal{G}$中任意子图的density 和cut ratio。\nProof: 一个图的Density定义为： $$ D = \\frac{2 |E|}{|V| \\cdot|V-1|} $$ 即图中实际存在的边数，占左右节点对可能构成的总边数的比例\n一个子图$\\mathcal{S}$的cut ratio定义为： $$ \\mathrm{CR}(\\mathcal{S}) = \\frac{|B_{\\mathcal{S}}|}{|\\mathcal{S}| \\cdot |\\mathcal{G} \\backslash \\mathcal{S}|} $$ 为子图和其他部分之间的边数$|B_{\\mathcal{S}}|$, 占子图和其他部分可能存在的总边数的比例。其中$B_{S}=\\{(u, v) \\in E \\mid u \\in S, v \\in G \\backslash S\\}$。 $$ \\begin{aligned} \\boldsymbol{a}_{v}^{(1)} \u0026amp;=\\sum_{u \\in N(v)}\\left(\\boldsymbol{l}_{u}^{(\\mathcal{S})}\\left[\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\end{array}\\right]+\\left[\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\end{array}\\right]\\right) = \\left[\\begin{array}{l} N(v)中存在于\\mathcal{S}的节点数量\\quad m \\\\ N(v)中\\mathcal{S}以外节点数量 \\quad n\\\\ 0 \\end{array}\\right] \\\\ \\boldsymbol{h}_{v}^{(1)} \u0026amp;=\\left[\\begin{array}{ccc} 1 \u0026amp; 0 \u0026amp; 0 \\\\ -1 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right] \\boldsymbol{a}_{v}^{(1)}+\\left[\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\end{array}\\right] = \\left[\\begin{array}{l} m \\\\ n-m \\\\ 1 \\end{array}\\right] \\\\ \\boldsymbol{h}_{\\mathcal{S}} \u0026amp;=\\sum_{v \\in \\mathbb{V}_{\\mathcal{S}}} \\boldsymbol{h}_{v}^{(1)} = \\left[\\begin{array}{l} 子图内边数 \\\\ 边界边数-子图内边数\\\\ 子图内节点数 \\end{array}\\right] \\end{aligned} $$ 其中$l_{u}^{(\\mathcal{S})}$为节点$u$的zero-one label，如果$u$在子图$\\mathcal{S}$中，那么为1不在为0。 因此子图$\\mathcal{S}$的Density $d$和 cut ratio $c$可以由上述定义的GLASS模型推导出： $$ \\begin{aligned} \u0026amp;d\\left(\\boldsymbol{h}_{\\mathcal{S}}\\right)=\\left(\\left[\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\end{array}\\right]^{T} \\boldsymbol{h}_{\\mathcal{S}}\\right) /\\left[\\left(\\left[\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\end{array}\\right]^{T} \\boldsymbol{h}_{\\mathcal{S}}\\right) \\cdot\\left(\\left[\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\end{array}\\right]^{T} \\boldsymbol{h}_{\\mathcal{S}}-1\\right)\\right] \\\\ \u0026amp;c\\left(\\boldsymbol{h}_{\\mathcal{S}}\\right)=\\left(\\left[\\begin{array}{c} 0 \\\\ 0.5 \\\\ 0 \\end{array}\\right]^{T} \\boldsymbol{h}_{\\mathcal{S}}\\right) /\\left[\\left(\\left[\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\end{array}\\right]^{T} \\boldsymbol{h}_{\\mathcal{S}}\\right) \\cdot\\left(n-\\left[\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\end{array}\\right]^{T} \\boldsymbol{h}_{\\mathcal{S}}-1\\right)\\right] . \\end{aligned} $$\nReference [1] Labeling trick: A theory of using graph neural networks for multi-node representation learning. NeurIPS2021\n","permalink":"https://JhuoW.github.io/posts/glass/","summary":"Paper\nIntroduction SubGNN在学习子图representation时保留子图的三种属性：Position，Neighborhood，Structure，每种属性包含Internal 和Border两方面，并且要精心设计不同的anchor patch，所以过于复杂。通过分析SubGNN和普通GNN，作者发现子图表示的核心可能是区分子图内部和外部节点。基于此发现，本文提出了一种labeling trick, 即max-zero-one，来提升子图GNN的表达能力和可拓展性。\nSubgraph representation task 如上图所示，目标子图$\\mathbb{S}$被嵌入在整个图中，并且可能拥有多个连通分量，目标是学习子图的表示向量，使其可以预测子图的属性。NeurIPS2020文章SubGNN提出子图级message-passing来代替节点级的message passing，并且设计了三个message passing通道，每个通道分为内部和边界模块，分别捕获子图分量间的交互，以及子图与图的其他部分之间的交互。尽管取得了比普通GNN更好的效果，但是SubGNN需要繁琐的预计算，因为SubGNN通过不同采样规则的anchor patch来传递子图分量之间，以及子图分量与图其他部分之间的相关性，而三个通道共6个aspects需要不同的采样规则，以及各自的message passing，计算十分冗长（这里有解读）。另外 SubGNN对每个aspects需要使用不同的anchor patch随机采样策略，无法保证采样的anchor patch是最优的，因此效果的方差较大，使得鲁棒性堪忧。\n通过对比SubGNN相较于普通GNN的优势，作者发现对于子图任务来说，区分子图内部节点和外部节点非常重要。基于这个发现，本文提出了一种labeling trick，即max-zero-one labeling trick，来标注每个节点是否在子图外或者子图内。\nLabeling Trick [1]: 使用GNN生成multi-node representations （即，为一组节点，例如子图生成表示向量），该方法说明了为高阶结构生成有表达能力的representation，需要捕获结构内不同节点间的交互。在实现上，labeling trick通过一个专门设计的label来表示节点的结构特征，这个label与node feature 结合作为新的feature输入GNN中。\n注： 本文只考虑诱导子图，即每个子图的每个连通分量保留原图中的所有边。\nPlain GNN and SubGNN 如上图所示，$G$是一个regular graph, 所以在没有节点feature的情况下，每个节点的embedding相同，所以GNN无法区分子图$\\mathcal{S}$和$\\mathcal{S}^\\prime$。如下图所示，Plain GNN 在message passing中子图$\\mathcal{S}$内部节点1同时接收来自子图内和子图外的邻居信息，并不会加以区分。同样$\\mathcal{S}^\\prime$中节点3也同时接收子图内外节点，因此对于Plain GNN ，它无法区分节点1和3，因此无法区分两个子图。\n而SubGNN引入了3个通道：position (P)，neighborhood (N), 和structure (S) 每个通道分别学习Internal 和Border两方面，共6个属性融入子图表示学习中。对于子图$\\mathcal{S}$，为了捕获某个通道$i$的属性，SubGNN首先随机采样$n_A$个anchor patches： $\\mathbb{A}_{i}=\\left\\{\\mathcal{A}_{i}^{(1)}, \\ldots, \\mathcal{A}_{i}^{\\left(n_{A}\\right)}\\right\\}$，然后学习$\\mathcal{S}$中的每个连通分量在这个属性$i$下的表示向量，通过子图内部连通分量和anchor patches之间的消息传递，来捕获子图内部连通分量的相对位置/邻域/结构信息，以及子图连通分量相对于子图外部分的位置/邻域/结构信息。如图2右边所示。对于通道$i$，它的Internal和border两方面采样的anchor patches表示为$\\mathbb{A}_{i}=\\left\\{\\mathcal{A}_{i}^{(1)}, \\ldots, \\mathcal{A}_{i}^{\\left(n_{A}\\right)}\\right\\}$，对于子图$\\mathcal{S}$的一个连通分量$\\mathcal{S}^{(c)}$，要学习该连通分量的表示，可使用一下subgraph-level message passing layer: $$ \\begin{aligned} \u0026amp;\\boldsymbol{a}_{i, \\mathcal{S}^{(c)}}=\\sum_{\\mathcal{A}_{i} \\in \\mathbb{A}_{i}} \\gamma_{i}\\left(\\mathcal{S}^{(c)}, \\mathcal{A}_{i}\\right) \\boldsymbol{g}_{\\mathcal{A}_{i}}, \\\\ \u0026amp;\\boldsymbol{h}_{i, \\mathcal{S}^{(c)}}^{(k)}=\\sigma\\left(W_{i} \\cdot\\left[\\boldsymbol{a}_{i, \\mathcal{S}^{(c)}}, \\boldsymbol{h}_{i, \\mathcal{S}^{(c)}}^{(k-1)}\\right]\\right) \\end{aligned} $$ 其中$\\gamma_{i}\\left(\\mathcal{S}^{(c)}, \\mathcal{A}_{i}\\right)$是子图分量$\\mathcal{S}^{(c)}$和一个anchor patch $\\mathcal{A}_{i}$的相似度。即每个子图分量依照与anchor patch 的相似度聚合来自anchor的信息。由于相似度函数的存在，SubGNN实际上是使用与子图分量$\\mathcal{S}^{(c)}$接近或结构相似的anchor patch对$\\mathcal{S}^{(c)}$的representation做平滑，即$\\mathcal{S}^{(c)}$聚合更多与它结构相似的anchor patches的信息。通过精心设计的anchor和subgraph-level message passing，6个属性可以被各自保留，然后在融合。","title":"ICLR2022 《GLASS：GNN with Labeling Tricks for Subgraph Representation Learning》 Reading Notes"},{"content":"paper\nIntroduction GNN通常关注节点级任务和图级任务，缺少针对子图级预测任务的方法。针对这个问题，本文提出SubGNNs用于解耦子图在不同结构aspect的表示。为了学习准确的子图表示，SubGNN在子图的连通分量和随机采样的anchor patches之间进行消息传递，从而学习高准确度的子图表示。SubGNN指定了三个通道，每个通道捕获子图不同的拓扑结构属性。\n从拓扑的角度来看，子图是非常具有挑战性的结构，对子图的预测存在以下挑战：\n 如要对更大且size不同的子图做联合预测，挑战在于如何表征含有多个分量，甚至分量间间隔较远的子图。 子图包含了高阶连通模式（connectivity patterns），这些连通模式不仅存在于子图内节点之间，也存在与子图内节点与子图外部节点之间， 挑战在于如何将子图边界信息和子图外部信息注入GNN中。 子图可能存在于图中的一个特定区域，也可能它的连通分量分布于多个局部邻域，挑战在于如何学习子图在图中的位置。 子图间共享边（sharing edges）和非边（non-edges）存在相关性，挑战在于如何将这种子图间的依赖融合进模型中，同时任然能够将特征信息考虑在内进行辅助归纳推理。  本文提出SubGNN以解决上述挑战， SubGNN的核心原则是子图级的消息传递，可以捕获子图位置、邻域、结构三种特征\nFormulating Subgraph Prediction 给定无向图$G=(V,E)$，它的一个子图表示为$S=\\left(V^{\\prime}, E^{\\prime}\\right)$，每个子图$S$有一个label $y_{S}$，并且子图$S$可能包含多个连通分量，连通分量表示为$S^{(C)}$。\nProblem (Subgraph Representations and Property Prediction) 给定子图集合 $\\mathcal{S} = \\left\\{S_{1}, S_{2}, \\ldots, S_{n}\\right\\}$，SubGNN $E_S$为每个子图$S\\in \\mathcal{S}$生成一个$d_s$维的表示向量$\\mathbf{Z}_S \\in \\mathbb{R}^{d_{s}}$， 然后用这些子图的表示向量学习一个子图分类器 $f: \\mathcal{S} \\rightarrow\\{1,2, \\ldots, C\\}$，使得输入子图得到预测label: $f(S)=\\hat{y}_{S}$。\n本文针对子图分类任务，所提出的模型为一个可学习的embedding函数$E_{S}: S \\rightarrow \\mathbb{R}^{d_{s}}$， 将每个子图映射为低维表示向量，这些表示向量可以捕获子图拓扑对预测重要的aspects。具体来说，对于一个子图，message再它的连通分量之间传递，这使得我们可以对多个连通分量的子图学习有意义的表示。\nSUBGNN: Properties of subgraph topology 子图拥有独特的内部结构，边界连通性，邻域概念，以及和图其他部分的相对位置。直觉上，我们的目标是以最大的似然保存保存特定的图属性。本文设计模型以考虑6种特定的图结构属性：\n具体来说：\n(1) Position.\nBorder Position: 该属性保留子图和图的其他部分之间的距离，通过这种距离关系，可以区分两个同构但处于不同位置的子图。\nInternal Position：子图自己连通分量之间的距离。\n(2) Neighborhood.\nBorder Neighborhood：为子图的边界邻域，表示子图$S$中任意节点的$k$跳邻域中（不属于子图$S$）的节点集合。\nInternal Neighborhood：子图内每个连通分量的边界邻域，每个连通分量$S^{(c)}$中任意节点的$k$跳邻域中（不属于子图$S^{(c)}$）的节点集合。\n(3) Structure.\nBorder Structure：子图内部节点和边界邻居之间的连通性。\nInternal Structure：每个连通分量的内部连通性。\n本文旨在将上述属性学习到子图表示向量中。\nSubGNN： Subgraph Neural Network SubGNN以层次的方式学习子图表示，将神经消息从anchor patch传递到子图分量中，并将所有子图分量的表示聚合为最终的子图表示。如图2（a）所示，独立考虑子图的每个连通分量的每个属性，从anchor patch中获得相应的属性信息，对于每个分量，聚合它的所以属性表示，得到该分量的表示，然后聚合子图所有分量的表示，得到最后的子图表示。\nProperty-Aware Routing 通过anchor patch向子图分量传递消息，使得子图分量可以实现结构属性感知，如图2（b）所示，每个通道$\\mathbf{z}_{i}$表示子图的第$i$个分量的输出表示，它由3部分构成，分别为该分量的位置属性（绿色），邻居属性（蓝色），结构属性（橙色）。将所有通道（分量）聚合，可以得到最终的子图表示。\n对于每个属性，我们定义一个anchor patch 采样函数$\\phi_{\\mathrm{X}}:\\left(G, S^{(c)}\\right) \\rightarrow A_{\\mathrm{X}}$，即对于子图$S$的一个连通分量$S^{(c)}$，$\\phi_{\\mathrm{X}}$为该子图分量输出属性$X$的anchor patch。\nPosition 为了捕获Internal Position, $\\phi_{\\mathrm{P}_{\\mathrm{I}}}$返回anchor patch $A_{P_{I}}$，每个anchor patch为 子图内的单个节点，所有的针对Internal Position属性的anchor patch集合为$\\mathcal{A}_{P_{I}} = \\{A^{(1)}_{P_{I}},A^{(2)}_{P_{I}}, \\cdots\\}$， 为子图$S$内随机采样的节点集合，即$A^{(i)}_{P_{I}}$为一个子图内的节点。由于Internal Position的目的是捕获子图连通分量之间的距离，因此不同的连通分量共享anchor，通过共享的anchor和不同连通分量之间的相似度，来将anchor的信息聚合到不同的连通分量中，这将允许子图中不同连通分量相互定位。 例如$S$有两个连通分量，有一个anchor $A^{(i)}_{P_{I}}$存在于其中一个分量中，那么两个连通分量分别依据和anchor的相似度来聚合anchor的表示，那么如果有很多anchor的情况下，可以较为准确的为这两个分量区分相对位置。子图$S$的anchor patch集合$\\mathcal{A}_{P_{I}}$对$S$的所有分量共享。\n为了捕获Border Position， 由于Border Position是子图整体和图的其他部分的距离，所以$\\phi_{\\mathrm{P}_{\\mathrm{B}}}$采样的节点在所有子图间共享，anchor集$\\mathcal{A}_{P_{B}} = \\{A^{(1)}_{P_{B}},A^{(2)}_{P_{B}}, \\cdots\\}$中每个anchor patch $A^{(i)}_{P_{B}}$是随机采样的节点，并且集合$\\mathcal{A}_{P_{B}}$所有子图都共享，即所有子图的所有分量都聚合来自$\\mathcal{A}_{P_{B}}$的消息，例如子图$S_1$和$S_2$， $S_1$的所有分量依据和anchor的相似度聚合所有anchor的信息，同样$S_2$的所有分量依据和anchor的相似度聚合所有anchor的信息，那么如果anchor数量足够多，并且在子图间共享，所以为子图$S_1$和$S_2$学习到的emb可以分别反映两个子图和原图中其他部分的相似度，从而区分两个子图的position。\n综上所述，对于Internal Positon，anchor在子图中采样，且在同一个子图的不同分量间共享，从而捕获同一个子图不同分量间的位置距离。\n对于Border Position， anchor在整个图中采样，且在所有子图间共享，从而捕获不同子图在原图中的位置距离。\nNeighborhood 为了捕获Internal Neighborhood，$\\phi_{\\mathrm{N}_{\\mathrm{I}}}$是从子图分量$S^{(c)}$中采样anchor patch，对于每个子图分量，它的anchor patch来自自己内部节点，聚合来自自己内部节点的消息，从而捕获内部邻域信息。\n而$\\phi_{\\mathrm{N}_{\\mathrm{B}}}$是从子图分量$S^{(c)}$的border neighborhood中采样anchor，即子图分量$S^{(c)}$中任意节点$k$-hop以内邻居（不在$S^{(c)}$）中的节点采样anchor，子图$S^{(c)}$聚合这些$k$-hop border neighborhood从而为每个子图分量捕获边界邻域信息。\nStructure $\\phi_{\\mathrm{S}}$采样anchor patch用于捕获子图的内部结构信息和边界结构信息，针对内部结构信息采样的anchor集合$\\mathcal{A}_{\\mathrm{S}_{1}}$以及针对边界结构信息采样的anchor集合$\\mathcal{A}_{\\mathrm{S}_{B}}$对所有子图也是共享的，具体来说，$\\phi_{\\mathrm{S}}$返回的是根据三角随机游走从图中抽取的连通部分，通过计算与这些anchor图的相似度来聚合这些anchor图，从而区分不同子图在结构上的相似度。具体来说，每个子图根据与共享anchor图之间的相似度，聚合anchor图，那么不同子图如果与共享的anchor子图集相似度越高，那么这这些子图的结构相似度就越高。例如，$S_1$和$S_2$为$G$中的两个子图，要使两个子图的embedding可以反映两个子图结构上的相似性，那么给定一大堆anchor子图，$S_1$基于它和这些anchor的相似性聚合所有anchor，$S_2$同样基于它和anchor的相似性聚合所有anchor，anchor set相当于一个相似度中介，两个子图的embedding分别反映了两个子图和共享anchor set之间的相似度 $Sim_1$和$Sim_2$，那么如果，如果$Sim_1$和$Sim_2$的差别较大，那么说明两个子图与anchor set的相似度相差太大，所以两个子图的结构差别较大。\nNeural Encoding of Anchor Patches 对提取出的anchor进行编码，对于Position anchor patches 和 Neighbor anchor patches，由于每个anchor patch是单一节点，所以节点特征就是anchor的representation。而对于 Structure属性的anchor patches，它是一个个子图，为了将他们编码，本文首先在每个anchor patch上进行长度为$w$参数为$\\beta$的三角随机游走，得到节点序列$\\left(u_{\\pi_{w}(1)}, \\ldots, u_{\\pi_{w}(n)}\\right)$，然后将节点序列输入双向LSTM中得到最终的patch 表示$\\mathbf{a}_{S}$。\nSubgraph-Level Message Passing SubGNN的消息传递定义在子图分量级。首先为每个结构属性采样一对anchor patches：$\\mathcal{A}_{\\mathrm{X}}=\\left\\{A_{\\mathrm{X}}^{(1)}, \\ldots, A_{\\mathrm{X}}^{\\left(n_{A}\\right)}\\right\\}$，每个属性的采样规则如Property-Aware Routing 中所示，即针对Position和Neighborhood属性，采样的anchor patch是节点，针对Structure，采样的anchor patch是子图。$\\mathcal{A}_{\\mathrm{P}}$、$\\mathcal{A}_{\\mathrm{N}}$和$\\mathcal{A}_{\\mathrm{S}}$分别为三个属性的anchor patch 集合。对于子图$S$的第$c$个分量$S^{(c)}$，定义从anchor patch $A_{\\mathrm{X}}$到$S^{(c)}$的消息： $$ \\mathrm{MSG}_{\\mathrm{X}}^{A \\rightarrow S}=\\gamma_{\\mathrm{X}}\\left(S^{(c)}, A_{\\mathrm{X}}\\right) \\cdot \\mathbf{a}_{\\mathrm{X}} $$ 其中$X$是结构属性，$\\gamma_{\\mathrm{X}}$是该结构属性的相似度函数，用于衡量anchor patch 和子图embedding之间的相似度值。将一个属性下所有anchor patches的消息聚合，然后与子图分量$S^{(c)}$结合成该子图分量在$X$属性上的表示： $$ \\begin{aligned} \u0026amp;\\mathbf{g}_{\\mathrm{X}, c}=\\mathrm{AGG}_{M}\\left(\\left\\{\\mathrm{MSG}_{\\mathrm{X}}^{A_{\\mathrm{X}} \\rightarrow S^{(c)}} \\forall A_{\\mathrm{X}} \\in \\mathcal{A}_{\\mathrm{X}}\\right\\}\\right) \\\\ \u0026amp;\\mathbf{h}_{\\mathrm{X}, c} \\leftarrow \\sigma\\left(\\mathbf{W}_{\\mathrm{X}} \\cdot\\left[\\mathbf{g}_{\\mathrm{X}, c} ; \\mathbf{h}_{\\mathrm{X}, c}\\right]\\right), \\end{aligned} $$ 以上图为例，给定一个子图$S$的第一个连通分量$S^{(1)}$在Position属性下的输入embedding为$\\mathbf{h}_{P,1}$， Position属性在Internal和Border方面一共有4个anchor patches，每个anchor patches将消息聚合到$S^{(1)}$中，得到4个Messages,即 $\\mathbf{m}_{P,1,1}$，$\\mathbf{m}_{P,1,2}$，$\\mathbf{m}_{P,1,3}$，$\\mathbf{m}_{P,1,4}$，分别表示4个Position属性的anchor patch聚合到子图$S$第1个连通分量的消息。\n$\\mathbf{h}_{\\mathrm{X}, c}$的顺序不变性是层到层消息传递的重要属性，但是它会限制捕捉子图结构和位置的能力。因此这里构造了property-aware的输出表征$\\mathbf{Z}_{X, c}$，通过将属性$X$在分量$c$上的所有anchor message拼接，得到anchor-set message 矩阵$\\mathbf{M}_{\\mathrm{X}}$， 如图二所示，$\\mathbf{M}_{\\mathrm{X}}$的每行都是anchor-set message （集合消息），然后传递给非线性激活函数（如算法1所示）。输出的表示向量的每一维都编码了anchor patch 的结构信息和位置信息，对于邻域通道，设定$\\mathbf{Z}_{\\mathrm{N}, c}=\\mathbf{h}_{\\mathrm{N}, c}$。最后SubGNN为连通分量拼接每个属性的消息，得到分量表示$\\mathbf{z}_c$。最后所有的分量表示通过$\\mathrm{READOUT}$聚合成最后的子图表示$\\mathbf{z}_{S}$。\n概括来说，先得到子图每个分量$S^{(c)}$在属性$X$上的表示$\\mathbf{z}_{X,c}$，然后1）对于每层SubGNN，将分量$S^{(c)}$的所有属性表示向量聚合，得到该层$c$分量的表示。2）将所有层的$S^{(c)}$分量的表示向量聚合，得到子图分量$S^{(c)}$的最终表示$\\mathbf{z}_c$。3）子图$S$所有分量的表示聚合，得到最终的子图表示$\\mathbf{z}_S$。\nConclusion 有点复杂~\n","permalink":"https://JhuoW.github.io/posts/subgnn/","summary":"paper\nIntroduction GNN通常关注节点级任务和图级任务，缺少针对子图级预测任务的方法。针对这个问题，本文提出SubGNNs用于解耦子图在不同结构aspect的表示。为了学习准确的子图表示，SubGNN在子图的连通分量和随机采样的anchor patches之间进行消息传递，从而学习高准确度的子图表示。SubGNN指定了三个通道，每个通道捕获子图不同的拓扑结构属性。\n从拓扑的角度来看，子图是非常具有挑战性的结构，对子图的预测存在以下挑战：\n 如要对更大且size不同的子图做联合预测，挑战在于如何表征含有多个分量，甚至分量间间隔较远的子图。 子图包含了高阶连通模式（connectivity patterns），这些连通模式不仅存在于子图内节点之间，也存在与子图内节点与子图外部节点之间， 挑战在于如何将子图边界信息和子图外部信息注入GNN中。 子图可能存在于图中的一个特定区域，也可能它的连通分量分布于多个局部邻域，挑战在于如何学习子图在图中的位置。 子图间共享边（sharing edges）和非边（non-edges）存在相关性，挑战在于如何将这种子图间的依赖融合进模型中，同时任然能够将特征信息考虑在内进行辅助归纳推理。  本文提出SubGNN以解决上述挑战， SubGNN的核心原则是子图级的消息传递，可以捕获子图位置、邻域、结构三种特征\nFormulating Subgraph Prediction 给定无向图$G=(V,E)$，它的一个子图表示为$S=\\left(V^{\\prime}, E^{\\prime}\\right)$，每个子图$S$有一个label $y_{S}$，并且子图$S$可能包含多个连通分量，连通分量表示为$S^{(C)}$。\nProblem (Subgraph Representations and Property Prediction) 给定子图集合 $\\mathcal{S} = \\left\\{S_{1}, S_{2}, \\ldots, S_{n}\\right\\}$，SubGNN $E_S$为每个子图$S\\in \\mathcal{S}$生成一个$d_s$维的表示向量$\\mathbf{Z}_S \\in \\mathbb{R}^{d_{s}}$， 然后用这些子图的表示向量学习一个子图分类器 $f: \\mathcal{S} \\rightarrow\\{1,2, \\ldots, C\\}$，使得输入子图得到预测label: $f(S)=\\hat{y}_{S}$。\n本文针对子图分类任务，所提出的模型为一个可学习的embedding函数$E_{S}: S \\rightarrow \\mathbb{R}^{d_{s}}$， 将每个子图映射为低维表示向量，这些表示向量可以捕获子图拓扑对预测重要的aspects。具体来说，对于一个子图，message再它的连通分量之间传递，这使得我们可以对多个连通分量的子图学习有意义的表示。\nSUBGNN: Properties of subgraph topology 子图拥有独特的内部结构，边界连通性，邻域概念，以及和图其他部分的相对位置。直觉上，我们的目标是以最大的似然保存保存特定的图属性。本文设计模型以考虑6种特定的图结构属性：\n具体来说：\n(1) Position.\nBorder Position: 该属性保留子图和图的其他部分之间的距离，通过这种距离关系，可以区分两个同构但处于不同位置的子图。\nInternal Position：子图自己连通分量之间的距离。\n(2) Neighborhood.\nBorder Neighborhood：为子图的边界邻域，表示子图$S$中任意节点的$k$跳邻域中（不属于子图$S$）的节点集合。\nInternal Neighborhood：子图内每个连通分量的边界邻域，每个连通分量$S^{(c)}$中任意节点的$k$跳邻域中（不属于子图$S^{(c)}$）的节点集合。","title":"NeurIPS2020 《Subgraph Neural Networks》 Reading Notes"},{"content":"paper\nIntroduction 现有的GNN在图和模型的size方面可拓展性有限。 对于大图，增加模型的深度对导致scope(感受野)大小成指数放大。深层model主要面临两个基本挑战： 1. oversmoothing导致表达能力下降，2. 邻域爆炸导致计算成本高昂。\n本文旨在结构GNN的depth 和 scope，首先提取子图作为有限大小（bounded-size）的scope, 然后将任意深度的GNN用于子图上。 由于提取出的局部子图是由少量关键邻居组成，且排除了不相关的邻居，所以深层GNN也可以学到informative representations。\n增加GNN层数会造成以下基本障碍：\n Expresivity (oversmoothing): 邻居的迭代混合导致不同节点的切入向量收敛到一个固定的低维子空间 Scalability (neighbor explosion): 多跳邻居递归导致感受野大小呈指数级增长  为了研究导致表达能力和可拓展性缺陷的根本原因，本文提出了以下insight:\nTwo views on the graph: 如果从全局视角来看两个节点，如果两个节点在同一个图的同一个连通分量中，那么这两个节点在随机游走中存在到达概率，无论他们间隔多远。而本文给出了图的局部视角，具体来说， 给定节点$v$的局部子图$\\mathcal{G}_{[v]}$，将$\\mathcal{G}_{[v]}$仅包含节点$v$的特性，整个图看一看做所有子图$\\mathcal{G}_{[v]}$的集合。那么$v$的邻域不在是所有节点$\\mathcal{V}$，而它的邻域只存在于$\\mathcal{V}_{[v]}$中。 如果节点$u$不在$\\mathcal{V}_{[v]}$中，$u$将永远不会被考虑为$v$的邻居，无论GNN有多深。\nScope of GNNs: 加深GNN层次所造成的的表达能力和可拓展性问题都和GNN不断扩大的感受野（scope）有关。随着层次变深，感受野不断变大，使得每个节点包含的信息重叠越多，最终收敛到同一个子空间，导致oversmoothing; 另外 感受野变大，导致每个节点的邻居数呈指数级上升，导致邻居爆炸。所以GNN的层数加深会导致感受野变大（耦合），即$L$层GNN的感受野为全部$L$-hop以内的邻居，层数深度（depth）和感受野大小(scope)的强耦合限制了GNN的设计。\nDecoupling the GNN depth and scope: 为了解耦GNN的深度（depth）与感受野(scope)，使得加层数与感受野无关。对于节点$v$，首先为它提取一个小的子图$\\mathcal{G}_{[v]}$，然后在小的子图上应用任意层数的GNN。若GNN的层数$L^\\prime$大于感受野的跳数，那么子图中的每对节点会交换多次信息，额外的消息传递有助于GNN更好的融合scope内的信息，从而增强表达能力。\nDecoupling the Depth and Scope of GNNs Definition (Depth of subgraph) ：假设子图$\\mathcal{G}_{[v]}$是连通的，$\\mathcal{G}_{[v]}$的depth定义为$\\max _{u \\in \\mathcal{V}_{[v]}} d(u, v)$, 其中$d(u, v)$表示$u$到$v$的最短路径。\n本文提出shaDow-GNN，它包含了一个子图提取器$\\text { EXTRACT}$。 shaDow-GNN的过程如下：\n 用子图提取器$\\operatorname{EXTRACT}(v, \\mathcal{G})$为节点$v$提取一个连通子图$\\mathcal{G}_{[v]}$，子图的深度（距离$v$最远的节点和$v$之间的跳数）为$L$。 构建一个$L^\\prime$层的GNN并应用在$\\mathcal{G}_{[v]}$上。 如果 $L^\\prime \u0026gt; L$那么可以反映decoupling，因为GNN层数此时与scope无关，层数加深不会影响感受野。  本文从三个不同的角度理论证明了shaDow-GNN可以提升GNN的表达能力。\nGraph Signal Processing Perspective  oversmoothing by deep GCN。 2. oversmoothing by repeated GCN-style propagation。  对于deep GCN, 包含了非线性激活，权重和bias。而带有bias参数的deep GCN 不会导致oversmoothing[1]， 但任然存在准确率下降的问题，这说明GCN的这种传播形式是导致学习困难的根本原因，而不是来自于激活函数或者bias。\n即，repeat-GCN-style propagation表示为$\\boldsymbol{M}=\\lim _{L \\rightarrow \\infty} \\widetilde{\\boldsymbol{A}}^{L} \\boldsymbol{X}$这种邻居迭代聚合的传播形式会导致学习困难。因此，这里在忽略激活函数和bias的情况下分析聚合矩阵的渐近性。\n对于子图$\\mathcal{G}_{[v]}$， 无限次特征聚合表示为：$\\boldsymbol{M}_{[v]}=\\lim_{L \\rightarrow \\infty} \\widetilde{\\boldsymbol{A}}^{L}_{[v]} \\boldsymbol{X}_{[v]}$。$\\boldsymbol{M}_{[v]}$为节点$v$的子图$\\mathcal{G}_{[v]}$的embedding矩阵。 由于$\\widetilde{\\boldsymbol{A}}_{[v]} = (\\boldsymbol{D}_{[v]} + \\boldsymbol{I}_{[v]})^{-\\frac{1}{2}} (\\boldsymbol{A}_{[v]} + \\boldsymbol{I}_{[v]}) (\\boldsymbol{D}_{[v]} + \\boldsymbol{I}_{[v]})^{-\\frac{1}{2}}$，$\\widetilde{\\boldsymbol{A}}_{[v]}$是实对称阵， 所以可以做特征分解， 即$\\widetilde{\\boldsymbol{A}}_{[v]}=\\boldsymbol{E}_{[v]} \\boldsymbol{\\Lambda} \\boldsymbol{E}_{[v]}^{-1}=\\boldsymbol{E}_{[v]} \\boldsymbol{\\Lambda} \\boldsymbol{E}_{[v]}^{\\top}$。那么: $$ \\widetilde{\\boldsymbol{A}}_{[v]}^{L \\to \\infty} =\\boldsymbol{E}_{[v]} \\boldsymbol{\\Lambda}^L \\boldsymbol{E}_{[v]}^{\\top} = \\boldsymbol{E}_{[v]} \\begin{bmatrix} \\lambda_1^L \u0026amp; \u0026amp; \u0026amp; \\\\ \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\lambda_N^L\u0026amp; \\end{bmatrix}\\boldsymbol{E}_{[v]}^{\\top} = e_{[v]}e_{[v]}^\\top $$ 因为归一化邻接矩阵$\\widetilde{\\boldsymbol{A}}_{[v]}$的最大特征值一定为1，且半正定，那么当$L \\to \\infty$时， 若$1 = \\lambda_1 \u0026gt; \\cdots \u0026gt; \\lambda_N$, 所以$\\lambda_1^L = 1$，$\\lambda_2^L,\\cdots,\\lambda_N^L \\to 0$。所以上式成立，其中$e_{[v]}$为$\\widetilde{\\boldsymbol{A}}_{[v]}$最大特征值对应特征向量。所以： $$ \\lim_{L\\to \\infty} \\widetilde{\\boldsymbol{A}}_{[v]}^{L} X_{[v]} = e_{[v]}e_{[v]}^\\top X_{[v]} = e_{[v]}(e_{[v]}^\\top X_{[v]}) = \\boldsymbol{M} $$ 因此在子图$\\mathcal{G}_{[v]}$上使用无限多层GNN的 shaDow-GNN得到的节点emb矩阵为$\\boldsymbol{M} = e_{[v]}(e_{[v]}^\\top X_{[v]})$, 那么 shaDow-GNN为节点$v$学习到的embedding为 节点$v$在其子图$\\mathcal{G}_{[v]}$中的对应embedding。即： $$ \\boldsymbol{M}_{[v]}=\\left[e_{[v]}\\right]_{v} \\cdot\\left(\\boldsymbol{e}_{[v]}^{\\top} \\boldsymbol{X}_{[v]}\\right) \\tag{1} $$ 考虑到这是无向图，$\\widetilde{\\boldsymbol{A}}_{[v]}$的最大特征值对应的特征向量表示中的每个元素表示$\\mathcal{G}_{[v]}$中对应节点的度。所以$\\left[e_{[v]}\\right]_{u}$是$\\mathcal{G}_{[v]}$中节点$u$的度$\\delta_{[v]}(u)$。所以$e_{[v]} = [\\delta_{[v]}(u)]$是$\\mathcal{G}_{[v]}$中节点的度向量。由于这里的粗体$e_{[v]}$是对$e_{[v]}$的normalization:\n$$ \\left[e_{[v]}\\right]_{u}=\\sqrt{\\frac{\\delta_{[v]}(u)}{\\sum_{w \\in \\mathcal{V}_{[v]}} \\delta_{[v]}(w)}} $$ 上面的$\\boldsymbol{M}_{[v]}$为 shaDow-GNN为节点$v$学到的embedding。\n对于普通的GCN propagation，可以看做是一个有足够大的hop $L$的$\\mathcal{G}_{[v]}$ 和一个有足够大hop $L$的$\\mathcal{G}_{[u]}$，使得在无限多层GCN时， $\\mathcal{G}_{[v]} = \\mathcal{G}_{[u]}$。即对于无限多层的普通GCN，两个不同节点的scope是一样的，这就说明 $e_{[v]} = e_{[u]}$，并且$\\boldsymbol{X}_{[u]}=\\boldsymbol{X}_{[v]}=\\boldsymbol{X}$。即对于普通的无限多层GNN来说，两个不同节点的局部子图就是整个图，无法保存节点特有的特征信息，根据公式一，由于$e_{[v]} = e_{[u]}$只和两个节点的度有关，特征信息又是一样的，所以在整个图上使用无限多层GNN的到的两个节点的embedding $\\boldsymbol{M}_{[v]}$和$\\boldsymbol{M}_{[u]}$只和两个节点的度有关。\n而对于shaDow-GNN，它实现一种局部平滑（local-smoothing）,由于是在子图上做特征聚合，那么无论多少层，都不会聚合到子图范围以外的邻居。从公式（1）可以看出，在子图内，目标节点的embedding实际上就是子图内所有节点embedding的线性组合，无限增加层数值只会使得线性组合的系数$e_{[v]}$收敛到一个固定值，即$\\widetilde{\\boldsymbol{A}}_{[v]}$的稳态分布向量。而稳态分布向量$e_{[v]}$的具体取值只与对应子图的度有关，与GNN层数无关。直观来看，子图提取器$\\text { EXTRACT}$会为不同的节点提取不同的子图，若$u$,$v$的子图不同，那么对应的$e_{[v]}$与$e_{[v]}$也不同，$\\boldsymbol{X}_{[u]}$和$\\boldsymbol{X}_{[v]}$也不同，所以shaDow-GNN在无限多层下依旧捕获局部特征信息。\nFunction Approximation Perspective GraphSAGE的每层定义为： $$ \\boldsymbol{h}_{v}^{(\\ell)}=\\sigma\\left(\\left(\\boldsymbol{W}_{1}^{(\\ell)}\\right)^{\\top} \\boldsymbol{h}_{v}^{(\\ell-1)}+\\left(\\boldsymbol{W}_{2}^{(\\ell)}\\right)^{\\top}\\left(\\frac{1}{\\mid \\mathcal{N}_{v}\\mid } \\sum_{u \\in \\mathcal{N}_{v}} \\boldsymbol{h}_{u}^{(\\ell-1)}\\right)\\right) $$ 对于$L^\\prime$层的shaDow-SAGE，，若$\\text { EXTRACT}$为节点提取$L$-hop的邻居，并且，对于shaDow-SAGE的$L+1 \\leq \\ell \\leq L^{\\prime}$层，令$\\boldsymbol{W}_{1}^{(\\ell)}=\\boldsymbol{I}$，$\\boldsymbol{W}_{2}^{(\\ell)}=\\mathbf{0}$， 此时$L^\\prime$层的shaDow-SAGE等价于$L$层的GraphSAGE。所以$L^\\prime$层的shaDow-SAGE可以表达GraphSAGE能表达的所有函数。\n要证明shaDow-SAGE可以表达一些GraphSAGE无法表达的函数，首先对于每个邻域子图$\\mathcal{G}_{[v]}$考虑一个目标函数：$\\tau\\left(\\boldsymbol{X}, \\mathcal{G}_{[v]}\\right)=C \\cdot \\sum_{u \\in \\mathcal{V}_{[v]}} \\delta_{[v]}(u) \\cdot \\boldsymbol{x}_{u}$，其中$C$是一个scaling常数，$\\delta_{[v]}(u)$是节点$u$在子图$\\mathcal{G}_{[u]}$中的度。\nGraphSAGE准确的学习函数$\\tau$，而shaDow-SAGE可以。令子图$\\mathcal{G}_{[v]}$的深度为$L$，对于GraphSAGE，只有它在原图上做$L$层的Message Passing，或者它做$L^\\prime$次MP的同时，$L^{\\prime}-L$层的$\\boldsymbol{W}_{2}$为0时，GraphSAGE才会遍历$\\mathcal{G}_{[v]}$中的节点，否则，$L^\\prime$层GraphSAGE将受到$v^{\\prime} \\notin \\mathcal{V}_{[v]}$的影响，那么它无法近似$\\tau$。那么问题就变为比较$L^\\prime$层shaDow-SAGE在$\\mathcal{G}_{[v]}$上的表达能力 和 $L$层GraphSAGE在原图上的表达能力， 因为他们的感受野都是$\\mathcal{G}_{[v]}$。接下来，假设GraphSAGE可以在一个局部子图$\\mathcal{G}_{[v]}^{\\prime}$学习一个函数$\\zeta$， 使得$\\zeta\\left(\\mathcal{G}_{[v]}^{\\prime}\\right)=\\tau\\left(\\mathcal{G}_{[v]}^{\\prime}\\right)$， 那么如果我们为子图$\\mathcal{G}_{[v]}^{\\prime}$添加一条边$e$来连接子图第$L$层的两个节点。 那么边$e$将会改变子图的度分布$\\delta_{[v]}(\\cdot)$，因此$\\tau\\left(\\mathcal{G}_{[v]}^{\\prime}\\right) \\neq \\tau\\left(\\mathcal{G}_{[v]}^{\\prime \\prime}\\right)$一定成立。对于在原图上$L$层GraphSAGE来说，第$L$-hop节点没有可能进行互相之间的message passing，除非GraphSAGE的层数增加，也就是GraphSAGE至少需要$L+1$层才有可能区分关于两个不同图的函数$\\tau$, $\\tau\\left(\\mathcal{G}_{-}[v]^{\\prime}\\right)$ 和$\\tau\\left(\\mathcal{G}_{-}[v]^{\\prime \\prime}\\right)$。所以原图上的$L$层GraphSAGE难以区分函数$\\tau$的不同输入，会将不同的输入图输出同一个值，导致表达能力下降。而对于子图上的$L^\\prime$层shaDow-SAGE， 直观上来看，它在最后一层后还有Message Passing，所以可以区分不同的输入。因此表达能力强于GraphSAGE。\nTopological Learning Perspective 对于Regular Graph, 总所周知它无法被1-WL test 区分， 因为“regular” property describes a global topological symmetry among nodes, 节点之间是全局拓扑对称的，任意两个节点的任意阶邻居数量完全一样，所以1-WL test无法区分regular graph 中的任意两个节点， 如图一中$\\mathcal{G}$所示。而对于shaDow-GNN来说，每个节点的局部子图不一定regular，如上图中$\\mathcal{G}_{[v]}^{1}$ 和$\\mathcal{G}_{[u]}^{1}$所示，使用$\\text { EXTRACT}$为节点提取一阶邻居后，可以区分两个节点。\n另外$L^\\prime$层shaDow-GNN 可以近似$L$层GIN。基于Universal Approximation Theorem, 先做$L^\\prime-L$次MLP，使得输入等于输出： $$ \\definecolor{energy}{RGB}{114,0,172} \\definecolor{freq}{RGB}{45,177,93} \\definecolor{spin}{RGB}{251,0,29} \\definecolor{signal}{RGB}{18,110,213} \\definecolor{circle}{RGB}{217,86,16} \\definecolor{average}{RGB}{203,23,206} \\definecolor{red}{RGB}{255,0,0} \\definecolor{black}{RGB}{0,0,0} \\begin{aligned} \\boldsymbol{h}_{v}^{(\\ell)} \u0026amp;=f_{1}^{(\\ell)}\\left(\\color{red}\\boldsymbol{h}_{v}^{(\\ell-1)}, \\color{black}\\sum_{u \\in \\mathcal{N}_{v}} f_{2}^{(\\ell)}\\left(\\boldsymbol{h}_{v}^{(\\ell-1)}, \\boldsymbol{h}_{u}^{(\\ell-1)}\\right)\\right) \\\\ \u0026amp;=\\color{red}\\boldsymbol{h}_{v}^{(\\ell-1)}\\color{black}, \\quad \\forall 1 \\leq \\ell \\leq L^{\\prime}-L \\end{aligned} $$\n在此基础上在做$L$层GIN。 所以$L^\\prime$层shaDow-GNN 可以表达$L$层GIN。\n子图提取算法 启发式$\\text { EXTRACT}$： 1. 根据最短路径，随机选取或者选取全部$L$-hop内的邻居作为$\\mathcal{G}_{[v]}$。 2. 基于PPR score为每个节点选取top-K 节点，构造诱导子图。同理 Katz index， SimRank都可以作为分数指标。\nArchitecture Subgraph Pooling 对于shaDow-GNN，通过READOUT子图中的所有节点embedding来作为target node的embedding、对于normal GNN, 则是直接将第$L$层的输出作为embedding。考虑一个问题，节点的$L$-hop邻域内的节点也可能存在交互，比如对于节点$v$来说，它的邻域内有两个节点$u$和$w$，这两个节点之间的距离可能有$2L$ hop。 但是$L$层GNN无法捕获$u$和$w$之间的交互，除非将层数扩大到$2L$，但这将引入相当多无关节点。 而shaDow-GNN在$L$-hop子图$\\mathcal{G}_{[v]}$中直接应用深层（$\u0026gt;2L$）GNN，此时可以捕获邻域内节点互相之间的交互，获得更加有意义的embedding。\nSubgraph Ensemble 单一的$\\text { EXTRACT}$不一定能够选取有意义的邻居， 所以可以使用多个候选$\\text { EXTRACT}$，若有$R$个候选$\\text { EXTRACT}$，最后每个节点将得到$R$个embedding，然后可以用一个基于attention的聚合函数将每个$\\text { EXTRACT}$输出的node embedding组合目标节点最终的node embedding。\nReference [1]Tackling Over-Smoothing for General Graph Convolutional Networks.\n","permalink":"https://JhuoW.github.io/posts/decouplinggcn/","summary":"paper\nIntroduction 现有的GNN在图和模型的size方面可拓展性有限。 对于大图，增加模型的深度对导致scope(感受野)大小成指数放大。深层model主要面临两个基本挑战： 1. oversmoothing导致表达能力下降，2. 邻域爆炸导致计算成本高昂。\n本文旨在结构GNN的depth 和 scope，首先提取子图作为有限大小（bounded-size）的scope, 然后将任意深度的GNN用于子图上。 由于提取出的局部子图是由少量关键邻居组成，且排除了不相关的邻居，所以深层GNN也可以学到informative representations。\n增加GNN层数会造成以下基本障碍：\n Expresivity (oversmoothing): 邻居的迭代混合导致不同节点的切入向量收敛到一个固定的低维子空间 Scalability (neighbor explosion): 多跳邻居递归导致感受野大小呈指数级增长  为了研究导致表达能力和可拓展性缺陷的根本原因，本文提出了以下insight:\nTwo views on the graph: 如果从全局视角来看两个节点，如果两个节点在同一个图的同一个连通分量中，那么这两个节点在随机游走中存在到达概率，无论他们间隔多远。而本文给出了图的局部视角，具体来说， 给定节点$v$的局部子图$\\mathcal{G}_{[v]}$，将$\\mathcal{G}_{[v]}$仅包含节点$v$的特性，整个图看一看做所有子图$\\mathcal{G}_{[v]}$的集合。那么$v$的邻域不在是所有节点$\\mathcal{V}$，而它的邻域只存在于$\\mathcal{V}_{[v]}$中。 如果节点$u$不在$\\mathcal{V}_{[v]}$中，$u$将永远不会被考虑为$v$的邻居，无论GNN有多深。\nScope of GNNs: 加深GNN层次所造成的的表达能力和可拓展性问题都和GNN不断扩大的感受野（scope）有关。随着层次变深，感受野不断变大，使得每个节点包含的信息重叠越多，最终收敛到同一个子空间，导致oversmoothing; 另外 感受野变大，导致每个节点的邻居数呈指数级上升，导致邻居爆炸。所以GNN的层数加深会导致感受野变大（耦合），即$L$层GNN的感受野为全部$L$-hop以内的邻居，层数深度（depth）和感受野大小(scope)的强耦合限制了GNN的设计。\nDecoupling the GNN depth and scope: 为了解耦GNN的深度（depth）与感受野(scope)，使得加层数与感受野无关。对于节点$v$，首先为它提取一个小的子图$\\mathcal{G}_{[v]}$，然后在小的子图上应用任意层数的GNN。若GNN的层数$L^\\prime$大于感受野的跳数，那么子图中的每对节点会交换多次信息，额外的消息传递有助于GNN更好的融合scope内的信息，从而增强表达能力。\nDecoupling the Depth and Scope of GNNs Definition (Depth of subgraph) ：假设子图$\\mathcal{G}_{[v]}$是连通的，$\\mathcal{G}_{[v]}$的depth定义为$\\max _{u \\in \\mathcal{V}_{[v]}} d(u, v)$, 其中$d(u, v)$表示$u$到$v$的最短路径。\n本文提出shaDow-GNN，它包含了一个子图提取器$\\text { EXTRACT}$。 shaDow-GNN的过程如下：\n 用子图提取器$\\operatorname{EXTRACT}(v, \\mathcal{G})$为节点$v$提取一个连通子图$\\mathcal{G}_{[v]}$，子图的深度（距离$v$最远的节点和$v$之间的跳数）为$L$。 构建一个$L^\\prime$层的GNN并应用在$\\mathcal{G}_{[v]}$上。 如果 $L^\\prime \u0026gt; L$那么可以反映decoupling，因为GNN层数此时与scope无关，层数加深不会影响感受野。  本文从三个不同的角度理论证明了shaDow-GNN可以提升GNN的表达能力。","title":"NeurIPS2021 《Decoupling the Depth and Scope of Graph Neural Networks》 Reading Notes"},{"content":"paper\nIntroduction 本文提出了一种新型的Graph Contrastive Learning构造Contrastive pairs的方式，即将跨图的同维度特征作为positive pairs， 不同维度特征作为negative pairs。 和过去的GCL方法相比，本文无需互信息估计器（MI Estimator），映射头（Projector），不对称结构（asymmetric structures）。 并且理论证明了该方法可以看做Information Bottleneck 原则在自监督设置下的实例。\n具体来说，受典型相关分析（From Canonical Correlation Analysis）的启发，本文提出了一种简单有效的GCL框架，从而是模型避免复杂难以理解多模块设计。 和过去的方法相同的是，为输入图以随机增强的方式生成两个view， 目的是为两个view学习共享的 node representations 通过共享的GNN Encoder。不同在于，本文利用了典型相关分析（CCA），具体来说，新目标旨在最大化同一输入的两个增强views之间的相关性，同时对单个视图表示的不同（特征）维度进行去相关（避免不同维度捕获相同信息，即同一个view内的不同维度channel互为negative pairs）。 这么做的目的是 1）本质上追求的是丢弃增强后变化的信息，同时保留增强后不变的信息，以及 2）防止维度崩溃（即不同维度捕获相同的信息）。\n和其他方法的对比如上图所示， 本文提出的CCA-SSG无需negative pairs， 参数化的互信息估计器， projection head或者不对称结构。对比对的数量仅为$O(D^2)$, 其中$D$为输出维度。\nCanonical Correlation Analysis CCA: Identify and Quantify the associations between two sets of variables， 即CCA用来衡量两组随机变量的相关性，每组可能有很多Random Variables.\n从相关系数引入：\nPearson 相关系数： 给定两组数据集$X$， $Y$。 其中$X \\in \\mathbb{R}^{N \\times 1}$ 表示只有一个随机变量（属性），样本数为$N$。 $Y \\in \\mathbb{R}^{M \\times 1}$: 一个随机变量，样本量为$M$。那么Pearson 相关系数$\\rho$定义为： $$ \\rho(X,Y)= \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y} $$ 其中$\\sigma_X$，$\\sigma_Y$分别为$X$和$Y$的标准差。$\\mathrm{Cov}(X,Y)$为$X$, $Y$的协方差。$\\rho \\in [-1,1]$。 $\\rho$越接近1， $X$和$Y$的线性相关性越高。$\\rho$越接近0，$X$和$Y$的线性相关性月底。\n相关系数存在问题：相关系数不适用于高维数据。 如果$X$是2维的（2个属性，例如身高和体重）， $Y$也是2维的，属性为(跑步，跳远)， $X \\in \\mathbb{R}^{N \\times 2}$, $Y \\in \\mathbb{R}^{M \\times 2}$。此时，相关系数$\\rho$無法計算2維隨機變量的相關程度。\nCCA 基本思想 $X$和$Y$ 为两个变量集合， 例如$X$中有两个随机变量（2维）， $Y$中也有两个随机变量。 要衡量变量间的相关性： 现将高维随机变量（即多个随机变量）降到一维（一个随机变量），再用相关系数计算相关性。\n令$X = \\{\\boldsymbol{x}_1,\\boldsymbol{x}_2\\} \\in \\mathbb{R}^{n_1\\times m}$， 表示$n_1=2$个随机变量，$m$个样本。 $Y = \\{\\boldsymbol{y}_1,\\boldsymbol{y}_2\\} \\in \\mathbb{R}^{n_2\\times m}$表示$n_2=2$个随机变量，$m$个样本。\n$U$为随机变量集合$X$的线性组合： $$ U = a_1 \\boldsymbol{x}_1 + a_2 \\boldsymbol{x}_2 = [a_1, a_2]\\begin{bmatrix} \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2\\end{bmatrix} = a^\\top X $$ $V$为随机变量集合$Y$的线性组合： $$ V = b_1 \\boldsymbol{y}_1 + b_2\\boldsymbol{x}_2 = b^\\top Y $$ CCA的优化目标： 找到一组最优解$a$和$b$， 使得$\\rho_{U,V}$最大： $$ \\arg \\max_{a,b} \\rho_{U,V} = \\frac{\\mathrm{Cov}(U,V)}{\\sigma_U \\sigma_V} $$ 得到的$a$, $b$是使得$X$与$Y$有最大关联的权重。\nCCA的表示与求解 输入：两个随机变量集合$X = \\{\\boldsymbol{x}_1 , \\cdots, \\boldsymbol{x}_n\\}$, $Y= \\{\\boldsymbol{y}_1 , \\cdots, \\boldsymbol{y}_m\\}$。 分别有$n$个和$m$个随机变量。\n$X$是一个$n \\times L$的矩阵， 即有$L$个样本， $n$个属性（$n$个随机变量）。\n$Y$是一个$m \\times L$的矩阵， $L$个样本， $m$个属性。\n$U = a^\\top X \\in \\mathbb{R}^{1 \\times L}$, $V= b^\\top Y \\in \\mathbb{R}^{1\\times L}$, 分别将组高维随机变量转为一维。 目标函数为 $$ \\arg \\max_{a,b} \\rho_{U,V} =\\arg \\max_{a,b} \\frac{\\mathrm{Cov}(U,V)}{\\sigma_U \\sigma_V} $$ 设 $\\Sigma_{XX} = \\mathrm{Cov}(X,X) = \\mathrm{Var}(X)$， $\\Sigma_{YY} = \\mathrm{Cov}(Y,Y) = \\mathrm{Var}(Y)$， $\\Sigma_{XY} = \\mathrm{Cov}(X,Y)$， $E[X] = \\mu_X \\in \\mathbb{R}^{n \\times 1}$ （样本均值）， $E[Y] = \\mu_Y \\in \\mathbb{R}^{m \\times 1}$。\n定义$X$ 为一个$n$个随机变量stack成的列向量： $$ X= \\begin{bmatrix} \\boldsymbol{x}_1 \\\\ \\cdots \\\\ \\boldsymbol{x}_n\\end{bmatrix} \\in \\mathbb{R}^{n \\times L} $$ $C$ 为$n$个scalars $c_1, \\cdots, c_n$ stack成的列向量： $$ C= \\begin{bmatrix} \\boldsymbol{c}_1 \\\\ \\cdots \\\\ \\boldsymbol{c}_n\\end{bmatrix} $$ $C^\\top X$是这$n$个Random Variables的线性组合。 $C^\\top X$的方差为： $$ \\mathrm{Var}(C^\\top X) = C^\\top \\Sigma_{XX} C = C^\\top \\mathrm{Var}(X) C $$ 那么$\\mathrm{Var}(U) = \\mathrm{Var}(a^\\top X) = a^\\top \\mathrm{Var}(X) a$。\n每个随机变量$\\boldsymbol{x}_i$为数据的第$i$个特征，每列为一个样本$X \\in \\mathbb{R}^{n \\times L}$。 有$L$个样本， 对特征维度做标准化，也就是对每个维度$\\boldsymbol{x}_i$做标准化， 可得$E(\\boldsymbol{x}_i) = 0$, $\\mathrm{Var}(\\boldsymbol{x}_i) = 1$。 $$ \\begin{aligned} \\mathrm{Var}(X) \u0026amp;= E(X-E(X))^2 \\\\ \u0026amp;= E(\\begin{bmatrix} \\boldsymbol{x}_1 \\\\ \\cdots \\\\ \\boldsymbol{x}_n\\end{bmatrix} -\\begin{bmatrix} \\boldsymbol{\\mu}_1 \\\\ \\cdots \\\\ \\boldsymbol{\\mu}_n\\end{bmatrix} )^2 \\\\ \u0026amp;= E (\\begin{bmatrix} \\boldsymbol{x}_1 \\\\ \\cdots \\\\ \\boldsymbol{x}_n\\end{bmatrix}^2) \\\\ \u0026amp;= E(XX^\\top) \\end{aligned} $$ 所以$\\mathrm{Var}(U) = a^\\top E(XX^\\top) a$， 同理$\\mathrm{Var}(V) = b^\\top E(YY^\\top) b$。另外： $$ E(a^\\top X) = E(a_1\\boldsymbol{x}_1 + \\cdots + a_n\\boldsymbol{x}_n) = a_1E(\\boldsymbol{x}_1) + \\cdots + a_n E(\\boldsymbol{x}_n) = 0 $$ 那么： $$ \\begin{aligned} \\mathrm{Cov}(U,V) \u0026amp;= \\mathrm{Cov}(a^\\top X, b^\\top Y) \\\\ \u0026amp;= E\\left[ \\langle a^\\top X - E(a^\\top X), b^\\top Y- E(b^\\top Y) \\rangle \\right] \\\\ \u0026amp;= E[\\langle a^\\top X, b^\\top Y \\rangle] \\\\ \u0026amp;= E[(a^\\top X)(b^\\top Y)^\\top] \\\\ \u0026amp;= E[a^\\top X Y^\\top b] \\\\ \u0026amp;= a^\\top E[XY^\\top]b \\end{aligned} $$\n$$ \\begin{aligned} \\mathrm{Var}(X) \u0026amp;= \\mathrm{Cov}(X,X) = E[XX^\\top] \\\\ \\mathrm{Var}(Y) \u0026amp;= \\mathrm{Cov}(Y,Y) = E[YY^\\top] \\\\ \\mathrm{Cov}(X,Y) \u0026amp;= E[\\langle X-\\mu_X, Y-\\mu_Y \\rangle] = E[XY^\\top] = \\Sigma_{XY}\\\\ \\mathrm{Cov}(Y,X) \u0026amp;=E[YX^\\top] \\end{aligned} $$\n优化目标转化为： $$ \\begin{aligned} \\arg \\max_{a,b} \\rho_{U,V} \u0026amp;=\\arg \\max_{a,b} \\frac{\\mathrm{Cov}(U,V)}{\\sigma_U \\sigma_V} \\\\ \u0026amp;=\\arg \\max_{a,b} \\frac{a^\\top \\Sigma_{XY}b}{\\sqrt{a^\\top \\Sigma_{XX} a} \\sqrt{b^\\top \\Sigma_{YY}b}} \\end{aligned} $$ 若对$a$， $b$同时放缩， 即$a$放缩$k$倍， $b$放缩$l$倍， 公式的值不会改变： $$ \\frac{ka^\\top \\Sigma_{XY}lb}{\\sqrt{ka^\\top \\Sigma_{XX} ka} \\sqrt{lb^\\top \\Sigma_{YY}lb}} = \\frac{a^\\top \\Sigma_{XY}b}{\\sqrt{a^\\top \\Sigma_{XX} a} \\sqrt{b^\\top \\Sigma_{YY}b}} $$ 所以， 可以直接对$a$做放缩，使得$a^\\top \\Sigma_{XX} a=1$, 对$b$做放缩，使得$b^\\top \\Sigma_{YY}b=1$（类似于SVM）。 那么优化目标转化为： $$ \\begin{aligned} \u0026amp;\\max_{a, b} a^{\\top} \\Sigma_{X Y} b, \\\\ \u0026amp;\\text{ s.t. } a^{\\top} \\Sigma_{X X} a=b^{\\top} \\Sigma_{Y Y} b=1 \\end{aligned} $$ 对于两个向量集合$X_1$和$X_2$， CCA 寻求两组向量最大化它们的相关性，并受到它们彼此不相关的约束。 后来的研究通过用神经网络代替线性变换，将 CCA 应用于具有深度模型的多视图学习。 具体来说，假设 $X_1$和$X_2$作为输入数据的两个视图，CCA的优化目标为： $$ \\max_{\\theta_{1}, \\theta_{2}} \\operatorname{Tr}\\left(P_{\\theta_{1}}^{\\top}\\left(X_{1}\\right) P_{\\theta_{2}}\\left(X_{2}\\right)\\right) \\quad \\text { s.t. } P_{\\theta_{1}}^{\\top}\\left(X_{1}\\right) P_{\\theta_{1}}\\left(X_{1}\\right)=P_{\\theta_{2}}^{\\top}\\left(X_{2}\\right) P_{\\theta_{2}}\\left(X_{2}\\right)=I \\text {. } \\tag{1} $$ 其中， $P_{\\theta_{1}}$和$P_{\\theta_{2}}$为两个Neural Network。尽管上式很精确，但这种计算确实很昂贵。Soft CCA 通过采用以下拉格朗日松弛, 消除了hard decorrelation constraint： $$ \\min_{\\theta_{1}, \\theta_{2}} \\mathcal{L}_{\\text {dist }}\\left(P_{\\theta_{1}}\\left(X_{1}\\right), P_{\\theta_{2}}\\left(X_{2}\\right)\\right)+\\lambda\\left(\\mathcal{L}_{S D L}\\left(P_{\\theta_{1}}\\left(X_{1}\\right)\\right)+\\mathcal{L}_{S D L}\\left(P_{\\theta_{2}}\\left(X_{2}\\right)\\right)\\right) $$ 其中$\\mathcal{L}_{\\text {dist }}$用于衡量两个view的representations之间的相关性，$\\mathcal{L}_{S D L}$ (stochastic decorrelation loss)计算$P_{\\theta_{i}}\\left(X_{i}\\right)$和identity matrix之间的$L_1$距离。\nApproach 模型包含3个模块 1. 随机图增强器$\\mathcal{T}$，2. GNN encoder $f_\\theta$, 3. 基于CCA的feature-level对比损失。\nGraph Augmentations 本文利用 edge droping和 node feature masking两种graph corruption方式来对输入图做增强。 $\\mathcal{T}$是所有可能的转换操作，$t \\sim \\mathcal{T}$表示图$G$的一种特定的转换。比如删除一条边的操作$t_r$就是$\\mathcal{T}$中的一个变换。\nTraining 从$\\mathcal{T}$随机采样两种图变换 $t_A$和$t_B$。 生成两个View: $\\tilde{\\mathbf{G}}_{A}=\\left(\\tilde{\\mathbf{X}}_{A}, \\tilde{\\mathbf{A}}_{A}\\right)$和$\\tilde{\\mathbf{G}}_{B}=\\left(\\tilde{\\mathbf{X}}_{B}, \\tilde{\\mathbf{A}}_{B}\\right)$，经过共享的GNN后，得到输出$\\mathbf{Z}_{A}=f_{\\theta}\\left(\\tilde{\\mathbf{X}}_{A}, \\tilde{\\mathbf{A}}_{A}\\right)$，$\\mathbf{Z}_{B}=f_{\\theta}\\left(\\tilde{\\mathbf{X}}_{B}, \\tilde{\\mathbf{A}}_{B}\\right)$。然后对feature dimensionzuo normalization (列标准化)， 是的每个特征维度均值为0， 标准差为$1 / \\sqrt{N}$：\n$$ \\tilde{\\mathbf{Z}}=\\frac{\\mathbf{Z}-\\mu(\\mathbf{Z})}{\\sigma(\\mathbf{Z}) * \\sqrt{N}} $$\nInference 基于公式（1）,使用公式(1)中的CCA目标函数，将向量集定义为输出$\\tilde{\\mathbf{Z}}$的列向量， 最终CCA-SSG的目标函数定义如下： $$ \\mathcal{L}=\\underbrace{\\left|\\left|\\tilde{\\mathbf{Z}}_{A}-\\tilde{\\mathbf{Z}}_{B}\\right|\\right|_{F}^{2}}_{\\text {invariance term }}+\\lambda \\underbrace{\\left(\\left|\\left|\\tilde{\\mathbf{Z}}_{A}^{\\top} \\tilde{\\mathbf{Z}}_{A}-\\mathbf{I}\\right|\\right|_{F}^{2}+\\left|\\left|\\tilde{\\mathbf{Z}}_{B}^{\\top} \\tilde{\\mathbf{Z}}_{B}-\\mathbf{I}\\right|\\right|_{F}^{2}\\right)}_{\\text {decorrelation term }} $$ 第二项中，要求不同特征之间的相似度尽可能低， 从而使得不同特征捕获不同的语义信息。\n","permalink":"https://JhuoW.github.io/posts/cca-ssg/","summary":"paper\nIntroduction 本文提出了一种新型的Graph Contrastive Learning构造Contrastive pairs的方式，即将跨图的同维度特征作为positive pairs， 不同维度特征作为negative pairs。 和过去的GCL方法相比，本文无需互信息估计器（MI Estimator），映射头（Projector），不对称结构（asymmetric structures）。 并且理论证明了该方法可以看做Information Bottleneck 原则在自监督设置下的实例。\n具体来说，受典型相关分析（From Canonical Correlation Analysis）的启发，本文提出了一种简单有效的GCL框架，从而是模型避免复杂难以理解多模块设计。 和过去的方法相同的是，为输入图以随机增强的方式生成两个view， 目的是为两个view学习共享的 node representations 通过共享的GNN Encoder。不同在于，本文利用了典型相关分析（CCA），具体来说，新目标旨在最大化同一输入的两个增强views之间的相关性，同时对单个视图表示的不同（特征）维度进行去相关（避免不同维度捕获相同信息，即同一个view内的不同维度channel互为negative pairs）。 这么做的目的是 1）本质上追求的是丢弃增强后变化的信息，同时保留增强后不变的信息，以及 2）防止维度崩溃（即不同维度捕获相同的信息）。\n和其他方法的对比如上图所示， 本文提出的CCA-SSG无需negative pairs， 参数化的互信息估计器， projection head或者不对称结构。对比对的数量仅为$O(D^2)$, 其中$D$为输出维度。\nCanonical Correlation Analysis CCA: Identify and Quantify the associations between two sets of variables， 即CCA用来衡量两组随机变量的相关性，每组可能有很多Random Variables.\n从相关系数引入：\nPearson 相关系数： 给定两组数据集$X$， $Y$。 其中$X \\in \\mathbb{R}^{N \\times 1}$ 表示只有一个随机变量（属性），样本数为$N$。 $Y \\in \\mathbb{R}^{M \\times 1}$: 一个随机变量，样本量为$M$。那么Pearson 相关系数$\\rho$定义为： $$ \\rho(X,Y)= \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y} $$ 其中$\\sigma_X$，$\\sigma_Y$分别为$X$和$Y$的标准差。$\\mathrm{Cov}(X,Y)$为$X$, $Y$的协方差。$\\rho \\in [-1,1]$。 $\\rho$越接近1， $X$和$Y$的线性相关性越高。$\\rho$越接近0，$X$和$Y$的线性相关性月底。","title":"NeurIPS2021 《From Canonical Correlation Analysis to Self-supervised Graph Neural Networks》 Reading Notes"},{"content":"paper\nIntroduction 本文旨在通过多视图Contrastive Learning 来学习节点表示和图表示。其中对比视图为结构视图（structural view）。本文发现，两个以上的对比视图不会提升性能（我觉得仅是针对本文的Diffusion-based view吧~）。 本文实验性的表明了基于一阶邻居和图扩散视图做CL可以达到最好的效果。\n为了将对比学习应用到图表示学习任务，本文提出通过最大化图的不同结构视角的互信息来作为监督信号。通过对提出框架的系统研究，本文展示了一些GCL和visual CL上的不同： （1）将view数量（即增强）增加到两个以上不会提高性能，最好的性能是通过对比来自一阶邻居view的embedding和graph diffusion的embedding，(2) 与对比图编码或多尺度编码相比，跨视图对比节点和图编码在node classification 和 graph classification上都能获得更好的结果。 (3) 与分层图池化方法（例如DiffPool相比）一个简单的Readout在这node classification 和 graph classification上实现了更好的性能，以及 (4) 应用正则化（early stopping除外） 或归一化层对性能有负面影响。\nMethod MVGRL通过最大化一个view的node embedding和另一个view的graph embedding之间的 互信息来学习节点和图表示。如上图所示，MVGRL由以下几个部分构成\n 增强机制：将样本图转化为同一个图的相关view， 这个view只是structural view， 不会改变原图中的node feature，然后对两个增强图中的相同节点（identical node）进行子采样，类似于CV中的域剪裁。 两个专用的GNNs， 每个view一个GNN，再接一个共享的MLP作为projection head，来为两个view学习representation。 图池化层， 在MLP后学习两个图的graph-level representation。 判别器 来对比一个图的embedding和另一个图的节点embedding,并对他们的一致性（agreement）评分。  Augmentations 考虑两种类型的图增强：(1) 对初始节点特征进行操作的特征空间增强，例如，mask或添加高斯噪声，以及 (2) 通过添加或删除连通性、子采样或使用最短路径或diffusion matrix生成全局视图来对做图结构增强。 前一种增强可能是有问题的，因为许多数据集不带有初始节点特征。 此外，观察到在任一空间上屏蔽或添加噪声都会降低性能。 因此，本文选择生成全局视图，然后进行子采样。\n实验表明，在大多数情况下，最好的结果是通过将邻接矩阵转化为扩散矩阵，并将这两个矩阵视为同一图的结构的两个一致view。因为邻接矩阵和扩散矩阵分别提供了图结构的局部和全局视图，从这两种view中学习到的表示之间最大一致性，从而鼓励模型同时编码的局部和全局信息。\nDiffusion matrix从全局角度提供了任意节点对之间的相关性，其中$\\mathbf{T} \\in \\mathbb{R}^{n \\times n}$是通用转移矩阵，$\\Theta$是权重系数，决定了全局和局部信息的比例，即对于每个节点，不同层次信息的比重， $\\Theta_{k}$越大，表示全局信息权重越大。 令$\\sum_{k=0}^{\\infty} \\theta_{k}=1, \\theta_{k} \\in[0,1]$，$\\lambda_{i} \\in[0,1]$,其中$\\lambda$是$\\mathbf{T}$的特征向量， 这样来保证$\\mathbf{S}$可以收敛到一个固定矩阵。扩散用快速近似值和稀疏化方法计算： $$ \\mathbf{S}=\\sum_{k=0}^{\\infty} \\Theta_{k} \\mathbf{T}^{k} \\in \\mathbb{R}^{n \\times n} $$ 给定一个邻接矩阵$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$和一个对角度矩阵$\\mathbf{D} \\in \\mathbb{R}^{n \\times n}$, Personalized PageRank (PPR)和Heat Kernel分别为两种不同的Diffusion matrix实例。对于PPR和HK，转移概率矩阵定义为$\\mathbf{T}=\\mathbf{A} \\mathbf{D}^{-1}$。PPR将第$k$层的权重系数设置为$\\theta_{k}=\\alpha(1-\\alpha)^{k}$, 而HK将第$k$层的权重系数设置为$\\theta_{k}=e^{-t} t^{k} / k !$。\nPPR的封闭阶如下所示： $$ \\mathbf{S}^{\\mathrm{PPR}}=\\alpha\\left(\\mathbf{I}_{n}-(1-\\alpha) \\mathbf{D}^{-1 / 2} \\mathbf{A} \\mathbf{D}^{-1 / 2}\\right)^{-1} $$ HK的封闭解如下所示： $$ \\mathbf{S}^{\\text {heat }}=\\exp \\left(t \\mathbf{A} \\mathbf{D}^{-1}-t\\right) $$\nSub-Sampling 从一个view中随机采样节点及其边，并从另一个view中选择exact的的节点和边 (如示意图所示， 从第一个图中采样节点和边的子图作为一个view，从第二个图中采样相同节点以及这些节点之间的边作为另一个view，来做对比学习)。这个过程允许MVGRL应用于具有图数据不适合GPU内存的inductive任务，也可以通过将子样本视为独立的图来考虑transductive任务。\nEncoder 和其他GCL方法不同的是，这里不同视图使用的是各自的GNN编码器， 邻接矩阵和Diffusion matrix是同一个图的两个一致视角，分别反映了局部和全局性质。首先，为两种view采样之后的子图分别定义GNN encoder：$g_{\\theta}(.), g_{\\omega}(.): \\mathbb{R}^{n \\times d_{x}} \\times \\mathbb{R}^{n \\times n} \\longmapsto \\mathbb{R}^{n \\times d_{h}}$， 使用最简单的GCN，传播矩阵分别为normalized adjacency matrix $\\sigma(\\tilde{\\mathbf{A} }X \\boldsymbol{\\Theta})$ 和 Diffusion Matrix: $\\sigma(\\mathbf{S} X \\boldsymbol{\\Theta})$。学习到的embedding输入projection head （MLP）$f_{\\psi}(.): \\mathbb{R}^{n \\times d_{h}} \\longmapsto \\mathbb{R}^{n \\times d_{h}}$中， 得到两个view的输出node embedding matrix: $\\mathbf{H}^{\\alpha}, \\mathbf{H}^{\\beta} \\in \\mathbb{R}^{n \\times d_{h}}$。\n接下来使用pooling $\\mathcal{P}(.): \\mathbb{R}^{n \\times d_{h}} \\longmapsto \\mathbb{R}^{d_{h}}$ 输出两个view的graph representations。 本文采用JKnet中的跳连机制，即GNN的每层输出做sum pooling, 然后将所有层拼起来做特征变换： $$ \\vec{h}_{g}=\\sigma\\left(||_{l=1}^{L}\\left[\\sum_{i=1}^{n} \\vec{h}_{i}^{(l)}\\right] \\mathbf{W}\\right) \\in \\mathbb{R}^{h_{d}} $$ 其中$\\vec{h}_{i}^{(l)}$是节点$i$的第$l$层输出，$||$是concatenation， $\\mathbf{W} \\in \\mathbb{R}^{\\left(L \\times d_{h}\\right) \\times d_{h}}$是特征变换参数，$\\sigma$是PReLU非线性激活。最终，将图表示输入到一个projection head $f_{\\phi}(.): \\mathbb{R}^{d_{h}} \\longmapsto \\mathbb{R}^{d_{h}}$ 中，得到最终的图表示：$\\vec{h}_{g}^{\\alpha}, \\vec{h}_{g}^{\\beta} \\in \\mathbb{R}^{d_{h}}$。\n在推理阶段， 由于两个view来自同一个图，可以把两个view的表示结合起来作为原图的表示：两个view的graph embedding直接相加，作为原图 embedding.。 两个view的node embedding 直接相加，作为原图的node embedding $\\vec{h}=\\vec{h}_{g}^{\\alpha}+\\vec{h}_{g}^{\\beta} \\in \\mathbb{R}^{n}$ 。 $\\mathbf{H}=\\mathbf{H}^{\\alpha}+\\mathbf{H}^{\\beta} \\in \\mathbb{R}^{n \\times d_{h}}$。 这里得到的原图embedding可以应用于下游任务。\nTraining 为了端到端训练encoder并学习与下游任务无关的丰富节点和图级表示，本文利用 Deep InfoMax 方法并通过对比一个视图的节点表示与图表示来最大化两个视图之间的 互信息。 实验表明，这种方法在节点和图分类上始终优于对比图-图或多尺度编码。 目标定义如下： $$ \\max_{\\theta, \\omega, \\phi, \\psi} \\frac{1}{|\\mathcal{G}|} \\sum_{g \\in \\mathcal{G}}\\left[\\frac{1}{|g|} \\sum_{i=1}^{|g|}\\left[\\operatorname{MI}\\left(\\vec{h}_{i}^{\\alpha}, \\vec{h}_{g}^{\\beta}\\right)+\\operatorname{MI}\\left(\\vec{h}_{i}^{\\beta}, \\vec{h}_{g}^{\\alpha}\\right)\\right]\\right] $$ 其中$\\theta, \\omega, \\phi, \\psi$为是GNN encoder和projection head的参数, $|\\mathcal{G}|$是图数量，$|\\mathcal{g}|$是图中节点数， $\\vec{h}_{i}^{\\alpha}, \\vec{h}_{g}^{\\beta}$分别表示view $\\alpha$中的节点$i$的representation， 和view $\\beta$的 graph representation。\n互信息判别器： $\\mathcal{D}(., .): \\mathbb{R}^{d_{h}} \\times \\mathbb{R}^{d_{h}} \\longmapsto \\mathbb{R}$简单的设置为表示向量间的内积相似度： $$ \\mathcal{D}\\left(\\vec{h}_{n}, \\vec{h}_{g}\\right)=\\vec{h}_{n} \\cdot \\vec{h}_{g}^{T} $$ 作者发现当判别器和projection head集成到双线性层中时，节点分类基准略有改进。 为了确定 MI 估计器，实验中调查了四个估计器并为每个基准选择了最好的一个。\n正样本采样自联合分布$x_{p} \\sim p\\left(\\left[\\mathbf{X}, \\tau_{\\alpha}(\\mathbf{A})\\right],\\left[\\mathbf{X}, \\tau_{\\beta}(\\mathbf{A})\\right]\\right)$， 从边际乘积中采样负样本 $x_{p} \\sim p\\left(\\left[\\mathbf{X}, \\tau_{\\alpha}(\\mathbf{A})\\right]\\right) p\\left(\\left[\\mathbf{X}, \\tau_{\\beta}(\\mathbf{A})\\right]\\right)$。利用小批量随机梯度下降法对模型参数进行优化。 MVGRL算法如下：\n","permalink":"https://JhuoW.github.io/posts/mvgrl/","summary":"paper\nIntroduction 本文旨在通过多视图Contrastive Learning 来学习节点表示和图表示。其中对比视图为结构视图（structural view）。本文发现，两个以上的对比视图不会提升性能（我觉得仅是针对本文的Diffusion-based view吧~）。 本文实验性的表明了基于一阶邻居和图扩散视图做CL可以达到最好的效果。\n为了将对比学习应用到图表示学习任务，本文提出通过最大化图的不同结构视角的互信息来作为监督信号。通过对提出框架的系统研究，本文展示了一些GCL和visual CL上的不同： （1）将view数量（即增强）增加到两个以上不会提高性能，最好的性能是通过对比来自一阶邻居view的embedding和graph diffusion的embedding，(2) 与对比图编码或多尺度编码相比，跨视图对比节点和图编码在node classification 和 graph classification上都能获得更好的结果。 (3) 与分层图池化方法（例如DiffPool相比）一个简单的Readout在这node classification 和 graph classification上实现了更好的性能，以及 (4) 应用正则化（early stopping除外） 或归一化层对性能有负面影响。\nMethod MVGRL通过最大化一个view的node embedding和另一个view的graph embedding之间的 互信息来学习节点和图表示。如上图所示，MVGRL由以下几个部分构成\n 增强机制：将样本图转化为同一个图的相关view， 这个view只是structural view， 不会改变原图中的node feature，然后对两个增强图中的相同节点（identical node）进行子采样，类似于CV中的域剪裁。 两个专用的GNNs， 每个view一个GNN，再接一个共享的MLP作为projection head，来为两个view学习representation。 图池化层， 在MLP后学习两个图的graph-level representation。 判别器 来对比一个图的embedding和另一个图的节点embedding,并对他们的一致性（agreement）评分。  Augmentations 考虑两种类型的图增强：(1) 对初始节点特征进行操作的特征空间增强，例如，mask或添加高斯噪声，以及 (2) 通过添加或删除连通性、子采样或使用最短路径或diffusion matrix生成全局视图来对做图结构增强。 前一种增强可能是有问题的，因为许多数据集不带有初始节点特征。 此外，观察到在任一空间上屏蔽或添加噪声都会降低性能。 因此，本文选择生成全局视图，然后进行子采样。\n实验表明，在大多数情况下，最好的结果是通过将邻接矩阵转化为扩散矩阵，并将这两个矩阵视为同一图的结构的两个一致view。因为邻接矩阵和扩散矩阵分别提供了图结构的局部和全局视图，从这两种view中学习到的表示之间最大一致性，从而鼓励模型同时编码的局部和全局信息。\nDiffusion matrix从全局角度提供了任意节点对之间的相关性，其中$\\mathbf{T} \\in \\mathbb{R}^{n \\times n}$是通用转移矩阵，$\\Theta$是权重系数，决定了全局和局部信息的比例，即对于每个节点，不同层次信息的比重， $\\Theta_{k}$越大，表示全局信息权重越大。 令$\\sum_{k=0}^{\\infty} \\theta_{k}=1, \\theta_{k} \\in[0,1]$，$\\lambda_{i} \\in[0,1]$,其中$\\lambda$是$\\mathbf{T}$的特征向量， 这样来保证$\\mathbf{S}$可以收敛到一个固定矩阵。扩散用快速近似值和稀疏化方法计算： $$ \\mathbf{S}=\\sum_{k=0}^{\\infty} \\Theta_{k} \\mathbf{T}^{k} \\in \\mathbb{R}^{n \\times n} $$ 给定一个邻接矩阵$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$和一个对角度矩阵$\\mathbf{D} \\in \\mathbb{R}^{n \\times n}$, Personalized PageRank (PPR)和Heat Kernel分别为两种不同的Diffusion matrix实例。对于PPR和HK，转移概率矩阵定义为$\\mathbf{T}=\\mathbf{A} \\mathbf{D}^{-1}$。PPR将第$k$层的权重系数设置为$\\theta_{k}=\\alpha(1-\\alpha)^{k}$, 而HK将第$k$层的权重系数设置为$\\theta_{k}=e^{-t} t^{k} / k !","title":"ICML2020 《Contrastive Multi-View Representation Learning on Graphs》 Reading Notes"},{"content":"paper\nIntroduction 图对比学习（GCL）已经成为图表示学习的主要技术，它最大化了共享相同语义的成对图增强之间的互信息。鉴于图数据的多样性，在增强过程中很难很好地保留语义。目前，GCL 中选择图增强方式的途径通常有以下三种。 1. 适用于不同数据集的图增强方式可能是不同的，需要在每个数据集上做验证，手动选择最适用于每个数据集的增强。2. 通过繁琐的搜索来选择增强方式。3. 通过邻域只是来选择增强方式。所有这些都限制了现有 GCL 方法的效率和通用性。为了解决该问题，本文提出了一种不需要对图做编辑， 而是对GNN编码器做扰动的增强方式： SimGRACE。并且对SimGRACE设计了对抗训练的方案：AT-SimGRACE。\n上图的实验中，两类图用不同的颜色标出，三种GCL模型分别在三个数据集上训练，训练完成后的分类效果如第一行所示。 对于GraphCL, 对边做扰动后再输入GraphCL 训练好的encoder,可以看出GraphCL的encoder对于扰动后的图数据集无法很好的保留分类语义。而对于SIMGRACE，不对图做扰动，而对训练好的encoder做扰动，扰动后的encoder对数据集的分类效果可以很好地保留语义信息。由此实验性表明了对encoder扰动可以保留比直接对图扰动更多的语义信息。\nGraphCL 表明 GNN 可以使用他们提出的框架获得鲁棒性。 但是，（1）他们没有解释为什么 GraphCL 可以增强鲁棒性； (2) GraphCL 似乎对随机攻击具有很好的免疫力，而对对抗性攻击的表现却不尽如人意。为了弥补这些缺陷，本文基于SimGRACE提出了一种新的算法 AT-SimGRACE通过对抗的方式来扰动编码器，从而是实现对抗训练的效果，它引入了更少的计算开销，同时显示出更好的鲁棒性。\nMethod SimGRACE 编码器扰动（Encoder perturbation） 给定一个GNN编码器$f(\\cdot;\\theta)$,它的参数扰动版本表示为$f(\\cdot;\\theta^\\prime)$。如图中所示，参数扰动版本的编码器不需要梯度反传训练参数，每次训练过程更新$f(\\cdot;\\theta)$，而$f(\\cdot;\\theta^\\prime)$的参数$\\theta^\\prime$只通过对$\\theta$扰动得到。第$l$层GNN的参数表示为$\\theta_l$，那么它的扰动后参数$\\theta^\\prime_l$有下式得到： $$ \\theta_{l}^{\\prime}=\\theta_{l}+\\eta \\cdot \\Delta \\theta_{l} ; \\quad \\Delta \\theta_{l} \\sim \\mathcal{N}\\left(0, \\sigma_{l}^{2}\\right) $$ 其中$\\eta$用来控制扰动的缩放，$\\Delta \\theta_{l}$是扰动项，扰动值采样自0均值$\\sigma_{l}^{2}$的Gaussian Distribution。$f(\\cdot;\\theta)$和$f(\\cdot;\\theta^\\prime)$的输出分别为$\\mathbf{h}$和$\\mathbf{h}^{\\prime}$： $$ \\mathbf{h}=f(\\mathcal{G} ; \\boldsymbol{\\theta}), \\mathbf{h}^{\\prime}=f\\left(\\mathcal{G} ; \\boldsymbol{\\theta}^{\\prime}\\right) $$ 从下图可以看出，如果不对编码器施加扰动，即超参数$\\eta=0$，效果会很差，扰动太多效果也会很差。\n映射头 （Projection Head） 和其他大多数GCL方法一样，该方法也要一个projection head来对GNN的output representation做一次变换，通常就是个MLP，得到输出$z$和$z^\\prime$： $$ z=g(\\mathbf{h}), z^{\\prime}=g\\left(\\mathbf{h}^{\\prime}\\right) $$\n对比损失（Contrastive loss） 和GraphCL一样，使用NT-Xent作为损失函数。具体来说，用$z_n$和$z_n^\\prime$分别表示表示图$n$在$f(\\cdot;\\theta)$和$f(\\cdot;\\theta^\\prime)$两个编码器下的输出， 用$z_n$和$z_{n^\\prime}$表示一个batch中两个不同图$n$和图$n^\\prime$在未扰动编码器$f(\\cdot;\\theta)$下的输出。在一个batch内，最大化同一个图的两个编码器（$f(\\cdot;\\theta)$和$f(\\cdot;\\theta^\\prime)$）输出间的相似度，同时最小化不同图在未扰动编码器$f(\\cdot;\\theta)$下输出的相似度： $$ \\ell_{n}=-\\log \\frac{\\left.\\exp \\left(\\operatorname{sim}\\left(z_{n}, z_{n}^{\\prime}\\right)\\right) / \\tau\\right)}{\\sum_{n^{\\prime}=1, n^{\\prime} \\neq n}^{N} \\exp \\left(\\operatorname{sim}\\left(z_{n}, z_{n^{\\prime}}\\right) / \\tau\\right)} $$ 即同一个图的两个输出为positive pair, 不同图的$f(\\cdot;\\theta)$输出为negative pair.\nWhy can SimGRACE work well? [1] 提供了两个属性来衡量对比学习学到的representation的质量： Alignment和Uniformity。其中Alignment metric直接定义为positive pairs之间的距离： $$ \\ell_{\\text {align }}(f ; \\alpha) \\triangleq \\underset{(x, y) \\sim p_{\\text {pos }}}{\\mathbb{E}}\\left[||f(x)-f(y)||_{2}^{\\alpha}\\right], \\quad \\alpha\u0026gt;0 $$ 其中$p_{\\text {pos }}$为positive pairs的分布，也就是positive pairs之间的距离越小，说明CL越好。 基于SimGRACE构造contrastive pairs的方式，alignment metric 可以定义为如下形式： $$ \\ell_{\\text {align }}(f ; \\alpha) \\triangleq \\underset{x \\sim p_{\\text {data }}}{\\mathbb{E}}\\left[\\left|\\left|f(x ; \\theta)-f\\left(x ; \\theta^{\\prime}\\right)\\right|\\right|_{2}^{\\alpha}\\right], \\quad \\alpha\u0026gt;0 $$ 另一个衡量指标是Uniformity， 定义为成对高斯势函数（Gaussian Potential）： $$ \\ell_{\\text {uniform }}(f ; \\alpha) \\triangleq \\log \\underset{x, y_{\\sim}^{i . i . d .} p_{\\text {data }}}{\\mathbb{E}}\\left[e^{-t||f(x ; \\theta)-f(y ; \\theta)||_{2}^{2}}\\right] . \\quad t\u0026gt;0 $$ 它要求随机样本的embedding应尽可能分散在hypersphere上， 即随机采样两个图在未扰动编码器输出的embedding距离要尽可能大。从下图可以看出，随着training epoch的增加，三种方法都呈现出正确的趋势。\nAT-SimGRACE 通过对抗训练（Adversarial Training， AT）来提升SimGRACE的鲁棒性。 对抗训练的优化问题定义如下： $$ \\min_{\\theta} \\mathcal{L}^{\\prime}(\\theta), \\quad \\text { where } \\quad \\mathcal{L}^{\\prime}(\\theta)=\\frac{1}{n} \\sum_{i=1}^{n} \\max_{ | |\\mathrm{x}_{i}^{\\prime}-\\mathrm{x}_{i} | |_{p} \\leq \\epsilon} \\ell_{i}^{\\prime}\\left(f\\left(\\mathrm{x}_{i}^{\\prime} ; \\theta\\right), y_{i}\\right) $$ 其中$n$是训练样本数，$\\mathrm{x}_{i}^{\\prime}$是对抗样本， 其中对抗样本在训练样本的$\\epsilon$-ball中，即$| |\\mathrm{x}_{i}^{\\prime}-\\mathrm{x}_{i} | |_{p} \\leq \\epsilon$, 表示对抗样本和原样本的变化不能超过$\\epsilon$。Adversarial Training: 优化$\\theta$，使得$f$可以在$\\mathrm{x}_{i}$的对抗样本$\\mathrm{x}_{i}^{\\prime}$上可以预测准确。其中$\\ell^{\\prime}(\\cdot)$为监督分类损失，$\\mathcal{L}^{\\prime}(\\theta)$为对抗损失。AT不能直接应用于CL上，因为（1）CL任务无标签，（2）对数据集中的每个样本扰动计算量太大。 为了解决这个问题，本文将AT loss中的损失函数部分换成NT-Xent对比学习损失，然后用对抗的方式来扰动encoder，从而无需对数据集中的所有样本扰动。\n假设$\\Theta$为GNN的权重空间(weight space)， 对于任意$\\mathbf{w}$任意正实数$\\epsilon$, 为$\\theta$定义半径为$\\epsilon$,中心为$\\mathbf{w}$的norm ball: $$ \\mathbf{R}(\\mathbf{w} ; \\epsilon):=\\{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}:||\\boldsymbol{\\theta}-\\mathbf{w}|| \\leq \\epsilon\\} $$ $\\theta \\in \\Theta$表示权重空间$\\Theta$中任意一组可能的GNN权重$\\theta$, $\\mathbf{R}(\\mathbf{w} ; \\epsilon)$表示GNN所有与$\\mathbf{w}$相似的权重，即所有与$\\mathbf{w}$的差距小于$\\epsilon$的权重。\n那么AT-SimGRACE的优化问题定义如下： $$ \\begin{gathered} \\min_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}+\\Delta) \\\\ \\text { where } \\mathcal{L}(\\boldsymbol{\\theta}+\\Delta)=\\frac{1}{M} \\sum_{i=1}^{M} \\max_{\\Delta \\in \\mathrm{R}(0 ; \\epsilon)} \\ell_{i}\\left(f\\left(\\mathcal{G}_{i} ; \\boldsymbol{\\theta}+\\Delta\\right), f\\left(\\mathcal{G}_{i} ; \\boldsymbol{\\theta}\\right)\\right) \\end{gathered} $$ 这里$\\mathrm{R}(0 ; \\epsilon)=\\{\\Delta \\in \\Theta: ||\\Delta|| \\leq \\epsilon\\}$ ，$\\mathcal{L}(\\boldsymbol{\\theta}+\\Delta)$表示在对GNN参数施加扰动$\\Delta$，使得GNN的效果最差，换句话说，找到一个扰动$\\Delta$，使得GNN的参数在被$\\Delta$扰动后（变为$\\theta+\\Delta$）两个图最不匹配（对比学习损失达到最大）。 $min_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}+\\Delta)$表示训练GNN参数，使得对比学习可以适应该扰动。算法如下：\n对抗训练：\n内层： 固定GNN参数，训练扰动参数$\\Delta$，使得GNN的对比学习loss上升\n外层： 固定扰动参数$\\Delta$， 训练GNN参数$\\theta$， 使得$\\theta$加上扰动$\\Delta$后的对比学习loss最小化。\nReference [1] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. ICML (2020)\n","permalink":"https://JhuoW.github.io/posts/simgrace/","summary":"paper\nIntroduction 图对比学习（GCL）已经成为图表示学习的主要技术，它最大化了共享相同语义的成对图增强之间的互信息。鉴于图数据的多样性，在增强过程中很难很好地保留语义。目前，GCL 中选择图增强方式的途径通常有以下三种。 1. 适用于不同数据集的图增强方式可能是不同的，需要在每个数据集上做验证，手动选择最适用于每个数据集的增强。2. 通过繁琐的搜索来选择增强方式。3. 通过邻域只是来选择增强方式。所有这些都限制了现有 GCL 方法的效率和通用性。为了解决该问题，本文提出了一种不需要对图做编辑， 而是对GNN编码器做扰动的增强方式： SimGRACE。并且对SimGRACE设计了对抗训练的方案：AT-SimGRACE。\n上图的实验中，两类图用不同的颜色标出，三种GCL模型分别在三个数据集上训练，训练完成后的分类效果如第一行所示。 对于GraphCL, 对边做扰动后再输入GraphCL 训练好的encoder,可以看出GraphCL的encoder对于扰动后的图数据集无法很好的保留分类语义。而对于SIMGRACE，不对图做扰动，而对训练好的encoder做扰动，扰动后的encoder对数据集的分类效果可以很好地保留语义信息。由此实验性表明了对encoder扰动可以保留比直接对图扰动更多的语义信息。\nGraphCL 表明 GNN 可以使用他们提出的框架获得鲁棒性。 但是，（1）他们没有解释为什么 GraphCL 可以增强鲁棒性； (2) GraphCL 似乎对随机攻击具有很好的免疫力，而对对抗性攻击的表现却不尽如人意。为了弥补这些缺陷，本文基于SimGRACE提出了一种新的算法 AT-SimGRACE通过对抗的方式来扰动编码器，从而是实现对抗训练的效果，它引入了更少的计算开销，同时显示出更好的鲁棒性。\nMethod SimGRACE 编码器扰动（Encoder perturbation） 给定一个GNN编码器$f(\\cdot;\\theta)$,它的参数扰动版本表示为$f(\\cdot;\\theta^\\prime)$。如图中所示，参数扰动版本的编码器不需要梯度反传训练参数，每次训练过程更新$f(\\cdot;\\theta)$，而$f(\\cdot;\\theta^\\prime)$的参数$\\theta^\\prime$只通过对$\\theta$扰动得到。第$l$层GNN的参数表示为$\\theta_l$，那么它的扰动后参数$\\theta^\\prime_l$有下式得到： $$ \\theta_{l}^{\\prime}=\\theta_{l}+\\eta \\cdot \\Delta \\theta_{l} ; \\quad \\Delta \\theta_{l} \\sim \\mathcal{N}\\left(0, \\sigma_{l}^{2}\\right) $$ 其中$\\eta$用来控制扰动的缩放，$\\Delta \\theta_{l}$是扰动项，扰动值采样自0均值$\\sigma_{l}^{2}$的Gaussian Distribution。$f(\\cdot;\\theta)$和$f(\\cdot;\\theta^\\prime)$的输出分别为$\\mathbf{h}$和$\\mathbf{h}^{\\prime}$： $$ \\mathbf{h}=f(\\mathcal{G} ; \\boldsymbol{\\theta}), \\mathbf{h}^{\\prime}=f\\left(\\mathcal{G} ; \\boldsymbol{\\theta}^{\\prime}\\right) $$ 从下图可以看出，如果不对编码器施加扰动，即超参数$\\eta=0$，效果会很差，扰动太多效果也会很差。\n映射头 （Projection Head） 和其他大多数GCL方法一样，该方法也要一个projection head来对GNN的output representation做一次变换，通常就是个MLP，得到输出$z$和$z^\\prime$： $$ z=g(\\mathbf{h}), z^{\\prime}=g\\left(\\mathbf{h}^{\\prime}\\right) $$\n对比损失（Contrastive loss） 和GraphCL一样，使用NT-Xent作为损失函数。具体来说，用$z_n$和$z_n^\\prime$分别表示表示图$n$在$f(\\cdot;\\theta)$和$f(\\cdot;\\theta^\\prime)$两个编码器下的输出， 用$z_n$和$z_{n^\\prime}$表示一个batch中两个不同图$n$和图$n^\\prime$在未扰动编码器$f(\\cdot;\\theta)$下的输出。在一个batch内，最大化同一个图的两个编码器（$f(\\cdot;\\theta)$和$f(\\cdot;\\theta^\\prime)$）输出间的相似度，同时最小化不同图在未扰动编码器$f(\\cdot;\\theta)$下输出的相似度： $$ \\ell_{n}=-\\log \\frac{\\left.","title":"WWW2022 《SimGRACE:A Simple Framework for Graph Contrastive Learning without Data Augmentation》 Reading Notes"},{"content":"paper\nIntroduction 在Deeper GNN中，residual connections通常可以缓解oversmoothing问题，但是，若图中的存在abnormal node features, 那么residual connections会放大abnormal features的影响。本文旨在设计AirGNN， 在自适应调整残差连接的权重，是的可以弹性适应存在abnormal node features的图。太多聚合（deep layers）会导致oversmoothing，但residual 对深层GNN有益，但是对于abnormal features是脆弱的。\nPreliminary Frobenius norm: $||\\mathbf{X}||_{F}=\\sqrt{\\sum_{i j} \\mathbf{X}_{i j}^{2}}$\n$\\ell_{21}$ norm: $||\\mathbf{X}||_{21}= \\sum_{i}\\left|\\left|\\mathbf{X}_{i}\\right|\\right|_{2}=\\sum_{i} \\sqrt{\\sum_{j} \\mathbf{X}_{i j}^{2}}$ 表示对每行算$\\ell_2$ norm 再对所有行算$\\ell_1$ norm。\nStudy 如图Figure 1所示， 对于具有abnormal node feature 的图，添加residual（蓝线）会导致性能巨大下降，因为abnormal node feature是与任务无关的，residual相对于无residual 保留了更多original abnormal features。\n对于node feature 都是normal的图， 没有residual的话，随着层次加深，GNN的性能会下降。\n综上，residual可以是GNN层次加深（容忍更多聚合），但是对abnormal features的鲁棒性较差。\nUnderstandings I: Feature aggregation as Laplacian smoothing 对于Laplacian Smoothing problem: $$ \\underset{\\mathbf{X} \\in \\mathbb{R}^{n \\times d}}{\\arg \\min } \\mathcal{L}_{1}(\\mathbf{X}):=\\frac{1}{2} \\operatorname{tr}\\left(\\mathbf{X}^{\\top}(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\mathbf{X}\\right)=\\frac{1}{2} \\sum_{\\left(v_{i}, v_{j}\\right) \\in \\mathcal{E}}\\left|\\left|\\frac{\\mathbf{X}_{i}}{\\sqrt{d_{i}+1}}-\\frac{\\mathbf{X}_{j}}{\\sqrt{d_{j}+1}}\\right|\\right|_{2}^{2} \\tag{1} $$ 其目标是找到最佳的$X$,使得$X$在图上最平滑。而对于GCN, GCNII (w/o residual)和APPNP (w/o residual), 他们的每一层都可以看做是如下的特征聚合方式： $$ \\mathbf{X}^{(k+1)}=\\tilde{\\mathbf{A}} \\mathbf{X}^{(k)} = \\tilde{\\mathbf{A}}=(\\hat{\\mathbf{D}}^{-\\frac{1}{2}} \\hat{\\mathbf{A}} \\hat{\\mathbf{D}}^{-\\frac{1}{2}})\\mathbf{X}^{(k)} \\tag{2} $$ 实际上，迭代多层GNN可以看做是以 step size =1的条件下，以梯度下降的方式求解Laplacian Smoothing问题，即以梯度下降的方式找到在图上最平滑的信号： $$ \\begin{equation} \\begin{aligned} \\mathbf{X}^{(k+1)} \u0026amp;= \\mathbf{X}^{(k)}-\\left.\\gamma \\frac{\\partial \\mathcal{L}_{1}}{\\partial \\mathbf{X}} \\right|_{\\mathbf{X}=\\mathbf{X}^{(k)}}\\\\ \u0026amp;= \\mathbf{X}^{(k)}-\\left.\\gamma \\frac{\\partial \\frac{1}{2} \\operatorname{tr}\\left(\\mathbf{X}^{\\top}(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\mathbf{X}\\right)}{\\partial \\mathbf{X}} \\right|_{\\mathbf{X}=\\mathbf{X}^{(k)}} \\\\ \u0026amp;= \\mathbf{X}^{(k)} - (\\mathbf{I}-\\tilde{\\mathbf{A}})\\mathbf{X}^{(k)} \\\\ \u0026amp;= \\tilde{\\mathbf{A}} \\mathbf{X}^{(k)}\n\\end{aligned}\\tag{3} \\end{equation} $$ 其中令$\\gamma = 1$, 所以，迭代多层GCN相当于以step size=1的方式迭代求解Laplacian Smoothing 问题。GCNII (w/o residual)和APPNP (w/o residual) 同理。\n堆叠GCN层来求解Laplacian smoothing问题可以被解释为图上信号的低通filter，即对于相邻节点，保留邻居节点间相似的特征(低频信号)，remove相邻节点间不同的特征（高频信号）。abnormal feature会导致图上的信号不平滑， 也就是若$v_i$的feature是abnormal，$v_j$的feature是normal, 且$v_i$与$v_j$相邻， 那么可能导致信号$k$的两个分量$X_{ik}$和$X_{jk}$ 差异较大， 所以abnormal feature可以看做图上的高频信号。 作为高频信号，它会被GCN等低筒滤波器过滤，即随着层数的加深，网络上的信号会越来越平滑，$X_{ik}^{out}$和$X_{jk}^{out}$的差距会变小，所以加深GCN可以缓解abnormal带来的问题。\nUnderstandings II: Residual connection maintains feature proximity 含有residual的APPNP形式如下： $$ \\mathbf{X}^{k+1}=(1-\\alpha) \\tilde{\\mathbf{A}} \\mathbf{X}^{k}+\\alpha \\mathbf{X}_{\\text {in }} \\tag{4} $$ 它也可以看做是对Laplaican Smoothing问题的求解，只不过加上的正则化项。 APPNP可以看做迭代求解如下regularized Laplacian smoothing problem: $$ \\underset{\\mathbf{X} \\in \\mathbb{R}^{n \\times d}}{\\arg \\min} \\mathcal{L}_{2}(\\mathbf{X}):=\\frac{1}{2} \\operatorname{tr}\\left(\\mathbf{X}^{\\top}(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\mathbf{X}\\right) + \\frac{\\alpha}{2(1-\\alpha)}\\left|\\left|\\mathbf{X}-\\mathbf{X}_{\\text {in }}\\right|\\right|_{F}^{2} \\tag{5} $$ 其中 step size $\\gamma = 1-\\alpha$: $$ \\mathbf{X}^{k+1}=\\mathbf{X}^{k}-(1-\\alpha)\\left(\\frac{\\alpha}{1-\\alpha}\\left(\\mathbf{X}^{k}-\\mathbf{X}_{\\text {in }}\\right)+(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\mathbf{X}^{k}\\right)=(1-\\alpha) \\tilde{\\mathbf{A}} \\mathbf{X}^{k}+\\alpha \\mathbf{X}_{\\text {in }} \\tag{6} $$ 上式的求解过程和公式(3)差不多。 其中$\\frac{\\alpha}{2(1-\\alpha)}\\left|\\left|\\mathbf{X}-\\mathbf{X}_{\\text {in }}\\right|\\right|_{F}^{2}$为正则化项， 上式要求 求得的$X$在图上尽可能平滑的同时， 要与输入尽可能接近。这样对平滑加以限制后，可以缓解深层GNN产生的oversmoothing 问题，因为保留了一些必要的高频信号。但是这些残差连接也携带了有害的异常特征，导致在含有abnormal feature的图上性能较差。\nThe Proposed Model Design Motivation 更多的feature aggregation 可以缓解abnormal feature但是oversmoothing。residual 可以缓解深层GNN，但是受abnormal feature影响。 如何设计MPNN使得node 可以自适应的特征聚合和residual?\n公式（5）中的正则化项$\\left|\\left|\\mathbf{X}-\\mathbf{X}_{\\text {in }}\\right|\\right|_{F}^{2} $决定了GNN的更新中含有residual，这样可以保持深层GNN的稳定性。虽然保持输入与每层输出之间的proximity对于加深层次很重要，但是，用Frobenius norm来惩罚偏差可能过于激进，即会使得输入和输出过于接近，从而削弱了Laplacian Smoothing项$\\operatorname{tr}\\left(\\mathbf{X}^{\\top}(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\mathbf{X}\\right)$去除abnormal feature的能力。因此本文提出用$\\ell_{21}$ norm 来替换Frobenius norm作为输入输出之间proximity的保留项。 相比于Frobenius norm，不那么激进，即最小化$\\ell_{21}$ norm 不会让输出过于接近输入。It also allows large deviations because the penalty on large values is less aggressive, leading to the potential removal of abnormal features：意思是$\\mathbf{X}-\\mathbf{X}_{\\text {in}}$ 如果很大的话，它的$\\ell_{21}$ norm在目标函数中不会占据过于大的分量，即 $\\left|\\left|\\mathbf{X}-\\mathbf{X}_{\\text {in }}\\right|\\right|_{F}^{2} \u0026gt; \\left|\\left|\\mathbf{X}-\\mathbf{X}_{\\text {in }}\\right|\\right|_{21} $。这样，最小化regularized Laplacian smoothing problem的目标函数时，不会过度倾向于最小化邻近度项。 所以regularized Laplacian smoothing 问题定义如下： $$ \\underset{\\mathbf{X} \\in \\mathbb{R}^{n \\times d}}{\\arg \\min } \\mathcal{L}(\\mathbf{X}):=\\underbrace{(1-\\lambda) \\operatorname{tr}\\left(\\mathbf{X}^{\\top}(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\mathbf{X}\\right)}_{可微 g(X)} + \\underbrace{\\lambda\\left|\\left|\\mathbf{X}-\\mathbf{X}_{\\text {in }}\\right|\\right|_{21}}_{不可微 h(X)} \\tag{7} $$ 其中$\\lambda \\in [0,1]$。\nAdaptive Massage Passing 公式（7）中，$ \\mathcal{L}(\\mathbf{X})$由可微和不可微凸函数组成，可以使用Proximal Gradient Descent（PGD）来优化 （PGD可以见这篇文章）。公式（7）中可微部分 记为$g(X) = (1-\\lambda) \\operatorname{tr}\\left(\\mathbf{X}^{\\top}(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\mathbf{X}\\right)$ ， 不可微部分记为 $h(X) = \\lambda\\left|\\left|\\mathbf{X}-\\mathbf{X}_{\\text {in }}\\right|\\right|_{21}$。 根据PGD, 是的(7)最小的$\\mathbf{X}$可以通过迭代方式求解： $$ \\definecolor{energy}{RGB}{114,0,172} \\definecolor{freq}{RGB}{45,177,93} \\definecolor{spin}{RGB}{251,0,29} \\definecolor{signal}{RGB}{18,110,213} \\definecolor{circle}{RGB}{217,86,16} \\definecolor{average}{RGB}{203,23,206} \\definecolor{red}{RGB}{255,0,0} \\boldsymbol{X}^{k+1}=\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}\\left(\\boldsymbol{X}^{k}-\\gamma \\nabla g\\left(\\boldsymbol{X}^{k}\\right)\\right) \\tag{8} $$ 其中， $$ \\begin{equation} \\begin{aligned} \\nabla g\\left(\\boldsymbol{X}^{k}\\right) \u0026amp;= \\nabla \\left[ (1-\\lambda) \\operatorname{tr}\\left((\\boldsymbol{X}^{k})^{\\top}(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\boldsymbol{X}^{k}\\right)\\right] \\\\ \u0026amp;=(1-\\lambda)(2(I-\\tilde{\\mathbf{A}})\\boldsymbol{X}^{k}) \\\\ \u0026amp;= 2\\boldsymbol{X}^{k} - 2 \\tilde{\\mathbf{A}}\\boldsymbol{X}^{k} - 2\\lambda \\boldsymbol{X}^{k}+ 2\\lambda \\tilde{\\mathbf{A}}\\boldsymbol{X}^{k} \\\\ \u0026amp;=2(1-\\lambda) (\\mathbf{I}-\\tilde{\\mathbf{A}}) \\boldsymbol{X}^{k} \\end{aligned} \\tag{9} \\end{equation} $$ 令 $$ \\begin{aligned} \\boldsymbol{Y}^{k} = \\boldsymbol{X}^{k}-\\gamma \\nabla g\\left(\\boldsymbol{X}^{k}\\right) \u0026amp;= \\boldsymbol{X}^{k} - 2\\gamma(1-\\lambda) (\\mathbf{I}-\\tilde{\\mathbf{A}}) \\boldsymbol{X}^{k} \\\\ \u0026amp;= (1-2 \\gamma(1-\\lambda)) \\mathbf{X}^{k}+2 \\gamma(1-\\lambda) \\tilde{\\mathbf{A}} \\mathbf{X}^{k}\n\\end{aligned}\\tag{10} $$ 那么： $$ \\begin{equation} \\begin{aligned} \\boldsymbol{X}^{k+1}\u0026amp;=\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}\\left(\\boldsymbol{Y}^{k}\\right) \\\\ \u0026amp;= \\underset{\\mathbf{X}}{\\arg \\min }\\left\\{\\lambda\\left|\\left|\\mathbf{X}-\\mathbf{X}_{\\text {in }}\\right|\\right|_{21}+\\frac{1}{2 \\gamma}\\left|\\left|\\mathbf{X}-\\mathbf{Y}^{k}\\right|\\right|_{F}^{2}\\right\\} \\end{aligned} \\tag{11} \\end{equation} $$ 这样，迭代公式（11）只和不可微函数$||\\cdot||_{21}$有关， 上式的第二项$\\left|\\left|\\mathbf{X}-\\mathbf{Y}^{k}\\right|\\right|_{F}^{2}$是proximity operator的固定计算，优化目标中的$g(\\cdot)$无关。\n接下来，对公式（11）做一个换元， 令$\\mathbf{Z}=\\mathbf{X}-\\mathbf{X}_{\\mathrm{in}}$， 那么公式（11）可以重写为： $$ \\begin{aligned} \\mathbf{Z}^{k+1} \u0026amp;=\\underset{\\mathbf{Z}}{\\arg \\min }\\left\\{\\lambda||\\mathbf{Z}||_{21}+\\frac{1}{2 \\gamma}\\left|\\left|\\mathbf{Z}-\\left(\\mathbf{Y}^{k}-\\mathbf{X}_{\\text {in }}\\right)\\right|\\right|_{F}^{2}\\right\\} \\\\ \u0026amp;=\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{21}\\color{energy}\\gamma }\\left(\\mathbf{Y}^{k}-\\mathbf{X}_{\\text {in }}\\right) \\\\ \\mathbf{X}^{k+1} \u0026amp;=\\mathbf{X}_{\\text {in }}+\\mathbf{Z}^{k+1} \\end{aligned} $$ 下面就是求解$\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{21}\\color{energy}\\gamma }\\left(\\mathbf{Y}^{k}-\\mathbf{X}_{\\text {in }}\\right)$。\n先看看$\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{21}\\color{energy}\\gamma }\\left(\\mathbf{X}\\right)$怎么算的, 参考[1,2]: $$ \\left[\\operatorname{prox}_{\\color{signal}||\\cdot||_{2}}\\left(\\mathbf{X}\\right)\\right]_i = \\begin{cases}\\mathbf{X}_i-\\frac{\\mathbf{X}_i}{\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}} \u0026amp; \\text { if }\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}\u0026gt;1 \\\\ 0 \u0026amp; \\text { if }\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2} \\leq 1\\end{cases} $$\n$$ \\begin{equation} \\begin{aligned} \\left[\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{21}\\color{energy}\\gamma }\\left(\\mathbf{X}\\right)\\right]_i \u0026amp; = \\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma }\\left(\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{2}\\color{energy}\\gamma }\\left(\\mathbf{X}\\right)_i\\right) \\\\ \u0026amp;= \\begin{cases} \\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma }\\left(\\mathbf{X}_i-\\frac{\\mathbf{X}_i}{\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}}\\right) \u0026amp; \\text { if }\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}\u0026gt;1 \\\\ \\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma }\\left(0\\right)=\\color{red}0 \u0026amp; \\text { if }\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2} \\leq 1\\end{cases} \\end{aligned} \\end{equation} $$ 其中： $$ \\begin{equation} \\begin{aligned} \\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma }\\left(\\mathbf{X}_i-\\frac{\\mathbf{X}_i}{\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}}\\right) \u0026amp;= \\frac{\\mathbf{X}_i}{\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}} \\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma }\\left( \\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}-\\underbrace{1}_{constant, 不影响结果}\\right) \\\\ \u0026amp;= \\frac{\\mathbf{X}_i}{\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}} \\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma }\\left( \\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}\\right) \\\\ \u0026amp;= \\frac{\\mathbf{X}_i}{\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}} \\cdot \\underbrace{\\boxed{\\begin{cases} \\left|\\left|\\mathbf{X}_i\\right|\\right|_{2} - \\lambda\\gamma \u0026amp; \\text { if }\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2} \u0026gt; \\lambda\\gamma \\\\ 0 \u0026amp; \\text { if }\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2} = \\lambda\\gamma \\\\ \\left|\\left|\\mathbf{X}_i\\right|\\right|_{2} + \\lambda\\gamma \u0026amp; \\text { if }\\left|\\left|\\mathbf{X}_i\\right|\\right|_{2} \\leq -\\lambda\\gamma \\end{cases}}}_{\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma }\\left( \\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}\\right)} \\end{aligned} \\tag{12} \\end{equation} $$ 公式（12）中，第三种情况不存在，所以 $$ \\begin{equation} \\begin{aligned} \\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma }\\left( \\left|\\left|\\mathbf{X}_i\\right|\\right|_{2}\\right) = \\max \\left(\\left|\\left|\\mathbf{X}_{i}\\right|\\right|_{2}-\\gamma \\lambda, 0\\right) \\end{aligned} \\end{equation} $$ 所以： $$ \\left(\\operatorname{prox}_{\\gamma \\lambda||\\cdot||_{21}}(\\mathbf{X})\\right)_{i}=\\frac{\\mathbf{X}_{i}}{\\left|\\left|\\mathbf{X}_{i}\\right|\\right|_{2}} \\max \\left(\\left|\\left|\\mathbf{X}_{i}\\right|\\right|_{2}-\\gamma \\lambda, 0\\right)=\\max \\left(1-\\frac{\\gamma \\lambda}{\\left|\\left|\\mathbf{X}_{i}\\right|\\right|_{2}}, 0\\right) \\cdot \\mathbf{X}_{i} \\tag{13} $$ 将公式（13）中的$\\mathbf{X}$替换为$\\mathbf{Y}^{k}-\\mathbf{X}_{\\text {in }}$, 计算$\\mathbf{Z}^{k+1}=\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{21}\\color{energy}\\gamma }\\left(\\mathbf{Y}^{k}-\\mathbf{X}_{\\text {in }}\\right)$ , 然后计算$\\mathbf{X}^{k+1} =\\mathbf{X}_{\\text {in }}+\\mathbf{Z}^{k+1}$ 可得： $$ \\mathbf{X}_{i}^{k+1}=\\left(\\mathbf{X}_{\\text {in }}\\right)_{i}+\\beta_{i}\\left(\\mathbf{Y}_{i}^{k}-\\left(\\mathbf{X}_{\\text{in }}\\right)_{i}\\right)=\\left(1-\\beta_{i}\\right)\\left(\\mathbf{X}_{\\text {in }}\\right)_{i}+\\beta_{i} \\mathbf{Y}_{i}^{k}, \\quad \\forall i \\in [n] $$ 其中 $\\beta_{i}:=\\max \\left(1-\\frac{\\gamma \\lambda}{\\left|\\left|\\mathbf{Y}_{i}^{k}-\\left(\\mathbf{X}_{\\text{in }}\\right)_{i}\\right|\\right|_{2}}, 0\\right)$。\n综上，最终Adaptive Message Passing（AMP）总结如下： $$ \\left\\{\\begin{aligned} \\mathbf{Y}^{k} \u0026amp;=(1-2 \\gamma(1-\\lambda)) \\mathbf{X}^{k}+2 \\gamma(1-\\lambda) \\tilde{\\mathbf{A}} \\mathbf{X}^{k} \\\\ \\beta_{i} \u0026amp;=\\max \\left(1-\\frac{\\gamma \\lambda}{\\left|\\left|\\mathbf{Y}_{i}^{k}-\\left(\\mathbf{X}_{\\text {in }}\\right)_{i}\\right|\\right|_{2}}, 0\\right) \\quad \\forall i \\in[n] \\\\ \\mathbf{X}_{i}^{k+1} \u0026amp;=\\left(1-\\beta_{i}\\right)\\left(\\mathbf{X}_{\\text {in }}\\right)_{i}+\\beta_{i} \\mathbf{Y}_{i}^{k} \\quad \\forall i \\in[n] \\end{aligned}\\right. $$\n文章证明了$\\gamma=\\frac{1}{4(1-\\lambda)}$或$\\gamma=\\frac{1}{2(1-\\lambda)}$可以保证收敛 （具体过程没看了）。这样AMP简化为$\\mathbf{Y}^{k}=\\frac{1}{2} \\mathbf{X}^{k}+\\frac{1}{2} \\tilde{\\mathbf{A}} \\mathbf{X}^{k}$和$\\mathbf{Y}^{k}=\\tilde{\\mathbf{A}} \\mathbf{X}^{k}$。\nReference [1] https://math.stackexchange.com/questions/2665254/proximal-operator-of-summation-of-l-1-norm-and-l-2-1-norm\n[2] https://math.stackexchange.com/questions/2190885/proximal-operator-of-the-euclidean-norm-l-2-norm\n","permalink":"https://JhuoW.github.io/posts/airgnn/","summary":"paper\nIntroduction 在Deeper GNN中，residual connections通常可以缓解oversmoothing问题，但是，若图中的存在abnormal node features, 那么residual connections会放大abnormal features的影响。本文旨在设计AirGNN， 在自适应调整残差连接的权重，是的可以弹性适应存在abnormal node features的图。太多聚合（deep layers）会导致oversmoothing，但residual 对深层GNN有益，但是对于abnormal features是脆弱的。\nPreliminary Frobenius norm: $||\\mathbf{X}||_{F}=\\sqrt{\\sum_{i j} \\mathbf{X}_{i j}^{2}}$\n$\\ell_{21}$ norm: $||\\mathbf{X}||_{21}= \\sum_{i}\\left|\\left|\\mathbf{X}_{i}\\right|\\right|_{2}=\\sum_{i} \\sqrt{\\sum_{j} \\mathbf{X}_{i j}^{2}}$ 表示对每行算$\\ell_2$ norm 再对所有行算$\\ell_1$ norm。\nStudy 如图Figure 1所示， 对于具有abnormal node feature 的图，添加residual（蓝线）会导致性能巨大下降，因为abnormal node feature是与任务无关的，residual相对于无residual 保留了更多original abnormal features。\n对于node feature 都是normal的图， 没有residual的话，随着层次加深，GNN的性能会下降。\n综上，residual可以是GNN层次加深（容忍更多聚合），但是对abnormal features的鲁棒性较差。\nUnderstandings I: Feature aggregation as Laplacian smoothing 对于Laplacian Smoothing problem: $$ \\underset{\\mathbf{X} \\in \\mathbb{R}^{n \\times d}}{\\arg \\min } \\mathcal{L}_{1}(\\mathbf{X}):=\\frac{1}{2} \\operatorname{tr}\\left(\\mathbf{X}^{\\top}(\\mathbf{I}-\\tilde{\\mathbf{A}}) \\mathbf{X}\\right)=\\frac{1}{2} \\sum_{\\left(v_{i}, v_{j}\\right) \\in \\mathcal{E}}\\left|\\left|\\frac{\\mathbf{X}_{i}}{\\sqrt{d_{i}+1}}-\\frac{\\mathbf{X}_{j}}{\\sqrt{d_{j}+1}}\\right|\\right|_{2}^{2} \\tag{1} $$ 其目标是找到最佳的$X$,使得$X$在图上最平滑。而对于GCN, GCNII (w/o residual)和APPNP (w/o residual), 他们的每一层都可以看做是如下的特征聚合方式： $$ \\mathbf{X}^{(k+1)}=\\tilde{\\mathbf{A}} \\mathbf{X}^{(k)} = \\tilde{\\mathbf{A}}=(\\hat{\\mathbf{D}}^{-\\frac{1}{2}} \\hat{\\mathbf{A}} \\hat{\\mathbf{D}}^{-\\frac{1}{2}})\\mathbf{X}^{(k)} \\tag{2} $$ 实际上，迭代多层GNN可以看做是以 step size =1的条件下，以梯度下降的方式求解Laplacian Smoothing问题，即以梯度下降的方式找到在图上最平滑的信号： $$ \\begin{equation} \\begin{aligned} \\mathbf{X}^{(k+1)} \u0026amp;= \\mathbf{X}^{(k)}-\\left.","title":"NeurIPS2021 《Graph Neural Networks with Adaptive Residual》 Reading Notes"},{"content":"Paper\nIntroduction 本文是自监督方法在GCNs上首次系统的探索，设计了3种自监督任务来将分析自监督在GCN中起到的作用。自监督旨在充分利用unlabeled数据中的知识来设计前置任务（pretext task），来帮助模型学习更具迁移性和泛化能力的表示。前置任务可以认为是对目标任务有帮助的辅助正则化网络，设计用于帮助原任务学习到更多下游任务相关的语义信息。\nGCN任务通常是直推半监督的（transductive semi-supervised）,含有大量unlabeled数据，而self-supervision(SSL)可以充分利用unlabeled data， 那么就产生了一个值得探索的问题：将自监督学习应用到GCN上是否也可以达到提升泛化能力和鲁棒能力的效果？\n先给结论\nQ1: 自监督学习可否在分类任务中提升GCN？ 如果可以，如何将其合并到 GCN 中以最大化增益？\nA1: 本文证明了通过多任务学习将自监督学习融入 GCN 是有效的，即多任务损失作为 GCN 训练中的正则化项。 这种作为自监督作为正则化项的方法，强于用自监督来预训练或者self-training。\nQ2: 前置任务的设计重要吗？ GCN 有哪些有用的自监督前置任务？\nA2: 本文研究了三个基于图属性的自监督任务。 分别是节点聚类node clustering, 图划分graph partitioning 和图补全graph completion。 并且进一步说明不同的模型和数据集倾向于不同的自监督任务。\nQ3: 自监督也会影响 GCN 的对抗鲁棒性吗？ 如果是，如何设计前置任务？\nA3: 本文进一步将上述发现推广到对抗性训练环境中。提供了广泛的结果，以表明自监督还可以提高 GCN 在各种攻击下的鲁棒性，而不需要更大的模型或额外的数据。\nMethod GCNs $\\boldsymbol{Z}=\\hat{\\boldsymbol{A}} \\operatorname{ReLU}\\left(\\hat{\\boldsymbol{A}} \\boldsymbol{X} \\boldsymbol{W}_{0}\\right) \\boldsymbol{W}_{1}$可以分为两块来看 (1) 特征提取模块$f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) = \\hat{\\boldsymbol{A}} \\operatorname{ReLU}\\left(\\hat{\\boldsymbol{A}} \\boldsymbol{X} \\boldsymbol{W}_{0}\\right)$ 参数为$\\theta = \\{\\boldsymbol{W}_{0}\\}$和（2）线性变换模块$\\boldsymbol{Z}=f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) \\boldsymbol{\\Theta}$ 其中 参数$ \\boldsymbol{\\Theta} = \\boldsymbol{W}_{1}$。 半监督GCN优化任务的目标函数为： $$ \\begin{aligned} \\boldsymbol{Z} \u0026amp;=f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) \\boldsymbol{\\Theta} \\\\ \\theta^{*}, \\boldsymbol{\\Theta}^{} \u0026amp;=\\arg \\min_{\\theta, \\boldsymbol{\\Theta}} \\mathcal{L}_{\\mathrm{sup}}(\\theta, \\boldsymbol{\\Theta}) \\\\ \u0026amp;=\\arg \\min_{\\theta, \\boldsymbol{\\Theta}} \\frac{1}{\\left|\\mathcal{V}_{\\text {label }}\\right|} \\sum_{v_{n} \\in \\mathcal{V}_{\\text {label }}} L\\left(\\boldsymbol{z}_{n}, \\boldsymbol{y}_{n}\\right) \\end{aligned} \\tag{1} $$ 其中$L(\\cdot, \\cdot)$是每个labeled node的损失函数。\nThree Schemes: Self-Supervision Meets GCNs 研究三种将SSL配置到GCNs的方式。 其中 给定输入$\\boldsymbol{X}_{ss}$, $\\hat{\\boldsymbol{A}}_{\\mathrm{ss}}$, label $\\boldsymbol{Y}_{ss}$和节点集$\\mathcal{V}_{ss}$。\nPretraining \u0026amp; Fintuning 预训练过程： $$ \\begin{aligned} \\boldsymbol{Z}_{\\mathrm{ss}} \u0026amp;=f_{\\theta}\\left(\\boldsymbol{X}_{\\mathrm{ss}}, \\hat{\\boldsymbol{A}}_{\\mathrm{ss}}\\right) \\boldsymbol{\\Theta}_{\\mathrm{Ss}} \\\\ \\theta_{\\mathrm{ss}}^{*}, \\boldsymbol{\\Theta}_{\\mathrm{ss}}^{*} \u0026amp;=\\arg \\min_{\\theta, \\boldsymbol{\\Theta}_{\\mathrm{ss}}} \\mathcal{L}_{\\mathrm{ss}}\\left(\\theta, \\boldsymbol{\\Theta}_{\\mathrm{ss}}\\right) \\\\ \u0026amp;=\\arg \\min_{\\theta, \\boldsymbol{\\Theta}} \\frac{1}{\\left|\\mathcal{V}_{\\mathrm{ss}}\\right|} \\sum_{v_{n} \\in \\mathcal{V}_{\\mathrm{ss}}} \\underbrace{L_{\\mathrm{ss}}\\left(\\boldsymbol{z}_{\\mathrm{ss}, n}, \\boldsymbol{y}_{\\mathrm{ss}, n}\\right)}_{\\text{loss of other task}}\n\\end{aligned} \\tag{2} $$ 也就是在另一个任务训练好的模型参数$\\theta_{\\mathrm{ss}}^{*}, \\boldsymbol{\\Theta}_{\\mathrm{ss}}^{*}$迁移到新任务（如半监督节点分类任务）上作为初始化参数训练新模型。\n上表中，可以看出用graph partitioning作为预训练任务，得到的模型fine-tuning到节点分类任务上之后，效果仅从79.10变成了79.19,是非常微小的。 本文推测可能原因有两个（1）.两个不同的任务的Loss function不一样，从$\\mathcal{L}_{\\mathrm{ss}}$变为$\\mathcal{L}_{\\mathrm{sup}}$会影响实验效果。（2）参数迁移前一句是在多层GCN上的训练结果了，迁移后再训练，相当于深层，易oversmoothing。\nSelf-Training 每次迭代为unlabeled samples分配高度可信的为标签，然后将这些分配了伪标签的节点纳入到下一次迭代的监督训练中，随迭代过程不断更新标签。\n表2可以看出Self-training的方式带来的提升有限\nMulti-task Learning 考虑一个目标task和一个自监督task. GCN的目标为公式（1）。该多任务的训练过程如下： $$ \\begin{aligned} \\boldsymbol{Z} \u0026amp;=f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) \\boldsymbol{\\Theta}, \\quad \\boldsymbol{Z}_{\\mathrm{ss}}=f_{\\theta}\\left(\\boldsymbol{X}_{\\mathrm{ss}}, \\hat{\\boldsymbol{A}}_{\\mathrm{ss}}\\right) \\boldsymbol{\\Theta}_{\\mathrm{ss}} \\\\ \\theta^{*}, \\boldsymbol{\\Theta}^{*}, \\boldsymbol{\\Theta}_{\\mathrm{ss}}^{*} \u0026amp;=\\arg \\min_{\\theta, \\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}_{\\mathrm{ss}}} \\alpha_{1} \\mathcal{L}_{\\mathrm{sup}}(\\theta, \\boldsymbol{\\Theta})+\\alpha_{2} \\mathcal{L}_{\\mathrm{ss}}\\left(\\theta, \\boldsymbol{\\Theta}_{\\mathrm{ss}}\\right) \\end{aligned} \\tag{3} $$ 其中任务的权重参数$\\alpha_{1}, \\alpha_{2} \\in \\mathbb{R}_{\u0026gt;0}$, 半监督目标任务的损失$\\mathcal{L}_{\\mathrm{sup}}$定义为公式（1）， 辅助自监督损失$\\mathcal{L}_{\\mathrm{ss}}$定义为公式（2）.其中特征提取器$f_{\\theta}(\\cdot, \\cdot)$对于自监督任务和目标任务是参数共享的，而线性变换参数$\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}_{\\mathrm{ss}}$是各自任务的。\n在公式(3)中，自监督任务的loss作为一个regularization term 与目标任务一同训练。正则化项在图信号处理中是广泛应用的， 常见的有Graph Laplaician Regularization（GLR）， 它用于惩罚相邻节点间的不平滑，用于在学习目标任务的同时保持特征在图结构上的smoothing。虽然GLR可以作为一个自监督任务，但是它是给予不涉及具体数据情况下的平滑先验，SSL的regularization term不用的是，SSL是给予unlabeled data,是一种引入数据驱动的先验知识。综上所述， 多任务学习是3种自监督方式中最通用的。\nGCN-Specific Self-Supervised Tasks 本文为 GCN 扩展了一个自监督任务的“工具包”。 通过利用图中的丰富节点和边信息，可以定义各种GCN特定的自监督任务（如表 3 所示），并且进一步证明了不同的自监督任务对不同类型的监督/下游任务有益。这些自监督任务会为节点分配伪标签来构造自监督损失$\\mathcal{L}_{ss}$, 如公式（3）所示。\nNode clustering 第一个任务为节点聚类， 给定节点集$\\mathcal{V}$以及feature set $\\boldsymbol{X}$, 一个预设值的簇数量$K \\in\\{1, \\ldots,|\\mathcal{V}|\\}$（是一个超参数）一定要小于等于节点数$|\\mathcal{V}|$。 聚类算法输出一个节点集合的集合$\\left\\{\\mathcal{V}_{\\text {clu }, 1}, \\ldots, \\mathcal{V}_{\\text {clu }, K} \\mid \\mathcal{V}_{\\text {clu }, n} \\subseteq \\mathcal{V}, n=1, \\ldots, K\\right\\}$， 其中$\\mathcal{V}_{\\text {clu }, i}$是一个集合表示在簇$i$中的节点集。 $$ \\begin{aligned} \u0026amp;\\mathcal{V}_{\\text {clu }, n} \\neq \\emptyset \\quad(n=1, \\ldots, K), \\quad \\cup_{n=1}^{K} \\mathcal{V}_{\\text {clu }, n}=\\mathcal{V} \\\\ \u0026amp;\\mathcal{V}_{\\text {clu }, i} \\cap \\mathcal{V}_{\\text {clu }, j}=\\emptyset \\quad(\\forall i, j=1, \\ldots, K \\text { and } i \\neq j) \\end{aligned} $$ 这$K$个簇互相之间没有公共的节点，SSL任务将每个节点所在的簇的index作为伪标签来构造自监督损失$\\mathcal{L}_{ss}$： $$ y_{\\mathrm{ss}, n}=k \\text { if } v_{n} \\in \\mathcal{V}_{\\mathrm{clu}, k}(\\forall n=1, \\ldots,|\\mathcal{V}|, \\forall k=1, \\ldots, K) $$\nGraph partitioning 上面的节点聚类任务，是基于特征的，与拓扑无关。 而这里的图划分任务，与feature无关，只与拓扑有关。 具体来说，通过“强”边连接的两个节点很可能属于同一标签类别。 因此，本文提出了一种使用图划分的基于拓扑的自监督任务。\n图划分是将图的节点划分为大致相等的子集，使得跨子集间的边数最小化（高聚类，低耦合，同时簇中节点数不能差别太大）。先预定义一个簇数量，$K \\in\\{1, \\ldots,|\\mathcal{V}|\\}$（超参数）。 和节点聚类任务类似，图划分算法也会输出一个节点集合的集合，用来标识每个节点属于哪个partition: $\\left\\{\\mathcal{V}_{\\text {par }, 1}, \\ldots, \\mathcal{V}_{\\text {par }, K} \\mid \\mathcal{V}_{\\text {par }, n} \\subseteq \\mathcal{V}, n=1, \\ldots, K\\right\\}$, 使得： $$ \\begin{aligned} \u0026amp;\\mathcal{V}_{\\text {par }, n} \\neq \\emptyset \\quad(\\forall n=1, \\ldots, K), \\quad \\cup_{n=1}^{K} \\mathcal{V}_{\\text {par }, n}=\\mathcal{V} \\\\ \u0026amp;\\mathcal{V}_{\\text {par }, i} \\cap \\mathcal{V}_{\\text {par }, j}=\\emptyset \\quad(\\forall i, j=1, \\ldots, K \\text { and } i \\neq j) \\end{aligned} $$ 上面的约束其实和node clustering任务差不多，Graph partitioning任务还需要两个约束，一个是平衡约束来保证簇不要太大： $$ K \\frac{\\max_{k}\\left|\\mathcal{V}_{\\text {par }, k}\\right|}{|\\mathcal{V}|} \\leqslant 1+\\epsilon, \\text { where } \\epsilon \\in(0,1) $$ 其中$\\max_{k}\\left|\\mathcal{V}_{\\text {par }, k}\\right|$是节点数最多的簇中的节点数。 另一个约束要保证簇间边要尽可能少，即最小化edgecut: $$ \\text { edgecut }=\\frac{1}{2} \\sum_{k=1}^{K} \\sum_{v_{i} \\in \\mathcal{V}_{\\text {par }, k}} \\quad\\sum_{\\left(v_{i}, v_{j}\\right) \\in \\mathcal{E} ,\\text {and } v_{j} \\notin \\mathcal{V}_{\\text {par }, k}} \\quad a_{i j} $$ 将每个节点所在的partition index作为label。\nGraph completion 图补全任务如下图所示。\n图补全首先通过删除目标节点的特征来mask目标节点。 然后，通过向 GCN 提供未掩蔽的节点特征（目前仅限于 2 层 GCN 的每个目标节点的二阶邻居）来恢复/预测被mask的节点特征。设计该自监督任务的原因如下：1）标签可以自由获取，也就是节点特征本身； 2）图补全可以帮助网络获得更好的特征表示，这可以教会网络从上下文中提取特征。\n最终多任务自监督GCN模型的框架如下图所示：\nSelf-Supervision in Graph Adversarial Defense 本文专注于Evasion Attack，在模型训练好后对目标节点$v_n$扰动， 实际上对于Evasion Attack，对扰动图重新训练或许可以纠正扰动的影响，但是本文这里不考虑重新训练。一个attacker $g$生成新的特征和邻接矩阵： $$ \\boldsymbol{X}^{\\prime}, \\boldsymbol{A}^{\\prime}=g\\left(\\boldsymbol{X}, \\boldsymbol{A}, \\boldsymbol{Y}, v_{n}, \\theta^{*}, \\boldsymbol{\\Theta}^{*}\\right) $$ 其中$ \\theta^{*}, \\boldsymbol{\\Theta}^{*}$是在clean 图上训练好的模型参数。\n对抗训练的目标函数定义为: $$ \\begin{aligned} \\boldsymbol{Z} \u0026amp;=f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) \\boldsymbol{\\Theta}, \\quad \\boldsymbol{Z}^{\\prime}=f_{\\theta}\\left(\\boldsymbol{X}^{\\prime}, \\boldsymbol{A}^{\\prime}\\right) \\boldsymbol{\\Theta} \\\\ \\theta^{*}, \\boldsymbol{\\Theta}^{*} \u0026amp;=\\arg \\min_{\\theta, \\boldsymbol{\\Theta}}\\left(\\mathcal{L}_{\\text {sup }}(\\theta, \\boldsymbol{\\Theta})+\\alpha_{3} \\mathcal{L}_{\\mathrm{adv}}(\\theta, \\boldsymbol{\\Theta})\\right) \\end{aligned} $$ 表示模型要同时在扰动图和训练图上都保持较好的效果。 本文将基于自监督的对抗训练定义为： $$ \\begin{aligned} \\boldsymbol{Z} \u0026amp;=f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) \\boldsymbol{\\Theta}, \\quad \\boldsymbol{Z}^{\\prime}=f_{\\theta}\\left(\\boldsymbol{X}^{\\prime}, \\boldsymbol{A}^{\\prime}\\right) \\boldsymbol{\\Theta} \\\\ \\boldsymbol{Z}_{\\mathrm{ss}}=\u0026amp; f_{\\theta}\\left(\\boldsymbol{X}_ \\mathrm{ss}, \\boldsymbol{A}_{\\mathrm{ss}}\\right) \\\\ \\theta^{*}, \\boldsymbol{\\Theta}^{*}, \\boldsymbol{\\Theta}_{\\mathrm{ss}}^{*}=\u0026amp; \\arg \\min_{\\theta, \\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}_{\\mathrm{ss}}}\\left(\\alpha_{1} \\mathcal{L}_{\\mathrm{sup}}(\\theta, \\boldsymbol{\\Theta})\\right.\\\\ \u0026amp;\\left.+\\alpha_{2} \\mathcal{L}_{\\mathrm{ss}}\\left(\\theta, \\boldsymbol{\\Theta}_{\\mathrm{ss}}\\right)+\\alpha_{3} \\mathcal{L}_{\\mathrm{adv}}(\\theta, \\boldsymbol{\\Theta})\\right) \\end{aligned} $$ 其中自监督损失被引入到以扰动图数据作为输入的训练中（自监督标签矩阵 $\\boldsymbol{Y}_{ss}$ 也是从扰动输入生成的）。\n","permalink":"https://JhuoW.github.io/posts/2020-04-03-ssgcns/","summary":"Paper\nIntroduction 本文是自监督方法在GCNs上首次系统的探索，设计了3种自监督任务来将分析自监督在GCN中起到的作用。自监督旨在充分利用unlabeled数据中的知识来设计前置任务（pretext task），来帮助模型学习更具迁移性和泛化能力的表示。前置任务可以认为是对目标任务有帮助的辅助正则化网络，设计用于帮助原任务学习到更多下游任务相关的语义信息。\nGCN任务通常是直推半监督的（transductive semi-supervised）,含有大量unlabeled数据，而self-supervision(SSL)可以充分利用unlabeled data， 那么就产生了一个值得探索的问题：将自监督学习应用到GCN上是否也可以达到提升泛化能力和鲁棒能力的效果？\n先给结论\nQ1: 自监督学习可否在分类任务中提升GCN？ 如果可以，如何将其合并到 GCN 中以最大化增益？\nA1: 本文证明了通过多任务学习将自监督学习融入 GCN 是有效的，即多任务损失作为 GCN 训练中的正则化项。 这种作为自监督作为正则化项的方法，强于用自监督来预训练或者self-training。\nQ2: 前置任务的设计重要吗？ GCN 有哪些有用的自监督前置任务？\nA2: 本文研究了三个基于图属性的自监督任务。 分别是节点聚类node clustering, 图划分graph partitioning 和图补全graph completion。 并且进一步说明不同的模型和数据集倾向于不同的自监督任务。\nQ3: 自监督也会影响 GCN 的对抗鲁棒性吗？ 如果是，如何设计前置任务？\nA3: 本文进一步将上述发现推广到对抗性训练环境中。提供了广泛的结果，以表明自监督还可以提高 GCN 在各种攻击下的鲁棒性，而不需要更大的模型或额外的数据。\nMethod GCNs $\\boldsymbol{Z}=\\hat{\\boldsymbol{A}} \\operatorname{ReLU}\\left(\\hat{\\boldsymbol{A}} \\boldsymbol{X} \\boldsymbol{W}_{0}\\right) \\boldsymbol{W}_{1}$可以分为两块来看 (1) 特征提取模块$f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) = \\hat{\\boldsymbol{A}} \\operatorname{ReLU}\\left(\\hat{\\boldsymbol{A}} \\boldsymbol{X} \\boldsymbol{W}_{0}\\right)$ 参数为$\\theta = \\{\\boldsymbol{W}_{0}\\}$和（2）线性变换模块$\\boldsymbol{Z}=f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) \\boldsymbol{\\Theta}$ 其中 参数$ \\boldsymbol{\\Theta} = \\boldsymbol{W}_{1}$。 半监督GCN优化任务的目标函数为： $$ \\begin{aligned} \\boldsymbol{Z} \u0026amp;=f_{\\theta}(\\boldsymbol{X}, \\hat{\\boldsymbol{A}}) \\boldsymbol{\\Theta} \\\\ \\theta^{*}, \\boldsymbol{\\Theta}^{} \u0026amp;=\\arg \\min_{\\theta, \\boldsymbol{\\Theta}} \\mathcal{L}_{\\mathrm{sup}}(\\theta, \\boldsymbol{\\Theta}) \\\\ \u0026amp;=\\arg \\min_{\\theta, \\boldsymbol{\\Theta}} \\frac{1}{\\left|\\mathcal{V}_{\\text {label }}\\right|} \\sum_{v_{n} \\in \\mathcal{V}_{\\text {label }}} L\\left(\\boldsymbol{z}_{n}, \\boldsymbol{y}_{n}\\right) \\end{aligned} \\tag{1} $$ 其中$L(\\cdot, \\cdot)$是每个labeled node的损失函数。","title":"ICML2020 《When Does Self-Supervision Help Graph Convolutional Networks?》 Reading Notes"},{"content":"当目标函数中有不可微部分时，可使用近端梯度下降来优化（Proximal Gradient Descent）\n假设目标函数如下： $$ \\definecolor{energy}{RGB}{114,0,172} \\definecolor{freq}{RGB}{45,177,93} \\definecolor{spin}{RGB}{251,0,29} \\definecolor{signal}{RGB}{18,110,213} \\definecolor{circle}{RGB}{217,86,16} \\definecolor{average}{RGB}{203,23,206} \\definecolor{red}{RGB}{255,0,0} f(w) = g(w) + h(w) $$ 其中$g(w)$是可微凸函数，$h(w)$是不可微（或局部不可微）凸函数。 以线性回归为例，\n给定$X \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^m$， Ridge Regression的目标函数为\n$$ f(\\boldsymbol{w})=\\underbrace{\\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w}||_{2}^{2}}_{g(\\boldsymbol{w})}+\\underbrace{\\lambda||\\boldsymbol{w}||_{2}}_{h(\\boldsymbol{w})} $$ 因为$\\ell_2$ norm处处可导，所以Ridge可以用SGD或GD来直接优化。但是若目标函数为Lasso，即正则化项定义为$\\ell_1$ norm: $$ f(\\boldsymbol{w})=\\underbrace{\\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w}||_{2}^{2}}_{g(\\boldsymbol{w})}+\\underbrace{\\lambda||\\boldsymbol{w}||_{1}}_{h(\\boldsymbol{w})} $$ 这里$h(w)=\\lambda||\\boldsymbol{w}||_{1}$在$w=0$处不可导，那么可用PGD来优化。\nProximity Operator 近端算子： 对于不可微函数$h(w)$, $h(w)$的proximity operator定义为：\n$$ u^* = \\operatorname{prox}_{\\color{signal}h}(w)=\\underset{u}{\\arg \\min }\\left(h(u)+\\frac{1}{2}||u-w||_{2}^{2}\\right) $$ 近端算子$\\operatorname{prox}_{\\color{signal}h}(w)$只和不可微凸函数$h(\\cdot)$有关。 上式含义，给定一个不可微凸函数$h(\\cdot)$, 给定向量$w \\in \\mathbb{R}^n$, 找到向量$u = u^*$, 使得公式$h(u)+\\frac{1}{2}||u-w||_{2}^{2}$最小。 这个$u^* = \\operatorname{prox}_{\\color{signal}h}(w)$就是$h(\\cdot)$在给定$w$条件下的近端算子（Proximity Operator）。$u^* = \\operatorname{prox}_{\\color{signal}h}(w)$要求最佳的$u^* $可以使得函数值$h(u^*)$尽可能小，同时$u^*$要尽可能接近给定的$w$。\n基于后面的公式推导，我们给$\\operatorname{prox}_{\\color{signal}h}(w)$添加一个参数$\\color{energy}\\gamma$: $$ u^* = \\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}(w)=\\underset{u}{\\arg \\min }\\left(h(u)+\\frac{1}{2\\color{energy}\\gamma}||u-w||_{2}^{2}\\right) $$ 上式表示，给定一个不可微凸函数$h(\\cdot)$， 一个给定的点$w$, 一个参数$\\gamma$, 要找到一个$u = u^*$, 使得$u^*$带入公式$h(u)+\\frac{1}{2\\color{energy}\\gamma}||u-w||_{2}^{2}$的到的结果最小。 $\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}(w)$是使得$h(u)+\\frac{1}{2\\color{energy}\\gamma}||u-w||_{2}^{2}$最小的输入$u$。\n因为$h(u)$和$||u-w||_{2}^{2}$都为凸函数，所以一定存在$u^*$使得函数值最小， 这个$u^* = \\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}(w)$要求使得$h(u^*)$尽可能小（第一项），同时$u^*$要尽可能接近给定的$w$（第二项）。\n例子：\n  若$h(w)=0$, $\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}(w) = u^* = w$。\n  当$h(w) = ||w||_{1}$时，$\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}(w) = \\operatorname{prox}_{\\color{signal}||\\cdot||_{1}\\color{energy}\\gamma}(w)$是软阈值操作\n  $$ u^* = \\left(\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}(w)\\right)_{i}= \\begin{cases}w_{i}-\\gamma \u0026amp; w_{i} \\geq \\gamma \\\\ 0 \u0026amp; \\left|\\mathrm{w}_{i}\\right| \\leq \\gamma \\\\ w_{i}+ \\gamma \u0026amp; w_{i} \\leq-\\gamma\\end{cases} $$\n如果在$\\ell_1$ norm前加上参数$\\lambda$， 即$h(w) = \\lambda||w||_{1}$， 那么近端算子为： $$ u^* = \\left(\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}(w)\\right)_{i}= \\begin{cases}w_{i}-\\lambda\\gamma \u0026amp; w_{i} \\geq \\lambda\\gamma \\\\ 0 \u0026amp; \\left|\\mathrm{w}_{i}\\right| \\leq \\lambda\\gamma \\\\ w_{i}+ \\lambda\\gamma \u0026amp; w_{i} \\leq-\\lambda\\gamma\\end{cases} $$\n近端梯度算法 回到Lasso 回归，要求解： $$ \\min_{w}(g(w)+h(w)) $$ $w$可以通过递推式求出： $$ \\boldsymbol{w}^{k}=\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}\\left(\\boldsymbol{w}^{k-1}-\\gamma \\nabla g\\left(\\boldsymbol{w}^{k-1}\\right)\\right) $$ 为什么可以通过不断迭代迭代上式来求解最佳的$w^{K}$, 使得$g(w)+h(w)$收敛到最小？下面先给出证明 $$ \\begin{equation} \\begin{aligned} \\boldsymbol{w}^{k} \u0026amp; =\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}\\left(\\boldsymbol{w}^{k-1}-\\gamma \\nabla g\\left(\\boldsymbol{w}^{k-1}\\right)\\right)\\\\ \u0026amp;=\\underset{u}{\\operatorname{argmin}}\\left(\\underbrace{h(u)}_{\\color{red}h(u)尽可能小}+\\frac{1}{2 \\gamma}\\left|\\left|\\underbrace{u-\\left(w^{k-1}-\\gamma \\nabla g\\left(w^{k-1}\\right)\\right)}_{\\color{red}u尽可能接近w^{k-1}-\\gamma \\nabla g\\left(w^{k-1}\\right)}\\right|\\right|_{2}^{2}\\right)\\\\ \u0026amp;= \\underset{u}{\\operatorname{argmin}}\\left(h(u)+\\frac{1}{2 \\gamma}\\left|\\left|(u-w^{k-1} )+\\gamma \\nabla g\\left(w^{k-1}\\right)\\right|\\right|_{2}^{2}\\right)\\\\ \u0026amp; = \\underset{u}{\\operatorname{argmin}}\\left(h(u)+\\underbrace{\\frac{\\gamma}{2}\\left|\\left|\\nabla g\\left(w^{k-1}\\right)\\right|\\right|_{2}^{2}}_{\\color{red}\\gamma和w^{k-1}给定， 所以该项与u无关，视为常数，可省略}+\\left(u-w^{k-1}\\right)^{T}\\nabla g\\left(w^{k-1}\\right)+\\frac{1}{2 \\gamma}\\left|\\left|u-w^{k-1}\\right|\\right|_{2}^{2}\\right) \\\\ \u0026amp;= \\underset{u}{\\operatorname{argmin}}\\left(h(u)+\\underbrace{g\\left(w^{k-1}\\right)}_{\\color{red}添加与u无关的项，不影响结果}+\\left(u-w^{k-1}\\right)^T\\nabla g\\left(w^{k-1}\\right)+\\frac{1}{2 \\gamma}\\left|\\left|u-w^{k-1}\\right|\\right|_{2}^{2}\\right) \\quad \\quad (5)\\\\ \u0026amp; \\approx \\underset{u}{\\arg \\min }(g(u)+h(u)) \\end{aligned} \\end{equation} $$ 整个过程不涉及对$h(u)$求梯度\n最后两步怎么来的？\n 泰勒展开式：\n$$f(x)=\\frac{f\\left(a\\right)}{0 !}+\\frac{f^{\\prime}\\left(a\\right)}{1 !}\\left(x-a\\right)+\\frac{f^{\\prime \\prime}\\left(a\\right)}{2 !}\\left(x-a\\right)^{2}+\\ldots+\\frac{f^{(n)}\\left(a\\right)}{n !}\\left(x-a\\right)^{n}$$\n 对$g(u)$做泰勒展开， 令$a=w^{k-1}$: $$ \\begin{equation} \\begin{aligned} g(u) \u0026amp;= g(w^{k-1}) + (u-w^{k-1})^{T}\\nabla g\\left(w^{k-1}\\right) + \\langle u-w^{k-1}, u-w^{k-1}\\rangle \\nabla^2 g\\left(w^{k-1}\\right) \\\\ \u0026amp; \\approx (5)式最后三项 \\end{aligned} \\end{equation} $$ 综上： $$ \\begin{equation} \\begin{aligned} \\boldsymbol{w}^{k} \u0026amp; =\\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}\\left(\\boldsymbol{w}^{k-1}-\\gamma \\nabla g\\left(\\boldsymbol{w}^{k-1}\\right)\\right) \\\\ \u0026amp; \\approx \\underset{u}{\\arg \\min }(g(u)+h(u)) \\end{aligned} \\end{equation} $$ 所以通过迭代的方式求$w^k = \\operatorname{prox}_{\\color{signal}h\\color{energy}\\gamma}\\left(\\boldsymbol{w}^{k-1}-\\gamma \\nabla g\\left(\\boldsymbol{w}^{k-1}\\right)\\right)$ 就是$\\min_{w}(g(w)+h(w))$的迭代递推求解过程。\n求解$\\ell_1$ 范数 将求解问题转为递推式。\n现在我们有问题，形式为： $$ \\min_{w}(\\underbrace{g(w)}_{\\color{red}凸可微}+\\underbrace{\\lambda \\left|\\left| w \\right|\\right|_1}_{\\color{red}h(w)凸不可微}) $$ 找到最佳$w$使得上式最小的过程可以迭代递推为： $$ \\begin{equation} \\begin{aligned} w^{k+1}\u0026amp;=\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma}\\left(w^{k}-\\gamma \\nabla g\\left(w^{k}\\right)\\right)\\\\ \u0026amp;= \\underset{u}{\\arg \\min }\\left(\\lambda \\left|\\left| u \\right|\\right|_1+\\frac{1}{2\\color{energy}\\gamma}||u-\\left(w^{k}-\\gamma \\nabla g\\left(w^{k}\\right)\\right)||_{2}^{2}\\right) \\end{aligned} \\end{equation} $$\n求解Lasso回归 待求解问题形如： $$ \\min_{w}\\left(\\frac{1}{2}||X w-y||_{2}^{2}+r||w||_1\\right) $$ 可见第一项可微，第二项为$\\ell_1$ norm 在$w=0$处不可微。\n根据递推式： $$ w^{k+1}=\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma}\\left(\\underbrace{w^{k}-\\gamma \\nabla g\\left(w^{k}\\right)}_{\\color{red}z^k}\\right) $$ 令$z^k = w^{k}-\\gamma \\nabla g\\left(w^{k}\\right)$, 上式可改写为\n因为learning step size $\\gamma$ 与$w$无关，所以上式可以改写为： $$ \\begin{equation} \\begin{aligned} w^{k+1}\u0026amp;=\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma}\\left(z^k\\right)\\\\ \u0026amp;= \\underset{w}{\\arg \\min } \\left( \\lambda \\left|\\left| w \\right|\\right|_1 + \\frac{1}{2\\color{energy}\\gamma}||w-z^k||_{2}^{2}\\right) \\\\ \u0026amp;= \\underset{w}{\\arg \\min } \\left( \\lambda \\gamma\\left|\\left| w \\right|\\right|_1 + \\frac{1}{2}||w-z^k||_{2}^{2}\\right) \\end{aligned} \\end{equation} $$\n$\\because g(w^k) = \\frac{1}{2}||X w-y||_{2}^{2}$\n$\\therefore \\nabla g(w^k) = X^{T}(Xw^k-y) = X^TXw^k-X^Ty$\n$\\therefore z^k = w^k-\\gamma (X^TXw^k-X^Ty)$\n把$z^k$带入$w^{k+1}$中： $$ \\begin{equation} \\begin{aligned} w^{k+1}\u0026amp;=\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma}\\left(z^k\\right)\\\\ \u0026amp;=\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma}\\left( w^k-\\gamma X^TXw^k+ \\gamma X^Ty\\right) \\end{aligned} \\end{equation} $$ 因为$\\left(\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma}(w)\\right)_{i}= \\begin{cases}w_{i}-\\lambda\\gamma \u0026amp; w_{i} \\geq \\lambda\\gamma \\\\ 0 \u0026amp; \\left|w_{i}\\right| \\leq \\lambda\\gamma \\\\ w_{i}+ \\lambda\\gamma \u0026amp; w_{i} \\leq-\\lambda\\gamma\\end{cases}$,\n所以$w^k$到$w^{k+1}$的迭代优化方式如下： $$ w_i^{k+1} = \\left(\\operatorname{prox}_{\\color{signal}\\lambda||\\cdot||_{1}\\color{energy}\\gamma}\\left( z^k\\right)\\right)_i = \\begin{cases} z^k_{i}-\\lambda\\gamma \u0026amp; z^k_{i} \\geq \\lambda\\gamma \\\\ 0 \u0026amp; \\left|z^k_{i}\\right| \\leq \\lambda\\gamma \\\\ z^k_{i}+ \\lambda\\gamma \u0026amp; z^k_{i} \\leq-\\lambda\\gamma \\end{cases} $$ 其中 $z^k_{i}$是$z^k$的第$i$行。\nReference https://blog.csdn.net/Chaolei3/article/details/81320940\nhttps://zhuanlan.zhihu.com/p/82622940\nhttp://roachsinai.github.io/2016/08/03/1Proximal_Method/\n","permalink":"https://JhuoW.github.io/posts/pgd/","summary":"当目标函数中有不可微部分时，可使用近端梯度下降来优化（Proximal Gradient Descent）\n假设目标函数如下： $$ \\definecolor{energy}{RGB}{114,0,172} \\definecolor{freq}{RGB}{45,177,93} \\definecolor{spin}{RGB}{251,0,29} \\definecolor{signal}{RGB}{18,110,213} \\definecolor{circle}{RGB}{217,86,16} \\definecolor{average}{RGB}{203,23,206} \\definecolor{red}{RGB}{255,0,0} f(w) = g(w) + h(w) $$ 其中$g(w)$是可微凸函数，$h(w)$是不可微（或局部不可微）凸函数。 以线性回归为例，\n给定$X \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^m$， Ridge Regression的目标函数为\n$$ f(\\boldsymbol{w})=\\underbrace{\\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w}||_{2}^{2}}_{g(\\boldsymbol{w})}+\\underbrace{\\lambda||\\boldsymbol{w}||_{2}}_{h(\\boldsymbol{w})} $$ 因为$\\ell_2$ norm处处可导，所以Ridge可以用SGD或GD来直接优化。但是若目标函数为Lasso，即正则化项定义为$\\ell_1$ norm: $$ f(\\boldsymbol{w})=\\underbrace{\\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w}||_{2}^{2}}_{g(\\boldsymbol{w})}+\\underbrace{\\lambda||\\boldsymbol{w}||_{1}}_{h(\\boldsymbol{w})} $$ 这里$h(w)=\\lambda||\\boldsymbol{w}||_{1}$在$w=0$处不可导，那么可用PGD来优化。\nProximity Operator 近端算子： 对于不可微函数$h(w)$, $h(w)$的proximity operator定义为：\n$$ u^* = \\operatorname{prox}_{\\color{signal}h}(w)=\\underset{u}{\\arg \\min }\\left(h(u)+\\frac{1}{2}||u-w||_{2}^{2}\\right) $$ 近端算子$\\operatorname{prox}_{\\color{signal}h}(w)$只和不可微凸函数$h(\\cdot)$有关。 上式含义，给定一个不可微凸函数$h(\\cdot)$, 给定向量$w \\in \\mathbb{R}^n$, 找到向量$u = u^*$, 使得公式$h(u)+\\frac{1}{2}||u-w||_{2}^{2}$最小。 这个$u^* = \\operatorname{prox}_{\\color{signal}h}(w)$就是$h(\\cdot)$在给定$w$条件下的近端算子（Proximity Operator）。$u^* = \\operatorname{prox}_{\\color{signal}h}(w)$要求最佳的$u^* $可以使得函数值$h(u^*)$尽可能小，同时$u^*$要尽可能接近给定的$w$。","title":"Proximal Gradient Descent"},{"content":"Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem. Another application is spectral matching that solves for graph matching.\nBasic notations We consider simple graphs (no multiple edges or loops), $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})$ :\n  $\\mathcal{V}(\\mathcal{G})=\\left\\{v_{1}, \\ldots, v_{n}\\right\\}$ is called the vertex set with $n=|\\mathcal{V}|$;\n  $\\mathcal{E}(\\mathcal{G})=\\left\\{e_{i j}\\right\\}$ is called the edge set with $m=|\\mathcal{E}|$;\n  An edge $e_{i j}$ connects vertices $v_{i}$ and $v_{j}$ if they are adjacent or neighbors. One possible notation for adjacency is $v_{i} \\sim v_{j}$;\n  The number of neighbors of a node $v$ is called the degree of $v$ and is denoted by $d(v), d\\left(v_{i}\\right)=\\sum_{v_{i} \\sim v_{j}} e_{i j}$. If all the nodes of a graph have the same degree, the graph is regular; The nodes of an Eulerian graph have even degree.\n  A graph is complete if there is an edge between every pair of vertices.\n  Subgraph of a graph   $\\mathcal{H}$ is a subgraph of $\\mathcal{G}$ if $\\mathcal{V}(\\mathcal{H}) \\subseteq \\mathcal{V}(\\mathcal{G})$ and $\\mathcal{E}(\\mathcal{H}) \\subseteq \\mathcal{E}(\\mathcal{G})$;\n  a subgraph $\\mathcal{H}$ is an induced subgraph of $\\mathcal{G}$ if two vertices of $\\mathcal{V}(\\mathcal{H})$ are adjacent if and only if they are adjacent in $\\mathcal{G}$.\n  A clique is a complete subgraph of a graph.\n  A path of $k$ vertices is a sequence of $k$ distinct vertices such that consecutive vertices are adjacent.\n  A cycle is a connected subgraph where every vertex has exactly two neighbors.\n  A graph containing no cycles is a forest. A connected forest is a tree.\n  A k-partite graph  A graph is called k-partite if its set of vertices admits a partition into $k$ classes such that the vertices of the same class are not adjacent. An example of a bipartite graph.  The adjacency matrix of a graph  For a graph with $n$ vertices, the entries of the $n \\times n$ adjacency matrix are defined by:  $$ \\mathbf{A}:= \\begin{cases}A_{i j}=1 \u0026amp; \\text { if there is an edge } e_{i j} \\\\ A_{i j}=0 \u0026amp; \\text { if there is no edge } \\\\ A_{i i}=0 \u0026amp; \\end{cases} $$\n$$ \\begin{aligned} \u0026amp; \\mathbf{A}=\\left[\\begin{array}{llll}0 \u0026amp; 1 \u0026amp; 1 \u0026amp; 0 \\\\1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1 \\\\1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\end{array}\\right] \\end{aligned} $$\nEigenvalues and eigenvectors   A is a real-symmetric matrix: it has $n$ real eigenvalues and its $n$ real eigenvectors form an orthonormal basis.\n  Let $\\left\\{\\lambda_{1}, \\ldots, \\lambda_{i}, \\ldots, \\lambda_{r}\\right\\}$ be the set of distinct eigenvalues.\n  The eigenspace $S_{i}$ contains the eigenvectors associated with $\\lambda_{i}$ :\n  $$ S_{i}=\\left\\{\\boldsymbol{x} \\in \\mathbb{R}^{n} \\mid \\mathbf{A} \\boldsymbol{x}=\\lambda_{i} \\boldsymbol{x}\\right\\} $$\n  For real-symmetric matrices, the algebraic multiplicity is equal to the geometric multiplicity, for all the eigenvalues.\n  The dimension of $S_{i}$ (geometric multiplicity) is equal to the multiplicity of $\\lambda_{i}$.\n  If $\\lambda_{i} \\neq \\lambda_{j}$ then $S_{i}$ and $S_{j}$ are mutually orthogonal.\n  Real-valued functions on graphs   We consider real-valued functions on the set of the graph\u0026rsquo;s vertices, $\\boldsymbol{f}: \\mathcal{V} \\longrightarrow \\mathbb{R}$. Such a function assigns a real number to each graph node.\n  $\\boldsymbol{f}$ is a vector indexed by the graph\u0026rsquo;s vertices, hence $\\boldsymbol{f} \\in \\mathbb{R}^{n}$.\n  Notation: $\\boldsymbol{f}=\\left(f\\left(v_{1}\\right), \\ldots, f\\left(v_{n}\\right)\\right)=(f(1), \\ldots, f(n))$.\n  The eigenvectors of the adjacency matrix, $\\mathbf{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x}$, can be viewed as eigenfunctions.\n  Matrix A as an operator and quadratic form  The adjacency matrix can be viewed as an operator  $$ \\boldsymbol{g}=\\mathbf{A} \\boldsymbol{f} ; g(i)=\\sum_{i \\sim j} f(j) $$\n It can also be viewed as a quadratic form:  $$ \\boldsymbol{f}^{\\top} \\mathbf{A} \\boldsymbol{f}=\\sum_{e_{i j}} f(i) f(j) $$\nThe incidence matrix of a graph   Let each edge in the graph have an arbitrary but fixed orientation;\n  The incidence matrix of a graph is a $|\\mathcal{E}| \\times|\\mathcal{V}|(m \\times n)$ matrix defined as follows:\n  $$ \\nabla:= \\begin{cases}\\nabla_{e v}=-1 \u0026amp; \\text { if } v \\text { is the initial vertex of edge } e \\\\ \\nabla_{e v}=1 \u0026amp; \\text { if } v \\text { is the terminal vertex of edge } e \\\\ \\nabla_{e v}=0 \u0026amp; \\text { if } v \\text { is not in } e\\end{cases} $$\n$$ \\begin{aligned} \u0026amp; \\nabla=\\left[\\begin{array}{cccc}-1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\1 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\0 \u0026amp; -1 \u0026amp; 1 \u0026amp; 0 \\\\0 \u0026amp; -1 \u0026amp; 0 \u0026amp; +1\\end{array}\\right] \\end{aligned} $$\nThe incidence matrix: A discrete differential operator   The mapping $\\boldsymbol{f} \\longrightarrow \\nabla \\boldsymbol{f}$ is known as the co-boundary mapping of the graph.\n  $(\\nabla \\boldsymbol{f})\\left(e_{i j}\\right)=f\\left(v_{j}\\right)-f\\left(v_{i}\\right)$\n  $$ \\left(\\begin{array}{c} f(2)-f(1) \\\\ f(1)-f(3) \\\\ f(3)-f(2) \\\\ f(4)-f(2) \\end{array}\\right)=\\left[\\begin{array}{cccc} -1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \u0026amp; +1 \\end{array}\\right]\\left(\\begin{array}{c} f(1) \\\\ f(2) \\\\ f(3) \\\\ f(4) \\end{array}\\right) $$\nThe Laplacian matrix of a graph   $\\mathbf{L}=\\nabla^{\\top} \\nabla$\n  $(\\mathbf{L} \\boldsymbol{f})\\left(v_{i}\\right)=\\sum_{v_{j} \\sim v_{i}}\\left(f\\left(v_{i}\\right)-f\\left(v_{j}\\right)\\right)$\n  Connection between the Laplacian and the adjacency matrices:\n  $$ \\mathbf{L}=\\mathbf{D}-\\mathbf{A} $$\n The degree matrix: $\\mathbf{D}:=D_{i i}=d\\left(v_{i}\\right)$.  $$ \\mathbf{L}=\\left[\\begin{array}{cccc} 2 \u0026amp; -1 \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp; 3 \u0026amp; -1 \u0026amp; -1 \\\\ -1 \u0026amp; -1 \u0026amp; 2 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right] $$\nThe Laplacian matrix of an undirected weighted graph   We consider undirected weighted graphs: Each edge $e_{i j}$ is weighted by $w_{i j}\u0026gt;0$.\n  The Laplacian as an operator:\n  $$ (\\mathbf{L} \\boldsymbol{f})\\left(v_{i}\\right)=\\sum_{v_{j} \\sim v_{i}} w_{i j}\\left(f\\left(v_{i}\\right)-f\\left(v_{j}\\right)\\right) $$\n As a quadratic form:  $$ \\boldsymbol{f}^{\\top} \\mathbf{L} \\boldsymbol{f}=\\frac{1}{2} \\sum_{e_{i j}} w_{i j}\\left(f\\left(v_{i}\\right)-f\\left(v_{j}\\right)\\right)^{2} $$\n  L is symmetric and positive semi-definite.\n  L has $n$ non-negative, real-valued eigenvalues: $0=\\lambda_{1} \\leq \\lambda_{2} \\leq \\ldots \\leq \\lambda_{n} .$\n  The Laplacian of a 3D discrete surface (mesh)   A graph vertex $v_{i}$ is associated with a 3D point $\\boldsymbol{v}_{i}$.\n  The weight of an edge $e_{i j}$ is defined by the Gaussian kernel:\n  $$ w_{i j}=\\exp \\left(-\\left|\\boldsymbol{v}_{i}-\\boldsymbol{v}_{j}\\right|^{2} / \\sigma^{2}\\right) $$\n  $0 \\leq w_{\\min } \\leq w_{i j} \\leq w_{\\max } \\leq 1$\n  Hence, the geometric structure of the mesh is encoded in the weights.\n  Other weighting functions were proposed in the literature.\n  The Laplacian of a cloud of points   3-nearest neighbor graph\n  $\\varepsilon$-radius graph\n  KNN may guarantee that the graph is connected (depends on the implementation)\n  $\\varepsilon$-radius does not guarantee that the graph has one connected component\n  The Laplacian of a graph with one connected component   $Lu =\\lambda \\boldsymbol{u}$.\n  $\\mathbf{L} \\mathbf{1}_{n}=\\mathbf{0}, \\lambda_{1}=0$ is the smallest eigenvalue.\n  The one vector: $\\mathbf{1}_{n}=(1 \\ldots 1)^{\\top}$.\n  $0=\\boldsymbol{u}^{\\top} \\mathbf{L} \\boldsymbol{u}=\\sum_{i, j=1}^{n} w_{i j}(u(i)-u(j))^{2}$.\n  If any two vertices are connected by a path, then $\\boldsymbol{u}=(u(1), \\ldots, u(n))$ needs to be constant at all vertices such that the quadratic form vanishes. Therefore, a graph with one connected component has the constant vector $\\boldsymbol{u}_{1}=\\mathbf{1}_{n}$ as the only eigenvector with eigenvalue 0 .\n  A graph with $k\u0026gt;1$ connected components  Each connected component has an associated Laplacian. Therefore, we can write matrix $\\mathbf{L}$ as a block diagonal matrix:  $$ \\mathbf{L}=\\left[\\begin{array}{lll} \\mathbf{L}_{1} \u0026amp; \u0026amp; \\\\ \u0026amp; \\ddots \u0026amp; \\\\ \u0026amp; \u0026amp; \\mathbf{L}_{k} \\end{array}\\right] $$\n  The spectrum of $\\mathbf{L}$ is given by the union of the spectra of $\\mathbf{L}_{i}$.\n  Each block corresponds to a connected component, hence each matrix $\\mathbf{L}_{i}$ has an eigenvalue 0 with multiplicity 1 .\n  The spectrum of $\\mathbf{L}$ is given by the union of the spectra of $\\mathbf{L}_{i}$.\n  The eigenvalue $\\lambda_{1}=0$ has multiplicity $k$.\n  The eigenspace of $\\lambda_{1}=0$ with multiplicity $k$  The eigenspace corresponding to $\\lambda_{1}=\\ldots=\\lambda_{k}=0$ is spanned by the $k$ mutually orthogonal vectors:  $$ \\begin{aligned} \\boldsymbol{u}_{1} \u0026amp;=\\mathbf{1}_{L_{1}} \\\\ \u0026amp; \\cdots \\\\ \\boldsymbol{u}_{k} \u0026amp;=\\mathbf{1}_{L_{k}} \\end{aligned} $$\n  with $\\mathbf{1}_{L_{i}}=(0000111110000)^{\\top} \\in \\mathbb{R}^{n}$\n  These vectors are the indicator vectors of the graph\u0026rsquo;s connected components.\n  Notice that $\\mathbf{1}_{L_{1}}+\\ldots+\\mathbf{1}_{L_{k}}=\\mathbf{1}_{n}$\n  The Fiedler vector of the graph Laplacian   The first non-null eigenvalue $\\lambda_{k+1}$ is called the Fiedler value.\n  The corresponding eigenvector $\\boldsymbol{u}_{k+1}$ is called the Fiedler vector.\n  The multiplicity of the Fiedler eigenvalue is always equal to $1 .$\n  The Fiedler value is the algebraic connectivity of a graph, the further from 0 , the more connected.\n  The Fidler vector has been extensively used for spectral bi-partioning\n  Theoretical results are summarized in Spielman \u0026amp; Teng 2007: http://cs-www.cs.yale.edu/homes/spielman/\n  Eigenvectors of the Laplacian of connected graphs   $\\boldsymbol{u}_{1}=\\mathbf{1}_{n}, \\mathbf{L} \\mathbf{1}_{n}=\\mathbf{0}$.\n  $\\boldsymbol{u}_{2}$ is the the Fiedler vector with multiplicity 1 .\n  The eigenvectors form an orthonormal basis: $\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{u}_{j}=\\delta_{i j}$.\n  For any eigenvector $\\boldsymbol{u}_{i}=\\left(\\boldsymbol{u}_{i}\\left(v_{1}\\right) \\ldots \\boldsymbol{u}_{i}\\left(v_{n}\\right)\\right)^{\\top}, 2 \\leq i \\leq n$ :\n  $$ \\boldsymbol{u}_{i}^{\\top} \\mathbf{1}_{n}=0 $$\n Hence the components of $\\boldsymbol{u}_{i}, 2 \\leq i \\leq n$ satisfy:  $$ \\sum_{j=1}^{n} \\boldsymbol{u}_{i}\\left(v_{j}\\right)=0 $$\n Each component is bounded by:  $$ -1\u0026lt;\\boldsymbol{u}_{i}\\left(v_{j}\\right)\u0026lt;1 $$\nLaplacian embedding: Mapping a graph on a line  Map a weighted graph onto a line such that connected nodes stay as close as possible, i.e., minimize $\\sum_{i, j=1}^{n} w_{i j}\\left(f\\left(v_{i}\\right)-f\\left(v_{j}\\right)\\right)^{2}$, or:  $$ \\arg \\min _{\\boldsymbol{f}} \\boldsymbol{f}^{\\top} \\mathbf{L} \\boldsymbol{f} \\text { with: } \\boldsymbol{f}^{\\top} \\boldsymbol{f}=1 \\text { and } \\boldsymbol{f}^{\\top} \\mathbf{1}=0 $$\n  The solution is the eigenvector associated with the smallest nonzero eigenvalue of the eigenvalue problem: $\\mathbf{L} \\boldsymbol{f}=\\lambda \\boldsymbol{f}$, namely the Fiedler vector $\\boldsymbol{u}_{2}$.\n  For more details on this minimization see Golub \u0026amp; Van Loan Matrix Computations, chapter 8 (The symmetric eigenvalue problem).\n  Example of mapping a graph on the Fiedler vector:\nLaplacian embedding   Embed the graph in a $k$-dimensional Euclidean space. The embedding is given by the $n \\times k$ matrix $\\mathbf{F}=\\left[\\boldsymbol{f}_{1} \\boldsymbol{f}_{2} \\ldots \\boldsymbol{f}_{k}\\right]$ where the $i$-th row of this matrix $-\\boldsymbol{f}^{(i)}-$ corresponds to the Euclidean coordinates of the $i$-th graph node $v_{i}$.\n  We need to minimize:\n  $$ \\arg \\min_{\\boldsymbol{f}_{1} \\ldots} \\sum_{k}^{n} \\sum_{i, j=1}^{n} w_{i j}\\left|\\left|\\boldsymbol{f}^{(i)}-\\boldsymbol{f}^{(j)}\\right|\\right|^{2} \\text { with: } \\mathbf{F}^{\\top} \\mathbf{F}=\\mathbf{I} $$\n The solution is provided by the matrix of eigenvectors corresponding to the $k$ lowest nonzero eigenvalues of the eigenvalue problem $\\mathbf{L} \\boldsymbol{f}=\\lambda \\boldsymbol{f}$.  Spectral embedding using the unnormalized Laplacian   Compute the eigendecomposition $\\mathbf{L}=\\mathbf{D}-\\mathbf{A}$.\n  Select the $k$ smallest non-null eigenvalues $\\lambda_{2} \\leq \\ldots \\leq \\lambda_{k+1}$\n  $\\lambda_{k+2}-\\lambda_{k+1}=$ eigengap.\n  We obtain the $n \\times k$ matrix $\\mathbf{U}=\\left[\\boldsymbol{u}_{2} \\ldots \\boldsymbol{u}_{k+1}\\right]$ :\n  $$ \\mathbf{U}=\\left[\\begin{array}{ccc} \\boldsymbol{u}_{2}\\left(v_{1}\\right) \u0026amp; \\ldots \u0026amp; \\boldsymbol{u}_{k+1}\\left(v_{1}\\right) \\\\ \\vdots \u0026amp; \u0026amp; \\vdots \\\\ \\boldsymbol{u}_{2}\\left(v_{n}\\right) \u0026amp; \\ldots \u0026amp; \\boldsymbol{u}_{k+1}\\left(v_{n}\\right) \\end{array}\\right] $$\n  $\\boldsymbol{u}_{i}^{\\top} \\boldsymbol{u}_{j}=\\delta_{i j}$ (orthonormal vectors), hence $\\mathbf{U}^{\\top} \\mathbf{U}=\\mathbf{I}_{k}$.\n  Column $i(2 \\leq i \\leq k+1)$ of this matrix is a mapping on the eigenvector $\\boldsymbol{u}_{i}$.\n  Euclidean L-embedding of the graph\u0026rsquo;s vertices  (Euclidean) L-embedding of a graph:  $$ \\mathbf{X}=\\boldsymbol{\\Lambda}_{k}^{-\\frac{1}{2}} \\mathbf{U}^{\\top}=\\left[\\begin{array}{llll} \\boldsymbol{x}_{1} \u0026amp; \\ldots \u0026amp; \\boldsymbol{x}_{j} \\ldots \u0026amp; \\boldsymbol{x}_{n} \\end{array}\\right] $$\nThe coordinates of a vertex $v_{j}$ are:\n$$ \\boldsymbol{x}_{j}=\\left(\\begin{array}{c} \\frac{\\boldsymbol{u}_{2}\\left(v_{j}\\right)}{\\sqrt{\\lambda_{2}}} \\\\ \\vdots \\\\ \\frac{\\boldsymbol{u}_{k+1}\\left(v_{j}\\right)}{\\sqrt{\\lambda_{k+1}}} \\end{array}\\right) $$\nJustification for choosing the L-embedding Both\n  the commute-time distance (CTD) and\n  the principal-component analysis of a graph (graph PCA)\n  are two important concepts; They allow to reason \u0026ldquo;statistically\u0026rdquo; on a graph. They are both associated with the unnormalized Laplacian matrix.\nThe commute-time distance   The CTD is a well known quantity in Markov chains;\n  It is the average number of (weighted) edges that it takes, starting at vertex $v_{i}$, to randomly reach vertex $v_{j}$ for the first time and go back;\n  The CTD decreases as the number of connections between the two nodes increases;\n  It captures the connectivity structure of a small graph volume rather than a single path between the two vertices - such as the shortest-path geodesic distance.\n  The CTD can be computed in closed form:\n  $$ \\operatorname{CTD}^{2}\\left(v_{i}, v_{j}\\right)=\\operatorname{vol}(\\mathcal{G})\\left|\\left|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}\\right|\\right|^{2} $$\nThe graph PCA  The mean (remember that $\\sum_{j=1}^{n} \\boldsymbol{u}_{i}\\left(v_{j}\\right)=0$ ):  $$ \\overline{\\boldsymbol{x}}=\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{x}_{j}=\\boldsymbol{\\Lambda}_{k}^{-\\frac{1}{2}}\\left(\\begin{array}{c} \\sum_{j=1}^{n} \\boldsymbol{u}_{2}\\left(v_{j}\\right) \\\\ \\vdots \\\\ \\sum_{j=1}^{n} \\boldsymbol{u}_{k+1}\\left(v_{j}\\right) \\end{array}\\right)=\\left(\\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array}\\right) $$\n The covariance matrix:  $$ \\mathbf{S}=\\frac{1}{n} \\sum_{j=1}^{n} \\boldsymbol{x}_{j} \\boldsymbol{x}_{j}^{\\top}=\\frac{1}{n} \\mathbf{X} \\mathbf{X}^{\\top}=\\frac{1}{n} \\boldsymbol{\\Lambda}_{k}^{-\\frac{1}{2}} \\mathbf{U}^{\\top} \\mathbf{U} \\boldsymbol{\\Lambda}_{k}^{-\\frac{1}{2}}=\\frac{1}{n} \\boldsymbol{\\Lambda}_{k}^{-1} $$\n The vectors $\\boldsymbol{u}_{2}, \\ldots, \\boldsymbol{u}_{k+1}$ are the directions of maximum variance of the graph embedding, with $\\lambda_{2}^{-1} \\geq \\ldots \\geq \\lambda_{k+1}^{-1}$.  Other Laplacian matrices  The normalized graph Laplacian (symmetric and semi-definite positive):  $$ \\mathbf{L}_{n}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{L} \\mathbf{D}^{-\\frac{1}{2}}=\\mathbf{I}-\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}} $$\n The transition matrix (allows an analogy with Markov chains):  $$ \\mathbf{L}_{t}=\\mathbf{D}^{-1} \\mathbf{A} $$\n The random-walk graph Laplacian:  $$ \\mathbf{L}_{r}=\\mathbf{D}^{-1} \\mathbf{L}=\\mathbf{I}-\\mathbf{L}_{t} $$\n These matrices are similar:  $$ \\mathbf{L}_{r}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{L} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{D}^{\\frac{1}{2}}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{L}_{n} \\mathbf{D}^{\\frac{1}{2}} $$\nEigenvalues and eigenvectors of $\\mathrm{L}_{n}$ and $\\mathrm{L}_{r}$  $\\mathbf{L}_{r} \\boldsymbol{w}=\\lambda \\boldsymbol{w} \\Longleftrightarrow \\mathbf{L} \\boldsymbol{w}=\\lambda \\mathbf{D} \\boldsymbol{w}$, hence:  $$ \\mathbf{L}_{r}: \\quad \\lambda_{1}=0 ; \\quad \\boldsymbol{w}_{1}=\\mathbf{1} $$\n $\\mathbf{L}_{n} \\boldsymbol{v}=\\lambda \\boldsymbol{v}$. By virtue of the similarity transformation between the two matrices:  $$ \\mathbf{L}_{n}: \\quad \\lambda_{1}=0 \\quad \\boldsymbol{v}_{1}=\\mathbf{D}^{\\frac{1}{2}} \\mathbf{1} $$\n More generally, the two matrices have the same eigenvalues:  $$ 0=\\lambda_{1} \\leq \\ldots \\leq \\lambda_{i} \\ldots \\leq \\lambda_{n} $$\n Their eigenvectors are related by:  $$ \\boldsymbol{v}_{i}=\\mathbf{D}^{\\frac{1}{2}} \\boldsymbol{w}_{i}, \\forall i=1 \\ldots n $$\nSpectral embedding using the random-walk Laplacian $\\mathbf{L}_{r}$  The $n \\times k$ matrix contains the first $k$ eigenvectors of $\\mathbf{L}_{r}$ :  $$ \\mathbf{W}=\\left[\\begin{array}{lll} \\boldsymbol{w}_{2} \u0026amp; \\ldots \u0026amp; \\boldsymbol{w}_{k+1} \\end{array}\\right] $$\n It is straightforward to obtain the following expressions, where $\\boldsymbol{d}$ and $\\mathbf{D}$ are the degree-vector and the degree-matrix:  $$ \\begin{gathered} \\boldsymbol{w}_{i}^{\\top} \\boldsymbol{d}=0, \\forall i, 2 \\leq i \\leq n \\\\ \\mathbf{W}^{\\top} \\mathbf{D W}=\\mathbf{I}_{k} \\end{gathered} $$\n The isometric embedding using the random-walk Laplacian:  $$ \\mathbf{Y}=\\mathbf{W}^{\\top}=\\left[\\begin{array}{lll} \\boldsymbol{y}_{1} \u0026amp; \\ldots \u0026amp; \\boldsymbol{y}_{n} \\end{array}\\right] $$\nThe normalized additive Laplacian  Some authors use the following matrix:  $$ \\mathbf{L}_{a}=\\frac{1}{d_{\\max }}\\left(\\mathbf{A}+d_{\\max } \\mathbf{I}-\\mathbf{D}\\right) $$\n This matrix is closely related to L:  $$ \\mathbf{L}_{a}=\\frac{1}{d_{\\max }}\\left(d_{\\max } \\mathbf{I}-\\mathbf{L}\\right) $$\n and we have:  $$ \\mathbf{L}_{a} \\boldsymbol{u}=\\mu \\boldsymbol{u} \\Longleftrightarrow \\mathbf{L} \\boldsymbol{u}=\\lambda \\boldsymbol{u}, \\mu=1-\\frac{\\lambda}{d_{\\max }} $$\nThe graph partitioning problem  The graph-cut problem: Partition the graph such that:  (1) Edges between groups have very low weight, and\n(2) Edges within a group have high weight.\n$\\operatorname{cut}\\left(A_{1}, \\ldots, A_{k}\\right):=\\frac{1}{2} \\sum_{i=1}^{k} W\\left(A_{i}, \\bar{A}_{i}\\right)$ with $W(A, B)=\\sum_{i \\in A, j \\in B} w_{i j}$\n Ratio cut:  $$ \\operatorname{RatioCut}\\left(A_{1}, \\ldots, A_{k}\\right):=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\bar{A}_{i}\\right)}{\\left|A_{i}\\right|} $$\n Normalized cut:  $$ \\operatorname{NCut}\\left(A_{1}, \\ldots, A_{k}\\right):=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\bar{A}_{i}\\right)}{\\operatorname{vol}\\left(A_{i}\\right)} $$\nWhat is spectral clustering? See my Blog of Spectral Clustering (in Chinese).\n  Both ratio-cut and normalized-cut minimizations are NP-hard problems\n  Spectral clustering is a way to solve relaxed versions of these problems:\n  (1) The smallest non-null eigenvectors of the unnormalized Laplacian approximate the RatioCut minimization criterion, and\n(2) The smallest non-null eigenvectors of the random-walk Laplacian approximate the NCut criterion.\nSpectral clustering using the random-walk Laplacian   For details see (von Luxburg \u0026lsquo;07)\n  Input: Laplacian $\\mathbf{L}_{r}$ and the number $k$ of clusters to compute.\n  Output: Cluster $C_{1}, \\ldots, C_{k}$.\n  (3) Compute W formed with the first $k$ eigenvectors of the random-walk Laplacian.\n(2) Determine the spectral embedding $\\mathbf{Y}=\\mathbf{W}^{\\top}$\n(3) Cluster the columns $\\boldsymbol{y}_{j}, j=1, \\ldots, n$ into $k$ clusters using the K-means algorithm.\nK-means clustering See Bishop'2006 (pages 424-428) for more details.\n  What is a cluster: a group of points whose inter-point distance are small compared to distances to points outside the cluster.\n  Cluster centers: $\\boldsymbol{\\mu}_{1}, \\ldots, \\boldsymbol{\\mu}_{k}$.\n  Goal: find an assignment of points to clusters as well as a set of vectors $\\mu_{i}$.\n  Notations: For each point $\\boldsymbol{y}_{j}$ there is a binary indicator variable $r_{j i} \\in{0,1}$.\n  Objective: minimize the following distorsion measure:\n  $$ J=\\sum_{j=1}^{n} \\sum_{i=1}^{k} r_{j i}\\left|\\left|\\boldsymbol{y}_{j}-\\boldsymbol{\\mu}_{i}\\right|\\right|^{2} $$\nThe K-means algorithm (1) Initialization: Choose initial values for $\\boldsymbol{\\mu}_{1}, \\ldots, \\boldsymbol{\\mu}_{k}$.\n(2) First step: Assign the $j$-th point to the closest cluster center:\n$$ r_{j i}= \\begin{cases}1 \u0026amp; \\text { if } i=\\arg \\min_{l}\\left|\\left|\\boldsymbol{y}_{j}-\\mu_{l}\\right|\\right|^{2} \\\\ 0 \u0026amp; \\text { otherwise }\\end{cases} $$\n(3) Second Step: Minimize $J$ to estimate the cluster centers:\n$$ \\boldsymbol{\\mu}_{i}=\\frac{\\sum_{j=1}^{n} r_{j i} \\boldsymbol{y}_{j}}{\\sum_{j=1}^{n} r_{j i}} $$\n(4) Convergence: Repeat until no more change in the assignments.\nThe Laplacian and the Rayleigh quotient As usual, for a graph $G=(V, E)$, let $A$ be its adjacency matrix and $D$ be the diagonal matrix with $D(v, v)=d_{v}$. Then, the random walk on $G$ will be taken according to the transition matrix $P=D^{-1} A$. We also define the stationary distribution $\\pi$ with $\\pi(x)=d_{x} / \\operatorname{vol} G$.\nOur discussion of random walks on $G$ left off with the result\n$$ \\left|\\left|f P^{t}-\\pi\\right|\\right|_{2} \\leq \\max_{i \\neq 0}\\left|\\rho_{i}\\right|^{t} \\frac{\\max_{x} \\sqrt{d_{x}}}{\\min_{y} \\sqrt{d_{y}}} $$\nwhere $f$ is a probability distribution (i.e. $f \\geq 0$ and $\\sum_{x} f(x)=1$ ) and $1=\\rho_{0} \\geq \\rho_{1} \\geq \\ldots \\geq \\rho_{n-1}$ are the eigenvalues of $P$. This inequality implies that convergence to the stationary distribution $\\pi$ will follow if $\\max \\left\\{\\left|\\rho_{1}\\right|,\\left|\\rho_{n-1}\\right|\\right\\}\u0026lt;1$.\nThe transition probability matrix $P$ is similar to the matrix $M=D^{\\frac{1}{2}} P D^{-\\frac{1}{2}}$, so $P$ and $M$ have the same eigenvalues. We previously introduced the Laplacian of the graph as $\\mathcal{L}=I-M$, so it has eigenvalues $0=\\lambda_{0} \\leq \\lambda_{1} \\leq \\ldots \\leq \\lambda_{n-1}$ (where $\\lambda_{i}=1-\\rho_{i}$ ).\nThe main tool we\u0026rsquo;ll use to study the spectrum of $\\mathcal{L}$ is the Rayleigh quotient $R(f)$ of $\\mathcal{L}$, defined (for our purposes) as\n$$ R(f)=\\frac{f L f^{*}}{f D f^{*}} $$\nwhere $L=D-A$ is the combinatorial Laplacian. This is the same as the usual sense of the Rayleigh quotient $g \\mathcal{L} g^{*} / g g^{*}$ with the subtitution $f=g D^{-\\frac{1}{2}}$. Following this equivalence, if the $\\phi_{i}$ are the eigenvectors of $\\mathcal{L}$, we\u0026rsquo;ll call the $\\psi_{i}=\\phi_{i} D^{-\\frac{1}{2}}$ the harmonic eigenvectors of $\\mathcal{L}$.\nEmploying the Rayleigh quotient, we see that the eigenvalue $\\lambda_{1}$ can be written as\n$$ \\lambda_{1}=\\inf_{\\substack{f \\\\ \\sum_{x} f(x) d_{x}=0}} R(f) . $$\nSince the eigenvector associated with $\\lambda_{0}$ is $\\phi_{0}=1 D^{\\frac{1}{2}}$, the condition $\\sum_{x} f(x) d_{x}=0$ is an orthogonality condition. Such variational characterizations can also be made for the other eigenvalues:\n$$ \\lambda_{n-1}=\\sup _{f} R(f) $$\nand, in general, $$ \\lambda_{i}=\\sup_{h_{0}, h_{1}, \\ldots, h_{i-1}} \\inf_{\\substack{f: \\\\ \\sum_{x} f(x) h_{j}(x) d_{x}=0 \\\\ \\forall j \\in{0, \\ldots, i-1}}} R(f) $$ The following characterization of the Rayleigh quotient (demonstrated last time) will be useful later: $$ R(f)=\\frac{\\sum_{x \\sim y}(f(x)-f(y))^{2}}{\\sum_{x} f^{2}(x) d_{x}} . $$\nTo this point, we have done a lot of linear algebra. We are not here to teach linear algebra; we are here to take linear algebra one step further to understand what is happening in the graph.\nThe Cheeger Ratio and The Cheeger Constant In many areas of mathematics the questions of \u0026ldquo;best\u0026rdquo; comes into play. What is the best bound for a given constant? What is the best way of row reducing a certain matrix? In this section, we will describe a way to make the \u0026ldquo;best possible cut\u0026rdquo; of a graph $G=(V, E)$, where a cut may be either an edge-cut or a vertex-cut, and this cut will split $G$ into two disconnected pieces.\nWe would like a way to measure the quality of a cut that is made to $G$. That is, would it be better to cut 4 edges which cause us to lose 20 vertices, or is it better to cut 10 edges which would result in the removal of 120 vetices?\nSuppose we are given a graph $G=(V, E)$ and a subset $S \\subseteq V$. We wish to define the folling two sets:\n$$ \\partial S={{u, v} \\mid u \\in S, v \\notin S} $$\nand\n$$ \\delta S={v \\notin S \\mid v \\sim u, u \\in S} . $$\nDefinition 1 For any vertex set $W$, the volume of $W$ is given by\n$$ \\operatorname{vol}(W)=\\sum_{x W} d_{x}, $$\nwhere $d_{x}$ is the degree of $\\mathrm{x}$ in $W$.\nDefinition 2 The Cheerger Ratio for a vertex set $S$ is\n$$ h(S)=\\frac{|\\partial S|}{\\min {\\operatorname{vol}(S), \\operatorname{vol}(\\bar{S})}}, $$\nwhere $\\bar{S}=V-S$.\nIt is first worth noting that in terms of this defintion of the Cheeger ratio, we are gauging the quality of our cut by taking a measure of what\u0026rsquo;s been cut off of $G$. There are other forms of the Cheeger ratio as well. For example, we can use $|\\delta S|$ instead of $|\\partial S|,|S|($ or $\\bar{S})$ instead of $\\operatorname{vol}(S)$ (or $\\operatorname{vol}(\\bar{S}))$, or $|S||\\bar{S}|$ instead of $\\min {\\operatorname{vol}(S), \\operatorname{vol}(\\bar{S})}$.\nDefinition 3 For any graph $G=(V, E)$, the Cheeger Constant of $G$ is given by\n$$ h_{G}=\\min_S h(S) . $$\nNow, if we consider the case where $\\operatorname{vol}(S) \\leq \\frac{1}{2} \\operatorname{vol}(G)$, then we can see that\n$$ |\\partial S| \\geq h_{G}(\\operatorname{vol}(S)) . $$\nThe Cheeger Inequality Given a graph $G$, we can define $\\lambda_{1}$ to be the first nontrivial eignevalue of the Laplacian, $\\mathcal{L}$, of $G$.\nFor any graph $G$,\n$$ 2 h_{G} \\geq \\lambda_{1} \\geq \\frac{h_{G}^{2}}{2} $$\nReference https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf\nhttps://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\nhttps://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf\nhttps://mathweb.ucsd.edu/~fan/teach/262/notes/paul/10_5_notes.pdf\n","permalink":"https://JhuoW.github.io/posts/laplacian/","summary":"Introduction The spectral graph theory studies the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices: the adjacency matrix and the graph Laplacian and its variants. The Laplacian allows a natural link between discrete representations, such as graphs, and continuous representations, such as vector spaces and manifolds. The most important application of the Laplacian is spectral clustering that corresponds to a computationally tractable solution to the graph partitionning problem.","title":"Everything about Graph Laplacian"},{"content":"Paper\nIntroduction 类别不均衡（Class Imbalance）是真实场景中非常常见的问题。一般在我们提及类别不均衡时，默认指的是数量不均衡：即不同类中训练样本数量的不一致带来的模型于不同类别学习能力的差异，由此引起的一个严重问题是模型的决策边界会主要由数量多的类来决定 。\n但是在图结构中，不同类别的训练样本不仅有在数量上的差异，也有在位置结构上的差异.这就使得图上的类别不均衡问题有了一个独特的来源：拓扑不均衡。这个工作最主要的动机就是研究拓扑不均衡的特点，危害以及解决方法，希望能够引起社区对拓扑不均衡问题的重视。\n本文提出Topology-Imbalance Node Representation Learning （TINL）, 主要关注拓扑不平衡导致的决策边界漂移。所谓拓扑不平衡值得是， labeled nodes的位置如果位于拓扑中的决策边界，那么会传播错误的影响。 如上图所示，颜色和色调分别表示节点从labeled node接收到的influence类型和强度，节点R1位于两类节点的拓扑边界，第一张图可以看出，两个$\\mathbf{x}$节点面临influence conflict问题，两个$\\mathbf{Y}$节点由于远离R2，面临影响力不足的问题。也就是，如果决策便捷有labeled node（如R1）, 那么他的影响力很容易传播给另一个类的边界unlabeled节点，导致影响力冲突，从而分类错误。 而冲突较小的labeled node更可能位于类的拓扑中心（如R2）,顾增加其权重，是的它在训练过程中发挥更大作用。\nUnderstanding Topology Imbalance via Label Propagation Label Propagation中，labels从labeled node延边传播， 看做label从labeled node开始的随机游走过程。LP最终收敛状态可以认为每个节点的soft-labels: $$ \\boldsymbol{Y}=\\alpha\\left(\\boldsymbol{I}-(1-\\alpha) \\boldsymbol{A}^{\\prime}\\right)^{-1} \\boldsymbol{Y}^{0} $$ 其中$\\boldsymbol{A}^{\\prime}=\\boldsymbol{D}^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$，其实就是PageRank的极限分布， $\\boldsymbol{Y}^{0}$为每个节点的初始one-hot label。 第$i$个节点的预测结果为$\\boldsymbol{q}_{i}=\\arg \\max _{j} \\boldsymbol{Y}_{i j}$，每个节点的预测向量反映了每个节点主要受哪个类的影响。图(a)反映了GCN与LP的预测一致性，所以LP的节点影响力边界可以作为GNN的决策边界。理想状态下，labeled node的影响力边界应与真实类边界一致，例如红色的labeled node 在LP下所传播的影响力范围，应与所有红色node的范围一致。但是如图(b)所示，蓝色的labeled node如果较多位于真实类边界，这些位于边界的节点也会传播影响力，从而导致位于边界的真是红色节点被预测为蓝色，预测边界向红色类偏移。\nMeasuring Topology Imbalance by Influence Conflict 可以看出，位于决策边界的labeled node 会不可避免的将影响力传播到其他类节点，因此需要衡量labeled node与其所属类的相对拓扑位置（位于类边缘还是中心）。由于Homophily， 位于类边缘的节点也具有和其邻居相似的性质，因此利用邻域特征差别来判断labeled node是否位于边缘是不可靠的。因此本文利用整个图中的节点影响力冲突，提出基于冲突检测的拓扑相对位置Conflict Detection-based Topology Relative Location metric (Totoro).\nPersonalized PageRank矩阵定义为： $$ \\boldsymbol{P} = \\alpha\\left(\\boldsymbol{I}-(1-\\alpha) \\boldsymbol{A}^{\\prime}\\right)^{-1} $$ $\\boldsymbol{P}_{ij} = \\boldsymbol{P}(j \\to^\\infty i)$， 可以用来反映拓扑中节点$i$对节点$j$的影响力（随机游走越有可能到达的两个节点，在拓扑中的越能相互影响）。\nNode influence conflict denotes topological position. $\\boldsymbol{P}$可以看做每个节点向外施加影响力的分布。 如果一个labeled node $v$ 在周围子图中受到了来自其他类中的labeled node的异质影响，而$v$本身也具有较大的影响力，那么可以认为$v$具有较大影响力冲突，他更可能位于所在类的拓扑边界。\n基于上述假设，本文将 从节点$v$开始在图上随机游走时， 节点$v$与其他类的labeled nodes之间的影响力冲突的期望作为节点$v$与其所在类的类中心的接近程度的度量。labeled node $v$ 的Totoro值定义如下： $$ \\boldsymbol{T}_{v}=\\mathbb{E}_{x \\sim \\boldsymbol{P}_{v, :}}\\left[\\sum_{j \\in[1, k], j \\neq \\boldsymbol{y}_{v}} \\frac{1}{\\left|\\mathcal{C}_{j}\\right|} \\sum_{i \\in \\mathcal{C}_{j}} \\boldsymbol{P}_{i, x}\\right] $$ 其中， $\\mathbb{E}_{x \\sim \\boldsymbol{P}_{v, :}}$： $x$节点受$v$的影响程度，$\\sum_{j \\in[1, k], j \\neq \\boldsymbol{y}_{v}}$表示其他所有类（不包括$v$所在的类）。 $\\frac{1}{\\left|\\mathcal{C}_{j}\\right|} \\sum_{i \\in \\mathcal{C}_{j}} \\boldsymbol{P}_{i, x}$表示类$\\mathcal{C}_{j}$中的labeled node对$x$的平均影响。 $\\boldsymbol{T}_{v}$越大，表示labeled node $v$对$x$的影响力很大，而且其他类的labeled node 对$x$的影响也很大，那么可以认为$v$越接近类边界。\n整个数据集的conflict可以表示为所有labeled node 的Totoro value之和：$\\sum_{v \\in \\mathcal{L}} \\boldsymbol{T}_{v}$\nNode Re-weighting Preliminary 余弦退火： $$ \\eta_{t}=\\eta_{\\min }^{i}+\\frac{1}{2}\\left(\\eta_{\\max }^{i}-\\eta_{\\min }^{i}\\right)\\left(1+\\cos \\left(\\frac{T_{\\text {cur }}}{T_{i}} \\pi\\right)\\right) $$ $\\eta_{\\min }$: 最小学习率\n$\\eta_{\\max }$: 最大学习率\n$T_{\\text {cur }}$: 当前执行多少个epoch\n$i$: 第$i$次迭代\nReNode 本文提出模型无关的训练权重re-weight 机制：ReNode.\n本文基于余弦退货算法来为训练节点（labeled nodes）加权： $$ \\boldsymbol{w}_{v}=w_{\\min }+\\frac{1}{2}\\left(w_{\\max }-w_{\\min }\\right)\\left(1+\\cos \\left(\\frac{\\operatorname{Rank}\\left(\\boldsymbol{T}_{v}\\right)}{|\\mathcal{L}|} \\pi\\right)\\right), \\quad v \\in \\mathcal{L} $$ 上式中$\\boldsymbol{T}_{v}$越大(越接近决策边界)，在所有labeled node $v \\in \\mathcal{L}$的排名越高，$\\operatorname{Rank}\\left(\\boldsymbol{T}_{v}\\right)$越大，$\\boldsymbol{w}_{v}$越小，越接近$w_{\\min }$。\n最终，对于一个quantity-balanced，topology-imbalanced (class labeled node 数量是平衡的，但拓扑不平衡) node classification task, the training loss $L_T$ is computed by: $$ L_{T}=-\\frac{1}{|\\mathcal{L}|} \\sum_{v \\in \\mathcal{L}} \\boldsymbol{w}_{v} \\sum_{c=1}^{k} \\boldsymbol{y}_{v}^{* c} \\log \\boldsymbol{g}_{v}^{c}, \\quad \\boldsymbol{g}=\\operatorname{softmax}(\\mathcal{F}(\\boldsymbol{X}, \\boldsymbol{A}, \\boldsymbol{\\theta})) $$ 其中$\\mathcal{F}$是任意GNN encoder,$g_i$为GNN对第$i$个节点的output。$\\boldsymbol{y}_{v}^{* c} \\log \\boldsymbol{g}_{v}^{c}$为cross-entropy。 对于每个training labeled node，计算它的CE loss时，用这个节点的权重为loss加权，说明越靠近决策边界的节点，他的损失权重尽可能小，意味着model倾向于把它当做一个unlabeled node，它的损失对于总损失贡献较小。\nReNode to Jointly Handle TINL and QINL 若要同时处理数量不平衡且拓扑不平衡问题， loss定义如下： $$ L_{Q}=-\\frac{1}{|\\mathcal{L}|} \\sum_{v \\in \\mathcal{L}} \\boldsymbol{w}_{v} \\frac{|\\overline{\\mathcal{C}}|}{\\left|\\mathcal{C}_{j}\\right|} \\sum_{c=1}^{k} \\boldsymbol{y}_{v}^{* c} \\log \\boldsymbol{g}_{v}^{c} $$ 与$L_T$的不同就是多了对类的加权，若labeled node所在的类 training node较少，那么增加权重。同时，接近拓扑边界的节点权重降低。\n","permalink":"https://JhuoW.github.io/posts/2022-04-02-tinl/","summary":"Paper\nIntroduction 类别不均衡（Class Imbalance）是真实场景中非常常见的问题。一般在我们提及类别不均衡时，默认指的是数量不均衡：即不同类中训练样本数量的不一致带来的模型于不同类别学习能力的差异，由此引起的一个严重问题是模型的决策边界会主要由数量多的类来决定 。\n但是在图结构中，不同类别的训练样本不仅有在数量上的差异，也有在位置结构上的差异.这就使得图上的类别不均衡问题有了一个独特的来源：拓扑不均衡。这个工作最主要的动机就是研究拓扑不均衡的特点，危害以及解决方法，希望能够引起社区对拓扑不均衡问题的重视。\n本文提出Topology-Imbalance Node Representation Learning （TINL）, 主要关注拓扑不平衡导致的决策边界漂移。所谓拓扑不平衡值得是， labeled nodes的位置如果位于拓扑中的决策边界，那么会传播错误的影响。 如上图所示，颜色和色调分别表示节点从labeled node接收到的influence类型和强度，节点R1位于两类节点的拓扑边界，第一张图可以看出，两个$\\mathbf{x}$节点面临influence conflict问题，两个$\\mathbf{Y}$节点由于远离R2，面临影响力不足的问题。也就是，如果决策便捷有labeled node（如R1）, 那么他的影响力很容易传播给另一个类的边界unlabeled节点，导致影响力冲突，从而分类错误。 而冲突较小的labeled node更可能位于类的拓扑中心（如R2）,顾增加其权重，是的它在训练过程中发挥更大作用。\nUnderstanding Topology Imbalance via Label Propagation Label Propagation中，labels从labeled node延边传播， 看做label从labeled node开始的随机游走过程。LP最终收敛状态可以认为每个节点的soft-labels: $$ \\boldsymbol{Y}=\\alpha\\left(\\boldsymbol{I}-(1-\\alpha) \\boldsymbol{A}^{\\prime}\\right)^{-1} \\boldsymbol{Y}^{0} $$ 其中$\\boldsymbol{A}^{\\prime}=\\boldsymbol{D}^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$，其实就是PageRank的极限分布， $\\boldsymbol{Y}^{0}$为每个节点的初始one-hot label。 第$i$个节点的预测结果为$\\boldsymbol{q}_{i}=\\arg \\max _{j} \\boldsymbol{Y}_{i j}$，每个节点的预测向量反映了每个节点主要受哪个类的影响。图(a)反映了GCN与LP的预测一致性，所以LP的节点影响力边界可以作为GNN的决策边界。理想状态下，labeled node的影响力边界应与真实类边界一致，例如红色的labeled node 在LP下所传播的影响力范围，应与所有红色node的范围一致。但是如图(b)所示，蓝色的labeled node如果较多位于真实类边界，这些位于边界的节点也会传播影响力，从而导致位于边界的真是红色节点被预测为蓝色，预测边界向红色类偏移。\nMeasuring Topology Imbalance by Influence Conflict 可以看出，位于决策边界的labeled node 会不可避免的将影响力传播到其他类节点，因此需要衡量labeled node与其所属类的相对拓扑位置（位于类边缘还是中心）。由于Homophily， 位于类边缘的节点也具有和其邻居相似的性质，因此利用邻域特征差别来判断labeled node是否位于边缘是不可靠的。因此本文利用整个图中的节点影响力冲突，提出基于冲突检测的拓扑相对位置Conflict Detection-based Topology Relative Location metric (Totoro).\nPersonalized PageRank矩阵定义为： $$ \\boldsymbol{P} = \\alpha\\left(\\boldsymbol{I}-(1-\\alpha) \\boldsymbol{A}^{\\prime}\\right)^{-1} $$ $\\boldsymbol{P}_{ij} = \\boldsymbol{P}(j \\to^\\infty i)$， 可以用来反映拓扑中节点$i$对节点$j$的影响力（随机游走越有可能到达的两个节点，在拓扑中的越能相互影响）。","title":"NeurIPS2021 《Topology-Imbalance Learning for Semi-Supervised Node Classification》 Reading Notes"},{"content":"最大似然估计MLE 数据 $X = \\{x_1, \\cdots x_N\\}$, 模型参数为$\\theta$，Likelihood 定义为 $P(X | \\theta)$：当参数为$\\theta$时，观测到给定数据$X$的概率。 $$ P(X|\\theta) = L(\\theta | X) = P_\\theta(X) \\tag{1} $$ 最大似然估计 （Maximum Likelihood Estimation, MLE）: $$ \\theta_{\\mathrm{MLE}} = \\arg \\max_\\theta P(X|\\theta) \\tag{2} $$\n 最大似然估计：给定一组样本$X$，模型的参数$\\theta$是研究对象。若能找到参数$\\theta_{\\mathrm{MLE}}$，使得样本发生的可能性最大，则此估计值$\\theta_{\\mathrm{MLE}}$为参数$\\theta$的最大似然估计。\n 举例来说，如果模型是单个Gaussian Distribution下，参数为Gaussian Distribution的参数（均值$\\mu$, 标准差$\\Sigma$， $\\theta = {\\mu, \\Sigma}$）. 给定一组数据$X$, 要计算$X$来自什么样的Gaussian，即：$P(\\cdot | \\theta) = f_\\theta(\\cdot) = \\mathcal{N}(\\cdot | \\mu,\\Sigma)$是一个Gaussian Distribution函数，目标为： $$ \\theta_{\\mathrm{MLE}} = \\mu^\\star, \\Sigma^\\star = \\arg \\max_{\\mu,\\Sigma} \\sum^N_{i = 1} \\log \\mathcal{N}(x_i|\\mu,\\Sigma) \\tag{3} $$ 即MLE的目标是找到最佳的高斯分布，是的从该分布中采样出数据$X$的概率最高。\n如果只需要用一个Gaussian来拟合$X$的分布的话，这个Gaussian可以很容易用求导的方式获得$\\theta_{\\mathrm{MLE}}$的解析解： 对$\\mu$求导：$\\frac{\\partial P(X|\\mu,\\Sigma)}{\\partial \\mu}$；对$\\Sigma$求导：$\\frac{\\partial P(X|\\mu,\\Sigma)}{\\partial \\Sigma}$，令导数为0，即可求得最佳的$\\mu$，$\\Sigma$，使得对应的高斯分布符合数据$X = {x_1, \\cdots x_N}$的分布。\n但是，要用更复杂的模型（更多参数）来更准确的拟合$X$的分布，例如Gaussian Mixture Model，即多个Gaussian的组合，其模型参数为： $$ \\theta = \\{\\underbrace{\\mu_1, \\cdots,\\mu_K}_{\\text{每个Gaussian的 mean参数}}, \\underbrace{\\Sigma_1,\\cdots, \\Sigma_K}_{\\text{每个Gaussian的 std参数}}, \\underbrace{\\alpha_1, \\cdots, \\alpha_{K-1}}_{\\text{每个Gaussian的权重}} \\} \\tag{4} $$ 假设是一个$K$个Gaussian的Gaussian Mixture Model，那么$\\sum^K_{k = 1}\\alpha_k = 1$。\n给定数据$X = \\{x_1, \\cdots x_N\\}$，若要用$K$维Gaussian Mixture Model来拟合该数据，就要优化所有$K$个Gaussian的均值参数，标准差参数，和权重参数，使得混合高斯分布采样出$X$的概率最大，即： $$ \\begin{aligned} \\theta_{\\mathrm{MLE}} \u0026amp;= \\mu_1^\\star,\\cdots \\mu_K^\\star,\\Sigma_1^\\star, \\cdots, \\Sigma_K^\\star, \\alpha^\\star_1,\\cdots,\\alpha^\\star_{K-1} \\\\ \u0026amp;=\\underset{\\theta}{\\arg\\max} \\sum^N_{i = 1} \\log \\sum^K_{k=1} \\alpha_k \\mathcal{N}(x_i|\\mu_k,\\Sigma_k) \\end{aligned} \\tag{5} $$ 如果要得到上式的解析解，要对$\\mu_1,\\cdots,\\mu_K, \\Sigma_1, \\cdots,\\Sigma_K, \\alpha_1, \\cdots, \\alpha_{K-1}$求导，再令导数为0来求解，这非常困难，由此引出EM算法。\n期望最大算法 求解MLE问题时，在最大化log-likelihood: $$ \\theta_{\\mathrm{MLE}} = \\arg \\max_\\theta \\log P(X|\\theta) \\tag{6} $$ 难以直接对$\\theta$求导来得到解析解时（如高斯混合模型情况），可以使用EM算法来迭代求解： $$ \\theta^{(t+1)}=\\underset{\\theta}{\\arg \\max} \\int_z \\log P(X,z | \\theta) \\cdot P(z|X,\\theta^{(t)}) dz \\tag{7} $$ $X$为观测数据， $z$为latent variables（隐变量），隐变量必须不会影响$X$的边缘分布,即 $P(X) = \\int_z P(X|z) P(z) dz$。\n而公式(7)中 $$ \\begin{aligned} \u0026amp;\\int_z \\underbrace{\\log P(X,z | \\theta)}_{\\text{每个z对应的值}} \\cdot \\underbrace{P(z|X,\\theta^{(t)})}_{\\text{z的分布}} dz\\\\ =\u0026amp;\\mathbb{E}_{P(z|X,\\theta^{(t)})} \\left[\\log P(X,z | \\theta)\\right] \\end{aligned} \\tag{8} $$ 所以，期望最大化算法求参数$\\theta$的迭代公式可改写为: $$ \\theta^{(t+1)}=\\arg \\max_\\theta \\mathbb{E}_{P(z|X,\\theta^{(t)})} \\left[\\log P(X,z | \\theta)\\right] \\quad \\text{期望最大化} \\tag{9} $$\n其中$P(z|X,\\theta^{(t)})$为后验分布posterior。\nEM算法收敛性证明 因为EM算法通过迭代的方式优化模型参数$\\theta$，使得对数似然$\\log P(X|\\theta)$最大。通过公式(7)，可以保证在$\\theta^{(t+1)}$参数下的模型比$\\theta^{(t)}$参数下的模型更拟合数据分布。通过公式(7)迭代更新参数$\\theta^{(t)} \\to \\theta^{(t+1)}$，可以使得$\\log P(X|\\theta)$变大。收敛性即证明： $$ \\log P(X|\\theta^{(t)}) \\leq \\log P(X|\\theta^{(t+1)}) \\tag{10} $$ 证明. $$ \\begin{aligned} \u0026amp;\\because P(X,z) = P(z|X) P(X)\\quad \\text{always true}, \\text{then}\\quad P(X) = \\frac{P(X,z)}{P(z|X)} \\\\ \u0026amp;\\therefore P(X|\\theta) = \\frac{P(X,z|\\theta)}{P(z|X,\\theta)} \\\\ \u0026amp;\\therefore \\log P(X|\\theta) = \\log P(X,z|\\theta) - \\log P(z|X,\\theta) \\end{aligned} \\tag{11} $$ 上式左右两边对分布$P(z|X,\\theta^{(t)})$求期望： $$ \\mathbb{E}_{z \\sim P(z|X,\\theta^{(t)})} \\underbrace{\\left[\\log P(X|\\theta)\\right]}_{\\text{与z无关}} = \\mathbb{E}_{z \\sim P(z|X,\\theta^{(t)})} \\left[\\log P(X,z|\\theta) - \\log P(z|X,\\theta)\\right] \\tag{12} $$ 上式左边$=\\log P(X|\\theta)$，右边： $$ \\begin{aligned} \u0026amp;\\mathbb{E}_{P(z|X,\\theta^{(t)})} \\left[\\log P(X,z|\\theta) - \\log P(z|X,\\theta)\\right] \\\\ =\u0026amp; \\underbrace{\\int_{z} P(z|X,\\theta^{(t)}) \\log P(X,z|\\theta) dz}_{Q(\\theta,\\theta^{(t)})} - \\underbrace{\\int_z P(z|X,\\theta^{(t)}) \\log P(z|X,\\theta) dz}_{H(\\theta,\\theta^{(t)})} \\end{aligned} \\tag{13} $$ 注意到$Q(\\theta,\\theta^{(t)}) = \\int_{z} P(z|X,\\theta^{(t)}) \\log P(X,z|\\theta) dz$ 就是EM算法的迭代更新函数，即$\\theta^{(t+1)} = \\arg \\max_\\theta Q(\\theta,\\theta^{(t)})$。结合公式(12)和公式(13)： $$ \\log P(X|\\theta) = Q(\\theta,\\theta^{(t)}) - H(\\theta,\\theta^{(t)}) \\tag{14} $$\n 因此log-likelihood under $\\theta^{(t)}$ and $\\theta^{(t+1)}$： $$ \\begin{aligned} \\log P(X|\\theta^{(t+1)}) \u0026amp;= Q(\\theta^{(t+1)},\\theta^{(t)}) - H(\\theta^{(t+1)},\\theta^{(t)}) \\\\ \\log P(X|\\theta^{(t)}) \u0026amp;= Q(\\theta^{(t)},\\theta^{(t)}) - H(\\theta^{(t)},\\theta^{(t)}) \\end{aligned} \\tag{15} $$\n 首先，根据EM的迭代求解公式，$\\theta^{(t+1)}$由 $$ \\theta^{(t+1)} = \\arg \\max_\\theta Q(\\theta,\\theta^{(t)}) \\tag{16} $$ 得到，所以$Q(\\theta^{(t+1)},\\theta^{(t)}) \\geq Q(\\theta,\\theta^{(t)})$一定成立。所以下式成立： $$ Q(\\theta^{(t+1)},\\theta^{(t)}) \\geq Q(\\theta^{(t)},\\theta^{(t)}) \\tag{17} $$ 对于$H(\\theta,\\theta^{(t+1)})$，首先介绍Jensen Inequality:\n Jensen Inequality：\nIf $g(x)$ is a convex function on $R_X$, and $\\mathbb{E}[g(x)]$ and $g(\\mathbb{E}[X])$ are finite, then $\\mathbb{E}[g(x)] \\geq g(\\mathbb{E}[X])$。\n显然$\\log$是concave，所以$\\mathbb{E}[\\log(\\cdot)]\\leq \\log(\\mathbb{E}[\\cdot])$。同理$-\\log$是convex，所以$\\mathbb{E}[-\\log(\\cdot)]\\geq -\\log(\\mathbb{E}[\\cdot])$。\n 下面，计算$H(\\theta^{(t)},\\theta^{(t)})-H(\\theta,\\theta^{(t)})$： $$ \\begin{aligned} \u0026amp;H(\\theta,\\theta^{(t)}) = \\int_z P(z|X,\\theta^{(t)}) \\log P(z|X,\\theta) dz \\\\ \u0026amp;H(\\theta^{(t)},\\theta^{(t)})-H(\\theta,\\theta^{(t)})\\\\ =\u0026amp;\\int_z P(z|X,\\theta^{(t)}) \\log P(z|X,\\theta^{(t)}) dz - \\int_z P(z|X,\\theta^{(t)}) \\log P(z|X,\\theta) dz \\\\ =\u0026amp; \\underbrace{\\int_z P(z|X,\\theta^{(t)}) \\log \\frac{P(z|X,\\theta^{(t)})}{P(z|X,\\theta)} dz}_{\\mathrm{KL}(P(z|X,\\theta^{(t)})|P(z|X,\\theta))}\\\\ =\u0026amp; -\\int_z P(z|X,\\theta^{(t)}) \\log \\frac{P(z|X,\\theta)}{P(z|X,\\theta^{(t)})} dz \\\\ =\u0026amp; \\mathbb{E}_{P(z|X,\\theta^{(t)})} \\left[-\\log \\frac{P(z|X,\\theta)}{P(z|X,\\theta^{(t)})}\\right]\\\\ \\geq \u0026amp; -\\log \\mathbb{E}_{P(z|X,\\theta^{(t)})} \\left[\\frac{P(z|X,\\theta)}{P(z|X,\\theta^{(t)})}\\right] \\\\ =\u0026amp; -\\log \\int_z \\frac{P(z|X,\\theta)}{P(z|X,\\theta^{(t)})} \\cdot P(z|X,\\theta^{(t)}) dz \\\\ =\u0026amp; -\\log \\int_z P(z|X,\\theta) dz \\\\ =\u0026amp; - \\log 1 \\\\ =\u0026amp; 0 \\end{aligned}\\tag{18} $$ 因此，下式成立： $$ \\begin{aligned} \u0026amp;\\therefore H(\\theta^{(t)},\\theta^{(t)})\\geq H(\\theta,\\theta^{(t)}) \\\\ \u0026amp;\\therefore H(\\theta^{(t+1)},\\theta^{(t)}) \\leq H(\\theta^{(t)},\\theta^{(t)}) \\\\ \u0026amp;\\because Q(\\theta^{(t+1)},\\theta^{(t)}) \\geq Q(\\theta^{(t)},\\theta^{(t)})\\\\ \u0026amp;\\therefore Q(\\theta^{(t+1)},\\theta^{(t)}) - H(\\theta^{(t+1)},\\theta^{(t)}) \\geq Q(\\theta^{(t)},\\theta^{(t)}) - H(\\theta^{(t)},\\theta^{(t)}) \\\\ \u0026amp;\\therefore \\log P(X|\\theta^{(t+1)}) \\geq \\log P(X|\\theta^{(t)}) \\end{aligned} \\tag{19} $$ 所以通过EM算法的迭代得到新的$\\theta^{(t+1)}$增大likelihood，使得模型更加拟合数据。\nEM算法公式推导 EM算法Maximize Likelihood Estimation迭代公式： $$ \\begin{aligned} \\theta^{(t+1)}\u0026amp;=\\underset{\\theta}{\\arg \\max} \\int_z \\log P(X,z | \\theta) \\cdot P(z|X,\\theta^{(t)}) dz\\\\ \u0026amp;=\\arg \\max_\\theta \\mathbb{E}_{P(z|X,\\theta^{(t)})} \\left[\\log P(X,z | \\theta)\\right]\n\\end{aligned} \\tag{20} $$\n E-Step： $\\mathbb{E}_{P(z|X,\\theta^{(t)})} \\left[\\log P(X,z | \\theta)\\right]$ M-Step：$\\arg \\max_\\theta \\mathbb{E}_{P(z|X,\\theta^{(t)})} \\left[\\log P(X,z | \\theta)\\right]$  其中，$X$是观测数据，$z$是隐变量，$(X,z)$为完整数据，$\\theta$为待优化模型参数，$P(\\cdot|X)$为后验。\n上一节通过收敛性证明，验证了上式每次迭代都朝着最大化log-likelihood的方向。本节推导EM的迭代公式。\n公式(11)中得到： $$ \\log P(X|\\theta) = \\log P(X,z|\\theta) - \\log P(z|X,\\theta) \\tag{21} $$ 引入一个关于隐变量$z$的分布$q(z)$，可以定义为任意关于$z$的非0分布。上式可以改写为： $$ \\log P(X|\\theta) = \\log \\frac{P(X,z|\\theta)}{q(z)} - \\log\\frac{ P(z|X,\\theta)}{q(z)} \\tag{22} $$ 左右两边对$q(z)$求期望： $$ \\text{左边} = \\mathbb{E}_{q(z)} \\log P(X|\\theta) = \\int_z q(z) \\log P(X|\\theta) dz = \\log P(X|\\theta) \\underbrace{\\int_z q(z) dz}_{=1} = \\log P(X|\\theta) \\tag{23} $$\n$$ \\begin{aligned} \\text{右边}\u0026amp;=\\mathbb{E}_{q(z)} \\left[\\log \\frac{P(X,z|\\theta)}{q(z)} - \\log\\frac{ P(z|X,\\theta)}{q(z)}\\right] \\\\ \u0026amp;= \\underbrace{\\int_z q(z) \\log \\frac{P(X,z|\\theta)}{q(z)} dz}_{ELBO=\\text{Evidence Lower Bound}} \\underbrace{- \\int_z q(z) \\log\\frac{ P(z|X,\\theta)}{q(z)} dz}_{\\mathrm{KL}(q(z)||P(z|X,\\theta))} \\end{aligned} \\tag{24} $$\n所以 $$ \\log P(X|\\theta) = ELBO + \\mathrm{KL}(q(z)||P(z|X,\\theta)) \\tag{25} $$ 其中$P(z|X,\\theta)$为后验（posterior）。而$\\mathrm{KL}(q(z)||P(z|X,\\theta)) \\geq 0$, 当分布$q(z) = P(z|X,\\theta)$时，等号成立。所以 $$ \\log P(X|\\theta) \\geq ELBO = \\int_z q(z) \\log \\frac{P(X,z|\\theta)}{q(z)} dz \\tag{26} $$ 因此，最大化log-likelihood $\\log P(X|\\theta)$问题可以转化为最大化$\\log P(X|\\theta)$的下界ELBO，即： $$ \\hat{\\theta} = \\arg \\max_\\theta \\log P(X|\\theta) \\Longleftrightarrow \\hat{\\theta} = \\arg \\max_\\theta ELBO \\tag{27} $$\n$$ \\begin{aligned} \\hat{\\theta} \u0026amp;= \\arg \\max_\\theta ELBO \\\\ \u0026amp; = \\arg \\max_\\theta \\int_z q(z) \\log \\frac{P(X,z|\\theta)}{q(z)} dz \\quad \\text{令关于}z\\text{的分布}q(z) = P(z|X,\\theta^{(t)})\\\\ \u0026amp;= \\arg \\max_\\theta \\int_z P(z|X,\\theta^{(t)}) \\left[\\log P(X,z|\\theta) - \\underbrace{P(z|X,\\theta^{(t)})}_{\\text{与}\\theta \\text{无关，去掉不影响结果}}\\right] dz \\\\ \u0026amp;= \\arg \\max_\\theta \\int_z P(z|X,\\theta^{(t)}) \\log P(X,z|\\theta) dz\\\\ \u0026amp;= \\text{公式(7)} \\end{aligned} \\tag{28} $$\n我把本文整理成了PDF\n参考 https://youtube.com/playlist?list=PLOxMGJ_8X74bhcPbpiX642NIfPlkpD1BC\nhttps://zhuanlan.zhihu.com/p/78311644\n","permalink":"https://JhuoW.github.io/posts/em-algo/","summary":"最大似然估计MLE 数据 $X = \\{x_1, \\cdots x_N\\}$, 模型参数为$\\theta$，Likelihood 定义为 $P(X | \\theta)$：当参数为$\\theta$时，观测到给定数据$X$的概率。 $$ P(X|\\theta) = L(\\theta | X) = P_\\theta(X) \\tag{1} $$ 最大似然估计 （Maximum Likelihood Estimation, MLE）: $$ \\theta_{\\mathrm{MLE}} = \\arg \\max_\\theta P(X|\\theta) \\tag{2} $$\n 最大似然估计：给定一组样本$X$，模型的参数$\\theta$是研究对象。若能找到参数$\\theta_{\\mathrm{MLE}}$，使得样本发生的可能性最大，则此估计值$\\theta_{\\mathrm{MLE}}$为参数$\\theta$的最大似然估计。\n 举例来说，如果模型是单个Gaussian Distribution下，参数为Gaussian Distribution的参数（均值$\\mu$, 标准差$\\Sigma$， $\\theta = {\\mu, \\Sigma}$）. 给定一组数据$X$, 要计算$X$来自什么样的Gaussian，即：$P(\\cdot | \\theta) = f_\\theta(\\cdot) = \\mathcal{N}(\\cdot | \\mu,\\Sigma)$是一个Gaussian Distribution函数，目标为： $$ \\theta_{\\mathrm{MLE}} = \\mu^\\star, \\Sigma^\\star = \\arg \\max_{\\mu,\\Sigma} \\sum^N_{i = 1} \\log \\mathcal{N}(x_i|\\mu,\\Sigma) \\tag{3} $$ 即MLE的目标是找到最佳的高斯分布，是的从该分布中采样出数据$X$的概率最高。","title":"Expectation Maximization"},{"content":"Paper\nIntroduction GNN的neighborhood aggregation中会引入邻域中任务无关的邻居，所以要移除图中有大量任务无关的边，顾本文提出NeuralSparse来解决该问题。\n在构造数据集时，两个节点连接的动机可能与拿到这张图后要进行的下游任务无关，例如下游任务是节点分类，那么和这个任务相关的连接应是同类节点间产生边。但是构造图数据集是的两个节点连接的动机可能是特征相似的节点（不一定同类）。\n下图给出一个例子，Blue 和 Red节点分别采样自两个独立的二维Gaussian distribution, 图1(a)显示两类节点的原始features有大量重合。对于每个节点，随机抽取10个其他节点作为他的邻居，这样生成的图（图1(b) )中的边和node label没有任何关系，即所有边都与节点分类任务无关。在这个图上做GCN得到(图1(b)下方)的node embedding，可以看到GCN学到的node embedding在任务无关的边上，无法区分两类节点。 图1(c)是DropEdge学到的node embedding，图1(d)为本文的NeuralSparse学到的node embedding。\nPresent work：根据监督信号来选择任务相关的边。 由sparsification network 和GNN两个模块组成. sparsification network旨在参数化稀疏过程，即在给定预算下， 为每个节点选择任务相关的一节邻居。在训练阶段，优化稀疏策略，即训练一个可以将图稀疏化的网络。在测试阶段，将测试图输入网络，得到一个稀疏化的图。\nNeuralSparse Theoretical justification 首先从统计学习角度建模问题。将预测任务定义为$P(Y|G)$, 其中$Y$是预测目标，$G$是输入图。本文利用稀疏化子图来移除任务无关的边, 问题形式化为: $$ P(Y \\mid G) \\approx \\sum_{g \\in \\mathbb{S}_{G}} P(Y \\mid g) P(g \\mid G) $$ $g$是一个稀疏化子图，$\\mathbb{S}_{G}$为$G$的稀疏化子图集合。 $P(Y \\mid g)$ 为给定一个稀疏化子图$g$，用$g$过GNN后预测为$Y$的概率。 $$ \\sum_{g \\in \\mathbb{S}_{G}} P(Y \\mid g) P(g \\mid G) \\approx \\sum_{g \\in \\mathbb{S}_{G}} Q_{\\theta}(Y \\mid g) Q_{\\phi}(g \\mid G) $$ 用函数来近似(计算) 分布， 即给定$g$ 预测为$Y$的概率$ P(Y \\mid g)$定义为一个参数为$\\theta$的函数$Q_{\\theta}(Y \\mid g)$, 从$G$中获得子图$g$的概率$P(g \\mid G)$定义为一个参数为$\\phi$的函数$Q_{\\phi}(g \\mid G)$。\n$Q_{\\phi}(g \\mid G)$表示输入$G$, 生成一个子图分布，从分布中采样得到子图$g$的概率， 为了使得分布中采样这个过程可微，本文采用reparameterization tricks,使得： $$ \\sum_{g \\in \\mathbb{S}_{G}} Q_{\\theta}(Y \\mid g) Q_{\\phi}(g \\mid G) \\propto \\sum_{g^{\\prime} \\sim Q_{\\phi}(g \\mid G)} Q_{\\theta}\\left(Y \\mid g^{\\prime}\\right) $$ $g^{\\prime} \\sim Q_{\\phi}(g \\mid G)$表示给定图$G$，生成一个子图分布（每种子图的采样概率）。$\\sum_{g^{\\prime} \\sim Q_{\\phi}(g \\mid G)} Q_{\\theta}\\left(Y \\mid g^{\\prime}\\right)$: 表示从子图分布中采样的子图来预测label $Y$的概率。\nGoal: 1. 找到合适的$Q_{\\phi}(g \\mid G)$， 使得它生成的分布可以采样到最佳的稀疏化子图 ， 即通过优化$\\phi$使得$Q_{\\phi}(g \\mid G)$生成的子图分布中采样到最佳子图的概率是最大的。 2. 找到合适的$Q_{\\theta}(Y \\mid g)$表示优化GNN，使得采样出的$g$可以最好的预测label。\nArchitecture 包含两个模块： sparsification network 和GNNs.\nSparsification Network 目的为输入图生成稀疏化子图，即为每个节点的边生成一个分布，表示边被采样的概率，然后为节点采样边，从而实现采样的系数子图。首先定义所有候选子图。\nk-neighbor subgraphs: 给定输入图$G$，一个$k$-neighbor subgraph和图$G$有相同的节点集，每个节点可以从他的邻居中选择不多于$k$条边。\n理由： 超参数$k$可以用来调整任务相关的图数据量。如果$k$是低估的，那么GNN处理的任务相关数据不足，如果$k$被高估，那么下游GNN会拟合更多无关数据。\nSampling k-neighbor subgraphs：给定$k$和一个图$G=(V, E, \\mathbf{A})$, 以节点$u$为例，令$\\mathbb{N}_u$为$u$的一阶邻居。\n $v \\sim f_{\\phi}\\left(V(u), V\\left(\\mathbb{N}_{u}\\right), \\mathbf{A}(u)\\right)$, 其中，$f_{\\phi}(\\cdot)$是一个函数，输入为节点$u$的节点属性$V(u)$，节点$u$的邻居属性$V\\left(\\mathbb{N}_{u}\\right)$, 和$u$的边属性$\\mathbf{A}(u)$。输出为$u$的邻居分布，$v$从该邻居分布中采样。 比如当前$u$有3个节点，$f_\\phi$生成这三个节点的采样分布[0.1, 0.3, 0.6], 那么从这个分布中随机采样一个节点$v$作为$u$的重构邻居。 采样出的节点$v$作为$u$的重构邻居，即$E(u,v)$作为边保留下来。 重复上述过程$k$次，得到$u$的$k$个重构邻居。  注意，上述采样过程为不放回过程（sampling without replacement），即邻居只能被选择一次， $f_{\\phi}(\\cdot)$对所有节点共享，即一个$f_{\\phi}(\\cdot)$，每个节点都输入它来获得邻居采样分布。\nMaking samples differentiable 为了使样本的采样过程可微，本文采用基于Gumbel-Softmax的NN来实现采样函数$f_{\\phi}(\\cdot)$。\nGumbel-Softmax [1,2] 是一种reparameterization trick，用于以可微的方式生成离散样本。参数$\\tau$越小，生成的连续向量越sharp，越接近one-hot。\n以节点$u$为例，$f_\\phi(\\cdot)$如下：\n  $\\forall v \\in \\mathbb{N}_u$： $$ z_{u, v}=\\operatorname{MLP}_{\\phi}(V(u), V(v), \\mathbf{A}(u, v)) $$\n  $\\forall v \\in \\mathbb{N}_u$，使用softmax来计算边被采样的概率： $$ \\pi_{u, v}=\\frac{\\exp \\left(z_{u, v}\\right)}{\\sum_{w \\in \\mathbb{N}_{u}} \\exp \\left(z_{u, w}\\right)} $$\n  使用Gumbel-Softmax来生成可微样本： $$ x_{u, v}=\\frac{\\exp \\left(\\left(\\log \\left(\\pi_{u, v}\\right)+\\epsilon_{v}\\right) / \\tau\\right)}{\\sum_{w \\in \\mathbb{N}_{u}} \\exp \\left(\\left(\\log \\left(\\pi_{u, w}\\right)+\\epsilon_{w}\\right) / \\tau\\right)} $$\n  其中， $x_{u, v}$是一个scalar，$\\epsilon_{v}=-\\log (-\\log (s))$，$s$从$\\mathrm{Uniform}(0,1)$中采样， $\\tau$是一个temperature超参数，$\\tau$越小，分布$x_u$越接近one-hot。\nAlgorithm 算法如下：\n对所有节点$u \\in \\mathbb{V}$逐个稀疏化： 先遍历$u$的每个邻居$v$, 对于每个$v$ 通过公式$z_{u, v}=\\operatorname{MLP}_{\\phi}(V(u), V(v), \\mathbf{A}(u, v))$ 计算它对于$u$的分数， 然后将$u$的所有邻居$v$的分数用softmax变成概率。\n为$u$做$k$次采样， 每次采样过程如下： 每次采样遍历$u$的所有邻居$v$，根据$x_{u, v}=\\frac{\\exp \\left(\\left(\\log \\left(\\pi_{u, v}\\right)+\\epsilon_{v}\\right) / \\tau\\right)}{\\sum_{w \\in \\mathbb{N}_{u}} \\exp \\left(\\left(\\log \\left(\\pi_{u, w}\\right)+\\epsilon_{w}\\right) / \\tau\\right)}$计算$u$到每个邻居的$x_{u,v}$, 每次迭代产生一个向量$\\left[x_{u, v}\\right]$,用来表示采样出来的边，经过$k$次迭代，产生$k$个表示边的向量，$\\tau$越小，每个向量越接近one-hot。 每个向量$\\left[x_{u, v}\\right]_{v \\in \\mathbb{N}_u}$表示$u$的一个采样邻居，每个$u$有$k$个这样的邻居表示向量，那么网络中的所有边$\\mathbb{H}$就有$|\\mathbb{V}|k$个这样的向量，每个向量表示要保留的一条边，得到稀疏化子图，反向传播时，先更新GNN参数，然后直接对$f_\\phi$的参数求梯度, 如上图所示。\nReference [1] Jang, E., Gu, S., and Poole, B. Categorical reparameteriza- tion with gumbel-softmax. In ICLR, 2017.\n[2] Maddison, C. J., Mnih, A., and Teh, Y. W. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.\n","permalink":"https://JhuoW.github.io/posts/neuralsparse/","summary":"Paper\nIntroduction GNN的neighborhood aggregation中会引入邻域中任务无关的邻居，所以要移除图中有大量任务无关的边，顾本文提出NeuralSparse来解决该问题。\n在构造数据集时，两个节点连接的动机可能与拿到这张图后要进行的下游任务无关，例如下游任务是节点分类，那么和这个任务相关的连接应是同类节点间产生边。但是构造图数据集是的两个节点连接的动机可能是特征相似的节点（不一定同类）。\n下图给出一个例子，Blue 和 Red节点分别采样自两个独立的二维Gaussian distribution, 图1(a)显示两类节点的原始features有大量重合。对于每个节点，随机抽取10个其他节点作为他的邻居，这样生成的图（图1(b) )中的边和node label没有任何关系，即所有边都与节点分类任务无关。在这个图上做GCN得到(图1(b)下方)的node embedding，可以看到GCN学到的node embedding在任务无关的边上，无法区分两类节点。 图1(c)是DropEdge学到的node embedding，图1(d)为本文的NeuralSparse学到的node embedding。\nPresent work：根据监督信号来选择任务相关的边。 由sparsification network 和GNN两个模块组成. sparsification network旨在参数化稀疏过程，即在给定预算下， 为每个节点选择任务相关的一节邻居。在训练阶段，优化稀疏策略，即训练一个可以将图稀疏化的网络。在测试阶段，将测试图输入网络，得到一个稀疏化的图。\nNeuralSparse Theoretical justification 首先从统计学习角度建模问题。将预测任务定义为$P(Y|G)$, 其中$Y$是预测目标，$G$是输入图。本文利用稀疏化子图来移除任务无关的边, 问题形式化为: $$ P(Y \\mid G) \\approx \\sum_{g \\in \\mathbb{S}_{G}} P(Y \\mid g) P(g \\mid G) $$ $g$是一个稀疏化子图，$\\mathbb{S}_{G}$为$G$的稀疏化子图集合。 $P(Y \\mid g)$ 为给定一个稀疏化子图$g$，用$g$过GNN后预测为$Y$的概率。 $$ \\sum_{g \\in \\mathbb{S}_{G}} P(Y \\mid g) P(g \\mid G) \\approx \\sum_{g \\in \\mathbb{S}_{G}} Q_{\\theta}(Y \\mid g) Q_{\\phi}(g \\mid G) $$ 用函数来近似(计算) 分布， 即给定$g$ 预测为$Y$的概率$ P(Y \\mid g)$定义为一个参数为$\\theta$的函数$Q_{\\theta}(Y \\mid g)$, 从$G$中获得子图$g$的概率$P(g \\mid G)$定义为一个参数为$\\phi$的函数$Q_{\\phi}(g \\mid G)$。","title":"ICML2020 《Robust Graph Representation Learning via Neural Sparsification》 Reading Notes"},{"content":"Paper\nIntroduction 由于GNNs过度强调平滑的节点特征而不是图结构，使得在Link Prediction任务上的表现甚至弱于heuristic方法。平滑邻居难以反映邻域结构的相关性以及其他拓扑特征。 Structural information, (e.g., overlapped neighborhoods, degrees, and shortest path), is crucial for link prediction whereas GNNs heavily rely on smoothed node features rather than graph structure。\n Link prediction heuristics: 基于预定义的假设的链路预测。举几个例子[1]：\n  Common Neighbors (CN)： 公共邻居较多的节点存在边（heuristic），则需要计算节点对间的公共邻居。 Preferential Attachment (PA): 一个节点当前的连接越多，那么它越有可能接受到新的连接（heuristic）,这需要统计每个节点的度, i.e., $P A(x, y)=|N(x)| *|N(y)|$ Katz Index heuristic: $\\sum^{\\infty}_{\\ell=1} \\beta^{\\ell}|walks(x,y)=\\ell|$ 表示从$x$到$y$的所有路径数， $0\u0026lt;\\beta\u0026lt;1$， 表示越长的路径权重越低。 katz Index作为Link prediction heuristic假设来作为边是否存在的预测。  本文提出了Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs)来从邻接矩阵中学习结构信息，并且估计重叠多跳邻域用于link prediction。\nPreliminaries GNNs for Link Prediction $$ \\hat{y}_{i j}=\\sigma\\left(s\\left(h_{i}^{(L)}, h_{j}^{(L)}\\right)\\right) $$\n其中$s(\\cdot, \\cdot)$ 是一个相似度计算函数 e.g., inner product or MLP. $h_{i}^{(L)}$为 $v_i$的 node embedding.\nNeighborhood Overlap-based Heuristic Methods 就是上面提到的CN heuristic。Common Neighbors 通过count节点的公共邻居来衡量两个节点之间的链路存在分数$\\mathrm{link}(u,v)$： $$ S_{C N}(u, v)=|\\mathcal{N}(u) \\cap \\mathcal{N}(v)|=\\sum_{k \\in \\mathcal{N}(u) \\cap \\mathcal{N}(v)} 1 $$ CN的缺点在于不能衡量公共节点的权重。\nResource Allocation (RA) 认为度叫小的节点因更加重要， 所以用度的倒数来加权公共节点： $$ S_{R A}(u, v)=\\sum_{k \\in \\mathcal{N}(u) \\cap \\mathcal{N}(v)} \\frac{1}{d_{k}} $$\nAdamic-Adar：通过使用节点 $u$ 和$v$之间的共同邻居度的倒数对数，与 RA 相比，Adamic-Adar 对更高度的惩罚相对减少： $$ S_{A A}(u, v)=\\sum_{k \\in \\mathcal{N}(u) \\cap \\mathcal{N}(v)} \\frac{1}{\\log d_{k}} $$ 上述基于公共邻居的方法存在两个局限，1. 需要手动设计邻居结构特征，比如CN的公共邻居结构特征为1， RA的结构特征为$\\frac{1}{d}$, AA 的邻居结构特征为$\\frac{1}{\\log d}$。 2. 忽略了node features\n本文提出的Neo-GNN从邻接矩阵中学习结构特征，并且结合了node feature信息来做Link prediction。\nModel: Neo-GNNs 定义structural feature generator $\\mathcal{F}_{\\theta}$: $$ x_{i}^{\\text {struct }}=\\mathcal{F}_{\\theta}\\left(A_{i}\\right)=f_{\\theta_{n o d e}}\\left(\\sum_{j \\in \\mathcal{N}_{i}} f_{\\theta_{e d g e}}\\left(A_{i j}\\right)\\right) $$ 输入节点$i$的邻居$A_i$，提取自邻接矩阵$A$, Neo-GNNs 只是用$A$作为输入来获得节点的结构特征。 其中，$f_{\\theta_{e d g e}}(A_{ij})$生成节点$i$的局部边特征，然后聚合起来用$f_{\\theta_{n o d e}}$生成节点$i$的总体结构特征$x_{i}^{\\text {struct }}$， 作为节点$i$的structural feature，表示反映了节点$i$的局部结构。其中$f_{\\theta_{n o d e}}$和$f_{\\theta_{e d g e}}$是两个MLP。 也可以把上面的$A$替换成$A$的幂的组合，那就是$k$跳以内邻域的结构特征。\n得到了节点的邻居结构特征$x_{i}^{\\text {struct }}$后， 要用重叠邻居的结构特征来计算两个节点的相似度分数。 传统的GNN无法计算重叠邻域的结构特征的原因有两个：1. normalized adjacency matrix: 归一化邻接矩阵阻止了GNN计数邻居数量（我的理解是因为Norm adj上的元素为小数）2. 远低于节点数的hidden representation维度$d \\ll N$：低维度的节点表示向量使得在neighborhood aggregration后 节点邻域特征难以区分。\n本文提出了邻域重叠感知的聚合模式。 注意，上面的节点邻域特征是一个scale, 即$x_{i}^{\\text {struct }} \\in \\mathbb{R}^1$, 整个图的节点邻域结构特征可以表示为$X^{struct} \\in \\mathbb{R}^{N \\times N}$, 为一个对角阵，对角线元素为每个节点的邻域结构特征，如Figure 1所示。也就是$X^{struct}$的每一行为一个节点的局部结构特征表示向量，作为这个节点的结构特征。\n那么$Z = AX^{struct}$就可以为节点聚合结构特征。 因为$X^{struct}_i$表示节点$v_i$的structural feature (neighborhood structural), 所以$Z_i$表示节点$i$的1-st neighborhood structural feature, 所以$z_{i}^{T} z_{j}=\\sum_{k \\in \\mathcal{N}(i) \\cap \\mathcal{N}(j)}\\left(x_{k}^{s t r u c t}\\right)^{2}$表可以表示节点$i$和节点$j$的重叠邻域。\n注意 $X_{i}^{\\text {struct }}$表示节点$i$自身的结构特征。 而$Z_i$表示节点$i$的邻居的结构特征聚合, 所以$z_{i}^{T} z_{j}$表示节点$i$邻居的结构特征和节点$j$邻居的结构特征的相似度。 $x_i^T x_j$表示节点$i$自身的结构特征和节点$j$自身的结构特征的相似度。\n进一步，考虑多跳邻居： $$ Z=g_{\\Phi}\\left(\\sum_{l=1}^{L} \\beta^{l-1} A^{l} X^{\\text {struct }}\\right) $$\n$A^lX^{struct}$的第$i$行表示节点$i$ 的$l$跳邻居特征。 $Z_i$表示节点$i$在$L$跳以内的邻居结构特征总和。\n除了考虑结构特征来预测链接外，还应考虑node features，直接用GNN： $$ H=\\operatorname{GNN}\\left(X, \\tilde{A}_{G N N} ; W\\right) $$ 最终节点$i$和节点$j$的相似度分数表示为： $$ \\left.\\hat{y}_{i j}=\\alpha \\cdot \\sigma\\left(z_{i}^{T} z_{j}\\right)+(1-\\alpha) \\cdot \\sigma\\left(s\\left(h_{i}, h_{j}\\right)\\right)\\right) $$ 即为邻域结构相似度 与 特征相似度 的加权平均。\n最终损失函数要求 3种相似度衡量标准（基于邻域结构，基于节点feature, 两者加权平均）都可以你和真实的相似度，即： $$ \\mathcal{L}=\\sum_{(i, j) \\in D}\\left(\\lambda_{1} B C E\\left(\\hat{y}_{i j}, y_{i j}\\right)+\\lambda_{2} B C E\\left(\\sigma\\left(z_{i}^{T} z_{j}\\right), y_{i j}\\right)+\\lambda_{3} B C E\\left(\\sigma\\left(s\\left(h_{i}, h_{j}\\right)\\right), y_{i j}\\right)\\right) $$ 其中$BCE(\\cdot, \\cdot)$为 binary cross entropy loss。\nReference [1] Link Prediction Based on Graph Neural Networks. NeurIPS 2018.\n","permalink":"https://JhuoW.github.io/posts/neo-gnns/","summary":"Paper\nIntroduction 由于GNNs过度强调平滑的节点特征而不是图结构，使得在Link Prediction任务上的表现甚至弱于heuristic方法。平滑邻居难以反映邻域结构的相关性以及其他拓扑特征。 Structural information, (e.g., overlapped neighborhoods, degrees, and shortest path), is crucial for link prediction whereas GNNs heavily rely on smoothed node features rather than graph structure。\n Link prediction heuristics: 基于预定义的假设的链路预测。举几个例子[1]：\n  Common Neighbors (CN)： 公共邻居较多的节点存在边（heuristic），则需要计算节点对间的公共邻居。 Preferential Attachment (PA): 一个节点当前的连接越多，那么它越有可能接受到新的连接（heuristic）,这需要统计每个节点的度, i.e., $P A(x, y)=|N(x)| *|N(y)|$ Katz Index heuristic: $\\sum^{\\infty}_{\\ell=1} \\beta^{\\ell}|walks(x,y)=\\ell|$ 表示从$x$到$y$的所有路径数， $0\u0026lt;\\beta\u0026lt;1$， 表示越长的路径权重越低。 katz Index作为Link prediction heuristic假设来作为边是否存在的预测。  本文提出了Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs)来从邻接矩阵中学习结构信息，并且估计重叠多跳邻域用于link prediction。","title":"NeurIPS2021 《Neo-GNNs:Neighborhood Overlap-aware Graph Neural Networks for Link Prediction》 Reading Notes"},{"content":"Paper\nIntroduction 加深GNN层数来增加感受野会导致优化不稳定性，比如梯度消失和oversmoothing。 因此本文采用Transformer-based self-attention来学习成对节点间的长距离关系，并且提出一种新型的readout池化机制来学习global graph embedding。即在一个GNN模块后接一个置换不变（permutation-invariant）Transformer, GNN模块捕获local信息，Transformer捕获global信息。\nGNN作为一种专门的架构医学系节点直接邻域结构的局部表示， 而Transformer作为全局推理模块以位置无关的方式计算所有成对节点的交互。作者认为，一个没有positional encoding的Transformer是置换不变的，因此很适合图。\nMotivation 强关系Inductive bias(我的理解是Homophily假设) 鼓励学习局部短距离的关联性。 而对于长距离相关性，结构化较低的模块（不需要过于考虑图的结构信息）更受欢迎。\nGraphTrans leaves learning long-range dependencies to Transformer, 通过Transformer来学习图中所有节点对的依赖关系而不是只关注局部邻居。\n下图中展示了一个子图的attention map。一共有17个节点，横坐标表示目标节点，纵坐标表示源节点，第$i$行第$j$列表示节点$i$在Transformer中聚合$j$的attention权重。第18行为一个特殊的$\u0026lt;CLS\u0026gt;$token 作为图的readout embedding。结合本文的SOTA效果，表面在学习长距离依赖时不考虑图结构先验（spatial priors）对Graph summarization（graph-level representation）是有必要的\nModel GNN Module 一个通用的GNN模块： $$ \\boldsymbol{h}_{v}^{\\ell}=f_{\\ell}\\left(\\boldsymbol{h}_{v}^{\\ell-1},\\left\\{\\boldsymbol{h}_{u}^{\\ell-1} \\mid u \\in \\mathcal{N}(v)\\right\\}\\right), \\quad \\ell=1, \\ldots, L_{\\mathrm{GNN}} $$\nTransformer Module 通过上面的GNN模块，我们可以得到每个节点的embedding $h_{v}^{L_{\\mathrm{GNN}}}$, 将所有节点作为Transformer的Input。传统Transformer中输入先计算Self-attention（当前输入的$Q$向量和所有节点的$K$向量做内积得到输入节点和其他节点的att值，再用这个att值来为当前输入节点加权聚合所有节点的$V$向量），聚合后再和自身相加做residual,然后在做Layer Norm, 即对节点$i$的表示做Layer Norm 为$x_i^\\prime = \\frac{x_i-m}{\\sigma}$, 其中$m$为$x_i$的均值， $\\sigma$为$x_i$的标准差。\n这里的Transformer不同的是， 先对所有节点做一次MLP，然后直接计算Layer Norm: $$ \\overline{\\boldsymbol{h}}_{v}^{0}=\\operatorname{LayerNorm}\\left(\\boldsymbol{W}^{\\text {Proj }} \\boldsymbol{h}_{v}^{L_{\\mathrm{GNN}}}\\right) $$ 其中$\\boldsymbol{W}^{\\text {Proj }} \\in \\mathbb{R}^{d_{\\mathrm{TF}} \\times d_{L_{\\mathrm{GNN}}}}$， 把GNN的输出维度转为TF的输入维度$d_{\\mathrm{TF}}$。将所有节点的GNN node embeddings作为Transformer的输入（无positional encoding）。 每个节点的$Q$, $K$和$V$向量分别用$\\boldsymbol{W}_{\\ell}^{Q}, \\boldsymbol{W}_{\\ell}^{K}, \\boldsymbol{W}_{\\ell}^{V} \\in \\mathbb{R}^{d_{\\mathrm{TF}} / n_{\\text {head }} \\times d_{\\mathrm{TF}} / n_{\\text {head }}}$计算， 对于第$\\ell$层 Transformer, 节点$v$ 的$Q$向量$Q_v = \\boldsymbol{W}_{\\ell}^{Q} \\overline{\\boldsymbol{h}}_{v}^{\\ell-1}$和节点$u$的$K$向量$K_u = \\boldsymbol{W}_{\\ell}^{K} \\overline{\\boldsymbol{h}}_{u}^{\\ell-1}$做内积，得到两个节点之间的attention。然后用$\\alpha_{v, u}^{\\ell}$来为节点$v$聚合其他所有节点的$V$向量$V_u = \\boldsymbol{W}_{\\ell}^{V} \\overline{\\boldsymbol{h}}_{u}^{\\ell-1}$, 如下所示: $$ a_{v, u}^{\\ell}=\\left(\\boldsymbol{W}_{\\ell}^{Q} \\overline{\\boldsymbol{h}}_{v}^{\\ell-1}\\right)^{\\top}\\left(\\boldsymbol{W}_{\\ell}^{K} \\overline{\\boldsymbol{h}}_{u}^{\\ell-1}\\right) / \\sqrt{d_{\\mathrm{TF}}} \\tag{1} $$\n$$ \\alpha_{v, u}^{\\ell}=\\operatorname{softmax}_{u \\in \\mathcal{V}}\\left(a_{v, u}^{\\ell}\\right) \\tag{2} $$\n$$ \\overline{\\boldsymbol{h}}_{v}^{\\prime \\ell}=\\sum_{w \\in \\mathcal{V}} \\alpha_{v, u}^{\\ell} \\boldsymbol{W}_{\\ell}^{V} \\overline{\\boldsymbol{h}}_{u}^{\\ell-1} \\tag{3} $$\n\u0026lt;CLS\u0026gt; embedding as a GNN “readout” method Graph Pooling 部分旨在基于node embedding，得到整个图的一个global embedding. 大多数pooling方法为简单的mean,sum, 或者构造一个virtual node连接到所有节点并参与训练，这个virtual node聚合所有节点的信息作为global embedding。\n本文提出special-token readout module。具体来说，对Transformer的输入$[\\overline{\\boldsymbol{h}}_{v}^{0}]_{v\\in V}$, where $\\overline{\\boldsymbol{h}}_{v}^{0} \\in \\mathcal{R}^{d_{TF}}$我们添加一个额外的可学习embedding （可以被认为是一个额外virtual node）$\\bar{h}_{\\langle\\mathrm{CLS}\\rangle} \\in \\mathbb{R}^{d_{\\mathrm{TF}}}$, 这样 Transformer 的输入就变为$[\\overline{\\boldsymbol{h}}_{v}^{0}]_{v \\in V} \\cup \\bar{h}_{\\langle\\mathrm{CLS}\\rangle}$, 因为训练过程中$\\overline{\\boldsymbol{h}}_{v}^{0}$回聚合来自所有节点的信息，所以用它来作为readout embedding。 最终Transformer输出的token embedding $\\overline{\\boldsymbol{h}}_{\u0026lt;\\mathrm{CLS}\u0026gt;}^{L_{\\mathrm{TF}}}$ 再过一层MLP后用Softmax输出图的prediction: $$ y=\\operatorname{softmax}\\left(\\boldsymbol{W}^{\\mathrm{out}} \\overline{\\boldsymbol{h}}_{\u0026lt;\\mathrm{CLS}\u0026gt;}^{L_{\\mathrm{TF}}}\\right) $$\n","permalink":"https://JhuoW.github.io/posts/graphtrans/","summary":"Paper\nIntroduction 加深GNN层数来增加感受野会导致优化不稳定性，比如梯度消失和oversmoothing。 因此本文采用Transformer-based self-attention来学习成对节点间的长距离关系，并且提出一种新型的readout池化机制来学习global graph embedding。即在一个GNN模块后接一个置换不变（permutation-invariant）Transformer, GNN模块捕获local信息，Transformer捕获global信息。\nGNN作为一种专门的架构医学系节点直接邻域结构的局部表示， 而Transformer作为全局推理模块以位置无关的方式计算所有成对节点的交互。作者认为，一个没有positional encoding的Transformer是置换不变的，因此很适合图。\nMotivation 强关系Inductive bias(我的理解是Homophily假设) 鼓励学习局部短距离的关联性。 而对于长距离相关性，结构化较低的模块（不需要过于考虑图的结构信息）更受欢迎。\nGraphTrans leaves learning long-range dependencies to Transformer, 通过Transformer来学习图中所有节点对的依赖关系而不是只关注局部邻居。\n下图中展示了一个子图的attention map。一共有17个节点，横坐标表示目标节点，纵坐标表示源节点，第$i$行第$j$列表示节点$i$在Transformer中聚合$j$的attention权重。第18行为一个特殊的$\u0026lt;CLS\u0026gt;$token 作为图的readout embedding。结合本文的SOTA效果，表面在学习长距离依赖时不考虑图结构先验（spatial priors）对Graph summarization（graph-level representation）是有必要的\nModel GNN Module 一个通用的GNN模块： $$ \\boldsymbol{h}_{v}^{\\ell}=f_{\\ell}\\left(\\boldsymbol{h}_{v}^{\\ell-1},\\left\\{\\boldsymbol{h}_{u}^{\\ell-1} \\mid u \\in \\mathcal{N}(v)\\right\\}\\right), \\quad \\ell=1, \\ldots, L_{\\mathrm{GNN}} $$\nTransformer Module 通过上面的GNN模块，我们可以得到每个节点的embedding $h_{v}^{L_{\\mathrm{GNN}}}$, 将所有节点作为Transformer的Input。传统Transformer中输入先计算Self-attention（当前输入的$Q$向量和所有节点的$K$向量做内积得到输入节点和其他节点的att值，再用这个att值来为当前输入节点加权聚合所有节点的$V$向量），聚合后再和自身相加做residual,然后在做Layer Norm, 即对节点$i$的表示做Layer Norm 为$x_i^\\prime = \\frac{x_i-m}{\\sigma}$, 其中$m$为$x_i$的均值， $\\sigma$为$x_i$的标准差。\n这里的Transformer不同的是， 先对所有节点做一次MLP，然后直接计算Layer Norm: $$ \\overline{\\boldsymbol{h}}_{v}^{0}=\\operatorname{LayerNorm}\\left(\\boldsymbol{W}^{\\text {Proj }} \\boldsymbol{h}_{v}^{L_{\\mathrm{GNN}}}\\right) $$ 其中$\\boldsymbol{W}^{\\text {Proj }} \\in \\mathbb{R}^{d_{\\mathrm{TF}} \\times d_{L_{\\mathrm{GNN}}}}$， 把GNN的输出维度转为TF的输入维度$d_{\\mathrm{TF}}$。将所有节点的GNN node embeddings作为Transformer的输入（无positional encoding）。 每个节点的$Q$, $K$和$V$向量分别用$\\boldsymbol{W}_{\\ell}^{Q}, \\boldsymbol{W}_{\\ell}^{K}, \\boldsymbol{W}_{\\ell}^{V} \\in \\mathbb{R}^{d_{\\mathrm{TF}} / n_{\\text {head }} \\times d_{\\mathrm{TF}} / n_{\\text {head }}}$计算， 对于第$\\ell$层 Transformer, 节点$v$ 的$Q$向量$Q_v = \\boldsymbol{W}_{\\ell}^{Q} \\overline{\\boldsymbol{h}}_{v}^{\\ell-1}$和节点$u$的$K$向量$K_u = \\boldsymbol{W}_{\\ell}^{K} \\overline{\\boldsymbol{h}}_{u}^{\\ell-1}$做内积，得到两个节点之间的attention。然后用$\\alpha_{v, u}^{\\ell}$来为节点$v$聚合其他所有节点的$V$向量$V_u = \\boldsymbol{W}_{\\ell}^{V} \\overline{\\boldsymbol{h}}_{u}^{\\ell-1}$, 如下所示: $$ a_{v, u}^{\\ell}=\\left(\\boldsymbol{W}_{\\ell}^{Q} \\overline{\\boldsymbol{h}}_{v}^{\\ell-1}\\right)^{\\top}\\left(\\boldsymbol{W}_{\\ell}^{K} \\overline{\\boldsymbol{h}}_{u}^{\\ell-1}\\right) / \\sqrt{d_{\\mathrm{TF}}} \\tag{1} $$","title":"NeurIPS2021 《Representing Long-Range Context for Graph Neural Networks with Global Attention》 Reading Notes"},{"content":"paper\nIntroduction 很多GNN易受图结构攻击的影响，本文首先证明了symmetric normalized laplacian的低频分量作为GCN的filter，在某个特征值区间内对于结构扰动更加robust。基于该理论提出GCN-LFR，通过一个辅助神经网络迁移低频分量的robustness。\nQ: 对抗扰动边是否会对graph spectrum产生同等的影响？过去的研究显示来自结构攻击的扰动在图谱上表达了一种隐含的趋势。如下图所示，结构扰动后，小的特征值（低频）变化较小， 高频变化较大，即高频对扰动更加敏感。\n本文证明了当normalized symmetric laplacian的特征值落于某个特定区间时，低频分量会更加robust。\nMethodology Poisoning Attack是指训练前扰动：\nProblem 1 （Poisoning Attack）: 给定一个扰动图 $\\mathcal{G}^\\prime$, 要对目标集合$\\mathcal{T}$做对抗防御的目的是设计一个更加鲁棒的模型，使得模型在扰动图上训练后对$\\mathcal{T}$中节点的预测结果和在原图上训练得到的预测结果相似： $$ \\min_{\\boldsymbol{\\Theta}^{r *}} \\sum_{u \\in \\mathcal{T}}\\left|\\left|\\mathcal{M}_{u}^{r}\\left(\\boldsymbol{A}^{\\prime}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}^{r *}\\right)-\\mathcal{M}_{u}\\left(\\boldsymbol{A}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}^{*}\\right)\\right|\\right| $$ 即模型可以尽可能避免扰动对预测带来的影响。 其中$\\mathcal{M}_{u}^{r}\\left(\\boldsymbol{A}^{\\prime}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}^{r *}\\right)=\\hat{\\boldsymbol{y}}_{u}^{r}$模型在扰动图上训练过后对节点$u$的预测，$\\mathcal{M}_{u}^{r}$是在扰动图上训练过后的模型，最佳参数为$\\boldsymbol{\\Theta}^{r *}$, $\\mathcal{M}$是在原图上训练过后的模型。\n观察$\\hat{\\boldsymbol{A}}$的特征值， 因为$\\hat{\\boldsymbol{A}} = I - L$, 所以$\\hat{\\boldsymbol{A}}$的大特征值对应于低频分量，小特征值对应于高频分量。 从图1可以看出$\\hat{\\boldsymbol{A}}$的大特征值对于结构扰动更加鲁棒, 因为扰动之后大特征值的变化较小。所以GCN-SVD只是用最低频的分量来做defense.\n接下来本文证明了只有一条边被扰动的情况下，一定存在低频$\\lambda_b$，比所有高频都robust, 鲁棒区间为： $$ \\max \\left(0, \\frac{d_{b}-d_{a}+c_{a} \\lambda_{a}}{c_{b}}\\right)\u0026lt;\\lambda_{b}\u0026lt;\\min \\left(\\frac{d_{b}+d_{a}-c_{a} \\lambda_{a}}{c_{b}}, 1\\right) $$ 即，当特征值落于这个区间中时，它一定比高频更加鲁棒。\n对于Non-targeted Perturbation, 图中有多条边被扰动，那么特征值的鲁棒区间为： $$ \\max_{v \\in \\mathcal{P}_{u}, u \\in \\mathcal{T}}\\left(0, \\frac{d_{b u v}-d_{a u v}+c_{a u v} \\lambda_{a}}{c_{b u v}}\\right)\u0026lt;\\lambda_{b}\u0026lt;\\min_{v \\in \\mathcal{P}_{u}, u \\in \\mathcal{T}}\\left(\\frac{d_{b u v}+d_{a u v}-c_{a u v} \\lambda_{a}}{c_{b u v}}, 1\\right) $$ 在得到不同扰动情况下的鲁棒区间后，我们可以基于鲁棒区间来增强GCN的鲁棒性。\nGCN-LFR 基于鲁棒区间，利用区间内频率分量可以设计更加鲁棒的GCN。\n给定一个结构扰动图$\\mathcal{G}^\\prime$,邻接矩阵为$A^\\prime$, GCN-LFR使用一个辅助正则化网络$\\mathcal{M}_{\\mathrm{LFR}}\\left(\\boldsymbol{A}^{\\prime}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}\\right)$来计算robust 区间， 但是鲁棒区间时基于原图的，在只给定扰动图的情况下无法计算。 为了解决该问题， 本文用可学习的参数$\\mathbf{F}$作为filters来学习鲁棒区间。其中，$\\mathbf{F} = \\left(\\begin{array}{cccc} f_{1} \u0026amp; \u0026amp; \u0026amp; \\\\ \u0026amp; f_{2} \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\\\ \u0026amp; \u0026amp; \u0026amp; f_{k} \\end{array}\\right)$。\n解释：假设$U$是$\\hat{\\boldsymbol{A}}$的特征向量，则 $\\hat{\\boldsymbol{A}}$可以分解为:\n$$\\hat{\\boldsymbol{A}} = [u_1, \\cdots, u_n] F \\left[\\begin{array}{l} u_1^\\top \\\\ \\cdots \\\\ u_n^\\top \\end{array}\\right] = f_1 u_1 u_1^\\top + \\cdots + f_n u_n u_n^\\top$$\n其中$u_1$对应拉普拉斯矩阵的最小特征值的特征向量（$\\hat{\\boldsymbol{A}}$最大特征值的特征向量），所以$[u_1, \\cdots, u_k]$表示最低频的$k$个特征向量。$f_i$是第$i$个频率滤波器的权重。 我们选择$k$个低频滤波器，并自适应的学习他们的权重，即 $U^\\prime_{low} = [u_1, \\cdots, u_k]$, $\\mathbf{F}$是$k$个低频filter的参数， 所以图卷积层$\\mathcal{M}_{\\text {LFR }}\\left(\\boldsymbol{A}^{\\prime}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}, \\boldsymbol{F}\\right)$可以写作: $$ \\boldsymbol{H}^{(l+1)}=\\sigma\\left(\\boldsymbol{U}_{\\text {low }}^{\\prime} \\boldsymbol{F} \\boldsymbol{U}_{\\text {low }}^{\\prime \\top} \\boldsymbol{H}^{\\prime(l)}\\Theta\\right) $$ 另外，本文提出交替训练策略，对于在原本扰动图上的模型$\\mathcal{M}_{\\mathrm{GCN}}\\left(\\boldsymbol{A}^{\\prime}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}\\right)$以及低频自适应学习的模型$\\mathcal{M}_{\\mathrm{LFR}}\\left(\\boldsymbol{A}^{\\prime}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}, \\boldsymbol{F}\\right)$, 这两个模型交替训练，损失函数分别用$\\alpha$和$1-\\alpha$加权： $$ \\mathcal{L}_{\\text {total }}=(1-\\alpha) \\mathcal{L}_{\\mathrm{GCN}}+\\alpha \\mathcal{L}_{\\mathrm{LFR}} $$\n","permalink":"https://JhuoW.github.io/posts/gcn-lfr/","summary":"paper\nIntroduction 很多GNN易受图结构攻击的影响，本文首先证明了symmetric normalized laplacian的低频分量作为GCN的filter，在某个特征值区间内对于结构扰动更加robust。基于该理论提出GCN-LFR，通过一个辅助神经网络迁移低频分量的robustness。\nQ: 对抗扰动边是否会对graph spectrum产生同等的影响？过去的研究显示来自结构攻击的扰动在图谱上表达了一种隐含的趋势。如下图所示，结构扰动后，小的特征值（低频）变化较小， 高频变化较大，即高频对扰动更加敏感。\n本文证明了当normalized symmetric laplacian的特征值落于某个特定区间时，低频分量会更加robust。\nMethodology Poisoning Attack是指训练前扰动：\nProblem 1 （Poisoning Attack）: 给定一个扰动图 $\\mathcal{G}^\\prime$, 要对目标集合$\\mathcal{T}$做对抗防御的目的是设计一个更加鲁棒的模型，使得模型在扰动图上训练后对$\\mathcal{T}$中节点的预测结果和在原图上训练得到的预测结果相似： $$ \\min_{\\boldsymbol{\\Theta}^{r *}} \\sum_{u \\in \\mathcal{T}}\\left|\\left|\\mathcal{M}_{u}^{r}\\left(\\boldsymbol{A}^{\\prime}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}^{r *}\\right)-\\mathcal{M}_{u}\\left(\\boldsymbol{A}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}^{*}\\right)\\right|\\right| $$ 即模型可以尽可能避免扰动对预测带来的影响。 其中$\\mathcal{M}_{u}^{r}\\left(\\boldsymbol{A}^{\\prime}, \\boldsymbol{X} ; \\boldsymbol{\\Theta}^{r *}\\right)=\\hat{\\boldsymbol{y}}_{u}^{r}$模型在扰动图上训练过后对节点$u$的预测，$\\mathcal{M}_{u}^{r}$是在扰动图上训练过后的模型，最佳参数为$\\boldsymbol{\\Theta}^{r *}$, $\\mathcal{M}$是在原图上训练过后的模型。\n观察$\\hat{\\boldsymbol{A}}$的特征值， 因为$\\hat{\\boldsymbol{A}} = I - L$, 所以$\\hat{\\boldsymbol{A}}$的大特征值对应于低频分量，小特征值对应于高频分量。 从图1可以看出$\\hat{\\boldsymbol{A}}$的大特征值对于结构扰动更加鲁棒, 因为扰动之后大特征值的变化较小。所以GCN-SVD只是用最低频的分量来做defense.\n接下来本文证明了只有一条边被扰动的情况下，一定存在低频$\\lambda_b$，比所有高频都robust, 鲁棒区间为： $$ \\max \\left(0, \\frac{d_{b}-d_{a}+c_{a} \\lambda_{a}}{c_{b}}\\right)\u0026lt;\\lambda_{b}\u0026lt;\\min \\left(\\frac{d_{b}+d_{a}-c_{a} \\lambda_{a}}{c_{b}}, 1\\right) $$ 即，当特征值落于这个区间中时，它一定比高频更加鲁棒。\n对于Non-targeted Perturbation, 图中有多条边被扰动，那么特征值的鲁棒区间为： $$ \\max_{v \\in \\mathcal{P}_{u}, u \\in \\mathcal{T}}\\left(0, \\frac{d_{b u v}-d_{a u v}+c_{a u v} \\lambda_{a}}{c_{b u v}}\\right)\u0026lt;\\lambda_{b}\u0026lt;\\min_{v \\in \\mathcal{P}_{u}, u \\in \\mathcal{T}}\\left(\\frac{d_{b u v}+d_{a u v}-c_{a u v} \\lambda_{a}}{c_{b u v}}, 1\\right) $$ 在得到不同扰动情况下的鲁棒区间后，我们可以基于鲁棒区间来增强GCN的鲁棒性。","title":"NeurIPS2021 《Not All Low-Pass Filters are Robust in Graph Convolutional Networks》 Reading Notes"},{"content":"这篇笔记用于收藏别人的博客\nTech Blog    Blog Author     https://michael-bronstein.medium.com/ Michael Bronstein   https://geometricdeeplearning.com/ Michael Bronstein   https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c Vitaly Kurin (Many Paper Notes)   https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html UvA DL Notebooks   https://graph-neural-networks.github.io/index.html GNN Books   http://prob140.org/sp17/textbook/ Probability for Data Science class at UC Berkeley   https://graphreason.github.io/schedule.html Learning and Reasoning with Graph-Structured Representations ICML 2019 Workshop   https://chuxuzhang.github.io/KDD21_Tutorial.html KDD2021 Tutorial: Data Efficient Learning on Graphs   http://songcy.net/posts/ Changyue Song (Kernel)   https://www.cs.mcgill.ca/~wlh/grl_book/ William L. Hamilton   https://kexue.fm/ BoJone   https://danielegrattarola.github.io/blog/ Daniele Grattarola (EPFL)   https://ai.googleblog.com/2022/03/robust-graph-neural-networks.html Google AI Blog   https://zhiyuchen.com/blogs/ Zhiyu Chen   https://andreasloukas.blog/ Andreas Loukas (EPFL)   https://irhum.pubpub.org/pub/gnn/release/4 Understanding Graph Neural Networks   https://lilianweng.github.io/ Lilian Weng   https://www.zhihu.com/column/marlin 深度学习与图网络   https://github.com/roboticcam/machine-learning-notes Yida Xu   https://www.dgl.ai/pages/index.html DGL   https://www.kexinhuang.com/tech-blog Kexin Huang   https://rish16.notion.site/a8c6fcb0c29c4764afa1926ad33084f8?v=bb27bb0444574fbd85f0c9d7e43b9da8 Rishabh Anand   https://saashanair.com/blog Saasha Nair   http://www.huaxiaozhuan.com/ 华校专   https://github.com/dglai/WWW20-Hands-on-Tutorial DGL   https://blog.csdn.net/CSDNTianJi/article/details/104195306 Meng Liu   https://www.chaitjo.com/post/ Chaitanya K. Joshi   https://scottfreitas.medium.com/ Scott Freitas   https://fabianfuchsml.github.io/ Fabian Fuchs   https://medium.com/@pantelis.elinas Pantelis Elinas   https://github.com/tianyicui/pack 背包9講   https://www.fenghz.xyz/    https://sakigami-yang.me/2017/08/13/about-kernel-01/ kernel   https://davidham3.github.io/blog    https://fenghz.github.io/index.html    https://archwalker.github.io/     Awesome-Awesomes    Repo Name     https://github.com/naganandy/graph-based-deep-learning-literature links to conference publications in graph-based deep learning (Very, Very, Very Important)   https://github.com/SherylHYX/pytorch_geometric_signed_directed PyTorch Geometric Signed Directed is a signed/directed graph neural network extension library for PyTorch Geometric.   https://github.com/EdisonLeeeee/Awesome-Fair-Graph-Learning Paper Lists for Fair Graph Learning   https://github.com/thunlp/PromptPapers Must-read papers on prompt-based tuning for pre-trained language models.   https://github.com/zhao-tong/graph-data-augmentation-papers A curated list of graph data augmentation papers.   https://github.com/Thinklab-SJTU/ThinkMatch Code \u0026amp; pretrained models of novel deep graph matching methods.   https://github.com/FLHonker/Awesome-Knowledge-Distillation Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)。   https://github.com/zlpure/awesome-graph-representation-learning A curated list for awesome graph representation learning resources.   https://github.com/basiralab/GNNs-in-Network-Neuroscience A review of papers proposing novel GNN methods with application to brain connectivity published in 2017-2020.   https://github.com/flyingdoog/awesome-graph-explainability-papers Papers about explainability of GNNs   https://github.com/yuanqidu/awesome-graph-generation A curated list of graph generation papers and resources.   https://github.com/benedekrozemberczki/awesome-decision-tree-papers A collection of research papers on decision, classification and regression trees with implementations.   https://github.com/AstraZeneca/awesome-explainable-graph-reasoning A collection of research papers and software related to explainability in graph machine learning.   https://github.com/LirongWu/awesome-graph-self-supervised-learning Awesome Graph Self-Supervised Learning   https://github.com/Chen-Cai-OSU/awesome-equivariant-network Paper list for equivariant neural network   https://github.com/mengliu1998/DL4DisassortativeGraphs Papers about developing DL methods on disassortative graphs   https://github.com/SunQingYun1996/Graph-Reinforcement-Learning-Papers A curated list of graph reinforcement learning papers.   https://github.com/ChandlerBang/awesome-self-supervised-gnn Papers about pretraining and self-supervised learning on Graph Neural Networks (GNN).   https://github.com/GRAND-Lab/Awesome-Graph-Neural-Networks Paper Lists for Graph Neural Networks   https://github.com/jwzhanggy/IFMLab_GNN Graph Neural Network Models from IFM Lab   https://github.com/ChandlerBang/awesome-graph-attack-papers Adversarial attacks and defenses on Graph Neural Networks.   https://github.com/safe-graph/graph-adversarial-learning-literature A curated list of adversarial attacks and defenses papers on graph-structured data.   https://github.com/benedekrozemberczki/awesome-graph-classification A collection of important graph embedding, classification and representation learning papers with implementations.   https://github.com/benedekrozemberczki/awesome-gradient-boosting-papers A curated list of gradient boosting research papers with implementations.   https://github.com/benedekrozemberczki/awesome-community-detection A curated list of community detection research papers with implementations.   https://github.com/giannifranchi/awesome-uncertainty-deeplearning This repository contains a collection of surveys, datasets, papers, and codes, for predictive uncertainty estimation in deep learning models.   https://sites.google.com/site/graphmatchingmethods/ Efficient Methods for Graph Matching and MAP Inference   https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering Awesome Deep Graph Clustering is a collection of SOTA, novel deep graph clustering methods (papers, codes, and datasets).   https://github.com/jwwthu/GNN4Traffic This is the repository for the collection of Graph Neural Network for Traffic Forecasting.   https://github.com/zwt233/Awesome-Auto-GNNs A paper collection about automated graph learning   https://github.com/chaitjo/awesome-efficient-gnn Efficient Graph Neural Networks - a curated list of papers and projects   https://github.com/Radical3-HeZhang/Awesome-Trustworthy-GNNs Awesome Resources on Trustworthy Graph Neural Networks   https://github.com/EdisonLeeeee/Awesome-Masked-Autoencoders A collection of literature after or concurrent with Masked Autoencoder (MAE) (Kaiming He el al.).   https://github.com/XiaoxiaoMa-MQ/Awesome-Deep-Graph-Anomaly-Detection Awesome graph anomaly detection techniques built based on deep learning frameworks.   https://github.com/mengliu1998/awesome-expressive-gnn A collection of papers studying/improving the expressiveness of graph neural networks (GNNs)    Useful Repo/Tools    Name Info     http://acronymify.com/ Model Name   https://csacademy.com/app/graph_editor/ Graph Editor   https://github.com/guanyingc/python_plot_utils A simple code for plotting figure, colorbar, and cropping with python   https://github.com/guanyingc/latex_paper_writing_tips Tips for Writing a Research Paper using LaTeX   https://github.com/JhuoW/Pytorch_Program_Templete Pytorch Program Templete GNN   https://github.com/graph4ai/graph4nlp Graph4nlp is the library for the easy use of Graph Neural Networks for NLP. Welcome to visit our DLG4NLP website (https://dlg4nlp.github.io/index.html) for various learning resources!   https://github.com/benedekrozemberczki/pytorch_geometric_temporal PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models (CIKM 2021)   https://github.com/ysig/GraKeL A scikit-learn compatible library for graph kernels   https://github.com/jajupmochi/graphkit-learn A python package for graph kernels, graph edit distances, and graph pre-image problem.   https://github.com/pliang279/awesome-phd-advice Collection of advice for prospective and current PhD students   https://github.com/MLEveryday/100-Days-Of-ML-Code 100-Days-Of-ML-Code中文版   https://github.com/d2l-ai/d2l-zh 《动手学深度学习》   https://github.com/lukas-blecher/LaTeX-OCR pix2tex: Using a ViT to convert images of equations into LaTeX code.   https://github.com/thunlp/OpenPrompt An Open-Source Framework for Prompt-Learning.   https://github.com/snap-stanford/GraphGym Platform for designing and evaluating Graph Neural Networks (GNN)   https://github.com/pygod-team/pygod A Python Library for Graph Outlier Detection (Anomaly Detection)   https://github.com/MLNLP-World/Paper_Writing_Tips latex写作建议   https://github.com/dair-ai/ML-YouTube-Courses A place to discover the latest machine learning courses on YouTube.    Miscellaneous    Name Desc     https://github.com/The-Run-Philosophy-Organization/run run学指南   https://10beasts.net/ 测评    ","permalink":"https://JhuoW.github.io/posts/2019-04-28-paper-unscramble/","summary":"这篇笔记用于收藏别人的博客\nTech Blog    Blog Author     https://michael-bronstein.medium.com/ Michael Bronstein   https://geometricdeeplearning.com/ Michael Bronstein   https://www.notion.so/Paper-Notes-by-Vitaly-Kurin-97827e14e5cd4183815cfe3a5ecf2f4c Vitaly Kurin (Many Paper Notes)   https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html UvA DL Notebooks   https://graph-neural-networks.github.io/index.html GNN Books   http://prob140.org/sp17/textbook/ Probability for Data Science class at UC Berkeley   https://graphreason.github.io/schedule.html Learning and Reasoning with Graph-Structured Representations ICML 2019 Workshop   https://chuxuzhang.github.io/KDD21_Tutorial.html KDD2021 Tutorial: Data Efficient Learning on Graphs   http://songcy.","title":"Blog, Tools and Survey"},{"content":"Paper\nCode\nIntroduction 无监督图学习算法基于重构损失，不可避免的需要图相似度计算（重构embedding和输入embedding的loss), 计算复杂度较高。本文提出一种通用的归纳式无监督图学习算法SEED（Sampling, Encoding, and Embedding Distributions）。通过计算采样子图的重构损失来代替整个图的重构损失。 即 先采样子图，在用GNN编码子图，最后计算子图分布的embedding来作为整个图的representation. 过程如下图所示：\nSEED: Sampling, Encoding, and Embedding Distributions Anonymous Random Walk Definition 1 (Random Anonymous Walks[1]): Given a random walk $\\mathbf{w}=(w_1, \\cdots, w_l)$ where $\\langle w_i, w_{i+1} \\rangle \\in E$, the anonymous walk for $\\mathbf{w}$ is defined as： $$ \\mathrm{aw}(\\mathbf{w}) = (\\mathrm{DIS}(\\mathbf{w}, w_1),\\mathrm{DIS}(\\mathbf{w}, w_2),\\cdots, \\mathrm{DIS}(\\mathbf{w}, w_l) ) $$ where $\\mathrm{DIS}(\\mathbf{w}, w_i)$ denotes the number of distinct nodes in $\\mathbf{w}$ when $w_i$ first appears in $\\mathbf{w}$, i.e. $$ \\mathrm{DIS}(\\mathbf{w}, w_i) = |{w_1, \\cdots w_p}|, \\quad p = \\min_j {w_j=w_i} $$ 匿名随机游走和随机游走的不同在于，匿名随机游走描述了随机游走的潜在“patterns”, 不管具体被访问的节点是什么。 距离来说，给定两条随机游走序列 $\\mathbf{w_1}=(v_1, v_2, v_3, v_4, v_2)$ 和$w_2=(v_2, v_1, v_3, v_4, v_1)$, 这两个RW相关联的匿名随机游走是一样的，即$\\mathrm{aw}(\\mathbf{w_1}) = \\mathrm{aw}(\\mathbf{w_2}) = (1,2,3,4,2)$, 即使$\\mathbf{w_1}$和$\\mathbf{w_2}$访问不同的节点。即每个节点在RW中首次被访问时的位置就是这个点在ARW中的id,如在$\\mathbf{w_2}$中，$v_1$首次访问是在第二个时刻，那么他的id就是2，在ARW中用2表示。\nSampling 本文提出WEAVE随机游走来表示子图\n上图中所有的$a$代表属性一样的节点， 所有的$b$也代表属性一样的节点，那么构造如图中两条vanilla random walks将得到两条完全相同的随机游走序列，因为序列中的节点属性排列完全一样（这里不会去构造induced subgraph）。为了可以区分两个图，提出了WEAVE, i.e., random Walk with EArliest Visit timE。实际上就是为每个随机游走序列上的节点拼接他在匿名随机游走序列中的index。这样就可以区分两个属性完全一样的随机游走序列。\n简单来说这种方法会记录节点首次被访问的时间，这个时间作为节点的index，从而随机游走序列可以反映子图结构。\n一个长度为$k$的WEAVE序列可以表示为：$X=\\left[\\mathbf{x}^{(0)}, \\mathbf{x}^{(1)}, \\cdots, \\mathbf{x}^{(k)}\\right]$, 其中$\\mathbf{x}^{(p)}$是序列上的第$p$个节点， $\\mathbf{x}^{(p)}=\\left[\\mathbf{x}_{a}^{(p)}, \\mathbf{x}_{t}^{(p)}\\right]\\in \\mathbb{R}^{k \\times (d+\\ell)}$, 是两个向量的拼接，$\\mathbf{x}_{a}^{(p)} \\in \\mathbb{R}^d$代表这个节点的node feature, $ \\mathbf{x}_{t}^{(p)} \\in \\mathbb{R}^\\ell$是是节点在匿名随机游走中的idx， 用onehot向量表示（即该节点首次被访问的时间）。\n最终，如果要从输入图中sample $s$条随机游走路径，将会生成$s$个子图，用矩阵表示为$\\left\\{X_{1}, X_{2}, \\ldots, X_{s}\\right\\}$。\nEncoding 用$s$个随机游走序列表示$\\mathcal{G}$的$s$个子图。对每个子图使用auto encoder 计算embedding: $$ \\mathbf{z}=f\\left(X ; \\theta_{e}\\right), \\quad \\hat{X}=g\\left(\\mathbf{z} ; \\theta_{d}\\right) $$ 其中$X$表示一个子图（WEAVE）, 先用$f_{\\theta_e}$得到这个子图的pooling embedding, 在用$g_{\\theta_d}$将子图的embedding重构为矩阵$\\hat{X}$。每个子图的重构损失为： $$ \\mathcal{L}=||X-\\hat{X}||_{2}^{2} $$ 通过对每个子图的$\\mathcal{L}$做SGD来优化$\\theta_e$和$\\theta_d$来使得重构误差最小。 最终对于图$\\mathcal{G}$我们可以得到它的$s$个子图表示：$\\left\\{\\mathbf{z}_{1}, \\mathbf{z}_{2}, \\cdots, \\mathbf{z}_{S}\\right\\}$.\nEmbedding Distribution 假设我们已经有了输入图$\\mathcal{G}$的子图表示向量集$\\left\\{\\mathbf{z}_{1}, \\mathbf{z}_{2}, \\cdots, \\mathbf{z}_{S}\\right\\}$, 要将他们融合成一个embedding来表示整个图。可以把这个图的子图集合看做一个distribution，每个子图是这个distribution中的一个样本。 如果两个Graph的子图分布相似，那么这两个Graph的相似度应该更高。 所以目标就变为，给定两个图$\\mathcal{G}$和$\\mathcal{H}$, 他们的子图表示分别为$\\left\\{\\mathbf{z}_{1}, \\mathbf{z}_{2}, \\cdots, \\mathbf{z}_{s}\\right\\}$和$\\left\\{\\mathbf{h}_{1}, \\mathbf{h}_{2}, \\cdots, \\mathbf{h}_{s}\\right\\}$。这是两个分布的样本，我们要计算两个分布的距离，本文使用MMD, 目的是求两个分布的distribution embeddings, 然后求两个distribution embeddings间的距离。MMD可以参考这里。\n用$P_{\\mathcal{}G}$和$P_{\\mathcal{H}}$分别表示这两个图的子图分布， 两个分布之间的MMD距离可以用下式计算得到。 $$ \\begin{aligned} \\widehat{MMD}\\left(P_{\\mathcal{G}}, P_{\\mathcal{H}}\\right)=\u0026amp; \\frac{1}{s(s-1)} \\sum_{i=1}^{s} \\sum_{j \\neq i}^{s} k\\left(\\mathbf{z}_{i}, \\mathbf{z}_{j}\\right)+\\frac{1}{s(s-1)} \\sum_{i=1}^{s} \\sum_{j \\neq i}^{s} k\\left(\\mathbf{h}_{i}, \\mathbf{h}_{j}\\right) \\\\ \u0026amp;-\\frac{2}{s^{2}} \\sum_{i=1}^{s} \\sum_{j=1}^{s} k\\left(\\mathbf{z}_{i}, \\mathbf{h}_{j}\\right) \\\\ =\u0026amp;\\left|\\left|\\hat{\\mu}_{\\mathcal{G}}-\\hat{\\mu}_{\\mathcal{H}}\\right|\\right|_{2}^{2} . \\end{aligned} $$ 该式表示的含义为，两个图中的样本$\\left\\{\\mathbf{z}_{1}, \\mathbf{z}_{2}, \\cdots, \\mathbf{z}_{S}\\right\\}$和$\\left\\{\\mathbf{h}_{1}, \\mathbf{h}_{2}, \\cdots, \\mathbf{h}_{s}\\right\\}$分别映射到一个RKHS空间中，两组样本在这个RKHS空间中的均值来表示这两个分布。即： $$ \\hat{\\mu}_{\\mathcal{G}}=\\frac{1}{s} \\sum_{i=1}^{s} \\phi\\left(\\mathbf{z}_{i}\\right), \\quad \\hat{\\mu}_{\\mathcal{H}}=\\frac{1}{s} \\sum_{i=1}^{s} \\phi\\left(\\mathbf{h}_{i}\\right) $$ 其中$\\phi(\\mathbf{z}_{i})$,$\\phi(\\mathbf{h}_{i})$分别表示 将向量$\\mathbf{z}_{i}$和$\\mathbf{h}_{i}$ 映射到一个RKHS中，所以$\\phi(\\cdot)$是一个kernel $k(\\cdot, \\cdot)$的feature map函数, i.e., $k(u,v) = \\langle \\phi(u), \\phi(v) \\rangle$。$\\phi(u) = k(\\cdot, u)$是kernel $k$对应RKHS中的一个函数（向量）。 所以只要确定一个kernel $k(\\cdot, \\cdot)$，上面的$\\widehat{MMD}(P_{\\mathcal{G}}, P_{\\mathcal{H}})$就可以求出确定值，表示两个distribution间的距离。 但是知道两个分布在RKHS中的距离还不够，需要知道这两个分布的在RKHS间的均值距离还不够， 我们需要知道这两个分布在RKHS中被映射成了什么向量，即我们要求$\\phi(\\cdot)$。\n假设我们已经有了一个kernel， 这个kernel对应的映射函数是一个恒等映射，那么$\\phi(u)=u$, 分布样本在RKHS中的表示就是他们本身，即 $\\phi(\\mathbf{z}_{i})=\\mathbf{z}_{i}$, $\\phi(\\mathbf{h}_{i})=\\mathbf{h}_{i}$。那么这分布的表示向量就是他们的样本在RKHS上的平均（均值平均误差）： $$ \\hat{\\mu}_{\\mathcal{G}}=\\frac{1}{s} \\sum_{i=1}^{s} \\mathbf{z}_{i}, \\quad \\hat{\\mu}_{\\mathcal{H}}=\\frac{1}{s} \\sum_{i=1}^{s} \\mathbf{h}_{i} $$ 如果$k$是一个其他通用kernel, 比如RBF kernel, 那么$k(u,v) = \\langle \\phi(u), \\phi(v) \\rangle$这里的$\\phi(\\cdot)$是不知道的，也就是仅能知道映射后的内积值，不能知道具体的映射是什么，为了求这个映射，本文用神经网络来近似这个映射。\n具体来说，定义$\\hat{\\phi}\\left(\\cdot ; \\theta_{m}\\right)$是一个参数为$\\theta_{m}$的MLP， 输入为分布的样本，那么用这个函数来对两个分布的样本$\\{\\mathbf{z_i}\\}$和$\\{\\mathbf{h_i}\\}$做映射, 然后用$\\hat{\\phi}\\left(\\cdot ; \\theta_{m}\\right)$来近似kernel真实的映射函数$\\phi(\\cdot)$。即： $$ \\hat{\\mu}_{\\mathcal{G}}^{\\prime}=\\frac{1}{s} \\sum_{i=1}^{s} \\hat{\\phi}\\left(\\mathbf{z}_{i} ; \\theta_{m}\\right), \\quad \\hat{\\mu}_{\\mathcal{H}}^{\\prime}=\\frac{1}{s} \\sum_{i=1}^{s} \\hat{\\phi}\\left(\\mathbf{h}_{i} ; \\theta_{m}\\right), \\quad D\\left(P_{\\mathcal{G}}, P_{\\mathcal{H}}\\right)=\\left|\\left|\\hat{\\mu}_{\\mathcal{G}}^{\\prime}-\\hat{\\mu}_{\\mathcal{H}}^{\\prime}\\right|\\right|_{2}^{2} $$ 上式中的$D\\left(P_{\\mathcal{G}}, P_{\\mathcal{H}}\\right)$表示两个分布中的样本在被$\\hat{\\phi}\\left(\\cdot; \\theta_{m}\\right)$映射后的均值误差。用这个均值误差来近似$\\widehat{MMD}(P_{\\mathcal{G}}, P_{\\mathcal{H}})$中由kernel $k$的映射$\\phi(\\cdot)$算出的Ground truth均值误差：\n$$J\\left(\\theta_{m}\\right)=\\left|\\left|D\\left(P_{\\mathcal{G}}, P_{\\mathcal{H}}\\right)-\\widehat{M M D}\\left(P_{\\mathcal{G}}, P_{\\mathcal{H}}\\right)\\right|\\right|_{2}^{2}$$\n通过最小化$J\\left(\\theta_{m}\\right)$,来优化$\\hat{\\phi}\\left(\\cdot; \\theta_{m}\\right)$,使其近似称为一个kernel的feature map函数， 即可以将样本映射到一个RKHS空间中的函数。\n训练结束后，用$\\hat{\\mu}_{\\mathcal{G}}^{\\prime}$来表示输入图$\\mathcal{G}$的最终embedding （子图分布embedding）。\nReference [1] Micali, S., and Zhu, Z. A. 2016. Reconstructing markov processes from independent and anonymous experiments. Discrete Applied Mathematics 200:108–122.\n","permalink":"https://JhuoW.github.io/posts/seed/","summary":"Paper\nCode\nIntroduction 无监督图学习算法基于重构损失，不可避免的需要图相似度计算（重构embedding和输入embedding的loss), 计算复杂度较高。本文提出一种通用的归纳式无监督图学习算法SEED（Sampling, Encoding, and Embedding Distributions）。通过计算采样子图的重构损失来代替整个图的重构损失。 即 先采样子图，在用GNN编码子图，最后计算子图分布的embedding来作为整个图的representation. 过程如下图所示：\nSEED: Sampling, Encoding, and Embedding Distributions Anonymous Random Walk Definition 1 (Random Anonymous Walks[1]): Given a random walk $\\mathbf{w}=(w_1, \\cdots, w_l)$ where $\\langle w_i, w_{i+1} \\rangle \\in E$, the anonymous walk for $\\mathbf{w}$ is defined as： $$ \\mathrm{aw}(\\mathbf{w}) = (\\mathrm{DIS}(\\mathbf{w}, w_1),\\mathrm{DIS}(\\mathbf{w}, w_2),\\cdots, \\mathrm{DIS}(\\mathbf{w}, w_l) ) $$ where $\\mathrm{DIS}(\\mathbf{w}, w_i)$ denotes the number of distinct nodes in $\\mathbf{w}$ when $w_i$ first appears in $\\mathbf{w}$, i.","title":"ICLR2020 《Inductive and Unsupervised Representation Learning on Graph Structured Objects》 Reading Notes"},{"content":"晕了 import networkx as nx import matplotlib.pyplot as plt n_clique, n_path = 10, 10 clique1 = nx.complete_graph(n_clique) clique1_pos = nx.circular_layout(clique1) clique2 = nx.complete_graph(n_clique) clique2_mapping = {node: node + n_clique for node in clique2} nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes x_diff, y_diff = 8, -1 clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2} path = nx.path_graph(n_path) path_mapping = {node: node + 2 * n_clique for node in path} nx.relabel_nodes(path, path_mapping, copy=False) # avoids repeated nodes path_nodes = list(path.nodes) path_half1_nodes = path_nodes[:n_path//2] path_half2_nodes = path_nodes[n_path//2:] path_dist = 0.9 clique2_entry = n_clique + n_clique // 2 path_half1_pos = {node: clique1_pos[0] + (path_dist + i * path_dist, 0) for i, node in enumerate(path_half1_nodes)} path_half2_pos = {node: clique2_pos[clique2_entry] - (path_dist + i * path_dist, 0) for i, node in enumerate(path_half2_nodes[::-1])} path_pos = {**path_half1_pos, **path_half2_pos} barbell = nx.Graph() barbell.add_edges_from(clique1.edges) barbell.add_edges_from(clique2.edges) barbell.add_edges_from(path.edges) barbell.add_edges_from([(path_half1_nodes[0], 0), (path_half2_nodes[-1], clique2_entry)]) clique_pos = {**clique1_pos, **clique2_pos} barbell_pos = {**clique_pos, **path_pos} plt.figure(figsize=(20, 6)) nx.draw(barbell, pos=barbell_pos, with_labels=True) ","permalink":"https://JhuoW.github.io/posts/barbell_graph/","summary":"晕了 import networkx as nx import matplotlib.pyplot as plt n_clique, n_path = 10, 10 clique1 = nx.complete_graph(n_clique) clique1_pos = nx.circular_layout(clique1) clique2 = nx.complete_graph(n_clique) clique2_mapping = {node: node + n_clique for node in clique2} nx.relabel_nodes(clique2, clique2_mapping, copy=False) # avoids repeated nodes x_diff, y_diff = 8, -1 clique2_pos = {node: clique1_pos[node-n_clique] + (x_diff, y_diff) for node in clique2} path = nx.path_graph(n_path) path_mapping = {node: node + 2 * n_clique for node in path} nx.","title":"Awesome Barbell Graph with Networkx"},{"content":"Mean Discrepancy (MD)均值差异 判断2个分布$p$ 和$q$是否相同。\n$p$分布生成一个样本空间$\\mathbb{P}$ (从$p$中采样$m$个样本)\n$q$分布生成一个样本空间$\\mathbb{Q}$（从$q$中采样$n$个样本）\n函数$f$的输入为 分布生成的样本空间\n如果 $$ \\begin{equation} \\begin{aligned} \\mathrm{mean}(f(\\mathbb{P})) == \\mathrm{mean}(f(\\mathbb{Q})) \\\\ i.e., \\frac{1}{m}\\sum^m_{i=1}f(p_i) = \\frac{1}{n}\\sum^n_{i=1}f(q_i) \\end{aligned} \\end{equation} $$\n则$p$和$q$是同一分布。\nMD can be defined as $$ \\begin{equation} \\begin{aligned} \\mathrm{MD}\u0026amp;=|\\mathrm{mean}(f(\\mathbb{P})) -\\mathrm{mean}(f(\\mathbb{Q})) | \\\\ \u0026amp;= |\\frac{1}{m}\\sum^m_{i=1}f(p_i) - \\frac{1}{n}\\sum^n_{i=1}f(q_i)| \\end{aligned} \\end{equation} $$\nMaximum Mean Discrepancy (MMD) 最大均值差异 定义 MMD: 在函数集$\\mathcal{F}=\\{f_1, f_2, \\cdots \\}$中， 找到一个函数$f^*$， 使得$|\\mathrm{mean}(f^*(\\mathbb{P})) -\\mathrm{mean}(f^*(\\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的最大均值差异（MMD）。MMD =0 表示两个分布相同。 $$ \\operatorname{MMD}[\\mathcal{F}, p, q]:=\\sup _{f \\in \\mathcal{F}}\\left(\\mathbf{E}_{x \\sim p}[f(x)]-\\mathbf{E}_{y \\sim q}[f(y)]\\right) $$ 其中$\\mathbf{E}_{x \\sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\\sup$为上确界直接理解为max就好。\n条件 为了准确判断分布$p$和$q$之间的距离，需要找到一个合适的函数，使得两个分布在这个函数上的距离尽可能大，但搜索空间不能过于大，所以函数空间$\\mathcal{F}$要满足两个条件：\nC1: 函数集$\\mathcal{F}$要足够丰富， 使得MMD尽可能准确\nC2: 考虑数据集样本数量，随着数据集的增大，MMD要能迅速收敛，要求$\\mathcal{F}$足够restrictive (函数集不能无限大)\n所以利用kernel 方法，即， 将两个分布的样本空间映射到一个高维或者无限维的空间$\\mathcal{H}$中，如果两个分布的样本在$\\mathcal{H}$中的均值依然相等，那么这两个分布相等，MMD=0。两个分布在$\\mathcal{H}$中的最大均值为MMD。\n因此，当$\\mathcal{F}$是再生核Hilbert Space 上的单位球（unit ball）时，可以满足以上两个条件。 即，将$\\mathcal{F}$定义为某个kernel对应的RKHS中的函数， 例如，\n给定一个Gaussian Kernel: $k(u,v) = \\{\\exp({-\\frac{||u-v||^2}{2\\sigma}})\\}_\\sigma$, 这个kernel函数是一个Hilbert Space的再生核，那么这个空间可以表示为\n$$ \\begin{equation} \\mathcal{H}_k = \\operatorname{span}({\\Phi(x): x \\in \\mathcal{X}})=\\left\\{f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right): m \\in \\mathbf{N}, x_{i} \\in \\mathcal{X}, \\alpha_{i} \\in \\mathbf{R}\\right\\} \\tag{1} \\end{equation} $$ 空间$\\mathcal{X}$中的每个元素$x_i$都对应于一个函数$k(\\cdot,x_i)=k_{x_i}(\\cdot)$, 那么$\\mathcal{X}$中的所有元素所产生的函数$\\{k_{x_i}(\\cdot)\\}_{x_i \\in \\mathcal{X}}$ 可以span成一个Function Space, 如公式1所示， 这个function space中的每个function可以由\u0026quot;basis functions\u0026quot;$\\{k_{x_i}(\\cdot)\\}_{x_i \\in \\mathcal{X}}$ 通过线性组合得到。那么\n$$f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right)$$\n可以表示kernel $k(\\cdot, \\cdot)$的RKHS中的每个function。 每个valid kernel都有一个RKHS $\\mathcal{H}_k$与它对应。\n我们将MMD的候选函数集$\\mathcal{F}$定义为某一个kernel $k(\\cdot,\\cdot)$所对应的RKHS $\\mathcal{H}_k$中的函数，这样就可以满足所有候选函数都在$\\mathcal{H}_k$中(足够多)，同时如果kernel是Gaussian Kernel, 相当于把样本空间映射到无限高维来做MD,更加准确。\n另外，我们限制范式norm$||f||_{\\mathcal{H}_k} \\leq 1$来避免上界取到无限大\n回到MMD 已知$\\mathcal{F}=\\{f_1(\\cdot), f_2(\\cdot), \\cdots \\}$中的每个函数都是一个高斯核函数$k(\\cdot,\\cdot)$的RKHS中的函数，要从$\\mathcal{H}_k$中选一个函数$f^*(\\cdot)$，使得两个分布的样本间距离在$k(\\cdot,\\cdot)$的RKHS上最大。\n因为$f(\\cdot)$是$\\mathcal{H}_k$中的一个函数，那么$f(\\cdot)$可以表示为$\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right)$, 此时，下式一定成立（参考这里）：\n$$ f(x) = \\langle f(\\cdot), k(\\cdot, x) \\rangle_{\\mathcal{H}_k} $$ $k(\\cdot, x) = \\Phi(x)$表示将$x$映射到空间$\\mathcal{H}_{k}$上的值，即$x$在$\\mathcal{H}_{k}$上的表示。 若$k$是Gaussian Kernel, 那么$k(\\cdot, x)$就是$x$在无限维空间上的表示。\n连续空间中$\\mathbf{E}_{x \\sim p}[f(x)]$可以写为： $$ \\begin{equation} \\begin{aligned} \\mathbf{E}_{x \\sim p}[f(x)] \u0026amp;= \\int_x p(x)f(x) dx\\\\ \u0026amp; = \\int_x p(x) \\langle f(\\cdot), k(\\cdot, x) \\rangle_{\\mathcal{H}_k} dx \\\\ \u0026amp;= \\langle \\int_x p(x)f(\\cdot) dx, \\int_x p(x)k(\\cdot, x) dx \\rangle_{\\mathcal{H}_k}\\\\ \u0026amp;= \\langle f(\\cdot), \\mu_p\\rangle_{\\mathcal{H}_k} \\end{aligned} \\end{equation} $$ 其中$\\mu_p = \\int_x p(x)k(\\cdot, x) dx$.\n因此，MMD可以改写为： $$ \\begin{equation} \\begin{aligned} \\operatorname{MMD}(\\mathrm{p}, \\mathrm{q}, \\mathcal{H})\u0026amp;:=\\sup_{f \\in \\mathcal{H},|f|_{\\mathcal{H}} \\leq 1}(\\underset{\\mathrm{p}(\\boldsymbol{x})}{\\mathbb{E}}[f(\\boldsymbol{x})]-\\underset{\\mathrm{q}(\\boldsymbol{y})}{\\mathbb{E}}[f(\\boldsymbol{y})])\\\\ \u0026amp;=\\sup_{f \\in \\mathcal{H},|f|_{\\mathcal{H}_k} \\leq 1}\\left(\\left\\langle\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}, f\\right\\rangle_{\\mathcal{H}_k}\\right) \\end{aligned} \\end{equation} $$ 利用内积性质：$\\langle a, b \\rangle \\leq ||a|| ||b||$， 因为 $$ ||f(\\cdot)||_{\\mathcal{H}_k}\\leq 1 $$ , $$ \\left\\langle\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}, f\\right\\rangle_{\\mathcal{H}_k} \\leq ||\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}||_{\\mathcal{H}_k}||f||_{\\mathcal{H}_k} $$ Then, $$ \\sup_{f \\in \\mathcal{H},|f|_{\\mathcal{H}_k} \\leq 1}\\left(\\left\\langle\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}, f\\right\\rangle_{\\mathcal{H}_k}\\right) =||\\mu_{\\mathrm{p}}-\\mu_{\\mathrm{q}}||_{\\mathcal{H}_k} $$ 其中$\\mu_p = \\int_x p(x)k(\\cdot, x) dx$, $\\mu_q = \\int_y q(y)k(\\cdot, y) dy$ 分别表示分布的期望(均值)。 然而期望无法直接计算，因此用样本空间的均值代替分布的期望： $$ \\begin{equation} \\begin{aligned} \\mathrm{M M D}(p,q,\\mathcal{H}_k) \u0026amp; \\approx \\mathrm{M M D}(X,Y,\\mathcal{F}_{\\mathcal{H}_k})\\\\ \u0026amp;=\\left|\\left|\\frac{1}{n} \\sum_{i=1}^{n} f(x_i)-\\frac{1}{m} \\sum_{j=1}^{m} f(x_j)\\right|\\right|_{\\mathcal{H}_k} \\end{aligned} \\end{equation} $$\n$$ \\begin{equation} \\begin{aligned} \\mathrm{M M D}^2(p,q,\\mathcal{H}_k) \u0026amp; \\approx \\mathrm{M M D}^2(X,Y,\\mathcal{F}_{\\mathcal{H}_k})\\\\ \u0026amp;=\\left|\\left|\\frac{1}{n} \\sum_{i=1}^{n} f(x_i)-\\frac{1}{m} \\sum_{j=1}^{m} f(x_j)\\right|\\right|_{\\mathcal{H}_k}^{2}\\\\ \u0026amp;= \\left|\\left|\\frac{1}{n^{2}} \\sum_{i}^{n} \\sum_{i^{\\prime}}^{n} \\left\\langle f(x_i),f(x_i^{\\prime})\\right\\rangle-\\frac{2}{n m} \\sum_{i}^{n} \\sum_{j}^{m} \\left\\langle f(x_i), f(y_j)\\right\\rangle+\\frac{1}{m^{2}} \\sum_{j}^{m} \\sum_{j^{\\prime}}^{m} \\left\\langle f(y_j), f(y_j^{\\prime})\\right\\rangle\\right|\\right|_{\\mathcal{H}_k} \\\\ \u0026amp; = \\frac{1}{n^2} K_{x, x^\\prime}-\\frac{2}{nm} K_{x, y}+\\frac{1}{m^{2}} K_{y, y^{\\prime}} \\end{aligned} \\end{equation} $$\n令 $$ K=\\begin{bmatrix} K_{x, x^{\\prime}} \u0026amp; K_{x, y} \\\\ K_{x, y}\u0026amp; K_{y, y^{\\prime}} \\end{bmatrix} $$\n$$ M=\\begin{bmatrix}\\frac{1}{n^{2}} \u0026amp;-\\frac{1}{n m} \\\\ -\\frac{1}{n m}\u0026amp; \\frac{1}{m^{3}} \\end{bmatrix} $$\n最后： $$ \\mathrm{M M D}^2(X,Y,\\mathcal{F}_{\\mathcal{H}_k}) = tr(KM) $$\n","permalink":"https://JhuoW.github.io/posts/mmd/","summary":"Mean Discrepancy (MD)均值差异 判断2个分布$p$ 和$q$是否相同。\n$p$分布生成一个样本空间$\\mathbb{P}$ (从$p$中采样$m$个样本)\n$q$分布生成一个样本空间$\\mathbb{Q}$（从$q$中采样$n$个样本）\n函数$f$的输入为 分布生成的样本空间\n如果 $$ \\begin{equation} \\begin{aligned} \\mathrm{mean}(f(\\mathbb{P})) == \\mathrm{mean}(f(\\mathbb{Q})) \\\\ i.e., \\frac{1}{m}\\sum^m_{i=1}f(p_i) = \\frac{1}{n}\\sum^n_{i=1}f(q_i) \\end{aligned} \\end{equation} $$\n则$p$和$q$是同一分布。\nMD can be defined as $$ \\begin{equation} \\begin{aligned} \\mathrm{MD}\u0026amp;=|\\mathrm{mean}(f(\\mathbb{P})) -\\mathrm{mean}(f(\\mathbb{Q})) | \\\\ \u0026amp;= |\\frac{1}{m}\\sum^m_{i=1}f(p_i) - \\frac{1}{n}\\sum^n_{i=1}f(q_i)| \\end{aligned} \\end{equation} $$\nMaximum Mean Discrepancy (MMD) 最大均值差异 定义 MMD: 在函数集$\\mathcal{F}=\\{f_1, f_2, \\cdots \\}$中， 找到一个函数$f^*$， 使得$|\\mathrm{mean}(f^*(\\mathbb{P})) -\\mathrm{mean}(f^*(\\mathbb{Q})) |$ 最大。 这个最大值就是两个分布之间的最大均值差异（MMD）。MMD =0 表示两个分布相同。 $$ \\operatorname{MMD}[\\mathcal{F}, p, q]:=\\sup _{f \\in \\mathcal{F}}\\left(\\mathbf{E}_{x \\sim p}[f(x)]-\\mathbf{E}_{y \\sim q}[f(y)]\\right) $$ 其中$\\mathbf{E}_{x \\sim p}[f(x)]$表示分布$p$在函数$f$下的均值， $\\sup$为上确界直接理解为max就好。","title":"Maximum Mean Discrepancy"},{"content":"Hilbert Space Definition 1 (Norm) Let $\\mathcal{F}$ be a vector space over $\\mathbb{R}$ (For example $\\mathcal{F}=\\mathbb{R}^n$ is a vector space). A function $||\\cdot||_{\\mathcal{F}}: \\mathcal{F} \\to [0, \\inf)$ is said to be a norm on $\\mathcal{F}$ if ($||\\cdot||_{\\mathcal{F}}$ 是一个有效norm算子要满足以下条件):\n For $f \\in \\mathcal{F}$, $||f||_{\\mathcal{F}}=0$ if and only if $f=0$. (norm separates points) $|\\lambda f|_{\\mathcal{F}}=|\\lambda||f|_{\\mathcal{F}}$, $\\forall \\lambda \\in \\mathbb{R}, \\forall f \\in \\mathcal{F}$ (positive homogeneity). $|f+g|_{\\mathcal{F}} \\leq|f|_{\\mathcal{F}}+|g|_{\\mathcal{F}}, \\forall f, g \\in \\mathcal{F}$ (triangle inequality).  向$||\\cdot||_{\\mathcal{F}}$中输入任意一个向量，只要满足以上条件，那么$||\\cdot||_{\\mathcal{F}}$是一个valid norm operator.\nInner Product An inner product takes two elements of a vector space $\\mathcal{X}$ and outputs a number. An inner product could be a usual dot product: $\\langle\\mathbf{u}, \\mathbf{v}\\rangle=\\mathbf{u}^{\\prime} \\mathbf{v}=\\sum_{i} u^{(i)} v^{(i)}$ (Inner Product can be Dot Product). Or the inner product could be something fancier（即内积不一定表示为点积的形式）. If an Inner Product $\\langle \\cdot,\\cdot \\rangle$ is valid, it MUST satisfy the following conditions:\n  Symmetry $$\\langle u, v\\rangle=\\langle v, u\\rangle \\quad \\forall u, v \\in \\mathcal{X}$$\n  Bilinearity $$\\langle\\alpha u+\\beta v, w\\rangle=\\alpha\\langle u, w\\rangle+\\beta\\langle v, w\\rangle \\quad \\forall u, v, w \\in \\mathcal{X}, \\forall \\alpha, \\beta \\in \\mathbf{R}$$\n  Strict Positive Definiteness $$ \\begin{gathered} \\langle u, u\\rangle \\geq 0 \\forall x \\in \\mathcal{X} \\\\ \\langle u, u\\rangle=0 \\Longleftrightarrow u=0 \\end{gathered}$$\n  An inner product space (or pre-Hilbert space) is a vector space together with an inner product. （包含内积运算的向量空间称为 内积空间，即可以定义内积运算的向量空间）。\nKernel is a kind of Inner Product. For example, the Gaussian kernel is defined as: $$ \\begin{equation} \\langle u, v \\rangle = k(u,v) = \\exp({-\\frac{||u-v||^2}{2\\sigma}}) \\tag{1} \\end{equation} $$\nHilbert Space Definition 2 (Hilbert Space) A Hilbert Space is an Inner Product space that is complete and separable with respect to the norm defined by the inner product.\n\u0026lsquo;Complete\u0026rsquo; means sequences converge to elements of the space - there aren\u0026rsquo;t any \u0026ldquo;holes\u0026rdquo; in the space.\nFinite States Given finite input space ${x_1, x_2, \\cdots x_m }$. I want to be able to take inner products between any two of them using my function $k$ as the inner product ($k$ is customized and satisfy three conditions. For example, $k$ is a Gaussian inner product as Eq.(1)). Inner products by definition are symmetric, so $k(x_i, x_j)=k(x_j, x_i)$ , which yields a symmetric matrix $\\mathbf{K}$.\nSince $\\mathbf{K}$ is real symmetric, and this means we can diagonalize it （实对称阵可以对角化，即特征分解）, and the eigendecomposition takes this form: $$ \\begin{equation} \\begin{aligned} \\mathbf{K} \u0026amp;=\\mathbf{V} \\Lambda \\mathbf{V}^T \\\\ \u0026amp;= \\mathbf{V} \\begin{bmatrix} \\lambda_1 \u0026amp; \u0026amp; \u0026amp; \\\\ \u0026amp; \\lambda_2 \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\cdots \u0026amp;\\ \u0026amp; \u0026amp; \u0026amp;\\lambda_m \\end{bmatrix} \\mathbf{V}^T \\\\ \u0026amp;= \\begin{bmatrix} v_1 \u0026amp; v_2 \u0026amp; \\cdots v_m \\end{bmatrix} \\begin{bmatrix} \\lambda_1 \u0026amp; \u0026amp; \u0026amp; \\\\ \u0026amp; \\lambda_2 \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\cdots \u0026amp;\\\\ \u0026amp; \u0026amp; \u0026amp;\\lambda_m \\end{bmatrix} \\begin{bmatrix} v_1^T\\\\ v_2^T\\\\ \\cdots \\\\ v_m^T \\end{bmatrix}\\\\ \u0026amp;=v_1\\lambda_1 v_1^T + \\cdots + v_m\\lambda_m v_m^T = \\sum_{t=1}^m v_t\\lambda_tv_t^T \\end{aligned} \\tag{2} \\end{equation} $$ Let the $i$-th element of vector $v$ as $v^{(i)}$, then $$ \\begin{equation} \\begin{aligned} \\mathbf{K}_{ij} = k(x_i, x_j) \u0026amp;= [\\sum_{t=1}^m v_t\\lambda_tv_t^T]_{ij}\\\\ \u0026amp;=\\sum^m_{t=1} v_t^{(i)} \\lambda_t v_t^{(j)} \\end{aligned} \\tag{3} \\end{equation} $$ If $\\mathbf{K}$ is a positive semi-definite (PSD) matrix, then $\\lambda_1, \\cdots \\lambda_m \\geq 0$.\n Assumption 1. All $\\lambda_t$ are nonnegative.\n We consider this feature map: $$ \\begin{equation} \\Phi\\left(x_{i}\\right)=\\left[\\sqrt{\\lambda_{1}} v_{1}^{(i)}, \\ldots, \\sqrt{\\lambda_{t}} v_{t}^{(i)}, \\ldots, \\sqrt{\\lambda_{m}} v_{m}^{(i)}\\right] \\in \\mathbb{R}^m \\tag{4} \\end{equation} $$ (writing it for $x_j$ too): $$ \\begin{equation} \\boldsymbol{\\Phi}\\left(x_{j}\\right)=\\left[\\sqrt{\\lambda_{1}} v_{1}^{(j)}, \\ldots, \\sqrt{\\lambda_{t}} v_{t}^{(j)}, \\ldots, \\sqrt{\\lambda_{m}} v_{m}^{(j)}\\right] \\in \\mathbb{R}^m \\tag{5} \\end{equation} $$ 即 $\\Phi: \\mathcal{X} \\to \\mathbb{R}^m$ 将$x\\in \\mathcal{X}$映射到$m$维向量空间$\\mathbb{R}^m$中的一个点。\nWith this choice, the inner product $k$ is just defined as a dot product in $\\mathbb{R}^m$: $$ \\begin{equation} \\left\\langle\\Phi\\left(x_{i}\\right), \\Phi\\left(x_{j}\\right)\\right\\rangle_{\\mathbf{R}^{m}}=\\sum_{t=1}^{m} \\lambda_{t} v_{t}^{(i)} v_{t}^{(j)}=\\left(\\mathbf{V} \\Lambda \\mathbf{V}^{\\prime}\\right)_{i j}=K_{i j}=k\\left(x_{i}, x_{j}\\right) \\tag{6} \\end{equation} $$ If there exists an eigenvalue $\\lambda_s \u0026lt;0$ (即$\\sqrt{\\lambda_s} = \\sqrt{|\\lambda_s|} i$). $\\lambda_s$对应的特征向量$v_s$。用$v_s \\in \\mathbb{R}^m$的$m$个元素$v_s = [v_s^{(1)},\\cdots, v_s^{(m)}]$, 来对$\\Phi(x_1),\\cdots, \\Phi(x_m)$做线性组合： $$ \\begin{equation} \\mathbf{z}=\\sum_{i=1}^{m} v_{s}^{(i)} \\boldsymbol{\\Phi}\\left(x_{i}\\right) \\tag{7} \\end{equation} $$\nIt is obvious that $\\mathbf{z} \\in \\mathbb{R}^m$. Then calculate $$ \\begin{equation} \\begin{aligned} |\\mathbf{z}|_{2}^{2} \u0026amp;=\\langle\\mathbf{z}, \\mathbf{z}\\rangle_{\\mathbf{R}^{m}}=\\sum_{i} \\sum_{j} v_{s}^{(i)} \\boldsymbol{\\Phi}\\left(x_{i}\\right)^{T} \\boldsymbol{\\Phi}\\left(x_{j}\\right) v_{s}^{(j)}=\\sum_{i} \\sum_{j} v_{s}^{(i)} K_{i j} v_{s}^{(j)} \\\\ \u0026amp;=\\mathbf{v}_{s}^{T} \\mathbf{K} \\mathbf{v}_{s}=\\lambda_{s}\u0026lt;0 \\end{aligned} \\tag{8} \\end{equation} $$ which conflicts with the geometry of the feature space.\n如果$\\mathbf{K}$不是半正定，那么feature space $\\mathbb{R}^m$存在小于0的值。所以假设Assumption不成立。即，若$k$表示有限集的内积，那么它的Gram Matrix一定半正定(PSD)，否则无法保证该空间中的norm大于0。\n有效的内积对应的Gram Matrix 必定PSD.\nKernel Definition 3. (Kernel) A function $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is a kernel if\n $k$ is symmetric: $k(x,y) = k(y,x)$. $k$ gives rise to a positive semi-definite \u0026ldquo;Gram matrix,\u0026rdquo; i.e., for any $m\\in \\mathbb{N}$ and any $x_1,\\cdots,x_m$ chosen from $X$, the Gram matrix $\\mathbf{K}$ defined by $\\mathbf{K}_{ij} = k(x_i,x_j)$ is positive semi-definite.  Another way to show that a matrix $\\mathbf{K}$ is positive semi-definite is to show that $$ \\begin{equation} \\forall \\mathbf{c} \\in \\mathbf{R}^{m}, \\mathbf{c}^{T} \\mathbf{K} \\mathbf{c} \\geq 0 \\tag{9} \\end{equation} $$ Here are some nice properties of $k$:\n $k(u,u) \\geq 0$ (Think about the Gram matrix of $m = 1$.) $k(u, v) \\leq \\sqrt{k(u, u) k(v, v)}$ (This is the Cauchy-Schwarz inequality.)  Reproducing Kernel Hilbert Space (RKHS) 给定一个kernel $k(\\cdot, \\cdot): \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$. 定义一个函数空间（space of functions）$\\mathbf{R}^{\\mathcal{X}}:={f: \\mathcal{X} \\rightarrow \\mathbb{R}}$. $\\mathbf{R}^{\\mathcal{X}}$ 是一个 Hilbert Space， 该空间中的每个元素是一个$\\mathcal{X}$映射到$\\mathbb{R}$的函数。\n令$k_x(\\cdot) = k(x, \\cdot)$, 假设$x$是一个定值（Constant），自变量（输入）用$\\cdot$表示。那么$k(x, \\cdot)$ 也是$\\mathbf{R}^{\\mathcal{X}}$空间中的一个函数。\n每个函数$k_x(\\cdot)$ 都与一个特定的$x \\in \\mathcal{X}$有关，即每个$x$对应于一个函数$k_x(\\cdot) = k(\\cdot, x)$. 这种对应关系表示为$\\Phi(x) = k_x(\\cdot) = k(x,\\cdot)$, 即： $$ \\begin{equation} \\Phi: x \\longmapsto k(\\cdot, x) \\tag{10} \\end{equation} $$ 即 $\\Phi$的输入为$x\\in \\mathcal{X}$, 输出一个函数, 输出的函数属于$\\mathbf{R}^{\\mathcal{X}}$空间。\n在连续空间$\\mathcal{X}$中，$x \\in \\mathcal{X}$ 有无穷多种情况，那么$\\Phi(x)=k_x(\\cdot)=k(x, \\cdot)$也有无穷多种情况，即无穷多种函数。 这些函数可以span 一个Hilbert Space: $$ \\begin{equation} \\mathcal{H}_k = \\operatorname{span}({\\Phi(x): x \\in \\mathcal{X}})=\\left\\{f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right): m \\in \\mathbf{N}, x_{i} \\in \\mathcal{X}, \\alpha_{i} \\in \\mathbf{R}\\right\\} \\tag{11} \\end{equation} $$ 其中$k(x,\\cdot)=\\Phi(x)$可以理解为将$x$映射为一个函数（or vector）。上述Hilbert Space是由任意$k(x, \\cdot)$线性组合而成的函数空间，该空间中的每个元素可以表示为 $$ \\begin{equation} f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right) \\tag{12} \\end{equation} $$ 所以$\\mathcal{H}$可以看作是kernel $k$对应的一个Hilbert Space。\n给定$\\mathcal{H}$中的任意两个函数$f(\\cdot)=\\sum_{i=1}^{m} \\alpha_{i} k\\left(\\cdot, x_{i}\\right)$, $g(\\cdot)=\\sum_{j=1}^{m^{\\prime}} \\beta_{j} k\\left(\\cdot, x_{j}^{\\prime}\\right)$。注意$f(\\cdot)$和$g(\\cdot)$可以表示$\\mathcal{H}$中任意两个元素。我们将$\\mathcal{H}$上的内积定义为： $$ \\begin{equation} \\langle f, g\\rangle_{\\mathcal{H}_{k}}=\\sum_{i=1}^{m} \\sum_{j=1}^{m^{\\prime}} \\alpha_{i} \\beta_{j} k\\left(x_{i}, x_{j}^{\\prime}\\right) \\tag{13} \\end{equation} $$ 由Proof证明了该内积符合三个条件，顾上式是$\\mathcal{H}$空间中一个有效的内积算子。注：$\\mathcal{H}_k$表示该Hilbert Space是由函数 $k(x,\\cdot)$ span而成的，与Kernel $k$有关.\n$k(x,\\cdot)$也是$\\mathcal{H}_k$中的一个函数，那么它与 $f$的内积为： $$ \\begin{equation} \\langle k(\\cdot, x), f\\rangle_{\\mathcal{H}_{k}}= \\sum_{i=1}^m \\alpha_i k(x,x_i) =f(x) \\tag{14} \\end{equation} $$ Theorem 1. $k(\\cdot, \\cdot)$ is a reproducing kernel of a Hilbert space $\\mathcal{H}_k$ if $f(x)=\\langle k(x, \\cdot), f(\\cdot)\\rangle$.\n$\\mathcal{H}_k$ 为$k(\\cdot, \\cdot)$的再生核希尔伯特空间。\n同理，$k(\\cdot, x_i)$, $k(\\cdot, x_j)$都为$\\mathcal{H}_k$中的函数， 计算他们的内积: $$ \\begin{equation} \\left\\langle k(\\cdot, x_i), k\\left(\\cdot, x_j\\right)\\right\\rangle_{\\mathcal{H}_{k}}=k\\left(x_i, x_j\\right) \\tag{15} \\end{equation} $$ 因为$ k(\\cdot, x_i) = \\Phi(x_i)$, $ k(\\cdot, x_j) = \\Phi(x_j)$, 所以 $$ \\begin{equation} k\\left(x_i, x_j\\right) = \\left\\langle \\Phi(x_i), \\Phi(x_j)\\right\\rangle_{\\mathcal{H}_{k}} \\tag{16} \\end{equation} $$ 表示将$x_i$和$x_j$ 映射成$\\mathcal{H}_k$中的函数（向量）后再做内积。\n参考文献 [1] https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf\n[2] http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf\n[3] https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf\n我把本文整理成了PDF\n","permalink":"https://JhuoW.github.io/posts/rkhs_kernel/","summary":"Hilbert Space Definition 1 (Norm) Let $\\mathcal{F}$ be a vector space over $\\mathbb{R}$ (For example $\\mathcal{F}=\\mathbb{R}^n$ is a vector space). A function $||\\cdot||_{\\mathcal{F}}: \\mathcal{F} \\to [0, \\inf)$ is said to be a norm on $\\mathcal{F}$ if ($||\\cdot||_{\\mathcal{F}}$ 是一个有效norm算子要满足以下条件):\n For $f \\in \\mathcal{F}$, $||f||_{\\mathcal{F}}=0$ if and only if $f=0$. (norm separates points) $|\\lambda f|_{\\mathcal{F}}=|\\lambda||f|_{\\mathcal{F}}$, $\\forall \\lambda \\in \\mathbb{R}, \\forall f \\in \\mathcal{F}$ (positive homogeneity). $|f+g|_{\\mathcal{F}} \\leq|f|_{\\mathcal{F}}+|g|_{\\mathcal{F}}, \\forall f, g \\in \\mathcal{F}$ (triangle inequality).","title":"Reproducing Kernel Hilbert Space"},{"content":"单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。\n在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?\n多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。\n  利用（Exploitation）： 保证在过去决策中得到最佳回报\n  探索（Exploration）：寄希望在未来能够得到更大的汇报\n  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。\n但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。\n悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \\cdots (i = 1,\\cdots, k)$。 在时刻$t=1,2,\\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \\max_{i=1,\\cdots,k} \\sum^n_{t = 1} X_{i,t}-\\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机\n$I_t$: $t$时刻选择的赌博机\n$\\max_{i=1,\\cdots,k} \\sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励\n$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward\n$\\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。\n$R_n$越大，就代表$n$次决策的结果越差。\n上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡\n在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\\max_{i \\in{1, \\ldots, k}}\\left\\{\\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \\right\\}. $$ 其中$I_{t}$为$t$时刻要摇的赌博机，\n$\\overline{X_{i, T_{i}(t-1)}}$要尽可能大：为$t$时刻之前赌博机$i$的平均reward 要尽可能大，\n$C_{t-1, T_{i}(t-1)}$要尽可能小：$t$时刻之前$i$出现的次数要尽可能少\n其中$C_{t,T_i(t)}$的取值定义如下： $$ C_{t,T_i(t)}=\\sqrt{\\frac{2 \\operatorname{In} t}{T_i(t)}} $$ 其中$T_i(t)$表示 $t$时刻以前（包括$t$），选到赌博机$i$的次数总和。\n若赌博机在$t$时刻之前（包括$t$）时刻摇动的次数越少，$C_{t,T_i(t)}$越大\n选出的$I_t$要满足在$t$之前的平均reward尽可能大（利用）， 且在$t$之前出现次数尽可能少（探索）。\n也就是说，在第时刻，UCB算法一般会选择具有如下最大值的第$j$个赌博机： $$ \\begin{aligned} U C B\u0026amp;=\\bar{X}_{j}+\\sqrt{\\frac{2 \\operatorname{Inn}}{n_{j}}} \\text { 或者 } U C B=\\bar{X}_{j}+C \\times \\sqrt{\\frac{2 \\operatorname{In} n}{n_{j}}} \\\\ I_t \u0026amp;= \\mathrm{argmax}_j UCB(j) \\end{aligned} $$ 其中$\\bar{X}_{j}$是第$j$个赌博机在过去时间内所获得的平均奖赏值，$n_j$是在过去时间内拉动第$j$个赌博机臂膀的总次数，$n$是过去时间内拉动所有赌博机臂膀的总次数。$C$是一个平衡因子，其决定着在选择时偏重探索还是利用。\n从这里可看出UCB算法如何在探索-利用(exploration-exploitation)之间寻找平衡：既需要拉动在过去时间内获得最大平均奖赏的赌博机，又希望去选择拉动臂膀次数最少的赌博机。\nMonte Carlo Tree Search MCTS has four step:\n Selection 选择 Expansion 拓展 Simulation（rollout) 模拟 Backpropagation 回溯  选择 从根节点R开始，向下递归选择子节点，直至选择一个叶子节点L。 具体来说，通常用UCBl(Upper Confidence Bound,上限置信区间)选择最具“潜力”的后续节点： $$ U C B=\\bar{X}_{j}+\\sqrt{\\frac{2 \\operatorname{In} n}{n_{j}}} $$\n拓展 如果L不是一个终止节点（即博弈游戏不)，则随机创建其后的一个未被访问节点，选择该节点作为后续子节点C。\n模拟 从节点C出发，对游戏进行模拟，直到博弈游戏结束。\n反向传播 用模拟所得结果来回溯更新导致这个结果的每个节点中获胜次数和访问次数。\n包含两种策略学习机制：\n搜索树策略：从已有的搜索树中选择或创建一个叶子结点（即蒙特卡洛中选择和拓展两个步骤).搜索树策略需要在利用和探索之间保持平衡。\n模拟策略：从非叶子结点出发模拟游戏，得到游戏仿真结果。\n例子： 围棋   以围棋为例，假设根节点是执黑棋方。\n  图中每一个节点都代表一个局面，每一个局面记录两个值A/B\n  A: 该局面被访问中黑棋胜利次数。对于黑棋表示己方胜利次数，对于白棋表示己方失败次数（对方胜利次数)；\nB: 该局面被访问的总次数\n初始状态：根节点（12/21）是当前局面，该局面被访问过21次，其中黑棋访问该局面后最终获胜12次。 黑执棋方遇到该局面（根节点局面）开始选择下一步action。\n选择 黑执棋方遇到（12/21）这个局面时有3个有效action, 黑执行这三个action会得到3种局面（7/10）,（5/8）, (0/3). 黑执棋方要选择其中一个action，是的棋局变为这三种中的一种，那么要如何选择action呢？我们先分别计算UCB1：\n左一： 7/10对应的局面Reward为： $$ \\frac{7}{10} + \\sqrt{\\frac{\\log (21)}{10}} = 1.252 $$ 3中局面共21次被模拟到，其中，该局面被访问过10次（探索， 第二项）， 黑棋遇到该局面后最终获胜7次（利用， 第一项）。\n左二：（5/8）对应局面Reward: $$ \\frac{5}{8} + \\sqrt{\\frac{\\log(21)}{8}} = 1.243 $$ 左三： （0/3）对应局面Reward: $$ \\frac{0}{3} + \\sqrt{\\frac{\\log(21)}{3}} = 1.007 $$ 由此可见，黑棋选择会导致局面（7/10）的action进行走琪。\n白棋遇到局面（7/10）时，有两个有效的action, 分别会导致局面A/B = 2/4和5/6, A为白棋访问该局面后失败的次数，所以白棋访问该局面最终胜利的次数应该是$B-A$：\n左一： (2/4)对应的局面Reward (白棋尽可能获胜)为： $$ (1-\\frac{2}{4}) + \\sqrt{\\frac{\\log(21)}{4}}=1.372 $$ 左二： (5/6)对应的局面Reward为： $$ (1-\\frac{5}{6}) + \\sqrt{\\frac{\\log(21)}{4}}=0.879 $$ 因此白棋会选择（2/4）局面\n即每一步都寻找最佳应对方式，来最终评估更节点局面的好坏\n白色执棋方选择action后到达局面（2/4）, 黑棋执棋方面对该局面，有两种action可选，分别会到达（1/3）和（1/1）， 分别计算UCB1 得：\n左一： (1/3)对应reward 为： $$ \\frac{1}{3} + \\sqrt{\\frac{\\log (21)}{3}} = 1.341 $$ 左二：(1/1)对应reward为： $$ \\frac{1}{1} + \\sqrt{\\frac{\\log (21)}{1}} = 2.745 $$ 则黑棋选择action 使得局面变为(1/1)。右图中可见(1/1)为叶子节点， 白棋面对该局面暂时无候选局面（action）,则进入下一个步骤，拓展。\n拓展 白棋执棋方面对局面(1/1),因为已经是叶子节点， 所以(1/1)会随机产生一个未被访问过的新节点，初始化为(0/0),接着在该节点下进行模拟。 进入第三部， 模拟。\n模拟 黑棋执棋方面局面（0/0），开始进行游戏仿真， 假设经过一些列仿真后，最终白棋获胜，即从更节点到最终模拟结束的叶子节点黑棋失败了。 进入第四部，回溯\n回溯 根据仿真结果来更新该仿真路径上的每个节点的A/B值。 因为该次模拟最终白棋获胜，所以向上回溯路径上的所有父节点，父节点的A不变，B+1。 若最终黑棋获胜，则父节点的A+1, B+1。\n在有限时间里， MCTS会不断重复4个步骤，所有节点的A,B 值会不断变化\n到某个局面（节点）后如何下棋（action）, 会选择所有有效Action中UCB1值最大的节点，作为下棋的下一步。\n","permalink":"https://JhuoW.github.io/posts/monte-carlo-tree-search/","summary":"单一状态Monte Carlo规划：多臂赌博机（Multi-armed bandits） 单一状态$s$, $k$种action（$k$个摇臂）。\n在摇臂赌博机问题中，$s$每次以随机采样形式采取一种行动$a$。 例如，随机拉动第$k$个赌博机，得到的reward为$R(s, a_k)$。问下一次拉动哪个赌博机，才能获得最大Reward?\n多臂赌博机问题是一种序列决策问题，这种问题需要在利用（exploitation）和探索（exploration）之间保持平衡。\n  利用（Exploitation）： 保证在过去决策中得到最佳回报\n  探索（Exploration）：寄希望在未来能够得到更大的汇报\n  例如， $s$摇动10次赌博机，其中第5次获得最大回报，那么下一次摇动赌博机可能选第5个赌博机，即“利用”（根据过去的最佳回报来施展下一次行为），第11次摇动的action为$a_5$。\n但是摇动10次后，可能有的赌博机尚未摇动，未摇动的赌博机可能会有更大回报，即“探索”。\n悔值函数 如果有$k$个赌博机， 这$k$个赌博机产生的操作序列为$X_{i,1}, X_{i,2}, \\cdots (i = 1,\\cdots, k)$。 在时刻$t=1,2,\\cdots$, 选择第$I_t$个赌博机后，可得到的奖励为$X_{I_t,t}$, 则在$n$次操作$I_1,\\cdots,I_n$后，可如下定义悔值函数： $$ R_n = \\max_{i=1,\\cdots,k} \\sum^n_{t = 1} X_{i,t}-\\sum^n_{t=1}X_{I_t, t} $$ $i$: 第$i$个赌博机\n$I_t$: $t$时刻选择的赌博机\n$\\max_{i=1,\\cdots,k} \\sum^n_{t = 1} X_{i,t}$: 最理想（大）的奖励\n$X_{I_t, t}$: $t$时刻选择赌博机$I_t$的reward\n$\\sum^n_{t=1}X_{I_t, t}$: $n$次选择的总奖励。\n$R_n$越大，就代表$n$次决策的结果越差。\n上置信区间（Upper Confidence Bound, UCB） UCB旨在探索和利用间去的平衡\n在UCB方法中，使$X_{i, T_{i}(t-1)}$ 来记录第$i$个赌博机在过去$t-1$时刻内的平均奖励，则在第$t$时刻，选择使如下具有最佳上限置信区间的赌博机： $$ I_{t}=\\max_{i \\in{1, \\ldots, k}}\\left\\{\\overline{X_{i, T_{i}(t-1)}}+C_{t-1, T_{i}(t-1)} \\right\\}.","title":"Monte Carlo Tree Search"},{"content":"论文地址： DiffPool\nIntroduction 传统的GNN算法在Node-level的任务如节点分类、链路预测上有着较好的效果。但是，现有的GNN方法由于其存在平面化的局限性，因此无法学习图的层级表示（意味着无法预测整个图的标签），顾无法实现图分类任务。举个栗子，一个Graph，可以分成600个subgraph，每个节点都存在于其中的某个subgraph（一个节点只存在于一个subgraph中），每个subgraph拥有一个标签，如何预测subgraph的标签是这篇文章主要想解决的问题。传统的GNN的图分类方法都是为Graph中的所有节点生成Embedding，然后将对这些Embedding做全局聚合（池化），如简单的把属于同一个subgraph的节点求和或者输入到MLP中生成一个标签向量来表示整个subgraph，但是这样可能忽略的图的层级结构信息。\n本文提出了一种端到端的可微可微图池化模块DiffPool，原理如下图所示：\n在深度GNN中的每层中为节点学习可微的软簇分配，将节点映射到簇中，这些簇作为新的节点作为下一层GNN的输入。上图的Original Network部分是一个Subgraph，传统的方法是直接求出这个Subgraph中每个节点的Embedding，然后相加或输入到一个神经网络中，得到一个预测向量，这种方法可以称为“全局池化”。DiffPool中，假设第$l$层的输入是$1000$个簇（如果是第一层输入就是1000个节点），我们先设置第$l+1$层需要输入的簇的个数（假设为$100$），也就是第$l$层输出的簇个数，然后在$l$层中通过一个分配矩阵将$1000$个簇做合并，合并成100个“节点”，然后将这100个节点输入到$l+1$层中，最后图中的节点数逐渐减少，最后，图中的节点只有一个，这个节点的embedding就是整个图的表示，然后将图输入到一个多层感知机MLP中，得到预测向量，在于真值的one-hot向量做cross-entropy，得到Loss。\nModel：DiffPool 一个Graph表示为$\\mathcal{G} = (A,F)$，其中$A \\in {0,1}^{n \\times n}$是Graph的邻接矩阵，$F \\in \\mathbb{R}^{n \\times d}$表示节点特征矩阵，每个节点有$d$维的特征。给定一个带标签的子图集$\\mathcal{D}=\\left\\{\\left(G_{1}, y_{1}\\right),\\left(G_{2}, y_{2}\\right), \\ldots\\right\\}$， 其中 $y_{i} \\in \\mathcal{Y}$表示每个子图$G_i \\in \\mathcal{G}$的标签，任务目标是寻找映射$f: \\mathcal{G} \\rightarrow \\mathcal{Y}$，将图映射到标签集。我们需要一个过程来将每个子图转化为一个有限维度的向量$\\mathbb{R}^D$。\nGraph Neural Networks 一般，GNN可以表示成\u0026quot;Message Passing\u0026quot;框架： $$ H^{(k)}=M\\left(A, H^{(k-1)} ; \\theta^{(k)}\\right) $$ 其中$H^{(k)} \\in \\mathbb{R}^{n \\times d}$表示GNN迭代$k$次后的node embedding，$M$是一个Message扩散函数，由邻接矩阵$A$和一个可训练的参数$\\theta^{(k)}$决定。$H^{(k-1)}$是由前一个message passing过程生成的node embedding。当$k = 1$时，第一个GNN的输入为$H^{(0)}$是原始的节点特征$H^{(0)} = F$。\nGNN的一个主要目标是设计一个Message Passage函数$M$，GCN（kipf.2016）是一种流行的GNN，$M$的实现方式是将线性变换和ReLU非线性激活结合起来: $$ H^{(k)}=M\\left(A, H^{(k-1)} ; W^{(k)}\\right)=\\operatorname{ReLU}\\left(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(k-1)} W^{(k-1)}\\right) $$ 其中，$\\tilde{A} = A+I$是一个加上自环的邻接矩阵，$\\tilde{D}=\\sum_{j} \\tilde{A}_{i j}$是$\\tilde{A}$的度矩阵，$W^{(k)} \\in \\mathbb{R}^{d \\times d}$是一个可训练的权重矩阵，$W$与节点个数以及每个节点的度无关，可以看做一个特征增强矩阵，用来规定GCN的输出维度。\n一个完整的GNN模型会迭代$K$次来输出最终的node embedding$Z = H^{(K)} \\in \\mathbb{R}^{n \\times d}$。对于GCN，GAT，GraphSage，$K$一般取2-6。文中为了简单表示，忽略了GNN的内部结构，用$Z=GNN(A,X)$来表示一个任意的执行$K$次的GNN模块。\nGNN和池化层的堆叠 这篇工作的目标是定义一个一般的，端到端的可微策略，允许以层级的方式堆叠多个GNN模块。给定原始的邻接矩阵$A \\in \\mathbb{R}^{n \\times n}$，$Z=GNN(A,X)$十一GNN模块的输出（假设这个GNN模块做了3次迭代）。我们需要定义一个策略来输出一个新的粗化图，这个粗化图包含$m$个节点，$m \u0026lt; n$，它的邻接矩阵一个带权重的邻接矩阵$A\u0026rsquo; \\in \\mathbb{R}^{m \\times m}$，同时，输出node embedding $Z\u0026rsquo; \\in \\mathbb{R}^{m \\times d}$。这个粗化图（$m$个节点的图）作为下一层GNN的输入 （将$A\u0026rsquo;$和$Z\u0026rsquo;$输入下一个GNN层）。最后所有节点粗化为只有一个节点的图，这个节点的embedding就是这个subgraph的表示。因此，目标为：如何使用上一层GNN的输出结果，对节点做合并或池化，是的图中的节点减少，再将粗化的图输入到下一个GNN中。\n基于可学习分配的可微分池化 DiffPool通过对一个GNN模块的输出学习一个聚类分配矩阵来解决这个问题。可微池化层根据$l-1$层的GNN模块（假设是一个3次迭代的GNN模块）产生的node embedding来对节点做合并，从而产生一个粗化图，这个粗化图作为$l$层GNN模块的输入，最终，整个subgraph被粗化为一个cluster，可以看做一个节点。\n用分配矩阵进行池化 $S^{(l)} \\in \\mathbb{R}^{n_{l} \\times n_{l+1}}$表示第$l$层的聚类分配矩阵，$S^{(l)}$的每一行表示第l层的每个节点（cluster）,每一列表示$l+1$层的每个cluster（节点）。$S^{(l)}_{ij}$表示第$l$层的节点$i$属于第$l+1$层cluster $j$的概率，所以$S^{(l)}$是个概率矩阵。\n假如已经有了第$l$层的节点分配矩阵$S^{(l)}$，将第$l$层的邻接矩阵表示为$A^{(l)}$，将第$l$层GNN模块的输出节点特征（node embedding）表示为$Z^{(l)}$，通过DiffPool层可以将第$l$层的图粗化为$\\left(A^{(l+1)}, X^{(l+1)}\\right)=\\operatorname{DIFFPOOL}\\left(A^{(l)}, Z^{(l)}\\right)$，其中，$A^{(l+1)}$是$l+1$层图的邻接矩阵，是一个粗化后的图，$X^{(l+1)}$是下一层的输入特征（node/cluster embedding）： $$ \\begin{aligned} \u0026amp;X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \\in \\mathbb{R}^{n_{l+1} \\times d}\\ \u0026amp;A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \\in \\mathbb{R}^{n_{l+1} \\times n_{l+1}} \\end{aligned} $$ 上面第一个公式将第$l$层节点嵌入$Z^{(l)}$转化为下一层的输入特征$X^{(l+1)}$。第二个公式将第$l$层的邻接矩阵转化为$l+1$层的粗化图邻接矩阵$A^{(l+1)}$。$n_{l+1}$是$l+1$层节点（cluster）的数量。最后，将$A^{(l+1)}$和$X^{(l+1)}$作为下一层GNN的输入。这样图中的节点就由$n_l$个下降到$n_{l+1}$个。\n学习分配矩阵S 第$l$层的输入特征$X^{(l)}$，用一个GNN模块（代码中是一个3层的GCN）得到node embedding： $$ Z^{(l)}=\\mathrm{GNN}_{l, \\text { embed }}\\left(A^{(l)}, X^{(l)}\\right) $$ 用另外一个GNN模块（代码中是一个3层的GCN）在用一个softmax转化为概率矩阵来的到节点分配矩阵： $$ S^{(l)}=\\operatorname{softmax}\\left(\\mathrm{GNN}_{l, \\mathrm{pool}}\\left(A^{(l)}, X^{(l)}\\right)\\right) $$ $S^{(l)}$是一个$n_l \\times n_{l+1}$的全链接矩阵，$S^{(l)}_{ij}$表示第$l$层的节点$i$属于第$l+1$层cluster $j$的概率。\n$l=0$时，第一层GNN的输入是subgraph的原始邻接矩阵$A$和特征矩阵$F$，倒数第二层$l=L-1$时的分配矩阵$S^{(L-1)}$是一个全1向量，那么最后将所以节点归为一类，产生一个代表整个图的嵌入向量。\n所以，把图节点的合并过程称为分层的图表示学习（Hierarchical Graph Representation Learning）。\n","permalink":"https://JhuoW.github.io/posts/diffpool/","summary":"论文地址： DiffPool\nIntroduction 传统的GNN算法在Node-level的任务如节点分类、链路预测上有着较好的效果。但是，现有的GNN方法由于其存在平面化的局限性，因此无法学习图的层级表示（意味着无法预测整个图的标签），顾无法实现图分类任务。举个栗子，一个Graph，可以分成600个subgraph，每个节点都存在于其中的某个subgraph（一个节点只存在于一个subgraph中），每个subgraph拥有一个标签，如何预测subgraph的标签是这篇文章主要想解决的问题。传统的GNN的图分类方法都是为Graph中的所有节点生成Embedding，然后将对这些Embedding做全局聚合（池化），如简单的把属于同一个subgraph的节点求和或者输入到MLP中生成一个标签向量来表示整个subgraph，但是这样可能忽略的图的层级结构信息。\n本文提出了一种端到端的可微可微图池化模块DiffPool，原理如下图所示：\n在深度GNN中的每层中为节点学习可微的软簇分配，将节点映射到簇中，这些簇作为新的节点作为下一层GNN的输入。上图的Original Network部分是一个Subgraph，传统的方法是直接求出这个Subgraph中每个节点的Embedding，然后相加或输入到一个神经网络中，得到一个预测向量，这种方法可以称为“全局池化”。DiffPool中，假设第$l$层的输入是$1000$个簇（如果是第一层输入就是1000个节点），我们先设置第$l+1$层需要输入的簇的个数（假设为$100$），也就是第$l$层输出的簇个数，然后在$l$层中通过一个分配矩阵将$1000$个簇做合并，合并成100个“节点”，然后将这100个节点输入到$l+1$层中，最后图中的节点数逐渐减少，最后，图中的节点只有一个，这个节点的embedding就是整个图的表示，然后将图输入到一个多层感知机MLP中，得到预测向量，在于真值的one-hot向量做cross-entropy，得到Loss。\nModel：DiffPool 一个Graph表示为$\\mathcal{G} = (A,F)$，其中$A \\in {0,1}^{n \\times n}$是Graph的邻接矩阵，$F \\in \\mathbb{R}^{n \\times d}$表示节点特征矩阵，每个节点有$d$维的特征。给定一个带标签的子图集$\\mathcal{D}=\\left\\{\\left(G_{1}, y_{1}\\right),\\left(G_{2}, y_{2}\\right), \\ldots\\right\\}$， 其中 $y_{i} \\in \\mathcal{Y}$表示每个子图$G_i \\in \\mathcal{G}$的标签，任务目标是寻找映射$f: \\mathcal{G} \\rightarrow \\mathcal{Y}$，将图映射到标签集。我们需要一个过程来将每个子图转化为一个有限维度的向量$\\mathbb{R}^D$。\nGraph Neural Networks 一般，GNN可以表示成\u0026quot;Message Passing\u0026quot;框架： $$ H^{(k)}=M\\left(A, H^{(k-1)} ; \\theta^{(k)}\\right) $$ 其中$H^{(k)} \\in \\mathbb{R}^{n \\times d}$表示GNN迭代$k$次后的node embedding，$M$是一个Message扩散函数，由邻接矩阵$A$和一个可训练的参数$\\theta^{(k)}$决定。$H^{(k-1)}$是由前一个message passing过程生成的node embedding。当$k = 1$时，第一个GNN的输入为$H^{(0)}$是原始的节点特征$H^{(0)} = F$。\nGNN的一个主要目标是设计一个Message Passage函数$M$，GCN（kipf.2016）是一种流行的GNN，$M$的实现方式是将线性变换和ReLU非线性激活结合起来: $$ H^{(k)}=M\\left(A, H^{(k-1)} ; W^{(k)}\\right)=\\operatorname{ReLU}\\left(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(k-1)} W^{(k-1)}\\right) $$ 其中，$\\tilde{A} = A+I$是一个加上自环的邻接矩阵，$\\tilde{D}=\\sum_{j} \\tilde{A}_{i j}$是$\\tilde{A}$的度矩阵，$W^{(k)} \\in \\mathbb{R}^{d \\times d}$是一个可训练的权重矩阵，$W$与节点个数以及每个节点的度无关，可以看做一个特征增强矩阵，用来规定GCN的输出维度。","title":"NIPS2018 《DiffPool:Hierarchical Graph Representation Learning with Differentiable Pooling》 Reading Notes"},{"content":"最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。\n本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments\nIntroduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。\n基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} \u0026gt; 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \\sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\\displaystyle \\left(\\begin{array}{ccc}{d_{1}} \u0026amp; {\\ldots} \u0026amp; {\\ldots} \\\\\\ {\\ldots} \u0026amp; {d_{2}} \u0026amp; {\\ldots} \\\\\\ {\\vdots} \u0026amp; {\\vdots} \u0026amp; {\\ddots} \\\\\\ {\\ldots} \u0026amp; {\\ldots} \u0026amp; {d_{n}}\\end{array}\\right) ^{n\\times n} $$ 是一个$n \\times n$的对角阵，对角元素是每个节点的度和。\n定义图的邻接矩阵为$W \\in \\mathbb{R}^{n \\times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \\subset V$， 定义： $$|A|=A 中的节点个数 $$\n$$vol(A) = \\sum_{i \\in A} d_i \\quad 表示A中所有节点的权重之和$$\n基础2：相似矩阵 再提下谱聚类的基本思想： 距离较远的两个点之间权重值较低，距离较近的两个点之间权重值较高\n但是在谱聚类中，我们只能获得数据点的定义，无法给出邻接矩阵，因为是离散的分布在空间中，无法得知其中的连接关系。\n一般来说，通过样本点距离度量的相似矩阵$S$来获得邻接矩阵$W$.\n构建邻接矩阵$W$有两种方法: $\\epsilon$-邻近法， K邻近法和全连接法。\n$\\epsilon$-邻近法 $\\epsilon$为距离的阈值，欧式距离$S_{ij}$表示两点$v_i$,$v_j$的坐标$x_i$,$x_j$之间的距离。 即 $S_{ij} =||x_i-x_j||^2_2$为相似矩阵$S \\in \\mathbb{R}^{n \\times n}$的第$i$行第$j$个元素，则邻接矩阵$W$可以表示为：\n$$ w_{ij}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {s_{i j}\u0026gt;\\epsilon} \\\\ {\\epsilon} \u0026amp; {s_{i j} \\leq \\epsilon}\\end{array}\\right. $$\n意思是如果两点之间的距离大于$\\epsilon$，那么他们之间的权重为0， 如果他们之间的距离小于$\\epsilon$，他们之间的权重为$\\epsilon$。 这种方法的弊端在于点之间的距离只有两种情况，不够精确，故很少采用。\nK邻近法 利用KNN算法遍历所有样本点，取每个样本点最近的$k$个点作为近邻，只有和样本点距离最近的$k$个点的$w_{ij} \u0026gt;0$，但会出现一种情况， $v_i$的k个近邻中有$v_j$，但$v_j$的k个近邻中没有$v_i$，这样会造成邻接矩阵$W$的不对称， 因此提供两种解决办法\n第一种： 只要$v_j$在$v_i$的K邻域中，那么不管$v_i$在不在$v_j$的K邻域中，都把$v_i$加入$v_j$的邻域中，即：\n$$ w_{i j}=w_{j i}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {x_{i} \\notin K N N\\left(x_{j}\\right) \\text { and } x_{j} \\notin K N N\\left(x_{i}\\right)} \\\\ {\\exp \\left(-\\frac{\\left||x_{i}-x_{j}\\right||^2_2}{2 \\sigma^{2}}\\right)} \u0026amp; {x_{i} \\in K N N\\left(x_{j}\\right) \\text { or } x_{j} \\in K N N\\left(x_{i}\\right)}\\end{array}\\right. $$ 第二种，必须$v_i$在$v_j$的K邻域中，且$v_j$在$v_i$的K邻域中，那么才保留两者间的权重，否则都为0，即：\n$$ w_{i j}=w_{j i}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {x_{i} \\notin K N N\\left(x_{j}\\right) \\text { or } x_{j} \\notin K N N\\left(x_{i}\\right)} \\\\ {\\exp \\left(-\\frac{||x_{i}-x_{j}||^2_2}{2 \\sigma^{2}}\\right)} \u0026amp; {x_{i} \\in K N N\\left(x_{j}\\right) \\text { and } x_{j} \\in K N N\\left(x_{i}\\right)}\\end{array}\\right. $$\n全连接法 设所有点之间的权重都大于0，可以选择不同的核函数来定义边的权重，常用的如多项式核函数，高斯核函数， Sigmod核函数。 最常用的为高斯核函数RBF，将两点之间的距离带入高斯核函数RBF中，即： $$ w_{i j}=w_{ji}=s_{i j}=s_{ji}=\\exp \\left(-\\frac{\\left|x_{i}-x_{j}\\right|_{2}^{2}}{2 \\sigma^{2}}\\right) $$ 其中，$sigma$为为函数的宽度参数 , 控制了函数的径向作用范围。\n基础3：拉普拉斯矩阵 拉普拉斯矩阵定义为$L = D-W$ 如上文所示，$D$为度矩阵，$W$为邻接矩阵。\n拉普拉斯矩阵具有如下性质：\n  $L$是对称阵 （因为$D$和$W$都是对称阵）\n  $L$的所有特征值都是实数 （因为$L$是对称阵）\n  对于任意向量$f$， 有$f^TLf = \\displaystyle \\frac{1}{2} \\sum_{i = 1}^n \\sum_{j = 1}^n w_{ij} (f_i-f_j)^2$\n推导： $$ \\begin{aligned} f^TLf \u0026amp;= f^TDf-f^TWf\\\\ \u0026amp;= \\sum^n_{i = 1}d_if_i^2 - \\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij}\\\\ \u0026amp;= \\frac{1}{2}\\left(\\sum^n_{i=1}d_if_i^2 -2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum^n_{i=1}d_if_i^2\\right)\\\\ \u0026amp;由于d_i = \\sum_{j = 1}^nw_{ij}, 将d_i带入上式得\\\\ f^TLf \u0026amp;= \\frac{1}{2} \\left(\\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2-2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2\\right)\\\\ \u0026amp; = \\frac{1}{2} \\left(\\sum_{i = 1}^n\\sum_{j =1}^n w_{ij}f_i^2-2\\sum_{i = 1}^n\\sum_{j =1}^n f_if_j w_{ij} + \\sum_{j = 1}^n\\sum_{i =1}^n w_{ji}f_j^2\\right)\\\\ \u0026amp;= \\frac{1}{2}\\left(\\sum_{i=1}^n\\sum_{j = 1}^n w_{ij} (f_i-f_j)^2\\right) \\end{aligned} $$\n  拉普帕斯矩阵是半正定的，且$n$个实数特征值都$\\geq$，即 $0=\\lambda_1 \\leq \\lambda_2 \\cdots \\leq \\lambda_n$，且最小的特征值为0。\n证明，因为$f^TLf \\geq 0$ 所以$L$半正定。\n  基础4：无向图切图 对于无向图$G$，目的是将图$G = (v,E)$切成相互没有连接的k个子图，每个子图是一个节点集合：$A_1,A_2,\\cdots, A_k$，满足$A_i \\cap A_j = \\phi$ 且$A_1 \\cup A_2 \\cup \\cdots \\cup A_k = V$，对于两个节点集合$A ,B \\subset V$, $A \\cap B = \\phi$，定义$A$,$B$之间的切图权重为： $$ W(A,B) = \\sum_{v_i\\in A, v_j \\in B} w_{ij} \\quad 表示A中节点到B中节点的权重和 $$ 对于$k$个子图节点集合$A_1,A_2,\\cdots, A_k$，定义切图$Cut$为： $$ Cut(A_1,A_2, \\cdots, A_k) = \\frac{1}{2}\\sum^k_{i = 1} W(A_i,\\overline{A_i}) $$ 其中$\\overline{A_i}$是$A_i$的补集，也就是除$A_i$外其他所有节点的集合，$Cut$计算的是$A_i$中每个节点到$\\overline{A_i}$中每个节点的权重总和，如果最小化$Cut$，就相当于最小化每个子集中节点到自己外节点的权重，但是会存在一个问题，如下图所示：\n如果按左边那条线分割，可以保证有边子图到左边子图的权重最小，但不是最佳分割。\n谱聚类：切图聚类 为了避免上述效果不佳的情况，提供两种切图方式，1.RatioCut, 2.Ncut.\nRatioCut 切图 最小化$Cut(A_1,A_2, \\cdots, A_k)$的同时，最大化每个子图中接待你的个数，即：A_{i}, \\overline{A}_{i}\n$$RatioCut\\left(A_{1}, A_{2}, \\ldots A_{k}\\right)=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\overline{A_i}\\right)}{\\left|A_{i}\\right|}$$\n目标是最小化$RatioCut\\left(A_{1}, A_{2}, \\ldots A_{k}\\right)$。\n为此，我们引入一个指示向量（indicator vector）$h_j \\in {h_1,h_2,\\cdots, h_k}$，其中$j = 1,2,\\cdots,k$，对于其中任意一个向量$h_j$，它是一个$n$维向量，即： $$ h_j = (h_{1j},h_{2j}, \\cdots, h_{nj})^T \\\\ h_{i j}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {v_{i} \\notin A_{j}} \\\\ {\\frac{1}{\\sqrt{\\left|A_{j}\\right|}}} \u0026amp; {v_{i} \\in A_{j}}\\end{array}\\right. $$ $h_ij$表示节点$v_i$ 是否属于子图$A_j$, 如果属于，那么$h_{ij} = \\frac{1}{\\sqrt{\\left|A_{j}\\right|}}$，如果不属于，那么$h_{ij} = 0$。\n那么对于$h_i^TLh_i$有： $$ \\begin{aligned} h_i^T L h_i \u0026amp;= \\frac{1}{2}\\sum_{m=1}\\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\\\ \u0026amp;= \\frac{1}{2}\\left(\\sum_{m\\in A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2+\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\\\ \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2 + \\sum_{m\\notin A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2\\right)\\\\ \u0026amp; 上式中的mn是图中任意选取的两个不同的节点， 针对子图A_i及其对应的指示向量h_i,\\\\ \u0026amp;任意选取的节点对有四种情况。其中 根据h_{ij}的定义，第一项和第四项为0，所以\\\\ \u0026amp;=\\frac{1}{2} \\left(\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(h_{im}-h_{in})^2\\right) \\\\ \u0026amp;=\\frac{1}{2} \\left(\\sum_{m\\in A_i}\\sum_{n \\notin A_i}w_{mn}(\\frac{1}{\\sqrt{\\left|A_{i}\\right|}})^2 + \\sum_{m\\notin A_i}\\sum_{n \\in A_i}w_{mn}(-\\frac{1}{\\sqrt{\\left|A_{i}\\right|}})^2\\right) \\\\ \u0026amp;=\\frac{1}{2}\\left(\\frac{1}{|A_i|}Cut(A_i,\\overline{A_i}) + \\frac{1}{|A_i|}Cut(A_i,\\overline{A_i})\\right)\\\\ \u0026amp;=\\frac{Cut(A_i,\\overline{A_i})}{|A_i|} = RatioCut(A_i) \\end{aligned} $$ 上式$h_i^TLh_i$可以看做是子图$A_i$的RatioCut，那么： $$ \\begin{aligned} RatioCut(A_1,A_2,\\cdots,A_k) \u0026amp;=\\frac{1}{2} \\sum_{i=1}^{k} \\frac{W\\left(A_{i}, \\overline{A_i}\\right)}{\\left|A_{i}\\right|} = \\sum_{i = 1}^k \\frac{Cut(A_i,\\overline{A_i})}{|A_i|} \\\\ \u0026amp;= \\sum_{i=1}^k h_i^TLh^i = \\sum_{i=1}^k (H^TLH)_{ii} = tr(H^TLH) \\end{aligned} $$ 每个$h_i^TLh^j$的值对应于矩阵$H^TLH$在位置$ij$处的值： $$ H=(h_1,h_2,\\cdots,h_k) \\in \\mathbb{R}^{n\\times k} $$ $$ h_i^TLh_j = (H^TLH)_{ij} \\to h^T_iLh_i = (H^TLH)_{ii} $$\n由于$h_i\\cdot h_j = 0, h_i \\cdot h_i = 1$, 所以$H^TH = I$是一个单位矩阵\n所以切图优化函数为： $$ \\underbrace{\\arg \\min }_{H} RatioCut\\left(A_1,A_2,\\cdots A_k\\right) = \\underbrace{\\arg \\min }_{H} \\operatorname{tr}\\left(H^{T} L H\\right) \\quad \\text { s.t. } \\quad H^{T} H=I $$ $H$中的每个指示向量$h$是$n$维的，每个向量中的元素有两种取值，分别是0和$\\frac{1}{\\sqrt{\\left|A_{j}\\right|}}$， 所以每个$h$有$2^n$种可能性，所以整个$H$有$k2^n$中，因此上述目标函数是个NP-hard问题。\n注意到$tr(H^TLH)$中每个优化子目标$h_i^TLh_i$，其中，$h$是单位正交基，基每个元素的平方和等于1，$L$为对称矩阵，此时$h^T_iLh_i$的最大值为$L$的最大特征值， $h^T_iLh_i$的最小值是$L$的最小特征值。在谱聚类中，我们的目标是找到目标函数的最小特征值，从而使目标函数最小，得到对应的特征向量。\n对于$h^T_iLh_i$，目标是找到$L$最小的特征值，这个值就是$h^T_iLh_i$的最小值，对于$tr(H^TLH) = \\sum^k_{i=1} h^T_iLh_i$，目标是找到$k$个最小的特征值，从而使$tr(H^TLH)$最小。\n通过找到$L$最小的$k$个特征值，可以对应得到$k$个特征向量，这$k$个特征向量可以组成一个$n \\times k$的矩阵，这个矩阵就是我们需要的指示向量矩阵$H$，里面包含了每个节点所属子图的信息。一般来说 我们需要对$H$按行做标准化： $$ h_{ij}^* = \\frac{h_{ij}}{\\sqrt{\\sum_{t=1}^kh^2_{it}}} $$ 注意到，$H$的每行是一个$k$维行向量，即$H_i = (H_{i1},H_{i2},\\cdots, H_{ik})$， 表示节点$i$属于每个子图的指标值， 我们可以把$H_i$当做节点$v_i$的表示向量， 由于归一化后的$H$还不能明确指示各个样本的归属， 我们还需要对$H$代表的所有节点做一次传统聚类，如K-Means。\nNCut切图 把$RatioCut$的分母从$|A_i|$换成$vol(A_i) = \\sum_{j \\in A_i}d_j$， 为$A_i$中所有节点的权重之和，一般来说$NCut$效果好于$RatioCut$: $$ NCut(A_1,A_2,\\cdots,A_k) = \\frac{1}{2}\\sum_{i=1}^k\\frac{W(A_i,\\overline{A_i})}{vol(A_i)} = \\sum^k_{i = 1}\\frac{Cut(A_i)}{vol(A_i)} $$ $NCut$指示向量$h$做了改进，$RatioCut$切图在指示向量中使用$\\frac{1}{\\sqrt{|A_i|}}$表示某节点归属于子图$A_i$，而$NCut$切图使用子图权重$\\frac{1}{\\sqrt{vol{A_i}}}$来表示某节点归属子图$A_i$如下： $$ h_{i j}=\\left\\{\\begin{array}{ll}{0} \u0026amp; {v_{i} \\notin A_{j}} \\\\ {\\frac{1}{\\sqrt{v o l\\left(A_{j}\\right)}}} \u0026amp; {v_{i} \\in A_{j}}\\end{array}\\right. $$ 上式表示如果节点$v_i$在子图$A_j$中，那么指示向量$h_j$的第$i$个元素为$\\frac{1}{\\sqrt{vol{A_j}}}$。\n那么对于$h_i^TLh_i$有： $$ h^T_iLh_i = \\frac{1}{2}\\sum_{m=1}\\sum_{n=1}w_{mn}(h_{im}-h_{in})^2 = \\frac{Cut(A_i)}{vol(A_i)} =NCut(A_i) $$ 目标函数： $$ NCut(A_i,A_2,\\cdots,A_k) = \\sum^k_{i = 1} NCut(A_i) = \\sum^k_{i=1}h^T_iLh_i =\\sum^k_{i=1}(H^TLH)_{ii} = tr(H^TLH) $$ 此时，$h_i \\cdot h_j = 0$，$h_i\\cdot h_i = \\frac{|A_i|}{vol(A_i)} \\neq 1$， 所以$H^TH \\neq I$。\n但是， 由于：$h^T_iDh_i = \\sum^n_{j = 1} h_{ij}^2d_j$, $d_j$为节点$v_j$的权重和，$h_{ij}$的值表示节点j是否在子图$A_i$中，如果在子图$A_i$中，那么$h_{ij}^2 = \\frac{1}{vol(A_i)}$，否则为0。\n$$h^T_iDh_i = \\frac{1}{vol(A_i)} \\sum_{v_j \\in A_i} d_j = \\frac{1}{vol(A_i)} \\cdot vol(A_i) = 1$$\n最终目标函数为： $$ \\underbrace{\\arg \\min } _{H}\\operatorname{tr}\\left(H^{T} L H\\right) \\quad \\text { s.t. }\\quad H^{T} D H=I $$ 由于$H$中的指示向量$h$不是标准正交基，所以RatioCut中加粗的定理不能直接使用，所以，令$H = D^{-\\frac{1}{2}}F$, $D^{-\\frac{1}{2}}$表示对$D$对角线上元素开方后求逆，那么： $$ H^TLH = F^TD^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}F $$\n$$ H^TDH = F^TD^{-\\frac{1}{2}}DD^{-\\frac{1}{2}}F = F^TF=I $$ 所以目标函数转化为： $$ \\underbrace{\\arg \\min }_{F} \\operatorname{tr}\\left(F^{T} D^{-1 / 2} L D^{-1 / 2} F\\right) \\quad \\text { s.t. } \\quad F^{T} F=I $$ 同$RatioCut$，$D^{-1 / 2} L D^{-1 / 2}$是对称矩阵，$F$中的每个向量为标准正交基，只需要求出$D^{-1 / 2} L D^{-1 / 2}$的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，最后得到特征矩阵$F$，然后对$F$的行向量做一次传统聚类，如K-means.\n一般来说， $D^{-1 / 2} L D^{-1 / 2}$相当于对拉普拉斯矩阵$L$做了一次标准化，即$\\frac{L_{i j}}{\\sqrt{d_{i} * d_{j}}}$.\n我把本文整理成了PDF\n","permalink":"https://JhuoW.github.io/posts/2019-09-07-spectral-clustering/","summary":"最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。\n本文主要参考了：[1] https://www.cnblogs.com/pinard/p/6221564.html#!comments\nIntroduction 谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。\n基础1： 无向权重图 对于边$(v_i,v_j)$, 它的权重$w_{ij} \u0026gt; 0$。对于没有边的节点$v_i$和$v_j$, 他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即： $$ d_i = \\sum_{j=1}^n w_{ij} $$ 根据所有节点的度值，我们可以得到一个度矩阵$D$: $$ D=\\displaystyle \\left(\\begin{array}{ccc}{d_{1}} \u0026amp; {\\ldots} \u0026amp; {\\ldots} \\\\\\ {\\ldots} \u0026amp; {d_{2}} \u0026amp; {\\ldots} \\\\\\ {\\vdots} \u0026amp; {\\vdots} \u0026amp; {\\ddots} \\\\\\ {\\ldots} \u0026amp; {\\ldots} \u0026amp; {d_{n}}\\end{array}\\right) ^{n\\times n} $$ 是一个$n \\times n$的对角阵，对角元素是每个节点的度和。\n定义图的邻接矩阵为$W \\in \\mathbb{R}^{n \\times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \\subset V$， 定义： $$|A|=A 中的节点个数 $$\n$$vol(A) = \\sum_{i \\in A} d_i \\quad 表示A中所有节点的权重之和$$","title":"Notes for Spectral Clustering"},{"content":"论文地址：M-NMF\nIntroduction Network Embedding的基本需求是保存网络机构和固有属性。而先前的方法主要保存了网络的围观结构（microscopic structure）如一阶相似度和二阶相似度（LINE，SDNE等）， 忽略了网络结构的介观社区结构（mesoscopic community structure）。针对这个问题，本文提出了一种模块化非负矩阵分解模型M-NMF，该模型将网络的社区结构融入network embedding中。利用节点表示和社区结构之间的一致关系（consensus relationship）， 联合优化基于非负矩阵分解的表示学习模型和基于模块化的社区发现模型， 这样就可以在节点表示中同时保存了微观网络结构和介观社区结构。\n具体来说，对于微观结构（节点对相似性），本文使用矩阵分解来融合节点的一节相似性和二阶相似性；对于介观社区结构，通过模块度约束项来检测社区。因此，上面的两项可以通过节点的表示和基于辅助社区表示矩阵社区结构之间的一致关系来联合优化。同时，本文给出了乘法更新策略，保证了再推导参数的时候的正确性和收敛性。\nM-NMF Model 对于一个无向图$G=(V,E)$, 图中有$n$个节点和$e$条边，用一个0,1二值邻接矩阵表示为$\\mathbf{A}=[A_{i,j}] \\in \\mathbb{R}^{n \\times n}$。 $\\mathbf{U} \\in \\mathbb{R}^{n \\times m}$ 为节点的表示矩阵，其中$m \\leq n$，$m$是节点的嵌入维度。\n建模社区结构 本文采用基于模块最大化的社区发现方法 Modularity， 给定一个邻接矩阵表示的网络$\\mathbf{A}$，$\\mathbf{A}$包含两个社区，根据Newman 2006b，模块度可以定义如下： $$ Q=\\frac{1}{4 e} \\sum_{i j}\\left(A_{i j}-\\frac{k_{i} k_{j}}{2 e}\\right) h_{i} h_{j} $$ 其中，$k_i$表示节点$i$的度，如果$i$在第一个社区中，那么$h_i=1$,否则$h_i = -1$。\n$k_ik_j$表示将所有边一分为二 参考模块度Q，那么节点$i$,$j$之间可能产生的边数。$\\frac{k_{i} k_{j}}{2 e}$如果所有边随机放置，节点$i$,$j$之间的期望边数。定义一个模块度矩阵$\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$，其中$B_{i,j}=A_{i,j}-\\frac{k_{i} k_{j}}{2 e}$，那么$Q=\\frac{1}{4 e} \\mathbf{h}^{T} \\mathbf{B h}$，其中$\\mathbf{h}=[h_i] \\in \\mathbb{R}^n$，表示社区成员指标器。\n如果将$Q$拓展到$k \u0026gt; 2$个社区，那么： $$ Q=\\operatorname{tr}\\left(\\mathbf{H}^{T} \\mathbf{B H}\\right), \\quad \\text { s.t. } \\quad \\operatorname{tr}\\left(\\mathbf{H}^{T} \\mathbf{H}\\right)=n $$ 其中$tr()$表示矩阵的迹（主对角线元素和），$\\mathbf{H}$是社区成员指标器，$\\mathbf{H} \\in \\mathbb{R}^{n \\times k}$，每行表示一个节点所属社区的one-hot编码。\n建模微观结构 一阶相似度 $$ S^{(1)} = \\mathbf{A} $$\n二阶相似度 表示为$S^{(2)}$，表示节点的邻域相似度， 用邻接向量的余弦相似度表示： $$ S_{i j}^{(2)}=\\frac{\\mathcal{N}_{i} \\mathcal{N}_{j}}{\\left|\\left|\\mathcal{N}_{i}\\right|\\right|\\left|\\left|\\mathcal{N}_{j}\\right|\\right|} $$\n结合网络的一阶结构的一阶二阶相似度，最终的网络相似度矩阵可以表示为： $$ \\mathbf{S}^{(1)}+\\eta \\mathbf{S}^{(2)} $$ 然后，文中引入了一个偏置矩阵$\\mathbf{M} \\in \\mathbb{R}^{n \\times m}$ 和一个非负表示矩阵$\\mathbf{U} \\in \\mathbb{R}^{n \\times m}$。所以微观结构的目标函数就是节点相似度和节点表示之间的误差： $$ \\min \\left|\\mathbf{S}-\\mathbf{M} \\mathbf{U}^{T}\\right|_{F}^{2} $$ 因为$\\mathbf{S} \\in \\mathbb{R}^{n \\times n}$， 矩阵$\\mathbf{U} \\in \\mathbb{R}^{n \\times m}$，矩阵$\\mathbf{M}$的作用是把$\\mathbf{U}$转成$n \\times n$，这样就可以计算损失函数了。\n统一的NE模型 引入一个非负辅助矩阵$\\mathbf{C} \\in \\mathbb{R}^{k \\times m}$, 即为社区表示矩阵，每一行$C_r$表示第$r$个社区的$m$维表示向量。 如果一个节点的表示向量和一个社区的表示向量接近，那么这个节点就很可能在这个社区中。我们把节点$i$和社区$r$之间的从属关系定义为： $$ \\mathbf{U}_{i} \\mathbf{C}_{r} $$ 如果两个向量正交，则$\\mathbf{U}_{i} \\mathbf{C}_{r} = 0$ 那么节点$i$不可能存在于社区$r$中。所以需要使$\\mathbf{U}\\mathbf{C}^T$更加近似社区指示器$\\mathbf{H}$,所以定义如下目标函数： $$ \\begin{aligned} \\min_{\\mathbf{M}, \\mathbf{U}, \\mathbf{H}, \\mathbf{C}}\\left|\\left|\\mathbf{S}-\\mathbf{M} \\mathbf{U}^{T}\\right|\\right|_{F}^{2}+\\alpha\\left|\\left|\\mathbf{H}-\\mathbf{U} \\mathbf{C}^{T}\\right|\\right|_{F}^{2}-\\beta \\operatorname{tr}\\left(\\mathbf{H}^{T} \\mathbf{B} \\mathbf{H}\\right) \\\\ s.t., \\quad \\mathrm{M} \\geqslant 0, \\mathrm{U} \\geqslant 0, \\mathrm{H} \\geqslant 0, \\mathrm{C} \\geqslant 0, \\operatorname{tr}\\left(\\mathrm{H}^{T} \\mathrm{H}\\right)=n \\end{aligned} $$ 其中$\\alpha$和$\\beta$是正参数，最后一项是要最大化模块度。\n","permalink":"https://JhuoW.github.io/posts/m-nmf/","summary":"论文地址：M-NMF\nIntroduction Network Embedding的基本需求是保存网络机构和固有属性。而先前的方法主要保存了网络的围观结构（microscopic structure）如一阶相似度和二阶相似度（LINE，SDNE等）， 忽略了网络结构的介观社区结构（mesoscopic community structure）。针对这个问题，本文提出了一种模块化非负矩阵分解模型M-NMF，该模型将网络的社区结构融入network embedding中。利用节点表示和社区结构之间的一致关系（consensus relationship）， 联合优化基于非负矩阵分解的表示学习模型和基于模块化的社区发现模型， 这样就可以在节点表示中同时保存了微观网络结构和介观社区结构。\n具体来说，对于微观结构（节点对相似性），本文使用矩阵分解来融合节点的一节相似性和二阶相似性；对于介观社区结构，通过模块度约束项来检测社区。因此，上面的两项可以通过节点的表示和基于辅助社区表示矩阵社区结构之间的一致关系来联合优化。同时，本文给出了乘法更新策略，保证了再推导参数的时候的正确性和收敛性。\nM-NMF Model 对于一个无向图$G=(V,E)$, 图中有$n$个节点和$e$条边，用一个0,1二值邻接矩阵表示为$\\mathbf{A}=[A_{i,j}] \\in \\mathbb{R}^{n \\times n}$。 $\\mathbf{U} \\in \\mathbb{R}^{n \\times m}$ 为节点的表示矩阵，其中$m \\leq n$，$m$是节点的嵌入维度。\n建模社区结构 本文采用基于模块最大化的社区发现方法 Modularity， 给定一个邻接矩阵表示的网络$\\mathbf{A}$，$\\mathbf{A}$包含两个社区，根据Newman 2006b，模块度可以定义如下： $$ Q=\\frac{1}{4 e} \\sum_{i j}\\left(A_{i j}-\\frac{k_{i} k_{j}}{2 e}\\right) h_{i} h_{j} $$ 其中，$k_i$表示节点$i$的度，如果$i$在第一个社区中，那么$h_i=1$,否则$h_i = -1$。\n$k_ik_j$表示将所有边一分为二 参考模块度Q，那么节点$i$,$j$之间可能产生的边数。$\\frac{k_{i} k_{j}}{2 e}$如果所有边随机放置，节点$i$,$j$之间的期望边数。定义一个模块度矩阵$\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$，其中$B_{i,j}=A_{i,j}-\\frac{k_{i} k_{j}}{2 e}$，那么$Q=\\frac{1}{4 e} \\mathbf{h}^{T} \\mathbf{B h}$，其中$\\mathbf{h}=[h_i] \\in \\mathbb{R}^n$，表示社区成员指标器。\n如果将$Q$拓展到$k \u0026gt; 2$个社区，那么： $$ Q=\\operatorname{tr}\\left(\\mathbf{H}^{T} \\mathbf{B H}\\right), \\quad \\text { s.","title":"AAAI2017 M-NMF:《Community Preserving Network Embedding》 Reading Notes"},{"content":"文章链接：Embedding_IC\nIntroduction 本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。\n对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：\n 用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。 用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。 不同应用中的级联长度变化很大，难以学习和预测。  本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：\n 影响传播是二元的（被感染或不被感染）， 扩散网络未知， 影响关系不依赖于传播的内容， 用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。  本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：\nModel Notations 传播事件集$\\mathcal{D}={D_1,D_2,\u0026hellip;,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。\n给定一个社交网络，有$N$个用户：$\\mathcal{U}={u_1,u_2,\u0026hellip;,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \\in \\mathcal{U} \\wedge t^D(u)\u0026lt; \\infty}$， 其中，$t^D: \\mathcal{U} \\to \\mathbb{R}^+$ 表示用户被传染的时间戳， $\\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \\quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \\in \\mathcal{U} | t^D(u)\u0026lt;t}$。对称地，用$\\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\\infty)$表示最终所有被感染的用户，$\\bar{D}(infty)$表示最终所有没被感染的用户。\nDiffusion Model 本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。\n在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \\in \\mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \\in \\mathcal{U} \\backslash I$: $$ P(v|I) = 1-\\prod_{u \\in I}(1-P_{u,v}) $$ 上式中，$\\prod_{u \\in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。\n接下来就需要给出$P_{u,v}$的定义了，即$v$被$u$传染的概率。$z_u \\in \\mathbb{R}^d$是传染源用户$u$的表示向量，$\\omega_{v} \\in \\mathbb{R}^d$是传染目标用户$v$的表示向量。那么$P_{u,v}$可以定义如下： $$ P_{u,v} = f(z_u,\\omega_{v}) $$ 其中，$f: \\mathbb{R}^d \\times \\mathbb{R}^d \\to [0,1]$，是一个映射函数，把两个表示向量映射到概率空间： $$ f\\left(z_{u}, \\omega_{v}\\right)=\\frac{1}{1+\\exp \\left(z_{u}^{(0)}+\\omega_{v}^{(0)}+\\sum_{i=1}^{d-1}\\left(z_{u}^{(i)}-\\omega_{v}^{(i)}\\right)^{2}\\right)} $$ 其中，$z_{u}^{(i)}$和$\\omega_{v}^{(i)}$分别表示$z_u$和$\\omega_v$的第$i$个分量。表示随距离增加而递减的传输概率，即$\\left(z_{u}^{(i)}-\\omega_{v}^{(i)}\\right)$越大$f$越小。上式使用了sigmoid函数:$\\frac{1}{1+e^{-x}}$返回一个$[0,1]$的概率。\n值得注意的是，偏置项$z_{u}^{(0)}$和$\\omega_{v}^{(0)}$的作用是反映$u$传入$v$的一般趋势，这样做的目的是避免不同的$u$和$v$产生相同的概率。\nLearning Algorithm 考虑所有节点对的传播概率$\\mathcal{P}={P_{u,v} | (u,v) \\in \\mathcal{U}^2}$ (涉及所有节点对)。那么对于特定级联$D$的概率为： $$ P(D)=\\prod_{v \\in D(\\infty)} P_{v}^{D} \\prod_{v \\in \\overline{D}(\\infty)}\\left(1-P_{v}^{D}\\right) $$ 上式中，$\\prod_{v \\in D(\\infty)} P_{v}^{D}$表示$D$中所有被影响的用户存在的概率，$\\prod_{v \\in \\overline{D}(\\infty)}\\left(1-P_{v}^{D}\\right)$表示$D$中所有未被影响的用户存在的概率。所以$P(D)$就是级联$D$存在的概率。同时，可以用对数似然来表示训练级联集$\\mathcal{D}$: $$ \\mathcal{L}(\\mathcal{P} ; \\mathcal{D})=\\sum_{D \\in \\mathcal{D}}\\left(\\sum_{v \\in D(\\infty)} \\log \\left(P_{v}^{D}\\right)+\\sum_{v \\in \\overline{D}(\\infty)} \\log \\left(1-P_{v}^{D}\\right)\\right) $$ 上式就是模型的目标函数。\n","permalink":"https://JhuoW.github.io/posts/2019-05-12-embedding-ic/","summary":"文章链接：Embedding_IC\nIntroduction 本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。\n对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：\n 用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。 用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。 不同应用中的级联长度变化很大，难以学习和预测。  本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为__学习表征用户间隐含的相互影响关系的概率分布__，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：\n 影响传播是二元的（被感染或不被感染）， 扩散网络未知， 影响关系不依赖于传播的内容， 用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。  本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：\nModel Notations 传播事件集$\\mathcal{D}={D_1,D_2,\u0026hellip;,D_n}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。\n给定一个社交网络，有$N$个用户：$\\mathcal{U}={u_1,u_2,\u0026hellip;,u_N}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = {(u,t^D(u)) | u \\in \\mathcal{U} \\wedge t^D(u)\u0026lt; \\infty}$， 其中，$t^D: \\mathcal{U} \\to \\mathbb{R}^+$ 表示用户被传染的时间戳， $\\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \\quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = {u \\in \\mathcal{U} | t^D(u)\u0026lt;t}$。对称地，用$\\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\\infty)$表示最终所有被感染的用户，$\\bar{D}(infty)$表示最终所有没被感染的用户。\nDiffusion Model 本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。\n在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \\in \\mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \\in \\mathcal{U} \\backslash I$: $$ P(v|I) = 1-\\prod_{u \\in I}(1-P_{u,v}) $$ 上式中，$\\prod_{u \\in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。","title":"WSDM2016 《Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model》 Reading Notes"},{"content":"影响力传播模型 社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择\nLC IT模型（独立级联模型和线性阈值模型） WC（权重级联模型） HD（热传播模型） SIR（传染病模型） MIA模型（路径相关） 投票模型 巴斯模型 影响力最大化算法 目前有的几个影响力最大化的算法\n   基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）\n  A note on maximizing a submodular set function subject to a knapsack constraint 这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样\n  Cost-effective outbreak detection in networks （CELF算法）\n  Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法） 这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路\n    基于中心性的启发式算法\n  Efficient influence maximization in social networks W.Chen （DegreeDiscount算法） 这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1\n  A potential-based node selection strategy for influence max- imization in a social network （TM算法） 这个算法也是一种启发式算法，它是以节点地潜在地影响力作为启发的因素去选择初始的节点，同时设置了一个参数c，分为两个阶段，第一个阶段启发式的选择潜在影响力最大的若干个节点，第二阶段用贪心算法对这些第一阶段选择的种子节点进行筛选，当参数c为0，也就是没有第一阶段，那么算法就变成了传统的贪心算法。算法的缺点也很明显，首先参数c的选择依赖于经验判断，其次是启发式，准确率没有保证，再次也需要使用贪心算法，因此时间复杂度很高\n  A new centrality measure for influence maxi- mization in social networks 考虑到了传播模型中的传播度，利用这个进行启发式选择传播度最高的节点，从而得到更加精确的结果。这个传播度包括了两个方面，第一是节点自己影响他的邻居，第二个是他的邻居影响其他节点，结合影响概率一起构建的模型。优点在于启发式的选择的同时考虑了传播模型。\n    基于影响路径的算法\n  Tractable models for information diffusion in social networks（SP1M算法） 基于影响路径的算法考虑某个节点只会尽可能地影响从这个节点开始地最短或者次最短路径上地节点，因此，可以递归地计算influenc spread而不用像贪心算法那样使用蒙特卡洛模拟，从而导致大量地计算时间，因此提高了算法地效率\n  Scalable influence maximization for prevalent viral marketing in large-scale social networks （MIA算法） 借鉴了基于路径地影响力最大化算法的思路，提出一种利用局部图结构的树状近似算法来近似influence spread从而也是避免了蒙特卡罗模拟。此算法中，每条路径具有一个传播概率，定义为在这条路径上的每条边的传播概率的乘积。只有具有最大传播概率的路径才能够作为影响力路径来扩散影响力。同时给每一个节点计算树状度，定义为从节点出发的各条路径中，路径传播概率大于阈值 $\\theta$的路径上的所有点的集合\n  Scalable and parallelizable processing of influence maximization for large-scale social networks （IPA算法）\n    该算法不同于chen等人提出的算法，认为每条路径是相互独立的，chen等人只选择了具有最大的propagation 概率的那条路径，但是本论文则选择所有大于阈值 $\\theta$的路径，并行的计算他们的influence spread。基于路径的算法也具有缺点，比如没有理论上的准确度保证，同时，针对于特别复杂的图，空间复杂度非常大\n  基于社区的算法\n Oasnet: an optimal allocation approach to in- fluence maximization in modular social networks（OASNET算法）  这个算法假设社交网络划分社区后的每个社区是相互独立的，社区之间不会存在相互的影响力传播，利用CNM算法进行社区发现。种子节点的选择则分为两个阶段，第一个节点在每个社区内部利用贪心算法选择k个节点，第二个阶段则使用动态规划的方法在$C \\times k$个节点中选择最终的k个节点\n Identifying influential nodes in complex networks with community structure  这个算法基于利用社区结构发现社交网络中的最具有影响力的几个节点的研究。首先根据加权图构造概率转移矩阵，然后使用$K-Mediods$聚类方法找到最具有影响力的若干个节点。\n Cim: community-based influence maximization in social networks（CIM算法）  chen等人基于HD（热传播）模型提出的基于社区结构的影响力最大化算法。算法分为好三个阶段，首先是社区发现，作者给出了一种$H_{Clustering}$算法用于社区发现，然后是候选节点迭代，作者根据节点的拓扑结构和它的社区特征进行选择，最后是种子节点的选择，同时考虑了诸多因素，个人认为是一个比较合理的影响力最大化算法。\n Conformity-aware influence maximization in online social networks （CINEMA算法）  基于节点的一致性来设计的算法。传播模型中的概率定义为让第一个节点的影响力指标乘以第二个节点的一致性指标作为传播概率。\n当然，基于社区发现的算法也有自己的缺点，首先是在社区内部进行初步的节点的选择，也需要进行蒙特卡洛模拟，因此时间复杂度也会比较大，其次，社区发现的思路，是用节点在社区内的influence spread去模拟它在whole network上的influence spread，近似效果依赖于网络结构，如果社区之间的连接边都比较少，那么近似结果是非常接近的，但是如果社区之间的连接边比较多，及即是hub节点比较多，那么近似效果可想而知\n   1.基于的贪心算法KK（kempe等学者提出的算法） 2.基于贪心算法的改进算法，利用启发式规则改进的NewGreedyIC，MixedGreedyIC，NewGreedyWC算法 相关论文：Efficient influence maximization in social networks 2009 3.基于贪心算法的改进算法，利用子模性质改进的CELF算法，改进的CELF++算法 相关论文：Cost-effective outbreak detection in networks 2007 CELF++：optimizing the greedy algorithm for influence maximization in social networks 2011 4.启发式算法：随机算法，度中心算法，MaxDegree算法，Degree Discount算法 相关论文：Efficient influence maximization in social networks 5.基于社区划分的算法：OASNET算法，CGA算法等（后面加上现阶段阅读的论文） 相关论文：Community-based greedy algorithm for mining top-k influential nodes in mobile social networks 2010 6.MIA算法 混合式算法 社区划分的算法   基于模块优化的算法 光谱聚类的算法 层次分级的算法 基于标签传播的算法  LPA算法（目前的最快的社区划分算法，几乎是线性时间复杂度）     论文思考的几个点 基于社区发现的影响力最大化算法的分析，论文研究的目的：\n 算法的效率保证，时间复杂度尽可能低。 算法的性能保证，尽可能接近最优解。（利用到子模拟性质？）  毕业论文的大致的框架，总体是基于社区划分的思路，具体需要做的工作如下：\n   传播模型的选择，如何改进传播模型使得切合实际的传播过程？IC，LT改进？结合PageRank算法或者思想？或者考虑改进HDM传播模型？引入时间空间的因素使得模型更加充分？\n（1）传播模型的改进，传播模型中，针对于某个节点的从邻居获得的影响力，不应该简单的直接叠加，而是考虑每个邻居并不是等同对待的，应当区分不同的权重，针对节点之间的互动频率，互动频率高的节点，应该具有更加高的信任度，同时，也可能存在负面的影响力，即反而让节点更不可能选择新产品，这点应该在改进的模型中有所反馈。\n（2）至于这个信任程度如何计算出来反应在传播模型中，则可以考虑，根据邻居之间的互动信息，每个节点的活跃程度，邻居节点本身是否是具有很高的度的节点，邻居节点和本节点的观点是正相关还是反相关，从而决定邻居对本节点的信任度。\n（3）应该考虑影响力的时效性，是否可以考虑结合HDM和LTM模型一起，加上信任度参数这个观点，一起构建一个新的传播模型。\n  社区发现算法的选择，社区发现的选择是非常重点的，社区发现本质上是社交网络节点的聚类，应该涉及比较有效率的聚类算法或者选择其他的距离算法？\n社区发现聚类算法，一般都是先设置每个节点作为单独的一个社区，然后进行合并，在进行社区聚类发现的时候，不应当单独仅仅考虑边，仅仅利用边的关系，比如CGA算法就利用到了传播模型，结合传播模型进行标签传播，然后获得相应的划分的社区。同时，可以加以改进的地方，比如，社交网络的社区发现不应当仅仅考虑到拓扑结构，还有考虑节点之间的互动交流的信息，互动程度越频繁，那么两个节点在一个社区内部的概率就越大，因此要考虑这个改进点。\n同时，借鉴了CGA算法的思想，一个节点的社区内部的影响力和整个社交网络的影响力如何区别？如何用社区内部的影响力去近似？或者考虑hub节点，社区之间的这些连接节点也有着非常重要的作用。\n  社区发现是否可以处理重叠社区的情况，重叠社区会导致影响力的重复传播，如何减少这种情况的出现，如何设计算法实现重叠社区的处理？\n  各个社区的重要性也是不同的，应该有选择的摒弃一些社区，先给出一个社区选择的模型，比如说利用PageRank先计算出哪些社区比较重要，有的社区人数多，有的社区人数少，但是处在中心位置，并且一些非重要的社区，往往会关注这些重要社区的传递出来的信息。考虑种子节点选择的时候，应当把社区这些因素考虑进去。我们可以忽略那些不重要的小社区，重要的社区给与比较大的加权值，同时注意影响力避免重叠传播。\n  社区发现之后，如何分配每个社区的种子节点数目？，直接按照比例分配？亦或是选择一种度量社区重要程度的模型？\nCGA算法，使用了动态规划进行贪心选择，在各个社区内部选择相应的种子节点。但是时间复杂度仍然是非常大的，是否可以考虑先在社区内部基于启发式规则，或者PageRank，计算重要的节点，然后全局进行贪心的选择？\n  基于社区发现的算法，实际上是利用节点在社区内的传播来近似它在整个网络上传播的效果，因此这种近似肯定存在误差，如何减少这种误差的产生？而且这种误差还是和网络中的社区结构有关系的。\n  注意充分利用社区结构的特点，划分社区之后，把社区也视为一个点，作为整体去考虑\n  社区内的候选节点选择，候选节点应该按照什么标准进行选择，是按照启发式的度选择？还是设计另外的模型？结合PageRank模型？\n  种子节点的获取策略，如何在这些候选节点上选择出最终的种子节点？直接按照贪心策略暴力选择还是参考CELF算法进行选择？或者是涉及其他的方法？\n   思路总结  基于社区发现的影响力最大化算法框架：\n 社区划分。 充分考虑社区作为一个整体性，来体现社区的一个作用，可以利用PageRank模型，来代表社区的重要程度，这个是社区的一个属性，利用这个模型，选择一些重要的社区，同时摒弃一些小的，没那么重要的社区。而且利用PageRank进行迭代，应该比传统的算法会快一些。 社区内部种子节点的选择，考虑的是社区内部的种子节点在社区内部的影响力传播。如何选择社区内部的种子节点。？？？？？这一点目前还需要多家考虑。 社区出现重叠，如何考虑？ 种子节点在社区内的影响力只是对种子节点在全局的影响力的近似，那么需要一种方式来弥补这种误差。显然，种子节点的影响力如果想要传播到另外的社区，那么是通过社区之间的边界节点进行传播的，同时和社区本身的重要性有关，那么这部分的误差通过边界节点来弥补。 最终会得到若干的候选节点，这些节点使用贪心算法进行选择出最终的种子节点，考虑贪心算法的时间复杂度，那么可以考虑使用CELF思路或者是其他的CGA这类的动态规划思路去求解，不过这还是基于蒙特卡罗模拟，时间复杂度仍然相对比较大，对算法进行加速。 影响力传播模型，基于线性阈值模型进行改变，加上信任度参数，因为每个影响力的叠加不是平权的，和用户之间的互动，观点信息，兴趣爱好是否一直存在着相关的关系，因此在影响力传播模型中加入这个考虑因素。   算法的实验部分，考虑一些经典的BaseLine Algorithm   传统贪心爬山算法-KK算法 CELF算法 CGA算法 CIM算法 启发式算法（度启发式，中心性启发式）DegreeDiscount算法 本论文的算法     算法中参数的选择的影响 控制变量对比实验   相关工作总结    Richardson和Domingos在2002年的论文，首次把这个问题作为一个研究方向提出。\n  首先应该数说到的式Kempe的2003年的论文，主要提出了\n  a）LT IC 模型，并说明了这个问题的NP完全性\nb）给出了贪心算法\nc）说明了影响力递增的边界递减性质，利用子模性质说明了算法的性能保证\n 近期论文阅读总结   传播模型的改进，基于PageRank的改进，传统PageRank在考虑某个节点的PR值是均匀分配给链出的节点的（链出的概率为出度的倒数）（即权重级联模型），但是实际上，PR高的节点具有更高的影响力，因此考虑链出的概率不用度，而用PR值的占比，从而更加切合实际的情况。\n  还有的改进算法，改进了PageRank计算模型，把节点自身的属性，节点之间互动的属性，加入到了PageRank模型计算中，使得PageRank能够适用于社交网络中节点重要性的计算。\n  我们可以考虑把以上的两者结合起来给出一种新的信息传播模型（给出概率计算的方法）。\n  数据集选择\n 参考宫秀云那篇文章\n   总结：基于PageRank思想的影响力计算，都是在PageRank的基础上进行模型的改进，加入其他的影响因子，给出不同的权重，从而更加符合实际的应用场景。\n  ","permalink":"https://JhuoW.github.io/posts/2019-05-06-im-conclusion/","summary":"影响力传播模型 社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择\nLC IT模型（独立级联模型和线性阈值模型） WC（权重级联模型） HD（热传播模型） SIR（传染病模型） MIA模型（路径相关） 投票模型 巴斯模型 影响力最大化算法 目前有的几个影响力最大化的算法\n   基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）\n  A note on maximizing a submodular set function subject to a knapsack constraint 这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样\n  Cost-effective outbreak detection in networks （CELF算法）\n  Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法） 这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路\n    基于中心性的启发式算法\n  Efficient influence maximization in social networks W.Chen （DegreeDiscount算法） 这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1","title":"Influence Maximization Conclusion"},{"content":"The Independent Cascade Model (IC Model) IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。\n在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。\n值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。\nThe Linear Threshold Model (LT Model) LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。\n形式上， 在图$G$中每条边$e=(u,v) \\in E$有一个权重$b_{u,v}$。 我们定义$\\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\\sum_{u \\in \\mathcal{N}_I (v)} b_{u,v} \\leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\\theta_v$。 LT模型首先为每个节点$v$的阈值$\\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。\n","permalink":"https://JhuoW.github.io/posts/2019-03-20-ic-lt/","summary":"The Independent Cascade Model (IC Model) IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。\n在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。\n值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。\nThe Linear Threshold Model (LT Model) LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。\n形式上， 在图$G$中每条边$e=(u,v) \\in E$有一个权重$b_{u,v}$。 我们定义$\\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\\sum_{u \\in \\mathcal{N}_I (v)} b_{u,v} \\leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\\theta_v$。 LT模型首先为每个节点$v$的阈值$\\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。","title":"社交网络影响最大化（Influence Maximization）中的IC，LT模型"},{"content":"论文地址：BiNE\nIntroduction Bipartite Network(二分网络):如下图所示：\n二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。\n另一个问题，\n如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。\n另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。\n针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过HITS来衡量。\nModel 如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \\times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\\overrightarrow{u_i}]$, $V=[\\overrightarrow{v_i}]$，结构如下图所示：\n（取自作者的讲解ppt)\nExplicit Relations 同LINE一样， 基于直接连接的目标函数表示为：\n$$\\mathrm{minimize} \\quad O_1=-\\sum_{e_{ij} \\in E}w_{ij}\\log \\hat{P}(i,j)$$\nImplicit Relations 构造随机游走序列 这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：\n$$w^U_{ij}=\\sum_{k \\in V}w_{ik}w_{jk}$$\n$$w^V_{ij}=\\sum_{k \\in U}w_{ki}w_{kj}$$\n其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。 其中$l=\\max(H(v_i)\\times \\max T,\\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。\n$$D_{v_i}=\\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$\n表示其中一次随机游走的节点集合$p$表示停止概率。\n通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。\n对间接关系建模 如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。 对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \\in C_s(u_i)$的条件概率。所以目标函数如下： $$\\mathrm{maximize} \\quad O_2 = \\prod_{u_i \\in S \\land S \\in D^U} \\prod_{u_c \\in C_s(u_i)}P(u_c|u_i)$$\n对于$D^V$同理。其中,$p(u_c|u_i) = \\frac{\\exp(\\overrightarrow{u}_i^T \\overrightarrow{\\theta}_c)}{\\sum^{|U|}_{k=1} \\exp(\\overrightarrow{u}_i^T \\overrightarrow{\\theta}_k))}$。\nNegative Sampling 本文的负采样方法是基于局部敏感哈希（LSH）来对与中心节点不相似的节点进行采样。 该方法的strategy是，给定一个中心节点，随机选取一个bucket（序列）并且这个序列不包含给定的中心节点，以此来获得和给定节点尽量不相似的负采样节点。\n$N^{ns}_S (u_i)$ 表示$ns$个负采样节点，对于中心节点$u_i$, 那么上文提到的条件概率$p(u_c|u_i)$可以被定义为下式：\n$$p(u_c,N^{ns}_S (u_i)|u_i) = \\prod_{z \\in {u_c} \\cup N^{ns}_S (u_i)} P(z|u_i)$$\n其中条件概率$P(z|u_i)$定义为：\n其中$\\sigma$表示sigmoid函数，这样就减少了上文softmax函数造成的计算量过大的问题。\n联合优化 通过随机梯度上升对3部分损失函数进行加权优化：\n$$\\mathrm{maximize} \\quad L = \\alpha \\log O_2+\\beta \\log O_3 - \\gamma O_1$$ 最终BiNE的整体算法流程如下：\nConclusion 这篇文章提出的分布式训练以及负采样策略还是很值得学习的。\n","permalink":"https://JhuoW.github.io/posts/bine/","summary":"论文地址：BiNE\nIntroduction Bipartite Network(二分网络):如下图所示：\n二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network)， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。\n另一个问题，\n如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex)也是完全相同的，这样无法反应网络的特征以及异构性。\n另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。\n针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过HITS来衡量。\nModel 如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \\times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\\overrightarrow{u_i}]$, $V=[\\overrightarrow{v_i}]$，结构如下图所示：\n（取自作者的讲解ppt)\nExplicit Relations 同LINE一样， 基于直接连接的目标函数表示为：\n$$\\mathrm{minimize} \\quad O_1=-\\sum_{e_{ij} \\in E}w_{ij}\\log \\hat{P}(i,j)$$\nImplicit Relations 构造随机游走序列 这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：\n$$w^U_{ij}=\\sum_{k \\in V}w_{ik}w_{jk}$$\n$$w^V_{ij}=\\sum_{k \\in U}w_{ki}w_{kj}$$\n其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。 其中$l=\\max(H(v_i)\\times \\max T,\\min T)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。\n$$D_{v_i}=\\mathrm{BiasedRandomWalk}(W^R,v_i,p)$$\n表示其中一次随机游走的节点集合$p$表示停止概率。\n通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。\n对间接关系建模 如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。 对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \\in C_s(u_i)$的条件概率。所以目标函数如下： $$\\mathrm{maximize} \\quad O_2 = \\prod_{u_i \\in S \\land S \\in D^U} \\prod_{u_c \\in C_s(u_i)}P(u_c|u_i)$$","title":"SIGIR18 《BiNE:Bipartite Network Embedding》 Reading Notes"},{"content":"论文地址:GraphGAN\nIntroduction GAN是CV上非常火的方法，作者将其引入Network Embedding领域。为了达到这个目的，首先，作者将当前的NE model分成了两类，分别是生成模型(generative model)和判别模型(discriminative model)。简单来说，生成模型。\n所谓生成模型，就是对于每个节点$v_c$，都存在一个潜在的真实连接分布$p_{true}(v|v_c)$, 这个分布反映了由条件生成样本的概率，因此，生成式模型的目的就是求网络中边的极大似然。比较经典的方法有DeepWalk，是通过随机游走来获得center node的上下文，然后最大化上下文节点的似然。Node2vec拓展了DeepWalk,提出了biased随机游走，使得模型在为给定节点生成上下文节点时具有更多的灵活性。\n所谓判别模型，目的是学习一个分类器来直接预测边是否存在。将两个节点视为输入特征，然后输出预测的两个节点之间有边的概率，即$p(edge|(v_i,v_j))$,常见的方法有SDNE，PPNE。\n于是，本文结合生成对抗网络(GAN),提出了GraphGAN来同一生成和判别器。其中生成器$G(v|v_c)$试图拟合真实的连接概率分布$p_{true}(v|v_c)$,生成最可能和$v_c$有链接的点。判别器$D(v,v_c)$尝试区分强连接节点对和若连接节点对，然后计算$v$和$v_c$存在边的可能性。简单来说，就是把生成器生成的可能与$v_c$有边的节点放入判别器计算两者有边的概率。\n除此之外GAN框架下，然而生成器的连接分布是通过softmax实现的，但是传统的softmax无法适配生成器，原因如下:\n(1).传统的softmax对网络中的所有节点一视同仁，缺乏对网络结构相似度信息的考虑。\n(2).计算成本太高。\n因此，论文中提出了新的生成器实现方法 Graph Softmax。并且证明GS具有归一化（normalization）、网络结构感知（graph structure awareness）和高计算效率（computational efficiency）的性质。相应的，文中也提出了基于随机游走(random walk)的生成器策略。\nModel 这里挑特别的来说。$\\mathcal{N}(v_c)$表示和$v_c$直接相连的节点集。我们把给定节点$v_c$与任意节点$v \\in \\mathcal{V}$的真实连接分布表示为$p_{true}(v|v_c)$。这么看$\\mathcal{N}(v_c)$可以看做从$p_{true}(v|v_c)$中采样的集合。文章的目的是学习两个模型:\nGenerator $G(v|v_c;\\theta_G)$ 生成器，尝试拟合真实连接分布$p_{true}(v|v_c)$，并且从节点集$\\mathcal{V}$中生成最有可能和$v_c$相连的节点。\nDiscriminator $D(v,v_c;\\theta_G)$ 判别器，目的是判断已对接点$(v,v_c)$的连接性。$D(v,v_c;\\theta_G)$输出$v$和$v_c$有边的概率。\n生成器$G$尝试完美拟合$p_{true}(v|v_c)$，并且生成和$v_c$真实邻居高度相似的节点来欺骗判别器。相反，判别器尝试发现输入的顶点是真实的节点对或者有生成器生成出的样本。这是一个极大极小游戏，目标函数$V(G,D)$:\n$$\\min_{\\theta_G} \\max_{\\theta_D} V(G,D)=\\sum_{c=1}^V (\\mathbb{E}_{v \\sim p_{true}(\\cdot|v_c)}[\\log D(v,v_c;\\theta_D)]+\\mathbb{E}_{v \\sim G(\\cdot|v_c;\\theta_G)}[\\log(1-D(v,v_c;\\theta_D))])$$\n上面这个公式是本文最关键的公式，以我的分析就是：在给定$\\theta_D$的情况下，对其最小化。先来分析$\\max_{\\theta_D}V(G,D)$,即给定$\\theta_G$,使原式最大。当给定$\\theta_G$时，通过改变$\\theta_D$,使$\\mathbb{E}_{v \\sim p_{true}(\\cdot|v_c)}[\\log D(v,v_c;\\theta_D)]$达到最大（是判别器D拟合真实分布），同时使$\\mathbb{E}_{v \\sim G(\\cdot|v_c;\\theta_G)}[\\log(1-D(v,v_c;\\theta_D))]$最大，即尽可能减小选定节点$v_c$与判别器中生成的$v_c$邻居节点与$v_c$连接的可能性。然后在给定$\\theta_D$的情况下，通过改变生成器$\\theta_G$继续生成节点，使得$\\mathbb{E}_{v \\sim G(\\cdot|v_c;\\theta_G)}[\\log(1-D(v,v_c;\\theta_D))]$尽可能小，即上一步的判别器尽可能把当前生成器生成的节点认为是邻居。 之后再更新判别器,以此类推。这样$G$和$D$各自提高性能，最终$G$生成的分布和真实连接分布无法区分，这样达到最好的效果。整体过程如下图所示:\nDiscriminator Optimization 对于判别器来说是个简单的sigmoid函数，来计算两个输入节点的表示向量:\n$$D(v,v_c;\\theta_D)=\\frac{1}{1+\\exp(-d^\\top_v d_{v_c})}$$\n其中，$d_v$,$d_{v_c}$是两个输入节点关于判别器$D$的表示向量，$\\theta_D$是所有节点表示向量的结合。注意到上面的公式只涉及$v$和$v_c$, 我们只需要更新$d_v$,$d_{v_c}$，通过梯度下降的方法可以实现：\nGenerator Optimization 对于生成器来说，目标是最小化判别器将生成的样本判断为负样本的对数似然（概率）。换句话说，生成器会调整自己的连接分布（通过调整生成的所有节点的向量表示$\\theta_G$）来提升对生成样本的判别分数。由于$v$的采样时离散的，所以使用policy gradient来计算$V(G,D)$关于$\\theta_G$的梯度：\n为了理解上述公式，注意到$\\nabla_{\\theta_G}V(G,D)$是一个由$\\log(1-D(v,v_c;\\theta_D))$加权的梯度$\\nabla_{\\theta_G}\\log G(v|v_c;\\theta_G)$的求和，直观上说，这意味着有高概率是负样本的节点会将生成器G“拉着”远离自己（因为我们在$\\theta_G$上执行梯度下降)。\nGraph Softmax graph softmax的核心思想是定义一种新的计算连接性概率的方式，满足以下性质:\n 归一化：$\\sum_{v \\neq v_c;\\theta_G}=1$。 图结构感知：生成器充分利用网络中的结构信息，来估计真实连接分布，如果两个节点在图上越远，那么他们间有边的概率越小。 高效的计算：和传统的softmax不同，$G(v|v_c;\\theta_G)$的计算应只涉及图中的一小部分点。  因此，本文以图中节点$v_c$为例，以$v_c$为根构建一棵BFS树$T_c$。$\\mathcal{N}_c(v)$为节点$v$在$T_c$上的邻居集合（包括他的父节点和所有子节点）。对于一个given vertex $v$和它的一个邻居$v_i \\in \\mathcal{N}_c(v)$,定义概率为:\n$$p_c(v_i|v)=\\frac{\\exp (g_{v_i}^\\top g_v)}{\\sum_{v_i \\in \\mathcal{N}_c(v)} \\exp(g_{v_j}^\\top g_v)}$$\n这是一个在$\\mathcal{N}_c(v)$上的softmax函数。\n为了计算$G(v|v_c;\\theta_G)$,注意到在$T_c$上，根节点$v_c$到每个节点$v$都有一条唯一的路径， 把这条路径记为$P_{v_c \\to v}=(v_{r_0},v_{r_1},\u0026hellip;,v_{r_m})$,其中$v_{r_0}=v_c$, $v_{r_m}=v$,那么在graph softmax中，将$G(v|v_c;\\theta_G)$定义为:\n$$G(v|v_c;\\theta_G)\\triangleq (\\prod^m_{j=1} p_c(v_{r_j}|v_{r_{j-1}})) \\cdot p_c(v_{r_{m-1}}|v_{r_m})$$\n","permalink":"https://JhuoW.github.io/posts/graphgan/","summary":"论文地址:GraphGAN\nIntroduction GAN是CV上非常火的方法，作者将其引入Network Embedding领域。为了达到这个目的，首先，作者将当前的NE model分成了两类，分别是生成模型(generative model)和判别模型(discriminative model)。简单来说，生成模型。\n所谓生成模型，就是对于每个节点$v_c$，都存在一个潜在的真实连接分布$p_{true}(v|v_c)$, 这个分布反映了由条件生成样本的概率，因此，生成式模型的目的就是求网络中边的极大似然。比较经典的方法有DeepWalk，是通过随机游走来获得center node的上下文，然后最大化上下文节点的似然。Node2vec拓展了DeepWalk,提出了biased随机游走，使得模型在为给定节点生成上下文节点时具有更多的灵活性。\n所谓判别模型，目的是学习一个分类器来直接预测边是否存在。将两个节点视为输入特征，然后输出预测的两个节点之间有边的概率，即$p(edge|(v_i,v_j))$,常见的方法有SDNE，PPNE。\n于是，本文结合生成对抗网络(GAN),提出了GraphGAN来同一生成和判别器。其中生成器$G(v|v_c)$试图拟合真实的连接概率分布$p_{true}(v|v_c)$,生成最可能和$v_c$有链接的点。判别器$D(v,v_c)$尝试区分强连接节点对和若连接节点对，然后计算$v$和$v_c$存在边的可能性。简单来说，就是把生成器生成的可能与$v_c$有边的节点放入判别器计算两者有边的概率。\n除此之外GAN框架下，然而生成器的连接分布是通过softmax实现的，但是传统的softmax无法适配生成器，原因如下:\n(1).传统的softmax对网络中的所有节点一视同仁，缺乏对网络结构相似度信息的考虑。\n(2).计算成本太高。\n因此，论文中提出了新的生成器实现方法 Graph Softmax。并且证明GS具有归一化（normalization）、网络结构感知（graph structure awareness）和高计算效率（computational efficiency）的性质。相应的，文中也提出了基于随机游走(random walk)的生成器策略。\nModel 这里挑特别的来说。$\\mathcal{N}(v_c)$表示和$v_c$直接相连的节点集。我们把给定节点$v_c$与任意节点$v \\in \\mathcal{V}$的真实连接分布表示为$p_{true}(v|v_c)$。这么看$\\mathcal{N}(v_c)$可以看做从$p_{true}(v|v_c)$中采样的集合。文章的目的是学习两个模型:\nGenerator $G(v|v_c;\\theta_G)$ 生成器，尝试拟合真实连接分布$p_{true}(v|v_c)$，并且从节点集$\\mathcal{V}$中生成最有可能和$v_c$相连的节点。\nDiscriminator $D(v,v_c;\\theta_G)$ 判别器，目的是判断已对接点$(v,v_c)$的连接性。$D(v,v_c;\\theta_G)$输出$v$和$v_c$有边的概率。\n生成器$G$尝试完美拟合$p_{true}(v|v_c)$，并且生成和$v_c$真实邻居高度相似的节点来欺骗判别器。相反，判别器尝试发现输入的顶点是真实的节点对或者有生成器生成出的样本。这是一个极大极小游戏，目标函数$V(G,D)$:\n$$\\min_{\\theta_G} \\max_{\\theta_D} V(G,D)=\\sum_{c=1}^V (\\mathbb{E}_{v \\sim p_{true}(\\cdot|v_c)}[\\log D(v,v_c;\\theta_D)]+\\mathbb{E}_{v \\sim G(\\cdot|v_c;\\theta_G)}[\\log(1-D(v,v_c;\\theta_D))])$$\n上面这个公式是本文最关键的公式，以我的分析就是：在给定$\\theta_D$的情况下，对其最小化。先来分析$\\max_{\\theta_D}V(G,D)$,即给定$\\theta_G$,使原式最大。当给定$\\theta_G$时，通过改变$\\theta_D$,使$\\mathbb{E}_{v \\sim p_{true}(\\cdot|v_c)}[\\log D(v,v_c;\\theta_D)]$达到最大（是判别器D拟合真实分布），同时使$\\mathbb{E}_{v \\sim G(\\cdot|v_c;\\theta_G)}[\\log(1-D(v,v_c;\\theta_D))]$最大，即尽可能减小选定节点$v_c$与判别器中生成的$v_c$邻居节点与$v_c$连接的可能性。然后在给定$\\theta_D$的情况下，通过改变生成器$\\theta_G$继续生成节点，使得$\\mathbb{E}_{v \\sim G(\\cdot|v_c;\\theta_G)}[\\log(1-D(v,v_c;\\theta_D))]$尽可能小，即上一步的判别器尽可能把当前生成器生成的节点认为是邻居。 之后再更新判别器,以此类推。这样$G$和$D$各自提高性能，最终$G$生成的分布和真实连接分布无法区分，这样达到最好的效果。整体过程如下图所示:\nDiscriminator Optimization 对于判别器来说是个简单的sigmoid函数，来计算两个输入节点的表示向量:\n$$D(v,v_c;\\theta_D)=\\frac{1}{1+\\exp(-d^\\top_v d_{v_c})}$$\n其中，$d_v$,$d_{v_c}$是两个输入节点关于判别器$D$的表示向量，$\\theta_D$是所有节点表示向量的结合。注意到上面的公式只涉及$v$和$v_c$, 我们只需要更新$d_v$,$d_{v_c}$，通过梯度下降的方法可以实现：\nGenerator Optimization 对于生成器来说，目标是最小化判别器将生成的样本判断为负样本的对数似然（概率）。换句话说，生成器会调整自己的连接分布（通过调整生成的所有节点的向量表示$\\theta_G$）来提升对生成样本的判别分数。由于$v$的采样时离散的，所以使用policy gradient来计算$V(G,D)$关于$\\theta_G$的梯度：\n为了理解上述公式，注意到$\\nabla_{\\theta_G}V(G,D)$是一个由$\\log(1-D(v,v_c;\\theta_D))$加权的梯度$\\nabla_{\\theta_G}\\log G(v|v_c;\\theta_G)$的求和，直观上说，这意味着有高概率是负样本的节点会将生成器G“拉着”远离自己（因为我们在$\\theta_G$上执行梯度下降)。\nGraph Softmax graph softmax的核心思想是定义一种新的计算连接性概率的方式，满足以下性质:\n 归一化：$\\sum_{v \\neq v_c;\\theta_G}=1$。 图结构感知：生成器充分利用网络中的结构信息，来估计真实连接分布，如果两个节点在图上越远，那么他们间有边的概率越小。 高效的计算：和传统的softmax不同，$G(v|v_c;\\theta_G)$的计算应只涉及图中的一小部分点。  因此，本文以图中节点$v_c$为例，以$v_c$为根构建一棵BFS树$T_c$。$\\mathcal{N}_c(v)$为节点$v$在$T_c$上的邻居集合（包括他的父节点和所有子节点）。对于一个given vertex $v$和它的一个邻居$v_i \\in \\mathcal{N}_c(v)$,定义概率为:","title":"AAAI2018 《GraphGAN:Graph Representation Learning with Generative Adversarial Nets》Reading Notes"},{"content":"论文地址: Enhanced Network Embeddings via Exploiting Edge Labels\nIntroduction 这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。\n具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个半监督NE模型。\nProblem Definition 假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,\u0026hellip;,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \\cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \\in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \\subseteq L$。目的还是一样，学习一个映射函数$\\Phi \\to \\mathbb{R}^{|V| \\times d}$, 其中$d \\ll |V|$。\nMethod 首先定义损失函数:\n$$\\mathcal{L}=(1-\\lambda)\\mathcal{L}_s+\\lambda\\mathcal{L}_r$$\n其中$\\mathcal{L}_s$表示预测节点邻域的损失。$\\mathcal{L}_r$表示预测边label的损失。$\\lambda$是两种损失的权重。\nStructural Loss 第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：\n$$\\mathcal{L}_s=-\\sum_{u \\in C(v)} \\log Pr(u|v)$$\n这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：\n$$Pr(u|v)=\\frac{\\exp(\\Phi(u) \\cdot \\Phi\u0026rsquo;(v))}{\\sum_{u\u0026rsquo; \\in V} \\exp(\\Phi(u\u0026rsquo;) \\cdot \\Phi\u0026rsquo;(v))}$$\n这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\\Phi(v)$和$\\Phi\u0026rsquo;(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。\n剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：\n$$C(v)={v_{i-w},\u0026hellip;,v_{i-1}} \\cup {v_{i+1},\u0026hellip;,v_{i+w}}$$\nRelational Loss 由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \\in E$,可以用一下方法来表示这条边:\n$$\\Phi(e)=g(\\Phi(u),\\Phi(v))$$\n其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：\n$$\\Phi(e)=\\Phi(u) \\oplus \\Phi(v)$$\n这样我们就获得了edge embedding。直接将它输入前馈神经网络，前馈神经网络第$k$层的定义为:\n$$h^{(k)}=f(W^{(k)}h^{(k-1)}+b^{(k)})$$\n其中 $h^{(0)}=\\Phi(e)$，$f$是除最后一层外采用relu激活函数，最后一层采用sigmoid函数激活,最后一层输出为$\\hat{y_i}$。最后最小化二元交叉熵损失函数：\n$$\\mathcal{L}_r=\\sum^{|L|}_{i=1} H(y_i,\\hat{y_i}) + (1-y_i) \\cdot \\log (1-\\hat{y_i})$$\n","permalink":"https://JhuoW.github.io/posts/2019-01-22-ne-edge-labels/","summary":"论文地址: Enhanced Network Embeddings via Exploiting Edge Labels\nIntroduction 这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。\n具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个半监督NE模型。\nProblem Definition 假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,\u0026hellip;,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \\cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \\in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \\subseteq L$。目的还是一样，学习一个映射函数$\\Phi \\to \\mathbb{R}^{|V| \\times d}$, 其中$d \\ll |V|$。\nMethod 首先定义损失函数:\n$$\\mathcal{L}=(1-\\lambda)\\mathcal{L}_s+\\lambda\\mathcal{L}_r$$\n其中$\\mathcal{L}_s$表示预测节点邻域的损失。$\\mathcal{L}_r$表示预测边label的损失。$\\lambda$是两种损失的权重。\nStructural Loss 第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：\n$$\\mathcal{L}_s=-\\sum_{u \\in C(v)} \\log Pr(u|v)$$\n这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：\n$$Pr(u|v)=\\frac{\\exp(\\Phi(u) \\cdot \\Phi\u0026rsquo;(v))}{\\sum_{u\u0026rsquo; \\in V} \\exp(\\Phi(u\u0026rsquo;) \\cdot \\Phi\u0026rsquo;(v))}$$\n这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\\Phi(v)$和$\\Phi\u0026rsquo;(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。\n剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：\n$$C(v)={v_{i-w},\u0026hellip;,v_{i-1}} \\cup {v_{i+1},\u0026hellip;,v_{i+w}}$$\nRelational Loss 由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \\in E$,可以用一下方法来表示这条边:\n$$\\Phi(e)=g(\\Phi(u),\\Phi(v))$$\n其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：","title":"CIKM2018 《Enhanced Network Embeddings via Exploiting Edge Labels》 Reading Notes"},{"content":"论文地址：SDNE\nIntroduction 这是一篇比较早的Network Embedding论文， 较早将深度模型应用到NE任务。 首先，本文提出了当前网络表示学习中遇到的三个问题：\n（1）. 高度非线性\n（2）. 尽可能保持网络结构\n（3）. 现实网络的高度稀疏性\nSDNE的主要目标就是保持网络结构的一阶相似性和二阶相似性。\n一阶相似性就是网络中边相连的节点对之间具有的相似性。\n二阶相似性就是在一个Graph中，拥有共同邻居但是不直接向相连的两个节点具有的相似性。\n其中，一阶相似性主要反映了网络的局部特征。 二阶相似性反映了网络的全局特征。\nModel 本文的模型主要如下图所示：\n这张图看上去有点复杂，实则原理非常简单。\n模型分为无监督部分和有监督部分，无监督部分是一个深度自编码器 用来捕获二阶相似度（保留全局结构），监督部分是一个拉普拉斯特征映射捕获一阶相似度（局部结构）。呃呃呃，Emmmmm ,不知道是我理解有问题还是其他原因，文章里说的和我理解的不太一样 /(ㄒoㄒ)/~~。 然后介绍一下具体的模型结构：\n深度自编码器的编码部分：\n$$y_i^{(k)}=\\sigma{(W^{(k)}y_i^{(k-1)}+b^{(k)})}, k=2,\u0026hellip;,K$$\n假设第$k$层是 节点$v$的表示向量（仅考虑全局信息），那么从第$k$层开始解码，最终得到$\\hat{x_i}$, 所以自编码器的误差就是输入节点$v$的邻接向量的重构误差。所以，二阶相似度损失函数定义为:\n$$\\mathcal{L}=\\sum_{i=1}^n{||\\hat{x_i}-x_i||^2_2}$$\n值得注意的是，由于网络的稀疏性，邻接矩阵中的0元素远多于非0元素，使用邻接矩阵作为输入的话要处理很多0，这样就做了太多无用功了。为了解决这个问题，对损失函数做了改进如下：\n$$\\mathcal{L_{2nd}}=\\sum_{i=1}^n||(\\hat{x_i}-x_i)\\odot{b_i}||^2_2=||\\hat{X}-X\\odot{B}||^2_F$$\n其中$\\odot$是哈马达乘积，表示对应元素相乘。$b_i={b_{i,j}}^n_{j=1}$， 邻接矩阵中的0对应$b=1$, 非0元素的$b\u0026gt;1$,这样的目的是对于有边连接的节点增加惩罚。可以理解为对有边连接的节点赋予更高权重。\n以上我们获得了二阶相似度的损失函数。在介绍一阶相似度之前，我们先来看看拉普拉斯映射（Laplacian Eigenmap） 其实LE也是一种经典的NRL方法，主要目的也是降维。其目标函数如下所示:\n$$\\sum_{i,j} W_{ij}||y_i-y_j||^2$$\nLE是通过构建相似关系图来重构局部特征结构,如果放在网络结构中来说,如果节点$v_i$和$v_j$很接近（有边），那么他们在embedding space中的距离也应该相应接近。$y_i$和$y_j$就表示他们在特征空间中的表示。因此，本文定义了保持一阶相似度的目标函数：\n$$\\mathcal{L_{1st}}=\\sum_{i,j=1}^n{s_{i,j}||y_i^{(K)}-y_j^{(K)}||^2_2}=\\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}$$\n具体来说，$K$就是自编码器第$K$层的输出，即编码结果，需要保持一条边的两个节点在嵌入空间中的表示相对接近。\n最终 结合一阶相似度和二阶相似度，本文给出了SDNE的目标函数：\n$$\\mathcal{L_{mix}}=\\mathcal{L_{2nd}+\\alpha{\\mathcal{L_{1st}}}}+\\nu{\\mathcal{L_{reg}}} =||(\\hat{X}-X)\\odot{B}||^2_F+\\alpha{\\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}}+\\nu{\\mathcal{L_{reg}}}$$\n其中，为了防止过拟合，添加了$\\mathcal L2$-norm单元$\\mathcal{L_{reg}}$来防止过拟合:\n$$\\mathcal{L_{reg}}=\\frac{1}{2}\\sum_{k=1}^k({||W^{(k)}||^2_F+||\\hat{W}^{k}||_F^2})$$\nOptimization 使用SGD来优化$\\mathcal{L_{mix}}$。具体算法如下：\n","permalink":"https://JhuoW.github.io/posts/sdne/","summary":"论文地址：SDNE\nIntroduction 这是一篇比较早的Network Embedding论文， 较早将深度模型应用到NE任务。 首先，本文提出了当前网络表示学习中遇到的三个问题：\n（1）. 高度非线性\n（2）. 尽可能保持网络结构\n（3）. 现实网络的高度稀疏性\nSDNE的主要目标就是保持网络结构的一阶相似性和二阶相似性。\n一阶相似性就是网络中边相连的节点对之间具有的相似性。\n二阶相似性就是在一个Graph中，拥有共同邻居但是不直接向相连的两个节点具有的相似性。\n其中，一阶相似性主要反映了网络的局部特征。 二阶相似性反映了网络的全局特征。\nModel 本文的模型主要如下图所示：\n这张图看上去有点复杂，实则原理非常简单。\n模型分为无监督部分和有监督部分，无监督部分是一个深度自编码器 用来捕获二阶相似度（保留全局结构），监督部分是一个拉普拉斯特征映射捕获一阶相似度（局部结构）。呃呃呃，Emmmmm ,不知道是我理解有问题还是其他原因，文章里说的和我理解的不太一样 /(ㄒoㄒ)/~~。 然后介绍一下具体的模型结构：\n深度自编码器的编码部分：\n$$y_i^{(k)}=\\sigma{(W^{(k)}y_i^{(k-1)}+b^{(k)})}, k=2,\u0026hellip;,K$$\n假设第$k$层是 节点$v$的表示向量（仅考虑全局信息），那么从第$k$层开始解码，最终得到$\\hat{x_i}$, 所以自编码器的误差就是输入节点$v$的邻接向量的重构误差。所以，二阶相似度损失函数定义为:\n$$\\mathcal{L}=\\sum_{i=1}^n{||\\hat{x_i}-x_i||^2_2}$$\n值得注意的是，由于网络的稀疏性，邻接矩阵中的0元素远多于非0元素，使用邻接矩阵作为输入的话要处理很多0，这样就做了太多无用功了。为了解决这个问题，对损失函数做了改进如下：\n$$\\mathcal{L_{2nd}}=\\sum_{i=1}^n||(\\hat{x_i}-x_i)\\odot{b_i}||^2_2=||\\hat{X}-X\\odot{B}||^2_F$$\n其中$\\odot$是哈马达乘积，表示对应元素相乘。$b_i={b_{i,j}}^n_{j=1}$， 邻接矩阵中的0对应$b=1$, 非0元素的$b\u0026gt;1$,这样的目的是对于有边连接的节点增加惩罚。可以理解为对有边连接的节点赋予更高权重。\n以上我们获得了二阶相似度的损失函数。在介绍一阶相似度之前，我们先来看看拉普拉斯映射（Laplacian Eigenmap） 其实LE也是一种经典的NRL方法，主要目的也是降维。其目标函数如下所示:\n$$\\sum_{i,j} W_{ij}||y_i-y_j||^2$$\nLE是通过构建相似关系图来重构局部特征结构,如果放在网络结构中来说,如果节点$v_i$和$v_j$很接近（有边），那么他们在embedding space中的距离也应该相应接近。$y_i$和$y_j$就表示他们在特征空间中的表示。因此，本文定义了保持一阶相似度的目标函数：\n$$\\mathcal{L_{1st}}=\\sum_{i,j=1}^n{s_{i,j}||y_i^{(K)}-y_j^{(K)}||^2_2}=\\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}$$\n具体来说，$K$就是自编码器第$K$层的输出，即编码结果，需要保持一条边的两个节点在嵌入空间中的表示相对接近。\n最终 结合一阶相似度和二阶相似度，本文给出了SDNE的目标函数：\n$$\\mathcal{L_{mix}}=\\mathcal{L_{2nd}+\\alpha{\\mathcal{L_{1st}}}}+\\nu{\\mathcal{L_{reg}}} =||(\\hat{X}-X)\\odot{B}||^2_F+\\alpha{\\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}}+\\nu{\\mathcal{L_{reg}}}$$\n其中，为了防止过拟合，添加了$\\mathcal L2$-norm单元$\\mathcal{L_{reg}}$来防止过拟合:\n$$\\mathcal{L_{reg}}=\\frac{1}{2}\\sum_{k=1}^k({||W^{(k)}||^2_F+||\\hat{W}^{k}||_F^2})$$\nOptimization 使用SGD来优化$\\mathcal{L_{mix}}$。具体算法如下：","title":"KDD2016 SDNE:《Structral Deep Network Embedding》 Reading Notes"},{"content":"论文地址：GAT\nIntroduction 本文介绍了一种新型的神经网络架构用来处理图结构。即__Graph Attention Networks__(GATs)。该方法利用masked self-attentional layer，即通过网络层的堆叠，可以获取网络中每个节点的领域特征，同时为领域中的不同节点指定不同的权重。这样做的好处是可以不需要各种高成本的矩阵运算也不依赖于的图结构信息。通过这种方式，GAT可以解决基于谱的图神经网络存在的问题，同时，GAT可以使用用归纳（inductive）和直推（transductive）问题。\n归纳学习:先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。\n直推学习:先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。\nArchitecture 图$G$中有$N$个节点，他们的特征向量为：$\\textbf{h}={\\vec{h_1},\\vec{h_2},\u0026hellip;,\\vec{h_N}}$，其中，$\\vec{h_i} \\in \\mathbb{R}^F$，$F$是每个节点特征数。我们的目的是输出一个新的节点特征向量集$\\textbf{h\u0026rsquo;}={\\vec{h_1\u0026rsquo;},\\vec{h_2\u0026rsquo;},\u0026hellip;,\\vec{h_N\u0026rsquo;}}$，其中$\\vec{h_i\u0026rsquo;} \\in \\mathbb{R}^{F\u0026rsquo;}$。 本质就是修改特征向量的维度（Network embedding）\n为了获得足够的表达能力以将输入特征变换为更高级别的特征，需要至少一个可学习的线性变换。因此，以任意节点$i$和$j$为例，分别对节点$i$和节点$j$的特征向量做线性变换$W \\in \\mathbb{R}^{F \\times F\u0026rsquo;}$，这样 就将$\\vec{h_i}$和$\\vec{h_j}$从$F$维的向量转化为$F\u0026rsquo;$维的向量： $$ e_{ij} = a(W\\vec{h_i},W\\vec{h_j}) $$ 上式中，分别对$\\vec{h_i}$和$\\vec{h_j}$做线性变换，然后使用self-attention为图中的每一个节点分配注意力（权重）。上式中，注意力机制$a$是一个$\\mathbb{R}^{F\u0026rsquo;} \\times \\mathbb{R}^{F\u0026rsquo;} \\to \\mathbb{R}$的映射。最终得到的$e_{ij}$是节点$j$对节点$i$的影响力系数（一个实数）。\n但是，上面的方法考虑其他节点对$i$的影响时，将图中的所有节点都纳入了考虑范围，这样就丢失了图的结构信息。因此，本文引入masked attention机制，即计算影响力系数$e_{ij}$时， 仅考虑节点$i$的一部分邻居节点 $j \\in \\mathcal{N}_i$（$i$也属于$\\mathcal{N}_i$）。使用softmax将节点$i$部分邻居的注意力系数分配到(0,1)上： $$ \\alpha_{ij} = \\mathrm{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i}\\exp(e_{ik})} $$ 在本文中，$a$是一个单层前馈神经网络，参数是一个权重向量$\\vec{\\text{a}} \\in \\mathbb{R}^{2F\u0026rsquo;}$，然后使用负半轴斜率为0.2的LeakyReLU作为非线性激活函数： $$ \\alpha_{ij} = \\frac{\\exp(\\mathrm{LeakyReLU}(\\vec{\\text{a}}^T[W\\vec{h_i}||W\\vec{h_j}]))}{\\sum_{k\\in \\mathcal{N}_i} \\exp(\\mathrm{LeakyReLU}(\\vec{\\text{a}}^T[W\\vec{h_i}||W\\vec{h_k}]))} $$ 其中$||$表示向量的连接操作。上述过程可以用下图表示：\n这样，我们就可以获得节点$j$对节点$i$的注意力系数$\\alpha_{ij}$，那么，节点$i$最终的输出特征$\\vec{h_i\u0026rsquo;}$就是对$\\mathcal{N}_i$中所有节点的加权（加注意力）求和： $$ \\vec{h_i\u0026rsquo;} = \\sigma (\\sum_{j \\in \\mathcal{N}_i}\\alpha_{ij} W\\vec{h_j}) $$\n另外，本文使用multi-head attention来稳定self-attention的学习过程，如下图所示：\n图中是$K=3$ heads的multi-head attention，不同颜色的箭头表示一个独立的attention计算，每个邻居节点做三次attention计算。每次attention计算就是一个普通的self-attention，输出的结果是一个$\\vec{h_i\u0026rsquo;}$。multi-head attention为每个节点$i$输出3个不同的$\\vec{h_i\u0026rsquo;}$,，然后将这三个向量做连接或者取平均，得到最终的$\\vec{h_i\u0026rsquo;}$： $$ \\vec{h_i\u0026rsquo;} = ||^K_{k=1} \\sigma\\left(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j}^{k} \\mathbf{W}^{k} \\vec{h}_{j}\\right) $$ 上式为把不同$k$的向量做连接操作，其中$\\alpha_{ij}^k$和$\\mathbf{W}^{k}$表示第$k$个head的结果，我们可以注意到，最终输出的结果是$KF\u0026rsquo;$维的。除了concat之外，我们还可以通过求平均的方式来获得$\\vec{h_i\u0026rsquo;}$: $$ \\vec{h^\\prime_i}=\\sigma\\left(\\frac{1}{K} \\sum_{k=1}^{K} \\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j}^{k} \\mathbf{W}^{k} \\vec{h}_{j}\\right) $$\nComparisions   GAT是计算高效的。self-attention是在所有边上并行计算，并且输出的特征在所有边上并行计算，从而不需要昂贵的矩阵计算和特征分解。单个head的GAT的时间复杂度为$O\\left(|V| F F^{\\prime}+|E| F^{\\prime}\\right)$，其中$F$是输入的特征数，$|V|$和$|E|$分别是节点数和边数。复杂度与GCN相同。\n  与GCN不同的是，GAT为同一邻域中的节点分配不同的重要性（different importance），提升了模型容量。\n  注意机制以共享的方式应用于图中的所有边（共享$\\mathbf{W}$），因此它不依赖于对全局图结构的预先访问或其所有节点的（特征）。这样有以下提升：\n 不必是无向图。如果$i \\to j$不存在,可以直接不用计算$\\alpha_{ij}$。 可直接应用于归纳学习。    GAT可以被描述为一种特殊的MoNet(Geometric deep learning on graphs and manifolds using mixture model cnns)。\n  Reference 参考：\nGCN：https://arxiv.org/abs/1609.02907\nhttps://zhuanlan.zhihu.com/p/34232818\nhttps://zhuanlan.zhihu.com/p/59176692\nhttps://arxiv.org/abs/1710.10903\n","permalink":"https://JhuoW.github.io/posts/gat/","summary":"论文地址：GAT\nIntroduction 本文介绍了一种新型的神经网络架构用来处理图结构。即__Graph Attention Networks__(GATs)。该方法利用masked self-attentional layer，即通过网络层的堆叠，可以获取网络中每个节点的领域特征，同时为领域中的不同节点指定不同的权重。这样做的好处是可以不需要各种高成本的矩阵运算也不依赖于的图结构信息。通过这种方式，GAT可以解决基于谱的图神经网络存在的问题，同时，GAT可以使用用归纳（inductive）和直推（transductive）问题。\n归纳学习:先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。\n直推学习:先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。\nArchitecture 图$G$中有$N$个节点，他们的特征向量为：$\\textbf{h}={\\vec{h_1},\\vec{h_2},\u0026hellip;,\\vec{h_N}}$，其中，$\\vec{h_i} \\in \\mathbb{R}^F$，$F$是每个节点特征数。我们的目的是输出一个新的节点特征向量集$\\textbf{h\u0026rsquo;}={\\vec{h_1\u0026rsquo;},\\vec{h_2\u0026rsquo;},\u0026hellip;,\\vec{h_N\u0026rsquo;}}$，其中$\\vec{h_i\u0026rsquo;} \\in \\mathbb{R}^{F\u0026rsquo;}$。 本质就是修改特征向量的维度（Network embedding）\n为了获得足够的表达能力以将输入特征变换为更高级别的特征，需要至少一个可学习的线性变换。因此，以任意节点$i$和$j$为例，分别对节点$i$和节点$j$的特征向量做线性变换$W \\in \\mathbb{R}^{F \\times F\u0026rsquo;}$，这样 就将$\\vec{h_i}$和$\\vec{h_j}$从$F$维的向量转化为$F\u0026rsquo;$维的向量： $$ e_{ij} = a(W\\vec{h_i},W\\vec{h_j}) $$ 上式中，分别对$\\vec{h_i}$和$\\vec{h_j}$做线性变换，然后使用self-attention为图中的每一个节点分配注意力（权重）。上式中，注意力机制$a$是一个$\\mathbb{R}^{F\u0026rsquo;} \\times \\mathbb{R}^{F\u0026rsquo;} \\to \\mathbb{R}$的映射。最终得到的$e_{ij}$是节点$j$对节点$i$的影响力系数（一个实数）。\n但是，上面的方法考虑其他节点对$i$的影响时，将图中的所有节点都纳入了考虑范围，这样就丢失了图的结构信息。因此，本文引入masked attention机制，即计算影响力系数$e_{ij}$时， 仅考虑节点$i$的一部分邻居节点 $j \\in \\mathcal{N}_i$（$i$也属于$\\mathcal{N}_i$）。使用softmax将节点$i$部分邻居的注意力系数分配到(0,1)上： $$ \\alpha_{ij} = \\mathrm{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i}\\exp(e_{ik})} $$ 在本文中，$a$是一个单层前馈神经网络，参数是一个权重向量$\\vec{\\text{a}} \\in \\mathbb{R}^{2F\u0026rsquo;}$，然后使用负半轴斜率为0.2的LeakyReLU作为非线性激活函数： $$ \\alpha_{ij} = \\frac{\\exp(\\mathrm{LeakyReLU}(\\vec{\\text{a}}^T[W\\vec{h_i}||W\\vec{h_j}]))}{\\sum_{k\\in \\mathcal{N}_i} \\exp(\\mathrm{LeakyReLU}(\\vec{\\text{a}}^T[W\\vec{h_i}||W\\vec{h_k}]))} $$ 其中$||$表示向量的连接操作。上述过程可以用下图表示：\n这样，我们就可以获得节点$j$对节点$i$的注意力系数$\\alpha_{ij}$，那么，节点$i$最终的输出特征$\\vec{h_i\u0026rsquo;}$就是对$\\mathcal{N}_i$中所有节点的加权（加注意力）求和： $$ \\vec{h_i\u0026rsquo;} = \\sigma (\\sum_{j \\in \\mathcal{N}_i}\\alpha_{ij} W\\vec{h_j}) $$\n另外，本文使用multi-head attention来稳定self-attention的学习过程，如下图所示：","title":"ICLR2018 《Graph Attention Networks》 Reading Notes"},{"content":"论文地址： metapath2vec\nIntroduction 真实网络中通常包含多种类型的节点和关系，这些节点和关系共同组成了异质信息网络（Heterogeneous Information Network），传统的NE方法如DeepWalk, Line , node2vec更多关注与同质网络从而忽略了网络的异质性。针对这个问题，这篇文章提出了metapath2vec基于meta-path的随机游走来构建节点的异质邻域，采用异质Skip-gram模型来学习节点的representation。 另外，在负采样时也考虑异质性，从而进一步提出了metapath2vec++。如下：\nDefinition Definition 1: 异质网络： $G = (V,E,T)$，其中每个节点和边分别对应一个映射 $\\phi(v) : V \\rightarrow T_{V}$ 和$\\varphi(e) : E \\rightarrow T_{E}$。 其中$T_V$和$T_E$分别表示节点的类型集合以及关系的类型集合。且$\\left|T_{V}\\right|+\\left|T_{E}\\right|\u0026gt;2$。\nDefinition 2: 异质网络表示学习 为网络中的节点学习一个$d$维的表示 $\\mathrm{X} \\in \\mathbb{R}^{|V| \\times d}$，$d \\ll|V|$。节点的向量表示可以捕获节点间结构和语义关系。\nModel metapath2vec Skip-Gram模型是给定一个节点，预测向下文节点的概率。metapath2vec的目的是考虑多种类型节点和多种类型关系的情况下，最大化网络概率。为了将异质信息融入skip-gram模型中，首先提出异质skip-gram。\nHeterogeneous Skip-Gram 对于节点类型$|T_V| \u0026gt; 1$的网络$G$， 给定一个节点$v$，需要最大化属于类型$t \\in T_V$的异质上下文节点的概率： $$ \\arg \\max_{\\theta} \\sum_{v \\in V} \\sum_{t \\in T_{V}} \\sum_{c_{t} \\in N_{t}(v)} \\log p\\left(c_{t} | v ; \\theta\\right) $$ 其中，$N_t(v)$表示$v$属于第$t$个类型的邻居节点。 $p\\left(c_{t} | v ; \\theta\\right)$ 表示给定节点$v$，它的$t$类型邻域概率： $p\\left(c_{t} | v ; \\theta\\right)=\\frac{e^{X_{c_{t}} \\cdot X_{v}}}{\\sum_{u \\in V} e^{X_{u} \\cdot X_{v}}}$，是一个softmax函数，$X_v$是节点$v$的表示。由于softmax函数的分母每一次都要遍历所有节点，所以采用负采样改进$\\log p(\\cdot)$ 如下： $$ \\log \\sigma\\left(X_{c_{t}} \\cdot X_{v}\\right)+\\sum_{m=1}^{M} \\mathbb{E}_{u^{m} \\sim P(u)}\\left[\\log \\sigma\\left(-X_{u^{m}} \\cdot X_{v}\\right)\\right] $$ 其中 $\\sigma(x)=\\frac{1}{1+e^{-x}}$。\nMeta-Path-Based Random Walks 在第$i$步时，转移概率$p(v^{i+1}|v^i)$表示为忽略节点类型情况下$v^i$的邻居分布。但是，PathSim提出，异质信息网络中的随机游走偏向于高度可见的节点，即具有主导数量路径的节点，所以 本文设计了基于元路径的随机游走来生成path，从而能够捕获不同类型节点间的结构联系和语义关系，提出了促进异构网络结构转换为metapath2vec的skip-gram。\n一个meta-path模式$\\mathcal{P}: V_{1} \\stackrel{R_{1}}{\\longrightarrow} V_{2} \\stackrel{R_{2}}{\\longrightarrow} \\dots V_{t} \\stackrel{R_{t}}{\\longrightarrow} V_{t+1} \\cdots \\stackrel{R_{l-1}}{\\longrightarrow} V_{l}$， 其中 $R=R_{1} \\circ R_{2} \\circ \\cdots \\circ R_{l-1}$ 节点类型$V_{1}$到$V_{l}$之间的组合关系。那么节点间的跳转概率定义为： $$ p\\left(v^{i+1} | v_{t}^{i}, \\mathcal{P}\\right)=\\left\\{\\begin{array}{cc}{\\frac{1}{\\left|N_{t+1}\\left(v_{t}^{i}\\right)\\right|}} \u0026amp; {\\left(v^{i+1}, v_{t}^{i}\\right) \\in E, \\phi\\left(v^{i+1}\\right)=t+1} \\\\ {0} \u0026amp; {\\left(v^{i+1}, v_{t}^{i}\\right) \\in E, \\phi\\left(v^{i+1}\\right) \\neq t+1} \\\\ {0} \u0026amp; {\\left(v^{i+1}, v_{t}^{i}\\right) \\notin E}\\end{array}\\right. $$ 其中$v^i_t \\in V_t$，$N_{t+1}\\left(v_{t}^{i}\\right)$表示属于$t$类型的节点$v$的属于$t+1$类型的邻居。如果下一个节点$v^{i+1}$和$v^i_t$之间有边，并且$v^{i+1}$是$t+1$类型的节点 那么转移概率服从平均分布。其中，$v^{i+1}$服从meta-path所定义的下移节点类型。如图（a）中，原路径为$OAPVPAO$，那么节点$a_4$的下一个节点必然要是$P$类。 由于meta-path的对称性，所以： $$ p\\left(v^{i+1} | v_{t}^{i}\\right)=p\\left(v^{i+1} | v_{1}^{i}\\right), \\text { if } t=l $$\nmetapath2vec++ 由于softmax做归一化时没有考虑节点类型，分母是对所有节点求和，所以为了融合节点类型，给出Heterogeneous negative sampling: $$ p\\left(c_{t} | v ; \\theta\\right)=\\frac{e^{X_{c_{t}} \\cdot X_{v}}}{\\sum_{u_{t} \\in V_{t}} e^{X_{u_{t}} \\cdot X_{v}}} $$ 如图（c）所示，metapath2vec++对每种类型节点指定不同的一组多项式分布，相当于在输出层根据节点类型，把异质网络分解成不同的同质网络，同样采用负采用的方法简化计算： $$ O(\\mathrm{X})=\\log \\sigma\\left(X_{c_{t}} \\cdot X_{v}\\right)+\\sum_{m=1}^{M} \\mathbb{E}_{u_{t}^{m} \\sim P_{t}\\left(u_{t}\\right)}\\left[\\log \\sigma\\left(-X_{u_{t}^{m}} \\cdot X_{v}\\right)\\right] $$ 算法如下：\n","permalink":"https://JhuoW.github.io/posts/metapath2vec/","summary":"论文地址： metapath2vec\nIntroduction 真实网络中通常包含多种类型的节点和关系，这些节点和关系共同组成了异质信息网络（Heterogeneous Information Network），传统的NE方法如DeepWalk, Line , node2vec更多关注与同质网络从而忽略了网络的异质性。针对这个问题，这篇文章提出了metapath2vec基于meta-path的随机游走来构建节点的异质邻域，采用异质Skip-gram模型来学习节点的representation。 另外，在负采样时也考虑异质性，从而进一步提出了metapath2vec++。如下：\nDefinition Definition 1: 异质网络： $G = (V,E,T)$，其中每个节点和边分别对应一个映射 $\\phi(v) : V \\rightarrow T_{V}$ 和$\\varphi(e) : E \\rightarrow T_{E}$。 其中$T_V$和$T_E$分别表示节点的类型集合以及关系的类型集合。且$\\left|T_{V}\\right|+\\left|T_{E}\\right|\u0026gt;2$。\nDefinition 2: 异质网络表示学习 为网络中的节点学习一个$d$维的表示 $\\mathrm{X} \\in \\mathbb{R}^{|V| \\times d}$，$d \\ll|V|$。节点的向量表示可以捕获节点间结构和语义关系。\nModel metapath2vec Skip-Gram模型是给定一个节点，预测向下文节点的概率。metapath2vec的目的是考虑多种类型节点和多种类型关系的情况下，最大化网络概率。为了将异质信息融入skip-gram模型中，首先提出异质skip-gram。\nHeterogeneous Skip-Gram 对于节点类型$|T_V| \u0026gt; 1$的网络$G$， 给定一个节点$v$，需要最大化属于类型$t \\in T_V$的异质上下文节点的概率： $$ \\arg \\max_{\\theta} \\sum_{v \\in V} \\sum_{t \\in T_{V}} \\sum_{c_{t} \\in N_{t}(v)} \\log p\\left(c_{t} | v ; \\theta\\right) $$ 其中，$N_t(v)$表示$v$属于第$t$个类型的邻居节点。 $p\\left(c_{t} | v ; \\theta\\right)$ 表示给定节点$v$，它的$t$类型邻域概率： $p\\left(c_{t} | v ; \\theta\\right)=\\frac{e^{X_{c_{t}} \\cdot X_{v}}}{\\sum_{u \\in V} e^{X_{u} \\cdot X_{v}}}$，是一个softmax函数，$X_v$是节点$v$的表示。由于softmax函数的分母每一次都要遍历所有节点，所以采用负采样改进$\\log p(\\cdot)$ 如下： $$ \\log \\sigma\\left(X_{c_{t}} \\cdot X_{v}\\right)+\\sum_{m=1}^{M} \\mathbb{E}_{u^{m} \\sim P(u)}\\left[\\log \\sigma\\left(-X_{u^{m}} \\cdot X_{v}\\right)\\right] $$ 其中 $\\sigma(x)=\\frac{1}{1+e^{-x}}$。","title":"KDD2017 《metapath2vec:Scalable Representation Learning for Heterogeneous Networks》 Reading Notes"},{"content":"最近遇到一个问题，如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积\n之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：\n我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：\n{% codeblock %}\nimport cv2 import numpy as np # Input image img = cv2.imread('cut.jpeg', cv2.IMREAD_GRAYSCALE) # Needed due to JPG artifacts _, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY) # Dilate to better detect contours temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) # Find largest contour _, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) largestCnt = [] for cnt in cnts: if len(cnt) \u0026gt; len(largestCnt): largestCnt = cnt # Determine center of area of largest contour M = cv2.moments(largestCnt) x = int(M[\u0026quot;m10\u0026quot;] / M[\u0026quot;m00\u0026quot;]) y = int(M[\u0026quot;m01\u0026quot;] / M[\u0026quot;m00\u0026quot;]) # Initiale mask for flood filling width, height = temp.shape mask = img2 = np.ones((width + 2, height + 2), np.uint8) * 255 mask[1:width, 1:height] = 0 # Generate intermediate image, draw largest contour, flood filled temp = np.zeros(temp.shape, np.uint8) temp = cv2.drawContours(temp, largestCnt, -1, 255, cv2.FILLED) _, temp, mask, _ = cv2.floodFill(temp, mask, (x, y), 255) temp = cv2.morphologyEx(temp, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) # Count pixels in desired region area = cv2.countNonZero(temp) # Put result on original image img = cv2.putText(img, str(area), (x, y), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, 255) cv2.imshow('Input', img) cv2.imshow('Temp image', temp) cv2.waitKey(0)  {% endcodeblock %}\n最后我们可以得到一个比较准确的轮廓：\n面积如图中所示：\n参考：\nhttps://stackoverflow.com/questions/55467031/how-to-get-the-area-of-the-contours\n","permalink":"https://JhuoW.github.io/posts/pic-closed-edge/","summary":"最近遇到一个问题，如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积\n之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：\n我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：\n{% codeblock %}\nimport cv2 import numpy as np # Input image img = cv2.imread('cut.jpeg', cv2.IMREAD_GRAYSCALE) # Needed due to JPG artifacts _, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY) # Dilate to better detect contours temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) # Find largest contour _, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) largestCnt = [] for cnt in cnts: if len(cnt) \u0026gt; len(largestCnt): largestCnt = cnt # Determine center of area of largest contour M = cv2.","title":"OpenCV轮廓提取并计算图片中某一封闭区域的面积"},{"content":"paper\nIntroduction 在现实世界的社交网络中，一个顶点在与不同的邻居顶点交互时可能表现出不同的方面 (aspect)，这是很直观的。例如，研究人员通常与各种合作伙伴就不同的研究主题进行合作（如下图所示），社交媒体用户与分享不同兴趣的各种朋友联系，一个网页出于不同目的链接到多个其它网页。然而，现有的大多数 NE 方法只为每个顶点安排一个 single embedding 向量，并产生以下两个问题：\n 这些方法在与不同邻居交互时，无法灵活转换不同的aspect 在这些模型中，一个顶点倾向于迫使它的所有邻居之间的 embedding彼此靠近，但事实上并非一直如此。例如下图中，左侧用户和右侧用户共享较少的共同兴趣，但是由于他们都链接到中间用户，因此被认为彼此接近。因此，这使得顶点 embedding 没有区分性。  为了解决上述问题，本文提出了一个 CANE框架，用于精确建模顶点之间的关系。更具体而言，论文在信息网络上应用 CANE。信息网络的每个顶点还包含丰富的外部信息，例如文本、标签 、或者其它元数据。在这种场景下，上下文的重要性对 network embedding 更为关键。在不失一般性的情况下，论文在基于文本的信息网络中实现了 CANE，但是 CANE可以很容易地扩展到其它类型的信息网络。\n在传统的 network embedding模型中，每个顶点都表达为一个静态的 embedding 向量，即 context-free embedding 。相反，CANE 根据与当前顶点交互的不同邻居，从而将动态的 embedding分配给当前顶点，称为 context-aware embedding。以一个顶点$u$为例：当与不同的邻居交互时， 的 context-free embedding保持不变；而当面对不同的邻居时， $u$的 context-aware embedding是动态的。\n当顶点$u$与它的邻居顶点$v$交互时，它们彼此相关的 context embedding 分别来自它们的文本信息。对于每个顶点，可以轻松地使用神经模型neural model ，例如卷积神经网络和循环神经网络来构建 context-free embedding 和 text-based embedding 。为了实现 context-aware text-based embedding，论文引入了 selective attention 方案，并在这些神经模型中建立了 和 之间的互注意力 mutual attention 。mutual attention 预期引导神经模型强调那些被相邻顶点 focus 的单词，并最终获得 context-aware embedding。每个顶点的 context-free embedding 和 context-aware embedding 都可以通过使用现有的 network embedding 方法学到（如 DeepWalk，LINE，node2vec）并拼接起来。\n论文对不同领域的三个真实数据集进行了实验。与其它 state-of-the-art 方法相比，链接预测的实验结果展示了论文框架的有效性。结果表明，context-aware embedding 对于网络分析至关重要，特别是对于那些涉及顶点之间复杂交互的任务，如链接预测。论文还通过顶点分类和案例研究来探索论文框架的性能，这再次证实了 CANE 模型的灵活性和优越性。\nModel 给定信息网络$G = (V,E,T)$, ，其中$V$为顶点集合， $E \\subseteq V \\times V$为边集合， $T$为顶点的文本信息。每条边表示顶点之间的关系，并且关联一个权重$w_{u,v}$ 。这里顶点$v \\in V$的文本信息表达为单词序列$\\mathcal{S}_{v}=\\left(t_{1}, t_{2}, \\cdots, t_{n_{v}}\\right)$，其中$n_{v}=\\left|\\mathcal{S}_{v}\\right|$为序列$\\mathcal{S}_{v}$的长度。\nNE旨在根据网络结构和关联信息（如文本和 label）为每个顶点$v \\in V$ 学习低维embedding$\\overrightarrow{\\mathbf{v}} \\in \\mathbb{R}^{d}$, 其中 $d \\ll |V|$为embedding的维度。\nContext-free Embedding 的定义：传统的 network representation learning 模型为每个顶点学习 context-free embedding 。这意味着顶点的 embedding 是固定的，并且不会根据顶点的上下文信息（即与之交互的另一个顶点）而改变。\nContext-aware Embedding 的定义：与现有的、学习 context-free embedding 的NE 模型不同，CANE 根据顶点不同的上下文来学习该顶点的各种 embedding 。具体而言，对于边$e_{u,v}$， CANE 学习 context-aware embedding $\\overrightarrow{\\mathbf{v}}_{(u)}$和$\\overrightarrow{\\mathbf{u}}_{(v)}$\nFramework 为了充分利用网络结构和关联的文本信息，为顶点$v$提供了两种类型的 embedding：structure-based embedding $\\overrightarrow{\\mathbf{v}}^{s}$, text-based embedding $\\overrightarrow{\\mathbf{v}}^{t}$。structure-based embeding 可以捕获网络结构中的信息，而 text-based embedding 可以捕获关联文本信息中的文本含义（textual meaning）。使用这些 embedding，可以简单地拼接它们并获得顶点 embedding 为： $$ \\overrightarrow{\\mathbf{V}}=\\overrightarrow{\\mathbf{V}}^{s} \\oplus \\overrightarrow{\\mathbf{V}}^{t} $$ 其中$\\oplus$为拼接算子。注意， text-based embedding $\\overrightarrow{\\mathbf{v}}^{t}$可以是 context-free 的、也可以是context-aware的，这将在后面详细介绍。当$\\overrightarrow{\\mathbf{v}}^{t}$是 context-aware时，整个顶点 embedding $\\overrightarrow{\\mathbf{v}}$也将是 context-aware。\n通过上述定义，CANE旨在最大化所有边上的目标函数，如下所示： $$ \\mathcal{L}=\\sum_{e \\in E} L(e) $$ 这里每条边的目标函数$L(e)$由两部分组成： $$ L(e) = L_s(e) + L_t(e) $$ $L_s(e)$表示基于结构的目标函数, $L_t(e)$表示基于文本的目标函数。 接下来分别对这两个目标函数进行详细介绍。\n基于结构的目标函数 为了不失一般性，假设网络是有向的，因为无向边可以被认为是两个方向相反、且权重相等的有向边。因此，基于结构的目标函数旨在使用 structure-based embedding来衡量观察到一条有向边的对数似然（log-likelihood），即： $$ L_{s}(e)=w_{u, v} \\log p\\left(\\overrightarrow{\\mathbf{v}}^{s} \\mid \\overrightarrow{\\mathbf{u}}^{s}\\right) $$ 遵从 LINE，将上式中的条件概率定义为： $$ p\\left(\\overrightarrow{\\mathbf{v}}^{s} \\mid \\overrightarrow{\\mathbf{u}}^{s}\\right)=\\frac{\\exp \\left(\\overrightarrow{\\mathbf{u}}^{s} \\cdot \\overrightarrow{\\mathbf{v}}^{s}\\right)}{\\sum_{z \\in V} \\overrightarrow{\\mathbf{u}}^{s} \\cdot \\overrightarrow{\\mathbf{z}}^{s}} $$\n基于文本的目标函数 现实世界社交网络中的顶点通常伴随着关联的文本信息。因此，本文提出了基于文本的目标函数来利用这些文本信息，并学习 text-based embedding 。\n基于文本的目标函数$L_{t}(e)$可以通过各种度量指标来定义。为了与$L_{s}(e)$ 兼容，将$L_{t}(e)$定义如下： $$ L_{t}(e)=\\alpha L_{t, t}(e)+\\beta L_{t, s}(e)+\\gamma L_{s, t}(e) $$ 其中：$\\alpha$，$\\beta$，$\\gamma$为对应部分的重要性，为超参数。\n$L_{t,t}(e)$，$L_{t, s}(e)$，$L_{s, t}(e)$定义为： $$ \\begin{aligned} \u0026amp;L_{t, t}(e)=w_{u, v} \\log p\\left(\\overrightarrow{\\mathbf{v}}^{t} \\mid \\overrightarrow{\\mathbf{u}}^{t}\\right) \\\\ \u0026amp;L_{t, s}(e)=w_{u, v} \\log p\\left(\\overrightarrow{\\mathbf{v}}^{t} \\mid \\overrightarrow{\\mathbf{u}}^{s}\\right) \\\\ \u0026amp;L_{s, t}(e)=w_{u, v} \\log p\\left(\\overrightarrow{\\mathbf{v}}^{s} \\mid \\overrightarrow{\\mathbf{u}}^{t}\\right) \\end{aligned} $$ 这里构建了 structure-based embedding 、 text-based embedding 之间的桥梁，使得信息在结构和文本之间流动。上式中的条件概率将两种类型的顶点 embedding 映射到相同的 representation 空间中，但是考虑到它们自身的特点，这里并未强制它们相同。类似地，使用 softmax 函数来计算概率，如公式$p\\left(\\overrightarrow{\\mathbf{v}}^{s} \\mid \\overrightarrow{\\mathbf{u}}^{s}\\right)$所示。\nstructure-based embedding 被视为 parameter，与传统的 network embedding 模型相同。但是对于 text-based embedding，本文从顶点的关联文本信息中获取它们。此外， text-based embedding 可以通过 context-free 的方式、或者 context-aware 的方式获取。\nContext-Free Text Embedding 有多种神经网络模型可以从单词序列 word sequence 中获取 text embedding，如卷积神经网络 CNN、循环神经网络RNN。在这项工作中，研究了用于文本建模的不同神经网络，包括 CNN、Bidirectional RNN、GRU，并采用了性能最好的 CNN。CNN 可以在单词之间捕获局部语义依赖性。\nCNN 以一个顶点的单词序列作为输入，通过 lookup、卷积、池化这三层得到基于文本的 embedding 。\n  lookup：给定一个单词序列 $\\mathcal{S}=\\left(t_{1}, t_{2}, \\cdots, t_{n}\\right)$, lookup layer 将每个单词$t_i$转换为对应的 word embedding $\\overrightarrow{\\mathbf{t}}_{i} \\in \\mathbb{R}^{d^{\\prime}}$, 并且获得 embedding序列$\\mathbf{S}=\\left(\\overrightarrow{\\mathbf{t}}_{1}, \\cdots, \\overrightarrow{\\mathbf{t}}_{n}\\right)$, 其中$d^{\\prime}$为word embedding的维度。\n  卷积：lookup之后，卷积层提取输入的embedding序列$\\mathbf{S}$局部特征 local feature。具体而言，它使用卷积矩阵$\\mathbf{C} \\in \\mathbb{R}^{d \\times\\left(l \\times d^{\\prime}\\right)}$在长度为$l$的滑动窗口上进行卷积操作，如下所示：\n  $$ \\overrightarrow{\\mathbf{x}}_{i}=\\mathbf{C} * \\mathbf{S}_{i: i+l-1}+\\overrightarrow{\\mathbf{b}} $$\n$\\mathbf{C}$表示$d$个卷积核，每个卷积核的尺寸为$l \\times d^\\prime$。 $d$为卷积核数量，也称作输出通道数。\n其中：$\\mathbf{S}_{i: i+l-1} \\in \\mathbb{R}^{l \\times d}$为第$i$个滑动窗口内所有word embedding拼接得到的矩阵。$\\overrightarrow{\\mathbf{b}} \\in \\mathbb{R}^{d}$为bias。注意，在单词序列的边缘添加了零填充向量。\n 最大池化：为了获得 text embedding $\\overrightarrow{\\mathbf{v}}^{t}$, 对 $\\left\\{\\overrightarrow{\\mathbf{x}}_{1}, \\cdots, \\overrightarrow{\\mathbf{x}}_{n}\\right\\}$进行最大池化和非线性变换，如下所示：  $$ \\begin{gathered} r_{i}=\\tanh \\left(\\max \\left(x_{1, i}, x_{2, i}, \\cdots, x_{n, i}\\right)\\right), \\quad i=1, \\cdots, d \\\\ \\overrightarrow{\\mathbf{v}}^{t}=\\left(r_{1}, \\cdots, r_{d}\\right)^{\\top} \\in \\mathbb{R}^{d} \\end{gathered} $$\n其中max是在 embedding维度上进行的，tanh为逐元素的函数。\nContext-Aware Text Embedding 假设顶点在与其它顶点交互时扮演不同的角色。换句话讲，对于给定的顶点，其它不同的顶点与它有不同的焦点，这将导致 context-aware text embedding。\n为了实现这一点，采用 mutual attention来获得 context-aware text embedding。mutual attention使 CNN 中的池化层能够感知 edge中的顶点 pair，从而使得来自一个顶点的文本信息可以直接影响另一个顶点的 text embedding，反之亦然。\n如上图所示，对于context-aware text embedding的生成过程， 给定一条边$e_{u,v}$和两个对应的文本序列$\\mathcal{S}_u$和$\\mathcal{S}_v$, 可以通过卷积层得到矩阵$\\mathbf{P} \\in \\mathbb{R}^{d \\times m}$和$\\mathbf{Q} \\in \\mathbb{R}^{d \\times n}$ 。这里$m$表示$\\mathcal{S}_u$的长度，$n$表示$\\mathcal{S}_v$的长度。通过引入一个注意力矩阵$\\mathbf{A} \\in \\mathbb{R}^{d \\times d}$，计算相关性矩阵$\\mathbf{F} \\in \\mathbb{R}^{m \\times n}$: $$ \\mathbf{F}=\\tanh \\left(\\mathbf{P}^{\\top} \\mathbf{A} \\mathbf{Q}\\right) $$ 其中$\\mathbf{F}$中的每个元素$\\mathbf{F}_{i,j}$表示两个隐向量（即$\\overrightarrow{\\mathbf{p}}_{i}$， $\\overrightarrow{\\mathbf{q}}_{j}$）之间的 pair-wise相关性得分。 然后，沿 $\\mathbf{F}$的行和列进行池化操作从而生成重要性向量，分别称作行池化row-pooling和列池化column-pooling。根据实验，均值池化的性能优于最大池化。因此，采用如下的均值池化操作： $$ \\begin{aligned} g_{i}^{p} \u0026amp;=\\operatorname{mean}\\left(F_{i, 1}, F_{i, 2}, \\cdots, F_{i, n}\\right) \\\\ g_{i}^{q} \u0026amp;=\\operatorname{mean}\\left(F_{1, i}, F_{2, i}, \\cdots, F_{m, i}\\right) \\end{aligned} $$ 其中：$\\overrightarrow{\\mathbf{g}}^{p}=\\left(g_{1}^{p}, \\cdots, g_{m}^{p}\\right)^{\\top} \\in \\mathbb{R}^{m}$表示行池化向量，为$\\mathbf{P}$的重要性向量。$\\overrightarrow{\\mathbf{g}}^{q}=\\left(g_{1}^{q}, \\cdots, g_{m}^{q}\\right)^{\\top} \\in \\mathbb{R}^{m}$表示列池化向量，为$\\mathbf{Q}$的重要性向量。\n接下来，使用softmax函数将重要性向量$\\overrightarrow{\\mathbf{g}}^{p}$和$\\overrightarrow{\\mathbf{g}}^{q}$转换为注意力向量 $\\overrightarrow{\\mathbf{a}}^{p}$和$\\overrightarrow{\\mathbf{a}}^{q}$： $$ \\begin{aligned} a_{i}^{p}=\\frac{\\exp \\left(g_{i}^{p}\\right)}{\\sum_{j=1}^{m} \\exp \\left(g_{j}^{p}\\right)}, \\quad a_{i}^{q}=\\frac{\\exp \\left(g_{i}^{q}\\right)}{\\sum_{j=1}^{n} \\exp \\left(g_{j}^{q}\\right)} \\ \\end{aligned} $$\n$$ \\begin{aligned} \\overrightarrow{\\mathbf{a}}^{p}=\\left(a_{1}^{p}, \\cdots, a_{m}^{p}\\right)^{\\top} \\in \\mathbb{R}^{m} \\\\ \\overrightarrow{\\mathbf{a}}^{q}=\\left(a_{1}^{q}, \\cdots, a_{n}^{q}\\right)^{\\top} \\in \\mathbb{R}^{n} \\end{aligned} $$ 最后，顶点$u$和$v$ 的 context-aware text embedding计算为： $$ \\overrightarrow{\\mathbf{u}}_{(v)}^{t}=\\mathbf{P} \\overrightarrow{\\mathbf{a}}^{p}, \\quad \\overrightarrow{\\mathbf{v}}_{(u)}^{t}=\\mathbf{Q} \\overrightarrow{\\mathbf{a}}^{q} $$ 现在，给定一条边$e_{u,v}$，可以获得 context-aware embedding，它是 structure embedding 和 context-aware text embedding的拼接： $$ \\overrightarrow{\\mathbf{u}}_{(v)}=\\overrightarrow{\\mathbf{u}}^{s} \\oplus \\overrightarrow{\\mathbf{u}}_{(v)}^{t}, \\quad \\overrightarrow{\\mathbf{v}}_{(u)}=\\overrightarrow{\\mathbf{v}}^{s} \\oplus \\overrightarrow{\\mathbf{v}}_{(u)}^{t} $$\nCANE优化过程 CANE 旨在最大化若干个条件概率。直观而言，使用 softmax函数优化条件概率在计算上代价太大。因此，使用负采样，并将目标函数转换为以下形式： $$ \\log \\sigma(\\overrightarrow{\\mathbf{u}} \\cdot \\overrightarrow{\\mathbf{v}})+k \\times \\mathbb{E}_{z \\sim P(v)}[\\log \\sigma(-\\overrightarrow{\\mathbf{u}} \\cdot \\overrightarrow{\\mathbf{z}})] $$ 其中$k$为负采样比例， $\\sigma$为sigmoid, $P(v) \\propto d_{v}^{3 / 4}$为负顶点的采样分布，$d_{v}$为顶点$v$的out-degree。\n","permalink":"https://JhuoW.github.io/posts/context-aware-ne/","summary":"paper\nIntroduction 在现实世界的社交网络中，一个顶点在与不同的邻居顶点交互时可能表现出不同的方面 (aspect)，这是很直观的。例如，研究人员通常与各种合作伙伴就不同的研究主题进行合作（如下图所示），社交媒体用户与分享不同兴趣的各种朋友联系，一个网页出于不同目的链接到多个其它网页。然而，现有的大多数 NE 方法只为每个顶点安排一个 single embedding 向量，并产生以下两个问题：\n 这些方法在与不同邻居交互时，无法灵活转换不同的aspect 在这些模型中，一个顶点倾向于迫使它的所有邻居之间的 embedding彼此靠近，但事实上并非一直如此。例如下图中，左侧用户和右侧用户共享较少的共同兴趣，但是由于他们都链接到中间用户，因此被认为彼此接近。因此，这使得顶点 embedding 没有区分性。  为了解决上述问题，本文提出了一个 CANE框架，用于精确建模顶点之间的关系。更具体而言，论文在信息网络上应用 CANE。信息网络的每个顶点还包含丰富的外部信息，例如文本、标签 、或者其它元数据。在这种场景下，上下文的重要性对 network embedding 更为关键。在不失一般性的情况下，论文在基于文本的信息网络中实现了 CANE，但是 CANE可以很容易地扩展到其它类型的信息网络。\n在传统的 network embedding模型中，每个顶点都表达为一个静态的 embedding 向量，即 context-free embedding 。相反，CANE 根据与当前顶点交互的不同邻居，从而将动态的 embedding分配给当前顶点，称为 context-aware embedding。以一个顶点$u$为例：当与不同的邻居交互时， 的 context-free embedding保持不变；而当面对不同的邻居时， $u$的 context-aware embedding是动态的。\n当顶点$u$与它的邻居顶点$v$交互时，它们彼此相关的 context embedding 分别来自它们的文本信息。对于每个顶点，可以轻松地使用神经模型neural model ，例如卷积神经网络和循环神经网络来构建 context-free embedding 和 text-based embedding 。为了实现 context-aware text-based embedding，论文引入了 selective attention 方案，并在这些神经模型中建立了 和 之间的互注意力 mutual attention 。mutual attention 预期引导神经模型强调那些被相邻顶点 focus 的单词，并最终获得 context-aware embedding。每个顶点的 context-free embedding 和 context-aware embedding 都可以通过使用现有的 network embedding 方法学到（如 DeepWalk，LINE，node2vec）并拼接起来。","title":"ACL2017 《CANE:Context-Aware Network Embedding for Relation Modeling》 Reading Notes"},{"content":"最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。\nGradient Desent(梯度下降) 目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。\n(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。\n(2).目标函数关于参数$x$在epoch $t$时的梯度：\n$$g_t = \\nabla_x f(x_t)$$\n(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：\n$$x_{t+1} = x_t-\\eta_t g_t$$\n其中$x_{t+1}$为$t+1$时刻的参数值。\nStochastic Gradient Desent(随机梯度下降) 梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。\n首先给出无偏估计的定义，稍后会用到：\n无偏估计：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。\n深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \\frac{\\displaystyle\\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：\n$$\\nabla f_{batch}(x) = \\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\nabla f_i(x)$$\n如果使用GD来优化：\n$$x_{t+1} = x_{t}- \\eta_t \\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\nabla f_i(x) \\ = x_t-\\eta_t \\nabla f_{batch}(x)$$ 上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。\n随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \\in {1, \\cdots,n}$, 每次从中随机选取一个样本来更新参数$x$。\n$$x_{t+1} = x_{t}-\\eta_t \\nabla f_i(x)$$\n这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\\nabla f_i(x)$是对梯度$\\nabla f_{batch}(x)$的无偏估计，因为：\n$$E_i \\nabla f_i(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\boldsymbol{x}) = \\nabla f_{batch}(\\boldsymbol{x})$$\n符合无偏估计的定义。\nMomentum(动量法) Exponentially weighted moving averages(EMA) EMA,指数加权移动平均数。\n在GD中,如果学习率过大，会导致目标函数发散，而无法逼近最小值，如下图所示：\n如果学习率很低，那么会缓慢接近最优点，如下图红色轨迹：\n我们希望在学习率较小的时候可以更快逼近最优点，在学习率大的时候自变量可以不发散，即在正确的方向上加速下降并且抑制震荡，也就是达到如下的效果：\n因此引入EMA。给定参数$0 \\leq \\gamma \u0026lt; 1$,当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合。\n$$y_t = \\gamma y_{t-1} + (1-\\gamma) x_t$$\n展开上式:\n$$\\begin{split}\\begin{aligned} y_t \u0026amp;= (1-\\gamma) x_t + \\gamma y_{t-1}\\\\ \u0026amp;= (1-\\gamma)x_t + (1-\\gamma) \\cdot \\gamma x_{t-1} + \\gamma^2y_{t-2}\\\\ \u0026amp;= (1-\\gamma)x_t + (1-\\gamma) \\cdot \\gamma x_{t-1} + (1-\\gamma) \\cdot \\gamma^2x_{t-2} + \\gamma^3y_{t-3}\\\\ \u0026amp;\\ldots \\end{aligned}\\end{split}$$\n上式可以看出当前时刻变量是对过去时刻变量做指数加权，离当前时刻越近，加权越大（越接近1）。\n在现实中，我们将$y_t$看作是最近$1/(1-\\gamma)$个时间步的$x_t$的加权平均，当$\\gamma = 0.95$时，是最近20个时间步的$x_t$值的加权平均。当$\\gamma=0.9$时,可以看做是最近10个时间步加权平均。\n动量法 $$\\begin{split}\\begin{aligned} \\boldsymbol{v}_t \u0026amp;= \\gamma \\boldsymbol{v}_{t-1} + \\eta_t \\boldsymbol{g}_t, \\\\ \\boldsymbol{x}_t \u0026amp;= \\boldsymbol{x}_{t-1} - \\boldsymbol{v}_t, \\end{aligned}\\end{split}$$\n其中$g_t = \\nabla f_i(x)$上式可以看出，如果$\\gamma=0$，则上式就是一个普通的随机梯度下降法。$0 \\leq \\gamma \u0026lt; 1$. $\\gamma$一般取0.9。\n一般，初始化$v_0=0$, 则\n$$v_1=\\eta_t g_t \\\\ v_2=\\gamma v_1+\\eta_t g_t = \\eta_t g_t(\\gamma+1) \\\\ v_3 = \\eta_t g_t (\\gamma^2+\\gamma+1) \\\\ v_{inf} = \\frac{(\\eta_t g_t)\\cdot(1-\\gamma^{inf+1})}{1-\\gamma}\\approx \\frac{(\\eta_t g_t)}{1-\\gamma}$$\n相比原始梯度下降算法，动量梯度下降算法有助于加速收敛。当梯度与动量方向一致时，动量项会增加，而相反时，动量项减少，因此动量梯度下降算法可以减少训练的震荡过程。\n换种方式理解动量法：\n如上图所示，A点为起始点，首先计算A点的梯度$\\nabla a$，下降到B点，\n$$\\theta_{new} = \\theta-\\eta\\nabla a$$\n其中$\\theta$为参数， $\\eta$为学习率\n到达B点后要加上A点的梯度，但是A点的梯度有个衰减值$\\gamma$,推荐取0.9，相当于加上一个来自A点递减的加速度。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。所以B点的参数更新公式是这样的：\n$$v_t = \\gamma v_{t-1}+\\eta \\nabla b$$\n$$\\theta_{new} = \\theta-v_t$$\n其中$v_{t-1}$表示之前所有步骤累计的动量和，$\\nabla b$为B点的梯度方向。这样一步一步下去，带着初速度的小球就会极速的奔向谷底。\nAdaGrad 假设目标函数有两个参数分别为$x_1$,$x_2$,若梯度下降迭代过程中，始终使用相同的学习率$\\eta$:\n$$x_{1_{new}} = x_1-\\eta \\frac{\\partial f}{\\partial x_1}$$ $$x_{2_{new}} = x_2-\\eta \\frac{\\partial f}{\\partial x_2}$$\nAdaGard算法根据自变量在每个维度的梯度值来调整各个维度上的学习率，避免学习率难以适应维度的问题。adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。\n$\\nabla_{\\theta_i} J(\\theta)$表示第$i$个参数的梯度，其中$\\theta=(\\theta_1,\\theta_2,\u0026hellip;)$有$n$个参数。如果使用SGD来优化第$i$个参数，我们可以表示为:\n$$\\theta_{i_new} = \\theta_i-\\eta \\nabla_{\\theta_i}J(\\theta)$$\n如果使用Adagrad，则可以表示为这样:\n$$\\theta_{i,t+1}=\\theta_{i,t}-\\frac{\\eta}{\\sqrt{G_{i,t}+\\epsilon}} \\nabla_{\\theta_{i,t}}J(\\theta)$$\n$i,t$ 表示优化参数$\\theta_i$时的第$t$次迭代，$\\epsilon$防止分母为0，可以取$10^{-6}$,$G_{i,t}$表示对参数$\\theta_i$优化的前$t$步的梯度的累加：\n$$G_{i,t} = G_{i,t-1}+\\nabla_{\\theta_{i,t}}J(\\theta) $$\n新公式可以简化成:\n$$\\theta_{t+1}= \\theta_t-\\frac{\\eta}{\\sqrt{G_t+\\epsilon}}\\nabla_{\\theta_t}J(\\theta)$$\n可以从上式看出，随着迭代的推移，新的学习率$\\frac{\\eta}{\\sqrt{G_t+\\epsilon}}$在缩小，说明Adagrad一开始激励收敛，到了训练的后期惩罚收敛，收敛速度变慢\nRMSprop 主要解决Adagrad学习率过快衰减问题，类似动量的思想，引入一个超参数，在积累梯度平方项进行衰减.\n$$s = \\gamma \\cdot s +(1-\\gamma) \\cdot \\nabla J(\\theta) \\odot \\nabla J(\\theta) $$\n参数$\\theta$的迭代目标函数可以改写为:\n$$\\theta_{new} = \\theta - \\frac{\\eta}{\\sqrt{s+\\varepsilon}} \\odot \\nabla J(\\theta)$$\n可以看出$s$是梯度的平方的指数加权移动平均值，$\\gamma$一般取0.9，有助于解决 Adagrad中学习率下降过快的情况。\nAdaptive moment estimation(Adam) Adam可以说是用的最多的优化算法，Adam通过计算一阶矩估计和二阶矩估计为不同的参数设计独立的自适应学习率。\nAdabound 正在学习中\n参考文献：\nhttps://zhuanlan.zhihu.com/p/32626442\nhttps://zhuanlan.zhihu.com/p/31630368\nhttps://zh.gluon.ai/\nhttps://blog.csdn.net/tsyccnh/article/details/76270707\n","permalink":"https://JhuoW.github.io/posts/optimizer/","summary":"最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。\nGradient Desent(梯度下降) 目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。\n(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。\n(2).目标函数关于参数$x$在epoch $t$时的梯度：\n$$g_t = \\nabla_x f(x_t)$$\n(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：\n$$x_{t+1} = x_t-\\eta_t g_t$$\n其中$x_{t+1}$为$t+1$时刻的参数值。\nStochastic Gradient Desent(随机梯度下降) 梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。\n首先给出无偏估计的定义，稍后会用到：\n无偏估计：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。\n深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \\frac{\\displaystyle\\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：\n$$\\nabla f_{batch}(x) = \\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\nabla f_i(x)$$\n如果使用GD来优化：\n$$x_{t+1} = x_{t}- \\eta_t \\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\nabla f_i(x) \\ = x_t-\\eta_t \\nabla f_{batch}(x)$$ 上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。\n随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \\in {1, \\cdots,n}$, 每次从中随机选取一个样本来更新参数$x$。\n$$x_{t+1} = x_{t}-\\eta_t \\nabla f_i(x)$$\n这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\\nabla f_i(x)$是对梯度$\\nabla f_{batch}(x)$的无偏估计，因为：","title":"深度学习中的优化算法总结"},{"content":"🌟自己好菜啊，希望可以多多刷paper\n💘希望可以长久的更新这个blog，如有错误欢迎评论或者发邮件给我~\n👋如果本blog有幸被大佬看到，求带飞 😊\n联系方式：  e-mail: jhuow@proton.me  Publication 2022 [1] ZHUO W, et at. Efficient Graph Similarity Learning with Alignment Regularization (NeurIPS 2022) [paper] [code] [poster]\n[2] ZHUO W, et at. Proximity Enhanced Graph Neural Networks with Channel Contrast (IJCAI 2022) [paper] [code]\n[3] ZHUO W, et at. Graph Contrastive Learning with Adaptive Proximity-based Graph Augmentation (IEEE Trans on NNLS)\n其他 🏀休斯顿火箭，⚽巴萨\n📷摄影爱好者\n🌜严重失眠患者 （顺便求助睡眠方法）\nMay The Force Be With You ","permalink":"https://JhuoW.github.io/about/","summary":"🌟自己好菜啊，希望可以多多刷paper\n💘希望可以长久的更新这个blog，如有错误欢迎评论或者发邮件给我~\n👋如果本blog有幸被大佬看到，求带飞 😊\n联系方式：  e-mail: jhuow@proton.me  Publication 2022 [1] ZHUO W, et at. Efficient Graph Similarity Learning with Alignment Regularization (NeurIPS 2022) [paper] [code] [poster]\n[2] ZHUO W, et at. Proximity Enhanced Graph Neural Networks with Channel Contrast (IJCAI 2022) [paper] [code]\n[3] ZHUO W, et at. Graph Contrastive Learning with Adaptive Proximity-based Graph Augmentation (IEEE Trans on NNLS)\n其他 🏀休斯顿火箭，⚽巴萨\n📷摄影爱好者\n🌜严重失眠患者 （顺便求助睡眠方法）\nMay The Force Be With You ","title":"About"},{"content":"I’ve been fascinated by photography.😄\nThe photos are taken with my Nikon F3 + 50/1.4 and FUJIFILM X-T30II + XF16-80mm F4.\n   1st\n      10th\n      11th\n      12th\n      13th\n      14th\n      15th\n      16th\n      17th\n      18th\n      19th\n      2nd\n      20th\n      21st\n      22nd\n      23rd\n      24th\n      25th\n      26th\n      27th\n      28th\n      29th\n      3rd\n      30th\n      31st\n      32nd\n      33rd\n      34th\n      35th\n      36th\n      37th\n      38th\n      39th\n      4th\n      40th\n      41st\n      42nd\n      43rd\n      44th\n      45th\n      5th\n      6th\n      7th\n      8th\n      9th\n                             ","permalink":"https://JhuoW.github.io/gallery/","summary":"I’ve been fascinated by photography.😄\nThe photos are taken with my Nikon F3 + 50/1.4 and FUJIFILM X-T30II + XF16-80mm F4.\n   1st\n      10th\n      11th\n      12th\n      13th\n      14th\n      15th\n      16th","title":"Photo Gallery"},{"content":"论文地址：HTNE\nIntroduction 本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。\n另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。\n因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。\n通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。\n另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。\n值得注意的是，本文目标是优化邻域生成序列的极大似然估计即条件强度函数（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数\nModel Definition 本文通过跟踪节点邻域的形成来捕获网络的形成过程。\nDefinition 1 : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \\in E$ 被表示为按时间顺序的时间序列，例如， $\\mathbf{a}_{x,y}={a_1\\to{a_2}\\to{…}}\\subset\\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。\n因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。\nDefinition 2 : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2\u0026hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\\to(y_2,t_2)\\to\u0026hellip;\\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。\nHawkes Process 点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。\n对于一个给定的节点$x \\in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：\n$$ \\tilde{\\lambda}_{y|x}(t)=\\mu_{x,y}+\\sum_{t_h\u0026lt;t}{\\alpha_{h,y}\\kappa(t-t_{h})}$$\n其中，$\\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\\sum_{t_h\u0026lt;t}$表示遍历t时刻前$x$的所有邻居。$\\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：\n$$\\kappa(t-t_h)=\\exp(-\\delta_s(t-t_h))$$\n其中，减少率 $\\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\\delta_s(t-t_h)$越大, $\\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。\n综上所述，$\\kappa$的具体意义是随时间衰减的影响，其中$\\delta_s$参数表示对于不同的源节点，影响是不同的。\n如果$\\tilde{\\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。\n直观的来看，基本率（base rate）$\\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\\mu_{x,y}=f(\\mathbf{e}_x,\\mathbf{e}_y)=-||\\mathbf{e}_x-\\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\\alpha_{h,y}=f(\\mathbf{e}_h,\\mathbf{e}_y)=-||\\mathbf{e}_h-\\mathbf{e}_y||^2$。\n因为条件强度函数必须为正，所以使用如下公式: $\\lambda_{y|x}(t)=\\exp(\\tilde\\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$. 这就与传统的NE方法差不多了。。。\nAttention 根据论文中（3）式，可以看出，$\\sum_{t_h\u0026lt;t}{\\alpha_{h,y}\\kappa(t-t_{h})}$这一部分主要描述了历史邻居对当前邻居的影响，但是完全忽略了源节点$x$，因为源节点$x$的变化也会影响到历史邻居对当前邻居的亲近程度(affinity)。因此，本文引入了attention model。as follows：\n$$w_{h,x} = \\frac{\\exp(-||\\mathbf{e}_x-\\mathbf{e}_h||^2)}{\\sum_{h\u0026rsquo;}{\\exp(-||\\mathbf{e}_x-\\mathbf{e}_{h\u0026rsquo;}||^2)}}$$\n这是一个softmax函数 来根据源节点$x$的不同为它的邻居赋予不同权重。\n最后， 历史邻居与当前邻居的连接紧密程度可以表示为: $$\\alpha_{h,y}=w_{h,x}f(\\mathbf{e}_h,\\mathbf{e}_y)$$\nOptimization 目标函数即为给定节点$x$以及基于邻域形成序列的霍克斯过程, 生成节点$y$的条件概率。 公式如下： $$p(y|x, \\mathcal{H}_x(t)) = \\frac{\\lambda_{y|x}(t)}{\\sum_{y\u0026rsquo;}{\\lambda_{y\u0026rsquo;|x}(t)}}$$ 目标函数即为所有节点对的极大似然： $$\\log \\mathcal{L}=\\sum_{x\\in{\\mathcal{V}}}{\\sum_{y\\in{\\mathcal{H}_x}}}{\\log{p(y|x,\\mathcal{H}(t))}}$$\n最后，由于softmax过程是calculating expensive，所以采用负采样优化损失函数。\n","permalink":"https://JhuoW.github.io/posts/htne/","summary":"论文地址：HTNE\nIntroduction 本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。\n另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。\n因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。\n通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。\n另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。\n值得注意的是，本文目标是优化邻域生成序列的极大似然估计即条件强度函数（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数\nModel Definition 本文通过跟踪节点邻域的形成来捕获网络的形成过程。\nDefinition 1 : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \\in E$ 被表示为按时间顺序的时间序列，例如， $\\mathbf{a}_{x,y}={a_1\\to{a_2}\\to{…}}\\subset\\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。\n因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。\nDefinition 2 : 对于给定节点$x$,邻域表示为$N(x)={y_i|i=1,2\u0026hellip;}$.$x$的目标邻居到达事件可以表示为${x:(y_1,t_1)\\to(y_2,t_2)\\to\u0026hellip;\\to(y_n,t_n)}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。\nHawkes Process 点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。\n对于一个给定的节点$x \\in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：\n$$ \\tilde{\\lambda}_{y|x}(t)=\\mu_{x,y}+\\sum_{t_h\u0026lt;t}{\\alpha_{h,y}\\kappa(t-t_{h})}$$\n其中，$\\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\\sum_{t_h\u0026lt;t}$表示遍历t时刻前$x$的所有邻居。$\\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：\n$$\\kappa(t-t_h)=\\exp(-\\delta_s(t-t_h))$$\n其中，减少率 $\\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\\delta_s(t-t_h)$越大, $\\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。\n综上所述，$\\kappa$的具体意义是随时间衰减的影响，其中$\\delta_s$参数表示对于不同的源节点，影响是不同的。\n如果$\\tilde{\\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。\n直观的来看，基本率（base rate）$\\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了**负平方欧式距离（negative squared Euclidean）**来反映表示向量间的相似度: $\\mu_{x,y}=f(\\mathbf{e}_x,\\mathbf{e}_y)=-||\\mathbf{e}_x-\\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\\alpha_{h,y}=f(\\mathbf{e}_h,\\mathbf{e}_y)=-||\\mathbf{e}_h-\\mathbf{e}_y||^2$。\n因为条件强度函数必须为正，所以使用如下公式: $\\lambda_{y|x}(t)=\\exp(\\tilde\\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$.","title":"SIGIR2018 《HTNE Embedding Temporal Network via Neighborhood Formation》 Reading Notes"}]